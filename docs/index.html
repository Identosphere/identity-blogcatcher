
<!DOCTYPE html>
<html>
<head>
  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 2.7.0 (2019-12-25) [x86_64-linux]'>

  <title>Identosphere Blogcatcher | Planet Identity Reboot</title>

  <link href="https://fonts.googleapis.com/css2?family=Libre+Baskerville&family=Volkhov&display=swap" rel="stylesheet">   
  <link rel='stylesheet' type='text/css' href='css/font-awesome.css'>
  <link rel='stylesheet' type='text/css' href='css/planetid.css'>
  
  <!-- fix: use script_tag  rails-style ??? -->
  <script defer src='js/jquery-2.0.3.min.js'></script>
  <script defer src='js/planetid.js'></script>

  
  
    <!-- Favicon-->
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/icon/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/icon/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/icon/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/icon/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/icon/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/icon/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/icon/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/icon/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/icon/apple-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icon/android-icon-192x192.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/icon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="/assets/icon/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/icon/favicon-16x16.png">
    <link rel="manifest" href="/manifest.json">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="/assets/icon/ms-icon-144x144.png">
    <meta name="theme-color" content="#ffffff">

  <script defer data-goatcounter="https://identosphere.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>
</head>
<body>
<div class='banner'>
<table cellspacing='0' cellpadding='0' width='100%'>
  <tr>
    <td>
    <!-- left/1st column -->
    <a href="https://identosphere.net"><b>Blogs</b></a> | <a href="/organizations/">Organizations</a> | <a href="/companies/">Companies</a> | <a href="/web3/">Web 3</a> | <a href="/media/">Media</a> • <a href="https://github.com/identosphere/planetid-reboot/">Github</a>
    </td>
    <td style='text-align:right;'>
    <!-- right/2nd column -->
    Style | <a href='/'><b>Full</b></a> &bull; <a href='overview.html'>Overview</a>
    </td>
  </tr>
</table>
</div>
<div id='navwrap'>
<div id='nav'>

    <div style='width: 100%; text-align: right;'>
    <img src='i/view-headlines.png' id='show-headlines' title='Show Headlines Only' width='24' height='24'>
    <img src='i/view-snippets.png' id='show-snippets'    title='Show Snippets' width='24' height='24'>
    <img src='i/view-standard.png' id='show-fulltext'    title='Show Full Text' width='24' height='24'>
    </div>  

    <h4>Subscriptions</h4>
    <ul class='subscriptions small' >
     
      <li>
        <a href='https://aaronparecki.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://aaronparecki.com'>Aaron Parecki</a>
      </li>
     
      <li>
        <a href='http://eternallyoptimistic.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://eternallyoptimistic.com'>Adventures of an Eternal Optimist</a>
      </li>
     
      <li>
        <a href='http://adistributedeconomy.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://adistributedeconomy.blogspot.com'>A Distributed Economy</a>
      </li>
     
      <li>
        <a href='https://altmode.org/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://altmode.org'>Altmode</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@blockadvocacy'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://blockadvocacy.medium.com/'>Ally Medina - Blockchain Advocacy</a>
      </li>
     
      <li>
        <a href='https://blog.aniljohn.com/feed/blog.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://blog.aniljohn.com'>Anil John</a>
      </li>
     
      <li>
        <a href='https://www.cyberforge.com/feed/cyberforge.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.cyberforge.com/'>Cyberforge (Anil John)</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@arjgovind'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@arjgovind'>Arjun Govind</a>
      </li>
     
      <li>
        <a href='http://asadistechspace.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://asadistechspace.blogspot.com'>My.Tech.Space</a>
      </li>
     
      <li>
        <a href='https://realestatecafe.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://realestatecafe.com/'>Bill Wendels Real Estate Cafe</a>
      </li>
     
      <li>
        <a href='https://bradtumy.wordpress.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://tumy-tech.com/posts/'>Brad Tumy</a>
      </li>
     
      <li>
        <a href='https://cybersocialstructure.org/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://cybersocialstructure.org/'>Virtual Democracy</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@cdonley'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@cdonley'>Clayton Donleys Blog</a>
      </li>
     
      <li>
        <a href='http://www.cloudidentity.com/blog/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.cloudidentity.com/blog'>Vittorio Bertocci - CloudIdentity</a>
      </li>
     
      <li>
        <a href='https://darius.dunlaps.net/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://darius.dunlaps.net'>Darius</a>
      </li>
     
      <li>
        <a href='https://damienbod.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://damienbod.com/'>Damien Bod</a>
      </li>
     
      <li>
        <a href='http://www.backalleycoder.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.backalleycoder.com/'>DANIEL BUCHNER</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@daniel-hardman'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://daniel-hardman.medium.com/'>Daniel Hardman</a>
      </li>
     
      <li>
        <a href='http://blog.deanland.com/blog/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://blog.deanland.com'>blog.deanland.com</a>
      </li>
     
      <li>
        <a href='http://feeds.soundcloud.com/users/soundcloud:users:66572473/sounds.rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://soundcloud.com/tim-bouma-1'>Definitely Identity</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@DickHardt'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@DickHardt'>Dick Hardt</a>
      </li>
     
      <li>
        <a href='http://www.discoveringidentity.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.discoveringidentity.com'>Discovering Identity</a>
      </li>
     
      <li>
        <a href='https://www.thedinglegroup.com/blog?format=rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.thedinglegroup.com/blog'>The Dingle Group</a>
      </li>
     
      <li>
        <a href='https://www.thedinglegroup.com/a-growth-journey?format=rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.thedinglegroup.com/a-growth-journey'>The Dingle Group - A Growth Journey</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@dkelts.id'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@dkelts.id'>David Kelts on ID</a>
      </li>
     
      <li>
        <a href='http://epeus.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://epeus.blogspot.com/'>"Epeus' epigone"</a>
      </li>
     
      <li>
        <a href='https://equalsdrummond.name/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://equalsdrummond.name'>Equals Drummond</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@expede'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@expede'>Brooklyn Zelenka - Programs with functions</a>
      </li>
     
      <li>
        <a href='https://medium.facilelogin.com/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.facilelogin.com'>FACILELOGIN</a>
      </li>
     
      <li>
        <a href='https://fossandcrafts.org/rss-feed.rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://fossandcrafts.org/'>Foss & Crafts</a>
      </li>
     
      <li>
        <a href='https://beuchelt.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://beuchelt.com/blog/'>Gerald Beuchelt</a>
      </li>
     
      <li>
        <a href='http://360tek.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://360tek.blogspot.com/'>Matt Flynn: InfoSec | IAM</a>
      </li>
     
      <li>
        <a href='http://meilinfung.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://meilinfung.blogspot.com'>Global Connections - Eclectic Selections</a>
      </li>
     
      <li>
        <a href='http://habitatchronicles.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://habitatchronicles.com'>Habitat Chronicles</a>
      </li>
     
      <li>
        <a href='https://hanszandbelt.wordpress.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://hanszandbelt.wordpress.com'>Hans Zandbelt</a>
      </li>
     
      <li>
        <a href='https://www.kyledenhartog.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://kyledenhartog.com/posts/'>Kyle Den Hartog</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@heathervescent'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.heathervescent.com/'>Heather Vescent</a>
      </li>
     
      <li>
        <a href='https://www.herbie.dev/blog/index.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.herbie.dev'>Herbies Blog</a>
      </li>
     
      <li>
        <a href='https://hyperonomy.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://hyperonomy.com/'>Hyperonomy Digital Identity Lab</a>
      </li>
     
      <li>
        <a href='http://identity-centric-architecture.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://identity-centric-architecture.blogspot.com/'>"Identity Driven Enterprise (Security) Architecture (IDEAs!!)"</a>
      </li>
     
      <li>
        <a href='http://identityhappens.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://identityhappens.blogspot.com/'>Identity Happens</a>
      </li>
     
      <li>
        <a href='http://identitymeme.org/feed/atom/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://identitymeme.org'>IdentityMeme.org</a>
      </li>
     
      <li>
        <a href='http://identitynetworks.wordpress.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://identitynetworks.wordpress.com'>Identity Networks</a>
      </li>
     
      <li>
        <a href='https://identitywoman.net/feed/atom/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://identitywoman.net'>Identity Woman</a>
      </li>
     
      <li>
        <a href='https://idimmusings.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://idimmusings.com'>IDIM Musings</a>
      </li>
     
      <li>
        <a href='http://idm-thoughtplace.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://idm-thoughtplace.blogspot.com/'>IdM Thoughtplace</a>
      </li>
     
      <li>
        <a href='http://idmdude.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://idmdude.com'>Easy Identity</a>
      </li>
     
      <li>
        <a href='http://feeds.feedburner.com/Ignisvulpis'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://ignisvulpis.blogspot.com/'>ignisvulpis</a>
      </li>
     
      <li>
        <a href='http://blog.independentid.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.independentid.com'>Independent Identity</a>
      </li>
     
      <li>
        <a href='https://infominer.xyz/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://infominer.xyz/'>infominer</a>
      </li>
     
      <li>
        <a href='https://www.informationanswers.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.informationanswers.com'>Information Answers</a>
      </li>
     
      <li>
        <a href='http://blog.is4u.be/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://blog.is4u.be/'>IS4U blog</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@isurakarunaratne'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@isurakarunaratne'>Isura Karunaratne</a>
      </li>
     
      <li>
        <a href='https://blog.joeandrieu.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://blog.joeandrieu.com'>Joe Andrieu</a>
      </li>
     
      <li>
        <a href='https://reb00ted.org/index.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://reb00ted.org/'>reb00ted</a>
      </li>
     
      <li>
        <a href='https://joipodgorny.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://joipodgorny.com'>Joi Podgorny</a>
      </li>
     
      <li>
        <a href='https://blog.jonudell.net/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://blog.jonudell.net'>Jon Udell</a>
      </li>
     
      <li>
        <a href='http://www.caballerojuan.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.caballerojuan.com/'>Juan Caballero</a>
      </li>
     
      <li>
        <a href='https://justatheory.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://justatheory.com'>Just a Theory</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@justinsecurity'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@justinsecurity'>Justin Richer</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@kago'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@kago'>Eddie Kago</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@karylfowler'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@karylfowler'>Karyl Fowler</a>
      </li>
     
      <li>
        <a href='https://katrynadow.me/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://katrynadow.me'>Katryna Dow</a>
      </li>
     
      <li>
        <a href='https://kentbull.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://kentbull.com/'>Kent Bull</a>
      </li>
     
      <li>
        <a href='https://kayaelle.medium.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://kayaelle.medium.com/'>Kerri Lemole</a>
      </li>
     
      <li>
        <a href='https://letshaveaplan.blog/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.letshaveaplan.blog'>Lets Have a Plan</a>
      </li>
     
      <li>
        <a href='http://feeds.feedburner.com/LifeWithAlacrity'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.lifewithalacrity.com/'>Life With Alacrity</a>
      </li>
     
      <li>
        <a href='https://ludopoitou.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://ludopoitou.com'>Ludo Sketches</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@macieklaskus'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@macieklaskus'>Maciek Laskus</a>
      </li>
     
      <li>
        <a href='https://madurangasblogs.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://madurangasblogs.blogspot.com'>Madurangas Blogs</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@margo.e.johnson'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@margo.e.johnson'>Margo Johnson</a>
      </li>
     
      <li>
        <a href='http://matthewschutte.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://matthewschutte.com'>Matthew Schutte</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@m-ruminer'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@m-ruminer/'>Michael Ruminer</a>
      </li>
     
      <li>
        <a href='https://www.moxytongue.com/feeds/posts/default?alt=rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.moxytongue.com/'>Moxy Tongue</a>
      </li>
     
      <li>
        <a href='http://www.sakimura.org/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.sakimura.org'>@_Nat Zone</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@CreatorNader'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@CreatorNader'>Nader Helmy</a>
      </li>
     
      <li>
        <a href='https://nosuchcommonsense.wordpress.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://nosuchcommonsense.wordpress.com'>nosuchcommonsense</a>
      </li>
     
      <li>
        <a href='http://notabob.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://notabob.blogspot.com/'>"Ceci n'est pas un Bob"</a>
      </li>
     
      <li>
        <a href='http://www.notsodistantfuture.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.notsodistantfuture.com/'>Not So Distant Future</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@OR13'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@OR13'>Orie Steele</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@paul.knowles_52509'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@paul.knowles_52509'>Paul Knowles</a>
      </li>
     
      <li>
        <a href='https://www.patrick-breyer.de/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@markus.sabadello'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@markus.sabadello'>Markus Sabadello on Medium</a>
      </li>
     
      <li>
        <a href='https://john.philpin.com/feed.json'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
      </li>
     
      <li>
        <a href='http://feeds.feedburner.com/PhillipGibb'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://phillipgibb.com/'>The Life, Adventure and Lessons of Phillip Gibb</a>
      </li>
     
      <li>
        <a href='https://newsletter.pragmaticengineer.com/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@pulasthi7'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@pulasthi7'>Pulasthi Mahawithana</a>
      </li>
     
      <li>
        <a href='https://www.rdegges.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.rdegges.com'>Randall Degges</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@rebeccarachmany'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@rebeccarachmany'>Rebecca Rachmany</a>
      </li>
     
      <li>
        <a href='http://riskhorizon.wordpress.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://riskhorizon.wordpress.com'>Risk Horizon Blog</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@rileyparkerhughes'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://rileyparkerhughes.medium.com'>Riley Hughes</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@Obstropolos'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://rocco.network'>Rocco, Gregory</a>
      </li>
     
      <li>
        <a href='https://sammantics.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://sammantics.com'>SAMMANTICS</a>
      </li>
     
      <li>
        <a href='http://samuelsmith.org/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://samuelsmith.org'>Meaningful Intelligence</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/selfrule'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/selfrule'>Sam Smith - Medium</a>
      </li>
     
      <li>
        <a href='http://openingmove.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://openingmove.blogspot.com'>Scott Maces IT Journal</a>
      </li>
     
      <li>
        <a href='http://www.seanbohan.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.seanbohan.com'>SeanBohan.com</a>
      </li>
     
      <li>
        <a href='https://blogs.harvard.edu/doc/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
      </li>
     
      <li>
        <a href='http://self-issued.info/?feed=atom'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://self-issued.info'>Mike Jones: self-issued</a>
      </li>
     
      <li>
        <a href='https://sepia.substack.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://sepia.substack.com/'>Sepia - Zach Brown</a>
      </li>
     
      <li>
        <a href='https://shanegreen.org/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://shanegreen.org'>Shane Green</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@shingaithornton'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@shingaithornton'>Shingai Thornton</a>
      </li>
     
      <li>
        <a href='https://simonwillison.net/atom/everything/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://simonwillison.net'>Simon Willison</a>
      </li>
     
      <li>
        <a href='https://socialmediaclarity.tumblr.com/rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://socialmediaclarity.tumblr.com'>Social Media Clarity</a>
      </li>
     
      <li>
        <a href='http://feeds.feedburner.com/superpat'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://blog.superpat.com'>Superpatterns</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@ssi-ambassador'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@ssi-ambassador'>SSI Ambassador</a>
      </li>
     
      <li>
        <a href='https://blog.talkingidentity.com/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://blog.talkingidentity.com'>Talking Identity</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@tarikk'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@tarikk'>Tarik Kurspahic</a>
      </li>
     
      <li>
        <a href='http://www.thread-safe.com/feeds/posts/default?alt=rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.thread-safe.com/'>Threadsafe</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@rufftimo'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@rufftimo'>Timothy Ruff</a>
      </li>
     
      <li>
        <a href='https://herestomwiththeweather.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://herestomwiththeweather.com'>Heres Tom with the Weather</a>
      </li>
     
      <li>
        <a href='https://www.mydigitalfootprint.com/feeds/posts/default?alt=rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.mydigitalfootprint.com/'>MyDigitalFootprint</a>
      </li>
     
      <li>
        <a href='http://blog.transparenthealth.org/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://blog.transparenthealth.org'>Transparent Health Blog</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@trbouma'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@trbouma'>Tim Boumas Blog</a>
      </li>
     
      <li>
        <a href='http://www.tuesdaynight.org/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.tuesdaynight.org'>ian glazers tuesdaynight</a>
      </li>
     
      <li>
        <a href='http://www.unclesamsblog.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.unclesamsblog.com'>Uncle Sams Blog</a>
      </li>
     
      <li>
        <a href='http://virtualsoul.org/blog/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://virtualsoul.org'>dale olds - virtualsoul</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@vishal144'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@vishal144'>Vishal Gupta</a>
      </li>
     
      <li>
        <a href='https://www.voidstar.com/module.php?mod=blog&op=feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.voidstar.com/module.php?mod=blog'>Voidstar: blog</a>
      </li>
     
      <li>
        <a href='https://werd.io/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://werd.io'>Ben Werdmüller</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@benwerd'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
      </li>
     
      <li>
        <a href='https://www.technometria.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
      </li>
     
      <li>
        <a href='https://wip-abramson.dev/rss.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://wip-abramson.dev/'>Wip Abramson</a>
      </li>
     
      <li>
        <a href='https://www.webistemology.com/rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
      </li>
     
      <li>
        <a href='https://wrenchinthegears.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
      </li>
     
      <li>
        <a href='http://xditao.blogspot.com/atom.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://xditao.blogspot.com/'>The Tao of XDI</a>
      </li>
     
      <li>
        <a href='https://www.xtseminars.co.uk/blog-feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.xtseminars.co.uk/'>XTSeminars</a>
      </li>
     
    </ul>
<center>
    <p><small><strong>Inspired by</strong> identity management feed aggregator <em><a href="https://web.archive.org/web/20161029051802/http://planetidentity.org/">planetidentity.org</a></em>.</small></p>
    <h4>Add Your Feed</h4>
    <p><small>Send Blog \ Feed to:  <code>blogcatcher (at) identosphere (dot) net</code>.</small></p>
</center>

<h4>Built with</h4>
<ul class='planetarium small'>
  <li><a href='https://github.com/feedreader'>Pluto Feedreader</a></li>
  <li><a href="https://github.com/web-work-tools/awesome-planet-pluto#github-actions">GitHub Actions</a></li>
</ul>

</div>
</div>
<sup>Last Update 6:44 AM June 04, 2024 (UTC)</sup>
<iframe src="https://identosphere.substack.com/embed" width="400" height="267" style="border:1px solid #EEE; background:#f0f0f0;" frameborder="0" align="right" scrolling="no"></iframe>

<h1>Identity Blog Catcher</h1>

<p>Brought to you by <a href="https://identitywoman.net">Identity Woman</a> and <a href="https://github.com/infominer33">Infominer</a>.<br><a href="https://www.patreon.com/user?u=35769676">Support this collaboration on Patreon</a>!!!</p>




 <h2 class='new-date'>
  Tuesday, 04. June 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/Jun/4/how-do-i-opt-into-full-text-search-on-mastodon/#atom-everything'>How do I opt into full text search on Mastodon?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   How do I opt into full text search on Mastodon? 
I missed this new Mastodon feature when it was released in 4.2.0 last September: you can now opt-in to a new setting which causes all of your future posts to be marked as allowed to be included in the Elasticsearch index provided by Mastodon instances that enable search. 
 It only applies to future posts because it works by adding an "indexable" fla
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://fedi.tips/how-do-i-opt-into-or-out-of-full-text-search-on-mastodon/">How do I opt into full text search on Mastodon?</a></p>
I missed this new Mastodon feature when it was released <a href="https://blog.joinmastodon.org/2023/09/mastodon-4.2/">in 4.2.0 last September</a>: you can now opt-in to a new setting which causes all of your future posts to be marked as allowed to be included in the Elasticsearch index provided by Mastodon instances that enable search.<p></p>
<p>It only applies to future posts because it works by adding an "indexable" flag to those posts, which can then be obeyed by other Mastodon instances that the post is syndicated to.</p>
<p>You can turn it on for your own account from the <code>/settings/privacy</code> page on your local instance.</p>
<p>The <a href="https://github.com/mastodon/mastodon/releases/tag/v4.2.0">release notes for 4.2.0</a> also mention new search operators:</p>
<blockquote>
<p><code>from:me</code>, <code>before:2022-11-01</code>, <code>after:2022-11-01</code>, <code>during:2022-11-01</code>, <code>language:fr</code>, <code>has:poll</code>,  or <code>in:library</code> (for searching only in posts you have written or interacted with)</p>
</blockquote>

    <p>Via <a href="https://front-end.social/@robinwhittleton/112556840499268599">@robinwhittleton</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   06:14 &bull;
   32 minutes ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/Jun/4/a-tip-from-neal-stephenson/#atom-everything'>A tip from Neal Stephenson</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   A tip from Neal Stephenson 
Twelve years ago on Reddit user bobbylox asked Neal Stephenson (in an AMA): 
 
 My ultimate goal in life is to make the Primer real. Anything you want to make sure I get right? 
 
 Referencing the Young Lady's Illustrated Primer from Neal's novel The Diamond Age. Stephenson replied: 
 
 Kids need to get answers from humans who love them. 
 
 (A lot of people in the AI s
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://www.reddit.com/r/Fantasy/comments/vdt11/comment/c53o23x/">A tip from Neal Stephenson</a></p>
Twelve years ago on Reddit user bobbylox asked Neal Stephenson (in an AMA):<p></p>
<blockquote>
<p>My ultimate goal in life is to make the Primer real. Anything you want to make sure I get right?</p>
</blockquote>
<p>Referencing the Young Lady's Illustrated Primer from Neal's novel <a href="https://en.wikipedia.org/wiki/The_Diamond_Age">The Diamond Age</a>. Stephenson replied:</p>
<blockquote>
<p>Kids need to get answers from humans who love them.</p>
</blockquote>
<p>(A lot of people in the AI space are taking inspiration from the Primer right now.)

    </p><p>Via <a href="https://twitter.com/noahlt/status/1797488714433909175">@noahlt</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   02:07 &bull;
   5 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/04/140216.html'> 📸 🌊 
</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   📸 🌊 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2024/a45a299300.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>📸 🌊</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2024/a45a299300.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   02:02 &bull;
   5 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/04/our-very-own.html'> Our very own @jthingelstad on the 🔗 🎙️ Omni Podcast </a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Our very own @jthingelstad on the 🔗 🎙️ Omni Podcast 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Our very own <a href="https://micro.blog/jthingelstad">@jthingelstad</a> on the <a href="https://overcast.fm/+K3-SPEjZA">🔗 🎙️ Omni Podcast</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   01:52 &bull;
   5 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/Jun/4/jon-christian/#atom-everything'>Quoting Jon Christian</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    computer scientists: we have invented a virtual dumbass who is constantly wrong   tech CEOs: let's add it to every product   — Jon Christian 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <blockquote cite="https://bsky.app/profile/jon-christian.bsky.social/post/3ktsxyw2pf423"><p>computer scientists: we have invented a virtual dumbass who is constantly wrong <br><br>tech CEOs: let's add it to every product</p></blockquote><p>— <a href="https://bsky.app/profile/jon-christian.bsky.social/post/3ktsxyw2pf423">Jon Christian</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   01:24 &bull;
   5 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/04/interesting-that-gig.html'> Interesting that Gig Young won 3 Oscar nominations (includin</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Interesting that Gig Young won 3 Oscar nominations (including 1 win). All three roles saw him playing some kind of drunk/ alcoholic. Life imitating art? 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Interesting that Gig Young won 3 Oscar nominations (including 1 win). All three roles saw him playing some kind of drunk/ alcoholic. Life imitating art?</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   01:09 &bull;
   6 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 03. June 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2024/06/03/archiving-a-way/'>Archiving a Way</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  My father, Allen H. Searls, was an archivist. Not a formal one, but good in the vernacular, at least when it came to one of the most consequential things he did in his life: helping build the George Washington Bridge. He did this by photographing his work and fellow workers. He shot with a Kodak […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<a href="https://www.flickr.com/photos/infrastructure/53764589985/in/album-72177720317536176/"></a>These are the men who strung and assembled the cables that hold up the George Washington Bridge roadway. After this was shot, on July 25, 1929, the cable bundles were compressed, sheathed, and draped with suspension cables to the new roadway that would be built below. The photo is from the collection of Allen H. Searls, the gap-toothed guy with a big grin at the center of the photo. He was 21 at the time.
<p>My father, <a href="https://flickr.com/photos/docsearls/albums/72157627459297180/">Allen H. Searls</a>, was an archivist. Not a formal one, but good in the vernacular, at least when it came to one of the most consequential things he did in his life: helping build the <a href="https://en.wikipedia.org/wiki/George_Washington_Bridge">George Washington Bridge</a>. He did this by photographing his work and fellow workers. He shot with a Kodak camera, and then developed and printed each shot, and assembled them into an album that survives to this day. All the shots in <a href="https://www.flickr.com/photos/infrastructure/albums/72177720317536176/">this collection</a> are from that album. I’ve archived them in <a href="https://www.flickr.com/photos/infrastructure/">my Flickr site focused on infrastructure</a>. I have also <a href="http://creativecommons.org">Creative Commons</a> licensed them to require only attribution. (Though I’d rather people credit Allen H. Searls than his son.)</p>
<p>Only two of the photos are dated. One is July 25, 1929, when workers and families celebrated the completion of the cable stringing operation. The other is July 25, 1930, presumably when the roadway was completed. I was able to piece together the significance of these dates, and guess at the date ranges of other pictures, by doing a deep dive into the New York Times archive (where I found that these guys were called “bridgemen,” and by reading my father’s copy of the <a href="https://www.roeblingmuseum.org/about">Roebling Cable</a> company’s book about how the bridge’s cables were made and put in place.</p>
<p>As we know now, almost too well, we live in an Age of AI, when the entire corpus of the Internet, and God only knows what else, has been ingested into large language models that are trained and programmed to answer questions about what they “know” (even though they don’t, really). Meanwhile what do we, as human beings, actually know? Or, better yet, where can we find what we need or want to know? Libraries of the physical kind are necessary but insufficient when our instruments of inquiry are entirely electronic. The World Wide Web has turned into the <a href="https://doc.searls.com/2024/05/02/world-wide-whiteboard/">World Wide Whiteboard</a>.</p>
<p>We need electronic archives. Simple as that.</p>
<p>We all know (and, I hope, appreciate) the Internet Archive. I was going to give my father’s copy of the Roebling book to the Archive for scanning, but discovered <a href="https://archive.org/details/RoeblingCablesForTheHudsonRiverBridgeProblemsAndRequirements/mode/2up">it’s already there</a>! That made the book easy to read (here on this flat screen), and saved my own fragile copy from further damage.</p>
<p>A heroic example of one person’s work to save the whole corpus of literature on broadcasting is David Gleason’s <a href="https://www.worldradiohistory.com/">World Radio History site</a>, without which much knowledge of broadcasting history would be lost.</p>
<p>On the contemporary front, <a href="https://bsquarebulletin.com/">Dave Askins</a>‘ work toward <a href="https://doc.searls.com/2024/03/19/the-online-local-chronicle/">online local news chronicles</a> may prove essential if we don’t want to see current and future news disappear down the same holes into which too many past publications’ work has fallen (or remains alive but trapped behind paywalls).</p>
<p>For more on where this might go, see my <a href="https://doc.searls.com/2024/04/21/archives-as-commons/">Archives as Commons</a> post. I’ll be talking about this, I hope, at the upcoming <a href="https://wow.indiana.edu/">WoW Conference</a> and <a href="https://dwebcamp.org/">DWeb Camp</a>.</p>
<p>&nbsp;</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://150108457.v2.pressablecdn.com/wp-content/uploads/2024/06/53764589985_d8a9835cc8_b.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:43 &bull;
   14 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/Jun/3/gpt-2-five-years-later/#atom-everything'>GPT-2 five years later</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   GPT-2 five years later 
Jack Clark, now at Anthropic, was a researcher at OpenAI five years ago when they first trained GPT-2. 
 In this fascinating essay Jack revisits their decision not to release the full model, based on their concerns around potentially harmful ways that technology could be used. 
 (Today a GPT-2 class LLM can be trained from scratch for around $20, and much larger models are 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://importai.substack.com/p/import-ai-375-gpt-2-five-years-later">GPT-2 five years later</a></p>
Jack Clark, now at Anthropic, was a researcher at OpenAI five years ago when they first trained GPT-2.<p></p>
<p>In this fascinating essay Jack revisits their decision not to release the full model, based on their concerns around potentially harmful ways that technology could be used.</p>
<p>(Today a GPT-2 class LLM can be trained from scratch <a href="https://simonwillison.net/2024/May/28/reproducing-gpt-2/">for around $20</a>, and much larger models are openly available.)</p>
<blockquote>
<p>There's a saying in the financial trading business which is 'the market can stay irrational longer than you can stay solvent' - though you might have the right idea about something that will happen in the future, your likelihood of correctly timing the market is pretty low. There's a truth to this for thinking about AI risks - yes, the things we forecast (as long as they're based on a good understanding of the underlying technology) will happen at some point but I think we have a poor record of figuring out a) when they'll happen, b) at what scale they'll happen, and c) how severe their effects will be. This is a big problem when you take your imagined future risks and use them to justify policy actions in the present!</p>
</blockquote>
<p>As an early proponent of government regulation around training large models, he offers the following cautionary note:</p>
<blockquote>
<p>[...] history shows that once we assign power to governments, they're loathe to subsequently give that power back to the people. Policy is a ratchet and things tend to accrete over time. That means whatever power we assign governments today represents the floor of their power in the future - so we should be extremely cautious in assigning them power because I guarantee we will not be able to take it back.</p>
</blockquote>
<p>Jack stands by the recommendation from the original GPT-2 paper for governments "to more systematically monitor the societal impact and diffusion of AI technologies, and to measure the progression in the capabilities of such systems."</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:22 &bull;
   14 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/protecting-artists-on-the-fediverse'>Protecting artists on the fediverse</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
   Over the weekend, I started to notice a bunch of artists moving to&nbsp;Cara, a social network for artists founded by Jingna Zhang, herself an accomplished photographer.  The fediverse is a decentralized cooperative of social networks that can interact with each other: a user on one network can follow, reply, like, and re-share content from a user on another network. The whole thing depends on
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://werd.io/file/665dd437da6a67804007c6f2/thumb.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
<p></p><p>Over the weekend, I started to notice a bunch of artists moving to&nbsp;<a href="https://cara.app/explore">Cara</a>, a social network for artists founded by <a href="https://www.zhangjingna.com/">Jingna Zhang</a>, herself <a href="https://en.wikipedia.org/wiki/Zhang_Jingna">an accomplished photographer</a>.</p><p>The fediverse is a decentralized cooperative of social networks that can interact with each other: a user on one network can follow, reply, like, and re-share content from a user on another network. The whole thing depends on an open standard called ActivityPub, shared community norms, and a cooperative culture.</p><p>Of course, my first reaction was that Cara should be compatible with the fediverse so that its content could be more easily discoverable by users on social networks like Threads, Flipboard, and Mastodon. Cara is explicitly set up to be a network for human artists, with no AI-generated content, which will be increasingly valuable as the web becomes flooded with machine-made art. The fediverse would allow them to publish on sites like Cara that are set up to support their needs, while finding a broad audience across the entire web.</p><p>From <a href="https://cara.app/about">its About page</a>:</p><blockquote><p>With the widespread use of generative AI, we decided to build a place that filters out generative AI images so that people who want to find authentic creatives and artwork can do so easily.</p><p>[…] We do not agree with generative AI tools in their current unethical form, and we won’t host AI-generated portfolios unless the rampant ethical and data privacy issues around datasets are resolved via regulation.</p></blockquote><p>I’d love to follow artists on Cara from my Mastodon or Threads accounts. But how does Cara’s AI stance square with the fediverse? How might artists on Cara find a broad audience for their work across the web without risking that art being used as training data without permission?</p><p>The first thing a site can do to prevent its content from being used as training data is to <a href="https://www.cyberciti.biz/web-developer/block-openai-bard-bing-ai-crawler-bots-using-robots-txt-file/">add exclusion rules to its robots.txt file</a>. These theoretically prevent crawlers owned by model vendors like OpenAI from directly accessing art from the site. There is <a href="https://www.robotstxt.org/faq/legal.html">nothing that legally binds crawlers from obeying robots.txt</a>; it’s less enforceable than a handshake agreement. Still, most claim that they voluntarily do.</p><p>But even if robots.txt was an ironclad agreement, content published to the fediverse doesn’t solely live on its originating server. If Cara was connected to the fediverse, images posted there could still be found on its servers, but they would also be syndicated to the home servers of anyone who followed its users. If a user on Threads followed a Cara user, the Cara user’s images would be copied to Threads; if a user on a Mastodon instance followed that user, the images would be copied to that Mastodon instance. The images are copied across the web as soon as they are published; even if Cara protects its servers from being accessed by AI crawlers, these other downstream fediverse servers are not guaranteed to be protected.</p><p>By connecting to the fediverse, one might argue that servers implicitly license their content to be reused across different services. This is markedly different from RSS, <a href="https://advertisinglaw.fkks.com/post/102gstp/implied-copyright-licenses-in-the-digital-world-blogs-rss-feeds-and-aggregator">where this is explicitly not the case</a>: there is legal precedent that says my RSS feed cannot be used to republish my content elsewhere without my permission (although you can, of course, access its content in a private feed reader; that’s the point). But on the fediverse, the ability to reshare across platforms is core functionality.</p><p>The following things are all true:</p>  Content published to the fediverse may be both re-copied to and served from other peoples’ servers 
 Those servers may have different policies regarding content use 
 In the absence of a robots.txt directive, AI crawlers will scrape a website’s data, even if they don’t have the legal right to 
 Some servers may themselves be owned by AI vendors and may use federated content to train generative models even without the use of a scraper 
 <p>As a result, there is no way an author can protect it from being used in an AI training set. The owners of a fediverse site wouldn’t have the right to make a deal with an AI vendor to sell the content it hosted because they wouldn’t have the copyright to all of that content in the first place. But because AI crawlers greedily scrape content without asking for permission, unless the site explicitly opts out with robots.txt, it doesn’t matter.</p><p>This leads me to a few conclusions:</p>  It is a moral obligation for every fediverse site to prevent crawling of federated content by robustly setting robots.txt directives at a minimum 
 Discussions about adding <a href="https://socialhub.activitypub.rocks/t/fep-c118-content-licensing-support/2903/1">content licensing support to the fediverse</a> are even more important than they appear 
 Someone needs to legally prevent AI vendors from using all available data as training fodder 
 <p>A fediverse (and a web!) where Cara can safely join while adhering to its principles is a more functional, safer network. To build it we’ll need to support explicit licensing on the fediverse, create a stronger standard for user protections across fediverse sites, and seek more robust legal protections against AI crawler activity. While these are ambitious goals, I believe they’re achievable — and necessary to support the artists and content creators who make the web their home.</p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://werd.io/file/665dd437da6a67804007c6f2/thumb.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:34 &bull;
   16 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/Jun/3/duckdb-10/#atom-everything'>DuckDB 1.0</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   DuckDB 1.0 
Six years in the making. The most significant feature in this milestone is stability of the file format: previous releases often required files to be upgraded to work with the new version. 

 This release also aspires to provide stability for both the SQL dialect and the C API, though these may still change with sufficient warning in the future.

      Via @duckdb 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://duckdb.org/2024/06/03/announcing-duckdb-100">DuckDB 1.0</a></p>
Six years in the making. The most significant feature in this milestone is stability of the file format: previous releases often required files to be upgraded to work with the new version.<p></p>

<p>This release also aspires to provide stability for both the SQL dialect and the C API, though these may still change with sufficient warning in the future.

    </p><p>Via <a href="https://twitter.com/duckdb/status/1797619191341551969">@duckdb</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   13:23 &bull;
   17 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/Jun/3/transformer-powered-predictive-text/#atom-everything'>A look at Apple’s new Transformer-powered predictive text model</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   A look at Apple’s new Transformer-powered predictive text model 
Jack Cook reverse engineered the tiny LLM used for the predictive text keyboard in the latest iOS. It appears to be a GPT-2 style custom model with 34M parameters and a 15,000 token vocabulary.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://jackcook.com/2023/09/08/predictive-text.html">A look at Apple’s new Transformer-powered predictive text model</a></p>
Jack Cook reverse engineered the tiny LLM used for the predictive text keyboard in the latest iOS. It appears to be a GPT-2 style custom model with 34M parameters and a 15,000 token vocabulary.
   
  
  
  

</div>
</div>

<div class='item-footer'>
   12:23 &bull;
   18 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/Jun/3/katherine-michels-pycon-us-2024-recap/#atom-everything'>Katherine Michel's PyCon US 2024 Recap</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Katherine Michel's PyCon US 2024 Recap 
An informative write-up of this year’s PyCon US conference. It’s rare to see conference retrospectives with this much detail, this one is great!

     Via @katimichel 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://katherinemichel.github.io/portfolio/pycon-us-2024-recap.html">Katherine Michel's PyCon US 2024 Recap</a></p>
An informative write-up of this year’s PyCon US conference. It’s rare to see conference retrospectives with this much detail, this one is great!

    <p>Via <a href="https://twitter.com/katimichel/status/1796931565227778378">@katimichel</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   09:31 &bull;
   21 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/03/you-know-what.html'> You know what this is - right? 
  
 Christmas WWDC is coming</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   You know what this is - right? 
  
 Christmas WWDC is coming. 
 All I really want is for it to STOP. 
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2024/cleanshot-2024-06-03-at-10.13.062x.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>You know what this is - right?</p>
<p></p>
<p>Christmas WWDC is coming.</p>
<p>All I really want is for it to STOP.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2024/cleanshot-2024-06-03-at-10.13.062x.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   04:42 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/03/about-that-show.html'> About That Show I Just Watched 
 
 Gratuitous Naked Shots ✅
</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   About That Show I Just Watched 
 
 Gratuitous Naked Shots ✅
 
 Tops and Bottoms ✅ 
 Male and Female ✅ 
 
 
 Clichés ✅ 
 Happy Endings ✅
 
 Arc of the shows story ✅ 
 Sex scenes ✅
 
 Champagne corks popping ✅ and 
 Crashing waves (see 2 - clichés) ✅ 
 
 
 
 
 Bad Acting ✅ 
 Plenty of military jingoism  ✅ 
 Nationalities of the baddies as expected  ✅ 
 Renegades in the Military who turn out to be th
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 About That Show I Just Watched 
 
 Gratuitous Naked Shots ✅
 
 Tops and Bottoms ✅ 
 Male and Female ✅ 
 
 
 Clichés ✅ 
 Happy Endings ✅
 
 Arc of the shows story ✅ 
 Sex scenes ✅
 
 Champagne corks popping ✅ and 
 Crashing waves (see 2 - clichés) ✅ 
 
 
 
 
 Bad Acting ✅ 
 Plenty of military jingoism  ✅ 
 Nationalities of the baddies as expected  ✅ 
 Renegades in the Military who turn out to be the best  ✅ 
 Lead straight guy discovers his best friend has been gay all along, even though the Lesbian team member knew all along   ✅ 
 The ‘nerd’ computer geek is the least attractive of the entire team  ✅ 
 Pretty compliant on ‘most’ politically correct casting for the core tea of 7   ✅ 
 
 That All Said, on the plus side. 
 
 
<p>Great twist that I didn’t see coming, that reinvigorated my waning (waned?) interest.</p>
 
 (Episode 1 was so bad that I have no idea why I started 2 - it likely just auto played.) 
 
 
 
<p>The actors spoke in English so no dubbing or subtitles needed (more on this in a future post).</p>
 
 
<p>The baddies weren’t muslim.</p>
 
 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   04:26 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/03/i-just-listened.html'> I just listened to Steven Hackett and David Sparks talking a</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   I just listened to Steven Hackett and David Sparks talking about Arc. Been a while, because though I am subscribed - I tend to listen to podcasts based on the topic per episode - not the podcast or host(s) of same. 
 Topic - Arc? Im in. 
 Nice summary - and a couple of things that I will try out - but most disappointed that no discussion on STILL WHY NO IPAD APP. I (don’t) get it - run the iPhone 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I just listened to Steven Hackett and David Sparks talking about Arc. Been a while, because though I am subscribed - I tend to listen to podcasts based on the topic per episode - not the podcast or host(s) of same.</p>
<p>Topic - Arc? Im in.</p>
<p>Nice summary - and a couple of things that I will try out - but most disappointed that no discussion on STILL WHY NO IPAD APP. I (don’t) get it - run the iPhone App. Yeah …. No. Have you seen how useless an iPhone window is on a big screen? Let alone browsing.</p>
<p>I like Arc - but I won’t even consider it as my default browser until there is a real iPad app.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   01:34 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/known-fizzled'>Known fizzled</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
 One of my biggest regrets is how the Known hosted service declined. The paid subscriptions came to an end, and eventually the hosting whimpered out. Behind the scenes, the database cluster was in need of more maintenance than I was able to provide.  Known itself has required more maintenance than I’ve been able to provide for quite some time. I wish I could spend more bandwidth on it, but the st
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
<p>One of my biggest regrets is how the Known hosted service declined. The paid subscriptions came to an end, and eventually the hosting whimpered out. Behind the scenes, the database cluster was in need of more maintenance than I was able to provide.</p><p>Known itself has required more maintenance than I’ve been able to provide for quite some time. I wish I could spend more bandwidth on it, but the state of my life right now is that it’s just not possible for me to dedicate the coding time for something that isn’t paying my bills and isn’t having the impact I wanted it to.</p><p>I wish we’d sent out a strong email at the end and allowed everyone to export their data automatically. I also wish Known had import/export that was reliable so that people could explore other platforms.</p><p>After attempting to claw the time to do it myself, I’d like to hire someone to build the latter, and then apply it to everyone who had a hosted account. The export function could be built into the Known UI or as a CLI tool. If this seems like something you might be able to do, let me know.</p><p>Overall, I have a ton of regrets about Known — something for a future post (or series of posts), maybe. This site is still powered by it, though, and I know other people still use it, too. So it’s not dead — just small.</p> 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   01:05 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 02. June 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/03/sometimes-i-dont.html'> 📸 🌊 Sometimes I don’t even know what to say … 
</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   📸 🌊 Sometimes I don’t even know what to say … 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2024/f1a9211b3f.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>📸 🌊 Sometimes I don’t even know what to say …</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2024/f1a9211b3f.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   23:30 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://john.philpin.com/2024/06/03/the-first-vision.html'>🎈155/366 The First Vision Pro Concert Movie</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   I have touched on this before but after reading a Lefsetz post over the weekend, decided to expand a little and log the thinking for posterity. 
 🔗 Lefsetz Letter - Company At The Sphere - that’s the link to where 🖇️ this quote came from that I published this weekend. I said I’d be back… 
 Around 2016 - ‘the term ‘spatial’ was being coined’. - though I personally think Apple’s ‘spatial’ is a light
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I have touched on this before but after reading a Lefsetz post over the weekend, decided to expand a little and log the thinking for posterity.</p>
<p><a href="https://lefsetz.com/wordpress/2024/05/31/dead-company-at-the-sphere/">🔗 Lefsetz Letter - Company At The Sphere</a> - that’s the link to where <a href="https://john.philpin.com/2024/06/02/ill-come-back.html">🖇️ this quote</a> came from that I published this weekend. I said I’d be back…</p>
<p><a href="https://john.philpin.com/2016/06/01/someone-else-who.html">Around 2016 - ‘the term ‘spatial’ was being coined’.</a> - though I personally think Apple’s ‘spatial’ is a light year or two ahead of where Mr. Scoble was at the time (He’s very quiet these days, isn’t he?)</p>
<p><a href="https://john.philpin.com/2024/02/01/the-vision-pro.html">🖇️ But February this year</a> I made some pretty logical predictions about the first Vision Pro Movie.</p>
<p><a href="https://john.philpin.com/2023/10/17/i-dont-make.html">🖇️ And in October last year I was predicting that The Sphere was a pretty good place to start on a ‘Vision Pro Concert Series’</a> - and I think Apple will be the one to do it. Well, maybe not ‘doing’ it - but certainly financing it.</p>
<p>I still think that.</p>
<p>But who will be the director and/or subject matter for this groundbreaking event?</p>
 Who’s Out? 
<p>Despite Tay Tay’s global appeal, it won’t be her. The Swifties don’t have the money to buy the Pro.</p>
<p>Peter Jackson is deep into the latest tech to do great things, definitely a friend of Apple - and his foray into music with The Beatles is interesting - but no, no Sphere experience and besides <a href="https://variety.com/2024/film/news/lord-of-the-rings-movie-2026-release-warner-bros-1235997102/">🔗 he is deep into his next Trilogy</a> .. as Producer - not Director,  but still ..</p>
<blockquote>
<p>Original “The Lord of the Rings” trilogy filmmaker <a href="https://variety.com/t/peter-jackson/">Peter Jackson</a> and his partners Fran Walsh and Philippa Boyens are producing the movie and “will be involved every step of the way,” Warner Bros. Discovery CEO David Zaslav said during an earnings call Thursday.</p>
</blockquote>
<p>Phish? I dunno - not seeing it.</p>
 So Who’s In? 
<p>Two very real possibilities …</p>
<p>U2 - deeply intertwined with Apple - remember the album? And then they were first up at the Sphere - and Zane Lowe hanging out with them, talking with them. We have seen some of the footage, but who knows what was taken off the ‘back stage’ for the first Vision Pro movie?</p>
<p>… but I am wondering - and well out of left field - but …</p>
<p>The Grateful Dead - who have always been at the bleeding edge of touring and spectacle</p>
 
 The ‘Home’ band for Bay Area based Apple 
 Sphere residency happening - even as I type 
 Residency coincided with Earth Day 2024 (very ‘Appley’) 
 Lots of San Francisco through time imagery featured in the show 
 <a href="https://www.huffpost.com/entry/the-dead-rise-again_b_11417820">🔗 and then there is this</a> - Huff Post - so maybe a pinch of salt is needed, written eight years ago …but written by <a href="https://en.wikipedia.org/wiki/Bill_Bradley">Bill Bradey</a>, so not his usual subject matter - but certainly no slouch. 
 Note that Mayer is also part of The Sphere residency. 
 
<p>It’s all fitting in for this bear.</p>
<p>The only hard thing to reconcile is that ‘Tay Tay’ problem. Is there an overlap?</p>
<p>I think there just might be.</p>
<p>What do you think?</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   23:06 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/Jun/2/andrej-karpathy/#atom-everything'>Quoting Andrej Karpathy</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    Turns out that LLMs learn a lot better and faster from educational content as well. This is partly because the average Common Crawl article (internet pages) is not of very high value and distracts the training, packing in too much irrelevant information. The average webpage on the internet is so random and terrible it's not even clear how prior LLMs learn anything at all.   — Andrej Karpathy 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <blockquote cite="https://twitter.com/karpathy/status/1797313173449764933"><p>Turns out that LLMs learn a lot better and faster from educational content as well. This is partly because the average Common Crawl article (internet pages) is not of very high value and distracts the training, packing in too much irrelevant information. The average webpage on the internet is so random and terrible it's not even clear how prior LLMs learn anything at all.</p></blockquote><p>— <a href="https://twitter.com/karpathy/status/1797313173449764933">Andrej Karpathy</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   21:09 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/03/today-i-learned.html'> Today I Learned that ‘A Boy Named Sue’ was written by Shel S</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Today I Learned that ‘A Boy Named Sue’ was written by Shel Silverstein … 
 … I’m also thinking that today most of you will be asking ‘ what’s ‘A Boy Named Sue’ ‘ 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Today I Learned that ‘A Boy Named Sue’ was written by Shel Silverstein …</p>
<p>… I’m also thinking that today most of you will be asking ‘ what’s ‘A Boy Named Sue’ ‘</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   20:47 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/03/magnet-fishing-had.html'> Magnet Fishing. 
 Had you heard of the term before now? </a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Magnet Fishing. 
 Had you heard of the term before now? 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Magnet Fishing.</p>
<p>Had you heard of the term before now?</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   20:34 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/the-21-best-science-fiction-books-of-all-time-according'>The 21 best science fiction books of all time – according to New Scientist writers</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 
 This list of the New Scientist's favorite science fiction books is brilliant. The books I've read that are included here are some of my favorites of all time; the others are on my to-read list. What's your favorite? 
 #Culture 
             [Link] 
         
     
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 
<p>This list of the New Scientist's favorite science fiction books is brilliant. The books I've read that are included here are some of my favorites of all time; the others are on my to-read list. What's your favorite?</p>
<p><a href="https://werd.io/tag/Culture">#Culture</a></p>
            <p>[<a href="https://www.newscientist.com/article/2433037-our-writers-pick-their-favourite-science-fiction-books-of-all-time/">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   20:25 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/03/the-surprising-thing.html'> The surprising thing about ICQ closing down is that it is 20</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   The surprising thing about ICQ closing down is that it is 2024 and it is still around to close down. 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The surprising thing about ICQ closing down is that it is 2024 and it is still around to close down.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   20:04 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/03/sounds-like-a.html'> Sounds like a grand plan. If memory serves he’s on a sticky </a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Sounds like a grand plan. If memory serves he’s on a sticky wicket when it comes to his coalition, If they go - the whole house o/ card is at risk - he loses power and will then have to face courts for other crimes - or something like that - I need to check. BUT. It sure smells like another example of a very small extreme minority holding sway over the vastly larger majority. 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2024/9cba0d44a5.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Sounds like a grand plan. If memory serves he’s on a sticky wicket when it comes to his coalition, If they go - the whole house o/ card is at risk - he loses power and will then have to face courts for other crimes - or something like that - I need to check. BUT. It sure smells like another example of a very small extreme minority holding sway over the vastly larger majority.</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2024/9cba0d44a5.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:33 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/how-black-lives-became-the-hidden-cost-of-clean-energy'>How Black Lives Became The Hidden Cost of Clean Energy</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 
  "The nation, fractured by war, disease, and famine, has seen more than 6 million people die since the mid-1990s, making the conflict the deadliest since World War II. But, in recent years, the death and destruction have been aided by the growing number of electric vehicles humming down American streets."   A good reminder that our desire for batteries and power has a human imp
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 
<blockquote><p>"The nation, fractured by war, disease, and famine, has seen more than 6 million people die since the mid-1990s, making the conflict the deadliest since World War II. But, in recent years, the death and destruction have been aided by the growing number of electric vehicles humming down American streets."</p></blockquote><p>A good reminder that our desire for batteries and power has a human impact, no matter which path we take. Renewable energy is still a far better choice, but we run the risk of thinking that "clean" tech is truly clean without doing the work necessary to ensure that everyone in the supply chain is well taken care of.</p><p>Solidarity campaigns and activism to protect peoples' lives are good, but it's notable that we never really get to hear about them, and this issue is rarely, if ever, mentioned in the tech press.</p><p>As the piece points out:</p><blockquote><p>"“We’re always on the menu, but we’re never at the table,” he said. “The space of transportation planning and climate change is mostly white people, or people of color that aren’t Black, so these discussions about exploitation aren’t happening in those spaces — it is almost like a second form of colonialism.”"</p></blockquote><p><a href="https://werd.io/tag/Technology">#Technology</a></p>            <p>[<a href="https://capitalbnews.org/congo-clean-energy-us-workers/">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   13:29 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/Jun/2/experimenting-with-local-alt-text-generation-in-firefox-nightly/#atom-everything'>Experimenting with local alt text generation in Firefox Nightly</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Experimenting with local alt text generation in Firefox Nightly 
The PDF editor in Firefox (confession: I did not know Firefox ships with a PDF editor) is getting an experimental feature that can help suggest alt text for images for the human editor to then adapt and improve on. 
 This is a great application of AI, made all the more interesting here because Firefox will run a local model on-device
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://hacks.mozilla.org/2024/05/experimenting-with-local-alt-text-generation-in-firefox-nightly/">Experimenting with local alt text generation in Firefox Nightly</a></p>
The PDF editor in Firefox (confession: I did not know Firefox ships with a PDF editor) is getting an experimental feature that can help suggest alt text for images for the human editor to then adapt and improve on.<p></p>
<p>This is a great application of AI, made all the more interesting here because Firefox will run a local model on-device for this, using a custom trained model they describe as "our 182M parameters model using a Distilled version of GPT-2 alongside a Vision Transformer (ViT) image encoder".</p>
<p>The model uses WebAssembly with ONNX running in <a href="https://huggingface.co/docs/transformers.js/en/index">Transfomers.js</a>, and will be downloaded the first time the feature is put to use.

    </p><p>Via <a href="https://twitter.com/mozhacks/status/1796774672639336804">@mozhacks</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   13:12 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2024/06/5803/'>[6月4日] ベルリンで開催のEuropean Identity & Cloud Conference 2024 でキーノートスピーチをします。題して「サイバー・フロンティアのレ・ミゼラブル: 分散型アイデンティティの対立する物語」</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  ヨーロッパ最大のデジタルアイデンティティに関するカンファレンス、EIC (European Identity and Cloud Conference) 2024が、今年もベルリンのbcc Berlin Congress Centerで開催されます。1500人以上が集まり、EUデ…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>ヨーロッパ最大のデジタルアイデンティティに関するカンファレンス、EIC (European Identity and Cloud Conference) 2024が、今年もベルリンのbcc Berlin Congress Centerで開催されます。1500人以上が集まり、EUデジタルアイデンティティウォレットをはじめ、様々なアイデンティティ関連のトピックについて議論が行われます。</p>



<p>私は、初日の夕方という最も注目を集める時間帯に、Danube TechのMarkus Sabbadello (Decentralized Identiy Foundation 理事）と共に、「Les Miserables of the Cyber Frontier: The Dueling Narratives of Decentralized Identities」（仮訳：サイバー・フロンティアのレ・ミゼラブル: 分散型アイデンティティの対立する物語）というタイトルでキーノートスピーチを行います。このタイトルは主催者によって決められたもので、どんな話になるかは当日のお楽しみという新たな試みです。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/06/image.png?ssl=1"></a>EICキーノートセッション「Les Miserables of the Cyber Frontier: The Dueling Narratives of Decentralized Identities」の紹介画像 (出所) KuppingerCole



<p>実はこのセッション、もともとは昨年行った「<a href="https://www.youtube.com/live/HziNa-RUA4A?t=715s">分散の誤謬</a>（<a href="https://www.youtube.com/watch?v=Cddx83R0jos">Falacy of Decentralisation</a>）」について、Markusが解決策を提示する場として企画されました。しかし、カンファレンスのコンテンツ責任者であるJoerg Reschに、別のコンテキストとして「無常社会と番号制度（The Miserable Society and the Identity System）」というネタがあると話したところ、彼がそのタイトルを気に入ったため、内容はそのままにタイトルが変更されることになったのです。</p>



<p>内容は「Markus SabbadelloとNat Sakimura、デジタルアイデンティティの先駆者たちによる対談。今日の『分散型』プロジェクトは本当にどれほど分散化されているのでしょうか？ここに至るまでの経緯は？そして、ここからどこへ向かうのでしょうか？」といったもので、私たち二人の対談を通じて、現在の分散型プロジェクトの現状と今後の展望について探っていきます。</p>



<p>先週の木曜日に行ったざっくりとした打ち合わせの中で、Markusが「んで、タイトル回収、どうする…？」と問いかけた場面もあり、果たしてどのような内容になるのか、今から楽しみです。</p>



<p>トークの時間と場所は：</p>



 
 タイトル：Les Miserables of the Cyber Frontier: The Dueling Narratives of Decentralized Identities 



 時間　　：2024年6月4日（火） 19:10—19:30 



 場所　　：bcc Berlin Congress Center メインホール (C 01) 
 



<p>当日、どのような展開になるのか、ぜひご期待ください。</p>



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/06/image.png?resize=546%2C254&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   12:06 &bull;
   2 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wrenchinthegears.com/2024/06/02/water-memory-wildflowers-regret-and-a-path-forward-with-bonus-material-food-for-thought-on-water-bears-and-consciousness/'>Water Memory, Wildflowers, Regret, And A Path Forward (With Bonus Material – Food For Thought On Water Bears And Consciousness)</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Good morning from Little Rock! It’s been a week of emotional ups and downs. Even as I’ve been pounding the pavement to find a new home, I’ve embarked on some exhilarating explorations with Stephers and Washington Sean around wearables, managed “wellness,” blockchain digital twins, tech billionaire-founded “smart” Montessori schools, forest intelligence, micro-myco biomes, photons, quantum [...]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Good morning from Little Rock! It’s been a week of emotional ups and downs. Even as I’ve been pounding the pavement to find a new home, I’ve embarked on some exhilarating explorations with Stephers and Washington Sean around wearables, managed “wellness,” blockchain digital twins, tech billionaire-founded “smart” Montessori schools, forest intelligence, micro-myco biomes, photons, quantum entanglement, metabolism, tardigrades (moss bears), substrate-independent fractal group minds, radiation, desiccation, resonant information fields, and water memory in the context of the “Three Body Problem,” Liu Cixin’s novel recently remade as a Netflix series. </p>
<p>I keep trying to focus and put my thoughts together, but have thus far only succeeded in compiling an hour or so of clips (Oops, look at that, now it’s three hours – shocker!) that outline the contours of our thinking, still in progress. Consider them video “notes.” I suspect without commentary it may be difficult for others to follow, but I’ll share the video below for curious ones who may want to wade into the flow of emergent, coalescing conjectures and have a look around. Hopefully in the next week I’ll be able to sort through our voluminous text thread and map out the concepts well enough to do a structured overview of our expanded thinking on the situation in which we find ourselves.</p>
<p></p>
<p>I texted Stephers, after a particularly fruitful exchange, that people are going to have a hard time keeping up. It was a comment with a bittersweet edge. I don’t mean to imply people are slow or ignorant. It’s just that our merry band has ventured far off into uncharted territory, and few are prepared to extend themselves past comfortable / comforting narratives. Beyond there be dragons and all that…</p>
<p>I experienced this with my own family, an ongoing heart break. You changed, you do research, you left us…was the refrain. Even though I was always right there in the living room. Yes, I was in the living room, but the information that surrounded me was…different. I wasn’t the NPR-listening mom I used to be. I followed the money. I saw patterns. I built my own understanding of what I sensed was unfolding. I wanted to know more, and in that quest I realized, too late, I had isolated myself from my former life with terrible finality.&nbsp;</p>
<p>The work I’ve done in partnership with fellow travelers; pulling threads, making maps, uncovering connections, and weaving a tapestry that continues to challenge everything I thought I knew about the world, gradually pushed me to the margins. Over time, it led to my painful cancellation as a daughter, wife, mother, and friend even as it has opened doors to productive new collaborations and adventures in new lands. I continue to grapple with that reality alone here in this efficiency apartment, awaiting my next chapter. Despite persistent, ineffectual, attempts to build bridges back to my loved ones, the vast chasms, for the moment at least, seem like they will not be overcome.&nbsp;</p>
<p>There is an ongoing refrain running in the back of my mind…Meg facing off against IT… “you are not my Charles Wallace.” </p>
<p>But also, “love wins.”</p>
<p>While touring homes this week, my realtor commiserated with me. </p>
<p>Yes, Covid, whatever it was, changed everything.&nbsp;</p>
<p>We are all brain-boxed to varying degrees. It’s challenging to integrate information we’re not socialized to see, have no context to see, into an updated world view. As a researcher, I freely offer questioners maps of my thinking with copious source material. I cannot, however, compel anyone to dive in and explore. Even with a framework that contextualizes my thought process laid out for consideration, learning a new landscape takes time; and that is bandwidth people may not have.&nbsp;</p>
<p>They may not want to make an investment in a journey that could result in them being jettisoned from their life, as I have been from mine. There are no shortcuts to getting familiar enough with a new lens that you can try it on and look at things from different angles to see if it works for you. And at the end of the day, few people want to put hours, days, weeks, or even years of effort into deconstructing the stories inside which they live. Because you risk being left standing out on your own, ideologically, emotionally, spiritually naked. Only those who are prepared to embark on an uncertain journey take that first step. No judgment. It’s just the way it is. We’re not all the same, and that’s ok. Not everyone is here to walk the same road. Blockchained or not, we are endowed with agency and the capacity to make choices. </p>
<p>Choose, pause, reflect, and choose again.</p>
<p>My intelligence forays keep looping me back to education, my starting point, particularly early childhood “education,” the centerpiece of the emerging social impact finance landscape. Each time I “level up” into an expanded understanding of this complex “game” I find myself revisiting work I’ve done before in a new light. I’ve been thinking about Montessori education melded with the forest school model in the context of invisible spatial computing, digital ID, terraformed synthetic telepathy, carbon credits, an emergent GAIA noosphere, and NVIDIA’s plans to give every child an AI companion that will help manifest bring their imaginary worlds into this realm.&nbsp;</p>
<p>Where does Sep Kamvar fit in with Wildflower Montessori (think – Evan Baehr’s “flourishing” Learn Capital investments, Regen Network’s Web3 soil sequestration shenanigans, systems theorist Dana Klisanin and her indigenous “rewilding” of the mind, not to mention cryptographer Ian Grigg’s tokenized laminated flowers).&nbsp;</p>
<p>Wildflower is gaining ground in charter micro-school franchises nationwide. Could MIT’s social computing project ultimately be paired with Celo’s “beautiful” digital community currency to manage the homeostasis of an emergent global superorganism? Are they after a network of youthful imaginary explorers, each classroom a fractal holon coordinated with token engineering and game mechanics? Will Oliver Reiser’s “won’t you be my sensor?” World Sensorium be run by the under-eight crowd, children just beginning to lose their baby teeth, their minds gradually domesticated by the printed word? Just what is this “science of reading” really about? It’s starting to feel like memetics and mind viruses melded with “personalized” AI literacy playlists of OER, EEGs, and IoT eye-tracking.</p>
<p>Kamvar speaks often about the intelligence of the forest, fungal networks, and decentralized collective intelligence. Think of communication reframed as a free market economy of underground signals – phosphorus routing through mycorrhiza and roots and soil and food and bones and around again it goes – dust to (nano?) dust…interdimensional communication…gut programming…unexpected data storage and retrieval mechanisms…hypercubes…crystal lattices navigated by entrained heart field energy…</p>
<p>Could the future of Moore’s law computing look like moss and lichen on a boulder? With the move to quantum photonics, there is a logic in putting photosynthesis front and center. I sense the system knows a sacred intelligence already exists and there is some pressure to harness, divert, and scale up nature’s secrets.&nbsp;</p>
<p></p>
<p>Again…for what purpose? I do not know. So please don’t ask me to stop looking and start judging, demanding I put a thumb down on the good/bad scale. That’s not who I am anymore. I don’t know enough yet to say, and ultimately I feel it is vital for each person to come to their own inner knowing of where things are headed with this Noosphere, gamified GAIA superorganism. I am more than happy to share my notes, and see yours if we are operating in the same problem space. I can’t tell you how to think about it all. It’s not my place.&nbsp;</p>
<p>Over the past few years I’ve swapped my previously assumed roles of judge, warrior, and activist for curious scout and pattern finder. My brain and my heart are full. There exists a shrinking circle of people who share enough common ground that we can compare notes and piece together more of this puzzle. For my fellow collaborators I continue to be profoundly grateful. For whatever reason I sense it is my fate to keep walking towards the horizon, my current base of operation being Hot Springs, Arkansas (the natural state) where I’ll poke around in the corners of philosophy of mind, quantum consciousness, information theory, extremophiles, biogeochemistry, and the origins of life against a backdrop of ancient, weathered mountains, clear water, and majestic trees.&nbsp;</p>
<p>An added bonus are the welcoming people here, some of whom have opened their hearts to me and seem more than happy to take a wayward Yankee under their wing and share their local wisdom of fishing, hiking, swimming holes, and t-ball.&nbsp;</p>
<p>I’ll end with some photos I took on walks this past week, seeking solace in the dappled shade of my new home place. One of the reasons I chose Hot Springs was the fact that despite being just an hour’s drive from Little Rock, it is surrounded by the wild nature of the Ouachita National Forest along with generous doses of pastoral beauty. These worn-down mountains are perfect for women of a certain age, like me. I adore the mix of hardwoods and towering southern pines interspersed with glints of distant sunlight sparkling on the lake.&nbsp;</p>
<p>Pics include a forest renewing itself from a devastating tornado 15 years ago, fungal decomposition, geological layers that reset one’s perception of time, cheerful rudbeckia, milky quartz, a tiny coral fungus of the most captivating purple, kids enjoying the first days of summer vacation at the lake, and a tiny heart of white pebbles!&nbsp;</p>
<p>Onward! This piece, “Portage” I saw in the riverfront sculpture garden today after grabbing some okra and squash at the farmer’s market seems to sum up the state of my life at the moment. Hopefully, I’ll be able to put my boat back in the water soon.&nbsp;</p>
<p>&nbsp;</p>
<p></p>
<p></p>
<p></p>
<p>&nbsp;</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p>&nbsp;</p>
<p></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/img_5777-1.jpg?resize=1020%2C765&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   07:52 &bull;
   2 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/02/just-to-say.html'> Just to say after writing that last post I suddenly realized</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Just to say after writing that last post I suddenly realized that Reminders has become my Task Manger of choice. 
 For years it’s been Things … which still lurks .. but Reminders has always been there because ‘family sharing’ .. and then a few weeks ago I asked Siri to remind me to buy something and Siri dropped it into Reminders - not Notes where the shared shopping lists live … I know I know … s
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Just to say after writing that last post I suddenly realized that Reminders has become my Task Manger of choice.</p>
<p>For years it’s been Things … which still lurks .. but Reminders has always been there because ‘family sharing’ .. and then a few weeks ago I asked Siri to remind me to buy something and Siri dropped it into Reminders - not Notes where the shared shopping lists live … I know I know … sorry … because ‘sharing’.</p>
<p>Not only that .. but turns out ‘Shopping’ has a whole auto categorizing thing going on that I had somehow missed … so I moved my quick lists in there and before you know it - other stuff.</p>
<p>It has come a long way since I last used it with vengeance .. not deleting Things yet though.</p>
<p>Two wishes for WWDC</p>
 
 however they are doing the shopping thing - let me set up other lists to allow similar categorization 
 Introduce time boxing and necessarily tighter calendar integration. 
 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   05:14 &bull;
   2 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/02/sitting-with-a.html'> Sitting with ‘a quiet one’, wandering through the reminders </a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Sitting with ‘a quiet one’, wandering through the reminders re prioritizing - as you do. Needed to remember to connect a couple of people to another couple and accidentally added them into my Shopping List. 
 Have to say, impeccable logic. 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2024/21c813e002.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Sitting with ‘a quiet one’, wandering through the reminders re prioritizing - as you do. Needed to remember to connect a couple of people to another couple and accidentally added them into my Shopping List.</p>
<p>Have to say, impeccable logic.</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2024/21c813e002.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   04:40 &bull;
   2 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/02/great-job-danielpunkass.html'> Great job @danielpunkass 
 New Mars Edit Release 
 Search &a</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Great job @danielpunkass 
 New Mars Edit Release 
 Search &amp; Find Improvements 
 NICE 
 Find me all the posts containing 😂 AND 📸 - which would find all my funny images is a great way to narrow down for whatI am looking for. (Words also work @devilgate 😂) - I wonder if ‘OR’ is in our future?) 
 Micropost Panel 
 I won’t typically use - but I can definitely see some workflows will love this 
 Dup
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Great job <a href="https://micro.blog/danielpunkass">@danielpunkass</a></p>
 New Mars Edit Release 
<p>Search &amp; Find Improvements</p>
<p>NICE</p>
<p>Find me all the posts containing 😂 AND 📸 - which would find all my funny images is a great way to narrow down for whatI am looking for. (Words also work <a href="https://micro.blog/devilgate">@devilgate</a> 😂) - I wonder if ‘OR’ is in our future?)</p>
<p>Micropost Panel</p>
<p>I won’t typically use - but I can definitely see some workflows will love this</p>
<p>Duplicate Posts</p>
<p>.. or Templates - as I like to call them - clever.</p>
<p>Options Side Panel</p>
<p>I am sure it is great - but .. ‘say what now’? Lost.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   02:58 &bull;
   2 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/02/remember-a-while.html'> Remember a while back there was this series called Sugar and</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Remember a while back there was this series called Sugar and there was a bit of a debate around - ‘well I did not see that coming’ at the end of Episode 6. 
 I have been in and out of another show that is absolutely nowhere near as good. Nowhere near. BUT … at the end of Episode 6 there is a 🤯 that I definitely did NOT see coming.
Suddenly viewing the plot through very different eyes. 
 Is ‘Episod
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Remember a while back there was this series called Sugar and there was a bit of a debate around - ‘well I did not see that coming’ at the end of Episode 6.</p>
<p>I have been in and out of another show that is absolutely nowhere near as good. Nowhere near. BUT … at the end of Episode 6 there is a 🤯 that I definitely did NOT see coming.
Suddenly viewing the plot through very different eyes.</p>
<p>Is ‘Episode 6’ the new black?</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   01:55 &bull;
   2 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/02/the-post-slashes.html'> The post 🔗 Slashes by Manu got me to thinking … 
 If 
 foo.c</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   The post 🔗 Slashes by Manu got me to thinking … 
 If 
 foo.com/contact 
 takes you to ‘contact’ - but by using a ‘redirect’ and the actual URL is more like 
 ‘stuff.weirdandcrazy.jcaourincag79q8i6iasicnAYT.com’ 
 … is it still a slash page? 
 Asking for a friend. 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The post <a href="https://manuelmoreale.com/@/page/hhm2chyyE0IwYUyA">🔗 Slashes by Manu</a> got me to thinking …</p>
<p>If</p>
<p>foo.com/contact</p>
<p>takes you to ‘contact’ - but by using a ‘redirect’ and the actual URL is more like</p>
<p>‘stuff.weirdandcrazy.jcaourincag79q8i6iasicnAYT.com’</p>
<p>… is it still a slash page?</p>
<p>Asking for a friend.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   01:51 &bull;
   2 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 01. June 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/02/part-of-the.html'> 📸 🌊 Part of the point of this series is to record a daily ‘e</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   📸 🌊 Part of the point of this series is to record a daily ‘ever changing mood’ (queue song?) of the weather from a consistent angle. On occasions I might post as many as 4 from a single day. I saw this and had to move fast before it disappeared. 

 I went back inside, turned around, I kid you not ONE MINUTE LATER. 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2024/2890c06c2704403e8893de2a69fe7252.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>📸 🌊 Part of the point of this series is to record a daily ‘ever changing mood’ (queue song?) of the weather from a consistent angle. On occasions I might post as many as 4 from a single day. I saw this and had to move fast before it disappeared.</p>

<p>I went back inside, turned around, I kid you not ONE MINUTE LATER.</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2024/2890c06c2704403e8893de2a69fe7252.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:10 &bull;
   2 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/02/ill-come-back.html'> I’ll come back to this later. Lefsetz has already connected </a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   I’ll come back to this later. Lefsetz has already connected some dots .. but he didn’t go on and introduce the lurking ‘hobby’ that ‘solves this’ and is a clear and present danger to the entertainment industry if they continue to focus on BAU. 
 The last paragraph. 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2024/248e0c67f2.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I’ll come back to this later. Lefsetz has already connected some dots .. but he didn’t go on and introduce the lurking ‘hobby’ that ‘solves this’ and is a clear and present danger to the entertainment industry if they continue to focus on BAU.</p>
<p>The last paragraph.</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2024/248e0c67f2.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:50 &bull;
   2 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/the-way-we-raise-the-money-at-the-guardian-is'>“The way we raise the money at The Guardian is different than any place I’ve ever been”</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 "The way we raise the money at The Guardian is different than any place I’ve ever been. This is truly a jointly owned responsibility among the business side and editorial." 

 Every non-profit newsroom needs to move their center of gravity from large contributions to smaller, distributed support from its reader base. The Guardian is doing it incredibly well, and there's a lot 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>"The way we raise the money at The Guardian is different than any place I’ve ever been. This is truly a jointly owned responsibility among the business side and editorial."</p>

<p>Every non-profit newsroom needs to move their center of gravity from large contributions to smaller, distributed support from its reader base. The Guardian is doing it incredibly well, and there's a lot to learn from how it's going about things.</p>

<p>I'm not sure about the idea of tracking revenue per article, but the idea of making the whole newsroom involved in its continued existence doesn't seem bad to me (even if it goes against accepted orthodoxy). The trick is not taking it too far, and being open to secondary or tertiary effects. There are some stories that are vitally important even if they aren't obvious moneymakers, and newsrooms must retain a strong argument for running them.</p>

<p>The Guardian's "epic" at the bottom of every article drives a ton of revenue for them, and I'd love to learn more about how they optimize it in practice.</p>

<p>Finally, this seems right to me, and something for all news (for-profit and non-profit alike) to emulate:</p>

<p>"Nine or 10 years ago, we did a lot of work to decide whether we should have a paywall or not. And we decided that we would both fulfill our mission better, but we would also generate more revenue, if there were no paywall. Now it’s part of our DNA and we talk about it every day."  <a href="https://werd.io/tag/Media">#Media</a></p>
            <p>[<a href="https://www.niemanlab.org/2024/05/the-way-we-raise-the-money-at-the-guardian-is-different-than-any-place-ive-ever-been/">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   13:48 &bull;
   3 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/Jun/1/how-some-good-corporate-engineering-blogs-are-written/#atom-everything'>How (some) good corporate engineering blogs are written</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   How (some) good corporate engineering blogs are written 
Dan Luu interviewed engineers from Cloudflare, Heap, and Segment—three companies with excellent technical blogs—and three other unnamed companies with blogs he categorized as lame. 

 His conclusion? The design of the process for publishing—most notable the speed and number of approvals needed to get something published—makes all the differe
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://danluu.com/corp-eng-blogs/">How (some) good corporate engineering blogs are written</a></p>
Dan Luu interviewed engineers from Cloudflare, Heap, and Segment—three companies with excellent technical blogs—and three other unnamed companies with blogs he categorized as lame.<p></p>

<p>His conclusion? The design of the process for publishing—most notable the speed and number of approvals needed to get something published—makes all the difference.

    </p><p>Via <a href="https://news.ycombinator.com/item?id=40524531">Hacker News</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   08:35 &bull;
   3 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/Jun/1/stealing-everything-youve-ever-typed/#atom-everything'>Stealing everything you’ve ever typed or viewed on your own Windows PC is now possible with two lines of code — inside the Copilot+ Recall disaster</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Stealing everything you’ve ever typed or viewed on your own Windows PC is now possible with two lines of code — inside the Copilot+ Recall disaster 
Recall is a new feature in Windows 11 which takes a screenshot every few seconds, runs local device OCR on it and stores the resulting text in a SQLite database. This means you can search back through your previous activity, against local data that ha
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://doublepulsar.com/recall-stealing-everything-youve-ever-typed-or-viewed-on-your-own-windows-pc-is-now-possible-da3e12e9465e">Stealing everything you’ve ever typed or viewed on your own Windows PC is now possible with two lines of code — inside the Copilot+ Recall disaster</a></p>
Recall is a new feature in Windows 11 which takes a screenshot every few seconds, runs local device OCR on it and stores the resulting text in a SQLite database. This means you can search back through your previous activity, against local data that has remained on your device.<p></p>
<p>The security and privacy implications here are still enormous because malware can now target a single file with huge amounts of valuable information:</p>
<blockquote>
<p>During testing this with an off the shelf infostealer, I used Microsoft Defender for Endpoint — which detected the off the shelve infostealer — but by the time the automated remediation kicked in (which took over ten minutes) my Recall data was already long gone.</p>
</blockquote>
<p>I like Kevin Beaumont's argument here about the subset of users this feature is appropriate for:</p>
<blockquote>
<p>At a surface level, it is great if you are a manager at a company with too much to do and too little time as you can instantly search what you were doing about a subject a month ago.</p>
<p>In practice, that audience’s needs are a very small (tiny, in fact) portion of Windows userbase — and frankly talking about screenshotting the things people in the real world, not executive world, is basically like punching customers in the face.</p>
</blockquote>

    <p>Via <a href="https://cyberplace.social/@GossiTheDog/112537206611365804">@GossiTheDog</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   07:48 &bull;
   3 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/01/tops-and-tails.html'> 📸 🌊 Tops and Tails … this one is the first of the day. 

 A </a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   📸 🌊 Tops and Tails … this one is the first of the day. 

 A little bit later the sun starts to break through 

 Then at the end of the day, all the boats that are here for the long weekend looked quite pretty IRL … 

 But it took another half hour for it to get dark enough for this old iPhone to pick up the boat lights. 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2024/3e31de1387944dd9bda76793f55d44d8.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>📸 🌊 Tops and Tails … this one is the first of the day.</p>

<p>A little bit later the sun starts to break through</p>

<p>Then at the end of the day, all the boats that are here for the long weekend looked quite pretty IRL …</p>

<p>But it took another half hour for it to get dark enough for this old iPhone to pick up the boat lights.</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2024/3e31de1387944dd9bda76793f55d44d8.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   05:53 &bull;
   3 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/06/01/systems-the-purpose.html'> 🔗 Systems: The Purpose of a System is What It Does - Anil Da</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   🔗 Systems: The Purpose of a System is What It Does - Anil Dash 
 Yes? 
 Yes! 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://anildash.com/2024/05/29/systems-the-purpose-of-a-system/">🔗 Systems: The Purpose of a System is What It Does - Anil Dash</a></p>
<p>Yes?</p>
<p>Yes!</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   01:07 &bull;
   3 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/user-research-is-storytelling'>User Research Is Storytelling</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 I shared this with my team and one of them said they had to check that I didn't write it. This is exactly how I think about (and ask my team to think about) rooting software development in human needs. 

 "All the elements of a good story are there in the three-act structure of user research." And if I'd written a post about it, it might look a little bit like this one. 

 The
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>I shared this with my team and one of them said they had to check that I didn't write it. This is exactly how I think about (and ask my team to think about) rooting software development in human needs.</p>

<p>"All the elements of a good story are there in the three-act structure of user research." And if I'd written a post about it, it might look a little bit like this one.</p>

<p>There's a reason for the closeness: both our processes were informed by Nancy Duarte, who is very clear about the role of the three act structure. The details of my approach are a little bit different to what’s laid out in this post - something I may write about in a future post.  <a href="https://werd.io/tag/Technology">#Technology</a></p>
            <p>[<a href="https://alistapart.com/article/user-research-is-storytelling/">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   00:37 &bull;
   3 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 31. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/31/will-larson/#atom-everything'>Quoting Will Larson</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    Engineering leaders, especially at large companies, are managing a team of a couple hundred people. That team might cost $50 to 100 million in salary a year. So as a CEO, when you hear from your eng leaders that ‘Engineering is an art, and you can’t predict how it’s going to work,’ it’s frustrating. They’re sitting there thinking, ‘They’re telling me this is art, but I’m spending $100 million on 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <blockquote cite="https://review.firstround.com/unexpected-anti-patterns-for-engineering-leaders-lessons-from-stripe-uber-carta/"><p>Engineering leaders, especially at large companies, are managing a team of a couple hundred people. That team might cost $50 to 100 million in salary a year. So as a CEO, when you hear from your eng leaders that ‘Engineering is an art, and you can’t predict how it’s going to work,’ it’s frustrating. They’re sitting there thinking, ‘They’re telling me this is art, but I’m spending $100 million on this art each year.’ That’s not reassuring.</p></blockquote><p>— <a href="https://review.firstround.com/unexpected-anti-patterns-for-engineering-leaders-lessons-from-stripe-uber-carta/">Will Larson</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:53 &bull;
   3 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/31/caught-in-scam/#atom-everything'>Man caught in scam after AI told him fake Facebook customer support number was legitimate</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Man caught in scam after AI told him fake Facebook customer support number was legitimate 
This one illustrates a nasty edge-case if you ship any kind of chatbot with your company's name attached to it. 
 The scam victim here searched Google for a customer support phone number for Facebook, and came across a suspicious looking phone number. 
 He pasted that number into the Meta AI chatbot in Faceb
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://www.cbc.ca/news/canada/manitoba/facebook-customer-support-scam-1.7219581">Man caught in scam after AI told him fake Facebook customer support number was legitimate</a></p>
This one illustrates a nasty edge-case if you ship any kind of chatbot with your company's name attached to it.<p></p>
<p>The scam victim here searched Google for a customer support phone number for Facebook, and came across a suspicious looking phone number.</p>
<p>He pasted that number into the Meta AI chatbot in Facebook Messenger and asked "is it a Facebook help line?" - and a Meta AI answered:</p>
<blockquote>
<p>The phone number 1-xxx-xxx-xxxx is indeed a legitimate Facebook support number. Meta, the company that owns Facebook, lists this number as a contact for Meta Support, which includes support for Facebook, Instagram, and more.</p>
</blockquote>
<p>This was a total hallucination, and the phone number was for a scammer who ran a classic "download this app so I can help you" attack.</p>
<p>It doesn't matter how many disclaimers you add to a chatbot: this kind of misunderstanding from users is inevitable.

    </p><p>Via <a href="https://news.ycombinator.com/item?id=40536860">Hacker News</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:53 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/unexpected-anti-patterns-for-engineering-leaders'>Unexpected Anti-Patterns for Engineering Leaders</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 "The key to effective engineering leadership lies in figuring out which scenarios are worth deliberately defying conventional logic, and when to simply follow the rules." 

 Lots of good food for thought here. I've definitely been guilty of some of the anti-patterns here - particularly trying to be an umbrella for my team, which can leave people out of the loop and let them fe
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>"The key to effective engineering leadership lies in figuring out which scenarios are worth deliberately defying conventional logic, and when to simply follow the rules."</p>

<p>Lots of good food for thought here. I've definitely been guilty of some of the anti-patterns here - particularly trying to be an umbrella for my team, which can leave people out of the loop and let them feel like they're lacking needed transparency.</p>

<p>The key is being able to jump in and get into the weeds when it's helpful, get out when it's not, and give everybody the context, culture, information, and resources they need in order to do their best work in service of the mission, vision, and strategy.</p>

<p>Speaking of, I love this:</p>

<p>"There’s this pervasive belief that there’s no strategy anywhere, but that’s not true. There is strategy everywhere, it’s just rarely written."</p>

<p>That's been true of every organization I've joined, and - if I'm honest with myself - every organization I've started.</p>

<p>"Complicating things even further, Larson also has found that many companies do have a habit of writing things down, they just aren’t the right things. “It’s the small decisions that end up getting documented,” Larson says. “You’d think it would be the opposite, but in my experience, the answers to important questions like, ‘Why did we go into this business? Why are we shutting down this business line? Why are we doing this services migration that's going to take five years?’ literally aren't written down anywhere.”"</p>

<p>Encouraging people to write reflections, to capture the "why" of decisions that were made, and, essentially, to journal the journey of the team and the company is rarely done, but I think forms part of a solution to many problems.  <a href="https://werd.io/tag/Technology">#Technology</a></p>
            <p>[<a href="https://review.firstround.com/unexpected-anti-patterns-for-engineering-leaders-lessons-from-stripe-uber-carta/">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:51 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.patrick-breyer.de/chatkontrolle-blockade-wackelt-chatkontrolle-verweigerer-sollen-mit-foto-und-linkverbot-bestraft-werden/'>Chatkontrolle-Blockade wackelt – Chatkontrolle-Verweigerer sollen mit Foto- und Linkverbot bestraft werden</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Die hochumstrittene verdachtslose Chatkontrolle könnte von den EU-Regierungen doch noch beschlossen werden, weil Frankreich sein bisheriges Veto aufgeben könnte. Das berichtet euractiv und bestätigen interne Unterlagen. Frankreich sieht den … 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Die hochumstrittene verdachtslose Chatkontrolle könnte von den EU-Regierungen doch noch beschlossen werden, weil Frankreich sein bisheriges Veto aufgeben könnte. Das <a href="https://www.euractiv.com/section/law-enforcement/news/audio-communications-excluded-in-latest-draft-of-child-sexual-abuse-material-law/">berichtet</a> euractiv und bestätigen <a href="https://netzpolitik.org/2024/interne-dokumente-frankreich-entscheidet-ueber-zukunft-der-chatkontrolle/">interne Unterlagen</a>. Frankreich sieht den neuen Vorschlag einer „Uploadmoderation“ im Grundsatz als gangbaren Weg an.</p>



<p>Nach dem neuesten Gesetzentwurf mit Datum 28. Mai (<a href="https://netzpolitik.org/wp-upload/2024/05/2024-05-28_Council_Presidency_LEWP_CSAR_Compromise-texts_9093.pdf">Dokument 9093/24</a>), der als „Uploadmoderation“ präsentiert wird, sollen Nutzer von Apps und Diensten mit Chatfunktionen gefragt werden, ob sie das verdachtslose und fehleranfällige Scannen und gegebenenfalls Ausleiten ihrer privat verschickten Bilder, Fotos und Videos akzeptieren. Mit „künstlicher Intelligenz“ sollen auch bisher unbekannte Bilder und Videos durchleuchtet werden. Lehnt ein Nutzer die Chatkontrolle ab, soll er gar keine Bilder, Fotos, Videos oder Links mehr verschicken oder empfangen können (Artikel 10). Ende-zu-Ende-verschlüsselte Dienste müssten die Chatkontrolle durch Einbau von Überwachungsfunktionen umsetzen, die „vor der Datenübertragung“ greifen sollen (sog. client-side scanning, Artikel 10a). Auf die Durchsuchung von Textnachrichten nach Hinweisen auf „Grooming“, die auch bisher kaum zum Einsatz kommt, soll genauso verzichtet werden wie auf das noch nie eingesetzte Scannen von Sprachkommunikation. Wohl als Zugeständnis an Frankreich sollen zudem die Chats der Mitarbeiter von Sicherheitsbehörden und Militär von der Chatkontrolle ausgenommen bleiben.</p>



<p>Bei der letzten Diskussion am 24. Mai machte der Juristische Dienst des Rates deutlich, dass eine verdachtslose massenhafte Chatkontrolle weiterhin vorgesehen sei und nach wie vor grundrechtswidrig bleibe. Die meisten EU-Regierungen halten trotzdem an der Chatkontrolle fest. Schon am 4. Juni wollen die EU-Regierungen weiter beraten.</p>



<blockquote>
<p>„Der neueste Vorstoß zeigt, dass der extreme Ausgangsentwurf der EU-Kommission zur in der freien Welt einzigartigen Chatkontrolle im Kern unverändert durchgedrückt werden soll. Die ‚Upload-Moderation‘ mit vermeintlicher Nutzerzustimmung erinnert an die Fernsehshow ‚Lass dich überwachen‘. Messengerdienste rein textbasiert mit Bilder- und Linkverbot zu nutzen, ist im 21. Jahrhundert keine ernsthafte Option.</p>



<p>Es bleibt dabei: Millionen privater Chats und Privatfotos unbescholtener Bürger sollen mit unzuverlässiger Technik durchsucht und ausgeleitet werden, ohne dass die Betroffenen auch nur entfernt mit Kindesmissbrauch zu tun haben – das zerstört unser digitales Briefgeheimnis. So landen unsere Nacktfotos und Familienbilder bei Fremden, in deren Hände sie nicht gehören und bei denen sie nicht sicher sind. Trotz Lippenbekenntnissen zu Verschlüsselung soll mit Client-Side-Scanning bisher sichere Ende-zu-Ende-Verschlüsselung allgemein ausgehebelt werden, um unsere Smartphones zu Spionen umzufunktionieren – das zerstört sichere Verschlüsselung.</p>



<p>Mich alarmiert, dass das bisher kritische Frankreich die umverpackten Pläne plötzlich lobt und dadurch die bisherige Sperrminorität zu kippen droht. Wenn die EU-Regierungen tatsächlich mit dieser radikalen Position zur Chatkontrolle in die Trilogverhandlungen gehen sollten, droht das Parlament hinter verschlossenen Türen erfahrungsgemäß seine Ausgangsposition schrittweise aufzugeben und sich auf schlechte und gefährliche Kompromisse einzulassen, die unsere Sicherheit im Netz auf Spiel setzen.</p>



<p>Jetzt ist die Zeit für unser digitales Briefgeheimnis und sichere Verschlüsselung zu kämpfen!“</p>
Dr. Patrick Breyer, Europaabgeordneter der Piratenpartei</blockquote>



<blockquote>
<p>„Nicht nur das eigentliche Ziel, auch der Zeitpunkt ist alarmierend. Genau jetzt, mitten im Endspurt des Wahlkampfes und in der anschließenden Phase, in der sich neu gewählte Abgeordnete erst zurechtfinden müssen, solch ein Tempo vorzulegen zeigt, worum es eigentlich geht. Den scheinbar schwächsten Moment des Parlamentes auszunutzen um diese demokratiegefährdenden Überwachungsphantasien Realität werden zu lassen. Ich bin entsetzt.”</p>
Anja Hirschel, Informatikerin und Spitzenkandidatin der Piratenpartei zur Europawahl</blockquote>



<p><a href="http://chatkontrolle.de/">Infoportal und Dokumentenarchiv zur Chatkontrolle</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   08:54 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/31/django-background-workers/#atom-everything'>Django Enhancement Proposal 14: Background Workers</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Django Enhancement Proposal 14: Background Workers 
Jake Howard's DEP has been approved and is moving into the implementation stage. 
 
 Django doesn't have a first-party solution for long-running tasks, however the ecosystem is filled with incredibly popular frameworks, all of which interact with Django in slightly different ways. Other frameworks such as Laravel have background workers built-in,
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://www.djangoproject.com/weblog/2024/may/29/django-enhancement-proposal-14-background-workers/#top">Django Enhancement Proposal 14: Background Workers</a></p>
Jake Howard's DEP has been approved and is moving into the implementation stage.<p></p>
<blockquote>
<p>Django doesn't have a first-party solution for long-running tasks, however the ecosystem is filled with incredibly popular frameworks, all of which interact with Django in slightly different ways. Other frameworks such as Laravel have background workers built-in, allowing them to push tasks into the background to be processed at a later date, without requiring the end user to wait for them to occur. [...]</p>
<p>This proposal sets out to provide an interface and base implementation for long-running background tasks in Django.</p>
</blockquote>
<p>Jake has an illustrative reference implementation called <a href="https://github.com/RealOrangeOne/django-tasks">django-tasks</a>.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   08:44 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/05/31/the-interesting-shots.html'> 📸 🌊 The interesting shots tend to be moving towards the even</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   📸 🌊 The interesting shots tend to be moving towards the evening … 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2024/d4d7ac6a82.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>📸 🌊 The interesting shots tend to be moving towards the evening …</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2024/d4d7ac6a82.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   05:43 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://john.philpin.com/2024/05/31/politics-again-a.html'>Politics - Again. A Reply That Got Out Of Hand</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   My reply to @ridwan got a little long, so I thought I might not reply and start a new thread. 
 🔗 This is Ridwan’s original post 
 This is my reply to his comment under that post … 
 
 We are in for a Labour government, which is a done deal. However, Is Keir Starmer’s the PM after 14 years of Tory squander? I see him as just another politician who will say anything to get elected and turn his back
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>My reply to <a href="https://micro.blog/ridwan">@ridwan</a> got a little long, so I thought I might not reply and start a new thread.</p>
<p><a href="https://ridwan.blog/2024/05/23/the-general-election.html">🔗 This is Ridwan’s original post</a></p>
<p>This is my reply to his comment under that post …</p>
<blockquote>
<p>We are in for a Labour government, which is a done deal. However, Is Keir Starmer’s the PM after 14 years of Tory squander? I see him as just another politician who will say anything to get elected and turn his back on the people who voted Labour. I guess, even if people do vote for Labour, how many really vote for Keir?</p>
</blockquote>
 My Reply 
<p>Like all good British elections - very few people will get to vote for The PM .. 😀</p>
<p>The problem however is the same as ever it was - who the hell would want to get into politics anywhere these days - so you are left with those that do and when they do - you want to know what is wrong with them - and why - and ….  I have a three thousand word introduction to the book about all of this - that I have in my head - but it will never reach paper or screen of any kind.</p>
 
 Here’s what I do know … 
 
 
<p>Vote or Don’t Vote - BUT if you don’t vote - you don’t get to complain later.</p>
 
 
<p>If you chose the first option - understand what principle(s) you are voting for - what problems you are wanting solved etc - and base your choice on the best of the candidates to do that. (There is no other choice.)</p>
 
 
<p>2.1) sometimes those calls are things like
- ‘he’ll bring back school lunches - the other guy took them away</p>
<p>2.2) sometimes they are broader in nature
- this candidate will ensure that billionaires pay the ‘same rate’ of tax as the rest of us, the other doesn’t get it</p>
<p>2.3) sometimes the choice is that there is a clear and present danger to me, society, my country and the world … with the other - not so much.</p>
<p>ALL ARE VALID</p>
<p>.1 , .2 and .3 will vary by voter - sometimes on the day but more importantly - because of their general inability to</p>
<p>a - conceive of,
b - understand,
c - see the impact on themselves should it come that,
d - concern themselves with others - so long as ‘I’ am ok
e - just grok …</p>
<p>The possible outcomes of the wrong solution to ‘2.3’ and/or don’t believe them.</p>
<p>So - if enough people at the 2.1 level voted for one person and enough people didn’t vote for the ‘other’ person because they are too old, trips on stairs, mumbles .. guess who wins?</p>
 
<p>(Apologies a US biased set of criteria that is very real at the moment - but you get the drift - e.g. - back in Blighty - compared to David Cameron, Theresa May and Boris (the THREE Prime Ministers that served when Jeremy was the Leader of the Opposition  for just 4 1/2 years .. how much worse off would we be?</p>
<p>I do know that JC was and is on the side of PEOPLE - and has served in Govt fighting the fight for over 40 years. When he was ousted - he didn’t slink off like the last 4 Tory PMs did to reap their financial rewards, cashing in with the Corporations .. he’s still there , serving the people of Islington and still fighting the fight.</p>
<p>At a broad character level who would you rather have run the country for the past FOURTEEN YEARS? Seriously - FOURTEEN?</p>
<p>Heres the thing - the country is being ALLOWED to vote for a party that with one outcome would put Kier in charge. That’s not a bad thing … since Cameron - every single PM has actually been APPOINTED by a few people - it is a few in the Tory party ..</p>
<p>Cameron won in 2010
He won again (just) in 2015 - and I recall resigned over Brexit
Theresa May was then appointed
As the incumbent she then won the next election, but not very well, so was ousted and Boris moved in
Who went on to win his election until Truss was appointed and then little Rishi.</p>
<p>Despite being in power for 14 years - Cameron was the last PM that actually won the election without first having a dry run as a PM because they had previously been appointed.</p>
<p>If Kier turns out to be no good - it is within the power of the Labour Party to remove him and put in someone else - it isn’t the first time - as the Tories continually demonstrate.</p>
 
<p>The UK isn’t the same as the US. Keep the prize in mind. 14 years of Tory Rule might be broken. Don’t not vote (for Labour) just because you think the leader is</p>
<blockquote>
<p>just another politician who will say anything to get elected and turn his back on the people who voted Labour.</p>
</blockquote>
<p>You vote for him to get Labour get in power. Don’t like Labour? Vote Tory … clearly they know exactly what they are doing. Just look around the UK - it is clear.</p>
 
<p>If anybody doesn’t like all of this? Then it is time for action and to do something about it. Get others to join your cause. Change things. Too hard?</p>
<p>That’s why we are where we are.</p>
<p>PS - I know you all know all this. Sometimes I just have to wave my cane!</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   05:09 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2532'>Using Standards: Some Assembly Required</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I gave the following presentation in the session Using Standards: Some Assembly Required at the 2024 Identiverse conference (PowerPoint) (PDF). The abstract was: Standards are about making choices. When building machines, we take for granted being able to use nuts, bolts, wires, light bulbs, and countless other parts made to industry standards. Standards contain choices […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I gave the following presentation in the session <a href="https://identiverse.com/idv24/session/2089616/">Using Standards: Some Assembly Required</a> at the 2024 <a href="https://identiverse.com/">Identiverse</a> conference <a href="https://self-issued.info/presentations/Identiverse_2024_Standards_are_about_making_choices.pptx">(PowerPoint)</a> <a href="https://self-issued.info/presentations/Identiverse_2024_Standards_are_about_making_choices.pdf">(PDF)</a>.  The abstract was:</p>
 
 Standards are about making choices. When building machines, we take for granted being able to use nuts, bolts, wires, light bulbs, and countless other parts made to industry standards. Standards contain choices about dimensions of screw threads, nut sizes, etc., enabling a marketplace of interoperable parts from multiple suppliers. Without these choices, every part would be custom-manufactured. The same is true of the identity and security standards we use to build the Identity Engine. However, the identity and security standards at our disposal differ wildly in the degree to which they do and don’t make choices. Some consistently define ONE way to do things, resulting in everyone doing it that way (interoperability!). Others leave critical choices unmade, passing the buck to implementers and applications (your mileage may vary). In this talk, I’ll name names and take prisoners, critiquing existing and emerging standards through the lens of the choices they made and failed to make. Hold on to your hats as we examine the pros and cons of the choices made by OAuth, SAML, X.509, OpenID Connect, Verifiable Credentials, DIDs, WebCrypto, JOSE, COSE, and many others through this lens! I believe you’ll agree with me that making choices matters. 
 
<p>The audience was highly engaged by the process of giving existing and emerging standards letter grades based on the choices they made (or failed to make)!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/Identiverse_Logo.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   02:14 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/05/31/trump-also-is.html'> 
 Trump also is a first-time offender with no criminal recor</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
 Trump also is a first-time offender with no criminal record. 
 
 💬 Wall Street Journal 
 Whilst I understand the thinking behind the first part of that sentence … 
 Surely it is only true in as much as he hasn’t previously been caught and held to account. 
 Semantics? Sure. But language is important. 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<blockquote>
<p>Trump also is a first-time offender with no criminal record.</p>
</blockquote>
<p>💬 Wall Street Journal</p>
<p>Whilst I understand the thinking behind the first part of that sentence …</p>
<p>Surely it is only true in as much as he hasn’t previously been caught and held to account.</p>
<p>Semantics? Sure. But language is important.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   01:29 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 30. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/30/why-after-6-years-im-over-graphql/#atom-everything'>Why, after 6 years, I’m over GraphQL</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Why, after 6 years, I’m over GraphQL 
I've seen many of these criticisms of GraphQL before - N+1 queries, the difficulty of protecting against deeply nested queries - but Matt Bessey collects them all in one place and adds an issue I hadn't considered before: the complexity of authorization, where each field in the query might involve extra permission checks: 
 
 In my experience, this is actually
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://bessey.dev/blog/2024/05/24/why-im-over-graphql/">Why, after 6 years, I’m over GraphQL</a></p>
I've seen many of these criticisms of GraphQL before - N+1 queries, the difficulty of protecting against deeply nested queries - but Matt Bessey collects them all in one place and adds an issue I hadn't considered before: the complexity of authorization, where each field in the query might involve extra permission checks:<p></p>
<blockquote>
<p>In my experience, this is actually the biggest source of performance issues. We would regularly find that our queries were spending more time authorising data than anything else.</p>
</blockquote>
<p>The 600+ comment <a href="https://news.ycombinator.com/item?id=40521518">Hacker News thread</a> is crammed with GraphQL war stories, mostly supporting the conclusions of the article.

    </p><p>Via <a href="https://news.ycombinator.com/item?id=40521518">Hacker News</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   10:36 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2024/05/30/today-i-got.html'> Today I got my first 🖇️ guestbook signature from a total str</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Today I got my first 🖇️ guestbook signature from a total stranger. 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Today I got my first <a href="https://jphilpin.pika.page/guestbook">🖇️ guestbook signature</a> from a total stranger.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   10:34 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/30/ai-survey/#atom-everything'>What does the public in six countries think of generative AI in news?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   What does the public in six countries think of generative AI in news? 
Fascinating survey by the Reuters Institute for the Study of Journalism at Oxford that asked ~12,000 people across six countries for their opinions on AI usage in journalism. 

 It’s also being interpreted as evidence that few members of the general public actually use these tools, because the opening survey questions ask about
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://reutersinstitute.politics.ox.ac.uk/what-does-public-six-countries-think-generative-ai-news">What does the public in six countries think of generative AI in news?</a></p>
Fascinating survey by the Reuters Institute for the Study of Journalism at Oxford that asked ~12,000 people across six countries for their opinions on AI usage in journalism.<p></p>

<p>It’s also being interpreted as evidence that few members of the general public actually use these tools, because the opening survey questions ask about personal usage.</p>

<p>I don’t think the numbers support that narrative, personally. For survey participants in the USA 7% used ChatGPT daily and 11% used it weekly, which is higher than I would expect for those frequencies. For the UK those were 2% daily and 7% weekly.</p>

<p>The 18-24 group were the heaviest users of these tools. Lots of other interesting figures to explore.

    </p><p>Via <a href="https://www.bbc.com/news/articles/c511x4g7x7jo.amp">BBC: AI products like ChatGPT much hyped but not much used, study says</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   07:38 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/30/andrej-karpathy/#atom-everything'>Quoting Andrej Karpathy</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    The realization hit me [when the GPT-3 paper came out] that an important property of the field flipped. In ~2011, progress in AI felt constrained primarily by algorithms. We needed better ideas, better modeling, better approaches to make further progress. If you offered me a 10X bigger computer, I'm not sure what I would have even used it for. GPT-3 paper showed that there was this thing that wou
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <blockquote cite="https://twitter.com/karpathy/status/1795980744436932871"><p>The realization hit me [when the GPT-3 paper came out] that an important property of the field flipped. In ~2011, progress in AI felt constrained primarily by algorithms. We needed better ideas, better modeling, better approaches to make further progress. If you offered me a 10X bigger computer, I'm not sure what I would have even used it for. GPT-3 paper showed that there was this thing that would just become better on a large variety of practical tasks, if you only trained a bigger one. Better algorithms become a bonus, not a necessity for progress in AGI. Possibly not forever and going forward, but at least locally and for the time being, in a very practical sense. Today, if you gave me a 10X bigger computer I would know exactly what to do with it, and then I'd ask for more.</p></blockquote><p>— <a href="https://twitter.com/karpathy/status/1795980744436932871">Andrej Karpathy</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   07:27 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/30/codestral/#atom-everything'>Codestral: Hello, World!</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Codestral: Hello, World! 
Mistral's first code-specific model, trained to be "fluent" in 80 different programming languages. 
 The weights are released under a new Mistral AI Non-Production License, which is extremely restrictive: 
 
 3.2. Usage Limitation 
 
 You shall only use the Mistral Models and Derivatives (whether or not created by Mistral AI) for testing, research, Personal, or evaluation
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://mistral.ai/news/codestral/">Codestral: Hello, World!</a></p>
Mistral's first code-specific model, trained to be "fluent" in 80 different programming languages.<p></p>
<p>The weights are released under a new <a href="https://mistral.ai/news/mistral-ai-non-production-license-mnpl/">Mistral AI Non-Production License</a>, which is extremely restrictive:</p>
<blockquote>
<p>3.2. Usage Limitation</p>
 
 You shall only use the Mistral Models and Derivatives (whether or not created by Mistral AI) for testing, research, Personal, or evaluation purposes in Non-Production Environments;  
 Subject to the foregoing, You shall not supply the Mistral Models or Derivatives in the course of a commercial activity, whether in return for payment or free of charge, in any medium or form, including but not limited to through a hosted or managed service (e.g. SaaS, cloud instances, etc.), or behind a software layer. 
 
</blockquote>
<p>To Mistral's credit at least they don't misapply the term "open source" in their marketing around this model - they consistently use the term "open-weights" instead. They also state that they plan to continue using Apache 2 for other model releases.</p>
<p>Codestral can be used commercially when accessed via their paid API.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   07:19 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 29. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2024/05/29/a-fun-ai-fail/'>A Fun AI Fail</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Here is me, trying to get ChatGPT (version 4o, which I pay for) to give me an illustration to use in my last post here, titled The People’s AI. But don’t go there yet (if you haven’t already). What I ended up using there is a punchline at the end of the dialog that starts […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Here is me, trying to get ChatGPT (version 4o, which I pay for) to give me an illustration to use in my last post here, titled <a href="https://doc.searls.com/2024/05/28/the-peoples-ai/">The People’s AI</a>. But don’t go there yet (if you haven’t already). What I ended up using there is a punchline at the end of the dialog that starts here—</p>
<p>Me:&nbsp;A vast field on which the ground spells the letters A and I, with people on it, having a good time.</p>
<p>ChatGPT:</p>
<p></p>
<p>Here’s the illustration of a whimsical outdoor scene with a vast field shaped into the letters ‘A’ and ‘I’, where people are enjoying various activities, creating a cheerful and lively atmosphere.</p>
<p>Me:&nbsp;Make the two letters upper case A and upper case I, both with serifs.</p>
<p>ChatGPT:&nbsp;</p>
<p></p>
<p>Here’s the updated illustration of a vast field shaped into the uppercase letters ‘A’ and ‘I’, both with serifs, where people are enjoying a sunny day outdoors. The setting is lively and festive, enhancing the creative landscape.</p>
<p>Me:&nbsp;Those are not the letters A or I, and lack serifs. Try again, with A on the left, and I on the right.</p>
<p>ChatGPT:&nbsp;</p>
<p></p>
<p>Here’s the updated illustration showing a vast field shaped into the uppercase serif letters ‘A’ and ‘I’, with ‘A’ on the left and ‘I’ on the right. The field is vibrant and bustling with people enjoying various outdoor activities.</p>
<p>Me: Better, but there is no I on the right.</p>
<p>ChatGPT:</p>
<p></p>
<p>Here’s the updated illustration showing a vast field clearly shaped into the uppercase serif letters ‘A’ and ‘I’, with ‘A’ on the left and ‘I’ on the right. The field is lively with people engaging in various activities, set in a sunny and cheerful environment.</p>
<p>Me: That’s an interesting design, but I need an A and an I next to each other.</p>
<p>ChatGPT:</p>
<p></p>
<p>Here’s the updated illustration showing the vast field clearly shaped into the adjacent uppercase serif letters ‘A’ and ‘I’. The setting is vibrant and festive, surrounded by people enjoying various activities.</p>
<p>Me:&nbsp;That’s one big A, not an A and an I. Give me both.</p>
<p>ChatGPT:</p>
<p></p>
<p>Here’s the updated illustration showing a vast field clearly shaped into the uppercase serif letters ‘A’ and ‘I’, side by side. The setting is lively and festive, filled with people enjoying various activities.</p>
<p>Me: That has no serifs and is still just an A. I want an A and an I. Separate.</p>
<p>ChatGPT:</p>
<p></p>
<p>Here’s the updated illustration showing a vast field clearly shaped into the uppercase serif letters ‘A’ and ‘I’, side by side. The setting is vibrant, with people enjoying various outdoor activities.</p>
<p>Me: Still not separate. That’s one big A. I want an I next to it.</p>
<p>ChatGPT:</p>
<p></p>
<p>Here’s the updated illustration, clearly showing the vast field shaped into the separate uppercase serif letters ‘A’ on the left and ‘I’ on the right. The scene is lively with people enjoying various activities under a sunny sky.</p>
<p>Me: Let’s try this: An “A” and an “I” in an open field full of people.</p>
<p>ChatGPT:&nbsp;</p>
<p></p>
<p>Here’s the illustration showing an open field shaped into the uppercase letters ‘A’ and ‘I’, with people enjoying various activities in the area. The scene is lively and sunny, capturing a cheerful outdoor gathering.</p>
<p>Me:&nbsp;Better! Now give me the same thing, with a complete “a.”</p>
<p>ChatGPT:&nbsp; We are experiencing heavy server load. To ensure the best experience for everyone, we have rate limits in place. Please wait for 2 minutes before generating more images. Let me know if there’s anything else you’d like to discuss in the meantime!</p>
<p>Me (after a 3 minute pause):&nbsp;Thanks. Now give me the same image, but with a complete “A.”</p>
<p>ChatGPT:&nbsp;</p>
<p></p>
<p></p>
<p>Here’s the updated illustration showing a vast open field creatively landscaped into two distinct uppercase ‘A’ letters, bustling with people enjoying various activities in a sunny, cheerful setting.</p>
<p>(Yes, it created both images.)</p>
<p>Me: Give me the same, but with people surrounding the “A” and the “I.” Also, please put serifs on the “I.”</p>
<p>ChatGPT:</p>
<p></p>
<p>Here’s the updated illustration showing a vast open field creatively landscaped into the shapes of the uppercase letters ‘A’ and ‘I’, both with serifs. The field is surrounded by people enjoying various activities in a sunny, cheerful setting.</p>
<p>Me:&nbsp;Try the same, but with “a” and “i” as separate letters.</p>
<p>ChatGPT:&nbsp;</p>
<p></p>
<p>Here’s the updated illustration showing a vast open field creatively landscaped into the shapes of the lowercase letters ‘a’ and ‘i’. The setting is surrounded by people enjoying various activities in a sunny, cheerful atmosphere.</p>
<p>Me:&nbsp;Closer. Only now take it out of the city. I want only countryside.</p>
<p>ChatGPT:</p>
<p></p>
<p>Here’s the updated illustration showing a vast open field in the countryside, creatively landscaped into the shapes of the lowercase letters ‘a’ and ‘i’. The setting is serene and picturesque, surrounded by people enjoying rural activities.</p>
<p>Me: Please make the “a” and the “i” separate.</p>
<p>ChatGPT:</p>
<p></p>
<p>Here’s the updated illustration showing a vast open field in the countryside, creatively landscaped into the distinctly separated lowercase letters ‘a’ and ‘i’. The setting is serene, surrounded by people enjoying rural activities in a picturesque and peaceful outdoor gathering.</p>
<p>At this point I gave up and decided to give <a href="https://copilot.microsoft.com/">Microsoft Copilot</a> a whack—</p>
<p>Me:&nbsp;A vast field on which the ground spells the letters A and I, with people on it, having a good time.</p>
<p>Microsoft Copilot:</p>
<p></p>
<p>I decided not to push my luck, and <a href="https://doc.searls.com/2024/05/28/the-peoples-ai/">used the one on the bottom right</a>.</p>
<p>It’s fun to know I am not alone at this. <a href="https://www.reddit.com/r/ChatGPT/comments/1b5l12u/oh_my_god/">Here is a brilliant video of somebody far more patient than I getting ChatGPT to spell HONDA in ASCII art</a>.</p>
<p>Early days, folks. Early days.</p>
<p>&nbsp;</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://150108457.v2.pressablecdn.com/wp-content/uploads/2024/05/a8ebc59f-58c1-4df6-8661-10a2890b2e17-1024x585.webp'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:41 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/29/jeremy-keith/#atom-everything'>Quoting Jeremy Keith</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    In their rush to cram in “AI” “features”, it seems to me that many companies don’t actually understand why people use their products. [...] Trust is a precious commodity. It takes a long time to build trust. It takes a short time to destroy it.   — Jeremy Keith 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <blockquote cite="https://adactio.com/journal/21160"><p>In their rush to cram in “AI” “features”, it seems to me that many companies don’t actually understand why people use their products. [...] Trust is a precious commodity. It takes a long time to build trust. It takes a short time to destroy it.</p></blockquote><p>— <a href="https://adactio.com/journal/21160">Jeremy Keith</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   11:06 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/29/training-not-chatting/#atom-everything'>Training is not the same as chatting: ChatGPT and other LLMs don't remember everything you say</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   I'm beginning to suspect that one of the most common misconceptions about LLMs such as ChatGPT involves how "training" works. 
 A common complaint I see about these tools is that people don't want to even try them out because they don't want to contribute to their training data. 
 This is by no means an irrational position to take, but it does often correspond to an incorrect mental model about ho
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>I'm beginning to suspect that one of the most common <a href="https://simonwillison.net/series/llm-misconceptions/">misconceptions about LLMs</a> such as ChatGPT involves how "training" works.</p>
<p>A common complaint I see about these tools is that people don't want to even try them out because they don't want to contribute to their training data.</p>
<p>This is by no means an irrational position to take, but it does often correspond to an incorrect mental model about how these tools work.</p>
<p>Short version: ChatGPT and other similar tools do not directly learn from and memorize everything that you say to them.</p>
<p>This can be quite unintuitive: these tools imitate a human conversational partner, and humans constantly update their knowledge based on what you say to to them. Computers have much better memory than humans, so surely ChatGPT would remember every detail of everything you ever say to it. Isn't that what "training" means?</p>
<p>That's not how these tools work.</p>
 LLMs are stateless functions 
<p>From a computer science point of view, it's best to think of LLMs as stateless function calls. Given this input text, what should come next?</p>
<p>In the case of a "conversation" with a chatbot such as ChatGPT or Claude or Google Gemini, that function input consists of the current conversation (everything said by both the human and the bot) up to that point, plus the user's new prompt.</p>
<p>Every time you start a new chat conversation, you clear the slate. Each conversation is an entirely new sequence, carried out entirely independently of previous conversations from both yourself and other users.</p>
<p>Understanding this is key to working effectively with these models. Every time you hit "new chat" you are effectively wiping the short-term memory of the model, starting again from scratch.</p>
<p>This has a number of important consequences:</p>
 
 There is no point at all in "telling" a model something in order to improve its knowledge for future conversations. I've heard from people who have invested weeks of effort pasting new information into ChatGPT sessions to try and "train" a better bot. That's a waste of time! 
 Understanding this helps explain why the "context length" of a model is so important. Different LLMs have different context lengths, expressed in terms of "tokens" - a token is about 3/4s of a word. This is the number that tells you how much of a conversation the bot can consider at any one time. If your conversation goes past this point the model will "forget" details that occurred at the beginning of the conversation. 
 Sometimes it's a good idea to start a fresh conversation in order to deliberately reset the model. If a model starts making obvious mistakes, or refuses to respond to a valid question for some weird reason that reset might get it back on the right track. 
 Tricks like <a href="https://simonwillison.net/tags/rag/">Retrieval Augmented Generation</a> and <a href="https://simonwillison.net/2024/Feb/14/memory-and-new-controls-for-chatgpt/">ChatGPT's "memory"</a> make sense only once you understand this fundamental limitation to how these models work. 
 If you're excited about local models because you can be certain there's no way they can train on your data, you're mostly right: you can run them offline and audit your network traffic to be absolutely sure your data isn't being uploaded to a server somewhere. But... 
 ... if you're excited about local models because you want something on your computer that you can chat to and it will learn from you and then better respond to your future prompts, that's probably not going to work. 
 
 So what is "training" then? 
<p>When we talk about model training, we are talking about the process that was used to build these models in the first place.</p>
<p>As a big simplification, there are two phases to this. The first is to pile in several TBs of text - think all of Wikipedia, a scrape of a large portion of the web, books, newspapers, academic papers and more - and spend months of time and potentially millions of dollars in electricity crunching through that "pre-training" data identifying patterns in how the words relate to each other.</p>
<p>This gives you a model that can complete sentences, but not necessarily in a way that will delight and impress a human conversational partner. The second phase aims to fix that - this can incorporate instruction tuning or Reinforcement Learning from Human Feedback (RLHF) which has the goal of teaching the model to pick the best possible sequences of words to have productive conversations.</p>
<p>The end result of these phases is the model itself - an enormous (many GB) blob of floating point numbers that capture both the statistical relationships between the words and some version of "taste" in terms of how best to assemble new words to reply to a user's prompts.</p>
<p>Once trained, the model remains static and unchanged - sometimes for months or even years.</p>
<p>Here's <a href="https://www.reddit.com/r/ClaudeAI/comments/1c3f1yc/comment/kzj7n4s/">a note</a> from Jason D. Clinton, an engineer who works on Claude 3 at Anthropic:</p>
<blockquote>
<p>The model is stored in a static file and loaded, continuously, across 10s of thousands of identical servers each of which serve each instance of the Claude model. The model file never changes and is immutable once loaded; every shard is loading the same model file running exactly the same software.</p>
</blockquote>
<p>These models don't change very often!</p>
 Reasons to worry anyway 
<p>A frustrating thing about this issue is that it isn't actually possible to confidently state "don't worry, ChatGPT doesn't train on your input".</p>
<p>Many LLM providers have terms and conditions that allow them to improve their models based on the way you are using them. Even when they have opt-out mechanisms these are often opted-in by default.</p>
<p>When OpenAI <a href="https://openai.com/policies/terms-of-use/">say</a> "We may use Content to provide, maintain, develop, and improve our Services" it's not at all clear what they mean by that!</p>
<p>Are they storing up everything anyone says to their models and dumping that into the training run for their next model versions every few months?</p>
<p>I don't think it's that simple: LLM providers don't want random low-quality text or privacy-invading details making it into their training data. But they are notoriously secretive, so who knows for sure?</p>
<p>The opt-out mechanisms are also pretty confusing. OpenAI try to make it as clear as possible that they won't train on any content submitted through their API (so you had better understand what an "API" is), but lots of people don't believe them! I wrote about <a href="https://simonwillison.net/2023/Dec/14/ai-trust-crisis/">the AI trust crisis</a> last year: the pattern where many people actively disbelieve model vendors and application developers (such as Dropbox and Slack) that claim they don't train models on private data.</p>
<p>People also worry that those terms might change in the future. There are options to protect against that: if you're spending enough money you can <a href="https://twitter.com/therealadamg/status/1794693810204909729">sign contracts with OpenAI</a> and other vendors that freeze the terms and conditions.</p>
<p>If your mental model is that LLMs remember and train on all input, it's much easier to assume that developers who claim they've disabled that ability may not be telling the truth. If you tell your human friend to disregard a juicy piece of gossip you've mistakenly passed on to them you know full well that they're not going to forget it!</p>
<p>The other major concern is the same as with any cloud service: it's reasonable to assume that your prompts are still logged for a period of time, for compliance and abuse reasons, and if that data is logged there's always a chance of exposure thanks to an accidental security breach.</p>

 What about "memory" features? 

<p>To make things even more confusing, some LLM tools are introducing features that attempt to work around this limitation.</p>

<p>ChatGPT recently added a <a href="https://help.openai.com/en/articles/8590148-memory-faq">memory feature</a> where it can "remember" small details and use them in follow-up conversations.</p>

<p>As with so many LLM features this is a relatively simple <a href="https://simonwillison.net/2024/Feb/14/memory-and-new-controls-for-chatgpt/">prompting trick</a>: during a conversation the bot can call a mechanism to record a short note - your name, or a preference you have expressed - which will then be invisibly included in the chat context passed in future conversations.</p>

<p>You can review (and modify) the list of remembered fragments at any time, and ChatGPT shows a visible UI element any time it adds to its memory.</p>

 Bad policy based on bad mental models 
<p>One of the most worrying results of this common misconception concerns people who make policy decisions for how LLM tools should be used.</p>
<p>Does your company ban all use of LLMs because they don't want their private data leaked to the model providers?</p>
<p>They're not 100% wrong - see <a href="https://simonwillison.net/2024/May/29/training-not-chatting/#reasons-to-worry-anyway">reasons to worry anyway</a> - but if they are acting based on the idea that everything said to a model is instantly memorized and could be used in responses to other users they're acting on faulty information.</p>
<p>Even more concerning is what happens with lawmakers. How many politicians around the world are debating and voting on legislation involving these models based on a science fiction idea of what they are and how they work?</p>
<p>If people believe ChatGPT is a machine that instantly memorizes and learns from everything anyone says to it there is a very real risk they will support measures that address invented as opposed to genuine risks involving this technology.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   10:51 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/29/a-year-of-building-with-llms/#atom-everything'>What We Learned from a Year of Building with LLMs (Part I)</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   What We Learned from a Year of Building with LLMs (Part I) 
Accumulated wisdom from six experienced LLM hackers. Lots of useful tips in here. On providing examples in a prompt: 
 
 If n is too low, the model may over-anchor on those specific examples, hurting its ability to generalize. As a rule of thumb, aim for n ≥ 5. Don’t be afraid to go as high as a few dozen. 
 
 There's a recommendation not
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/">What We Learned from a Year of Building with LLMs (Part I)</a></p>
Accumulated wisdom from six experienced LLM hackers. Lots of useful tips in here. On providing examples in a prompt:<p></p>
<blockquote>
<p>If n is too low, the model may over-anchor on those specific examples, hurting its ability to generalize. As a rule of thumb, aim for n ≥ 5. Don’t be afraid to go as high as a few dozen.</p>
</blockquote>
<p>There's a recommendation not to overlook keyword search when implementing RAG - tricks with embeddings can miss results for things like names or acronyms, and keyword search is much easier to debug.</p>
<p>Plus this tip on using the LLM-as-judge pattern for implementing automated evals:</p>
<blockquote>
<p>Instead of asking the LLM to score a single output on a Likert scale, present it with two options and ask it to select the better one. This tends to lead to more stable results.</p>
</blockquote>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   08:59 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/29/chris-perry/#atom-everything'>Quoting Chris Perry</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    Sometimes the most creativity is found in enumerating the solution space. Design is the process of prioritizing tradeoffs in a high dimensional space. Understand that dimensionality.   — Chris Perry 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <blockquote cite="https://twitter.com/thechrisperry/status/1795661635602059664"><p>Sometimes the most creativity is found in enumerating the solution space. Design is the process of prioritizing tradeoffs in a high dimensional space. Understand that dimensionality.</p></blockquote><p>— <a href="https://twitter.com/thechrisperry/status/1795661635602059664">Chris Perry</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   07:17 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/can-we-at-least-agree-that-killing-is-wrong'>Can we at least agree that killing is wrong?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
 I don’t think it’s possible to morally support the ongoing bombardment of Gaza, but that’s too weak a statement. What’s happening there seems to be — based on what I’ve read through the news, what I’ve seen in video, the stories that have been sent back to us — an atrocity. The latest story, from Rafah, is of an airstrike on a civilian evacuation zone where displaced families were sheltering in 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
<p>I don’t think it’s possible to morally support the ongoing bombardment of Gaza, but that’s too weak a statement. What’s happening there seems to be — based on what I’ve read through the news, what I’ve seen in video, the stories that have been sent back to us — an atrocity. The latest story, from Rafah, is of an airstrike on a civilian evacuation zone where displaced families <a href="https://www.nbcnews.com/news/world/israeli-tanks-enter-central-rafah-palestinians-tent-camp-blaze-rcna154235">were sheltering in tents</a>:</p><blockquote><p>Images showed the area engulfed in flames as screaming Palestinians fled for safety, with some video shared on social media showing disturbing images, including severely burned corpses and a man holding what appeared to be the headless body of a small child.</p></blockquote><p>There’s nothing flippant to say about this. This isn’t sports, where you root for a team. It’s not a theoretical debate: certainly not for the families who have no way to escape, kettled as they are into a small strip of land under constant military bombardment.</p><p>The bombardment on Gaza is disproportionate and indefensible. <a href="https://www.unicefusa.org/stories/more-13000-children-reported-dead-gaza-famine-nears">Thirteen thousand children alone</a> have been killed. A quarter of surviving children have acute malnutrition. There’s nowhere for them to go, and nowhere for them to get the care they need. In the face of these conditions, there must be a ceasefire. Obviously there must be a ceasefire.</p><p>Making statements like this is fraught. It sometimes seems like we’re being asked to fall into weird ideological lines that have little to do with the humanity of the people involved. Following the events of October 7, I unfollowed multiple progressive Instagram accounts that not only described the attack and kidnappings as the necessary work of de-colonization, but applauded the action. It’s clear to me that Palestine has been annexed, its land <a href="https://en.wikipedia.org/wiki/Israeli_settlement">illegally settled</a>, and its people made to suffer at the hands of increasingly-conservative Israeli policy. Protest and resistance are inevitable and justifiable. Regardless, I can’t support the killing and kidnapping of civilians, let alone accept cheerleading for it. Not ever.</p><p>By the same token, I see some people online call for an end to the state of Israel. What would that entail, exactly? Assuming it was a desirable goal, how might one go about achieving that? Dismantling it would involve unthinkable bloodshed.</p><p>Some people talk about how Hamas is the local government, and how the people there voted for them, so they deserve what’s happening to them. That it’s okay to bomb hospitals <a href="https://www.cnn.com/interactive/2024/01/middleeast/gaza-hospitals-destruction-investigation-intl-cmd/">because Hamas is hiding out in them</a> — regardless of <a href="https://en.wikipedia.org/wiki/Human_shield">international law related to protecting the lives of human shields</a>.</p><p>The history, today’s political issues, and the road to a solution are far more complicated than can be conveyed by memes and soundbites. I have no solutions to the problems in this region or how to get to a lasting peace.</p><p>But some things are not complicated at all.</p><p>Don’t kill. Don’t subjugate. Don’t dehumanize. Don’t reduce lives, in all their complexity and beauty, to points and sides.</p><p>The core of this issue right now is — or should be — concern for human life. Everyone, regardless of nationality or political affiliation, should be appalled when children burn to death or are decapitated (whether they’re in an evacuation zone or not). The ruining of cities should never yield applause.</p><p>The protests on university campuses are the latest in a long line of campus anti-war protests, and I’m strongly in favor of them. Except, because of course this is true, there are people there who conflate the protest over policy with protests of anyone who is Israeli, or even anyone who is a Jew. I’ve personally heard stories of at least one person being spat on, not because of any rhetoric they were espousing, but simply because of who they were.</p><p>This all has the potential to escalate. I worry that it will. This is all already so horrific.</p><p>These are human beings. The Palestinian people are human beings. The Israeli people are human beings. Arabs are human beings. Jews are human beings. They are not their leaders; they are not their circumstances. They all - like all people - deserve to live, and live well. The death of any human being is never something to celebrate or to praise as a strategy. It’s all just endless tragedy.</p><p>Stop the killing. Find another way.</p> 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   02:31 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 28. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/28/weeknotes/#atom-everything'>Weeknotes: PyCon US 2024</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Earlier this month I attended PyCon US 2024 in Pittsburgh, Pennsylvania. I gave an invited keynote on the Saturday morning titled "Imitation intelligence", tying together much of what I've learned about Large Language Models over the past couple of years and making the case that the Python community has a unique opportunity and responsibility to help try to nudge this technology in a positive dire
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>Earlier this month I attended <a href="https://us.pycon.org/2024/">PyCon US 2024</a> in Pittsburgh, Pennsylvania. I gave an invited keynote on the Saturday morning titled "Imitation intelligence", tying together much of what I've learned about Large Language Models over the past couple of years and making the case that the Python community has a unique opportunity and responsibility to help try to nudge this technology in a positive direction.</p>
<p>The video isn't out yet but I'll publish detailed notes to accompany my talk (using my <a href="https://simonwillison.net/tags/annotatedtalks/">annotated presentation format</a>) as soon as it goes live on YouTube.</p>
<p>PyCon was a really great conference. Pittsburgh is a fantastic city, and I'm delighted that PyCon will be in the same venue next year so I can really take advantage of the opportunity to explore in more detail.</p>
<p>I also realized that it's about time Datasette participated in the PyCon sprints - the project is mature enough for that to be a really valuable opportunity now. I'm looking forward to leaning into that next year.</p>
<p>I'm on a family-visiting trip back to the UK at the moment, so taking a bit of time off from my various projects.</p>
 LLM support for new models 
<p>The big new language model releases from May were OpenAI GPT-4o and Google's Gemini Flash. I released <a href="https://github.com/simonw/llm/releases/tag/0.14">LLM 0.14</a>, <a href="https://github.com/datasette/datasette-extract/releases/tag/0.1a7">datasette-extract 0.1a7</a> and <a href="https://github.com/datasette/datasette-enrichments-gpt/releases/tag/0.5">datasette-enrichments-gpt 0.5</a> with support for GPT-4o, and <a href="https://github.com/simonw/llm-gemini/releases/tag/0.1a4">llm-gemini 0.1a4</a> adding support for the new inexpensive Gemini 1.5 Flash.</p>
<p>Gemini 1.5 Flash is a particularly interesting model: it's now <a href="https://twitter.com/lmsysorg/status/1795512202465845686">ranked 9th</a> on the LMSYS leaderboard, beating Llama 3 70b. It's inexpensive, <a href="https://simonwillison.net/2024/May/14/llm-gemini-01a4/">priced close to Claude 3 Haiku</a>, and can handle up to a million tokens of context.</p>
<p>I'm also excited about GPT-4o - half the price of GPT-4 Turbo, around twice as fast and it appears to be slightly more capable too. I've been getting particularly good results from it for structured data extraction using <a href="https://datasette.io/plugins/datasette-extract">datasette-extract</a> - it seems to be able to more reliably produce a longer sequence of extracted rows from a given input.</p>
 Blog entries 
 
 <a href="https://simonwillison.net/2024/May/15/chatgpt-in-4o-mode/">ChatGPT in "4o" mode is not running the new features yet</a> 
 <a href="https://simonwillison.net/2024/May/8/slop/">Slop is the new name for unwanted AI-generated content</a> 
 
 Releases 
 
 
<a href="https://github.com/datasette/datasette-permissions-metadata/releases/tag/0.1">datasette-permissions-metadata 0.1</a> - 2024-05-15<br>Configure permissions for Datasette 0.x in metadata.json 
 
<a href="https://github.com/datasette/datasette-enrichments-gpt/releases/tag/0.5">datasette-enrichments-gpt 0.5</a> - 2024-05-15<br>Datasette enrichment for analyzing row data using OpenAI's GPT models 
 
<a href="https://github.com/datasette/datasette-extract/releases/tag/0.1a7">datasette-extract 0.1a7</a> - 2024-05-15<br>Import unstructured data (text and images) into structured tables 
 
<a href="https://github.com/simonw/llm-gemini/releases/tag/0.1a4">llm-gemini 0.1a4</a> - 2024-05-14<br>LLM plugin to access Google's Gemini family of models 
 
<a href="https://github.com/simonw/llm/releases/tag/0.14">llm 0.14</a> - 2024-05-13<br>Access large language models from the command-line 
 
 TILs 
 
 
<a href="https://til.simonwillison.net/ios/listen-to-page">Listen to a web page in Mobile Safari</a> - 2024-05-21 
 
<a href="https://til.simonwillison.net/ham-radio/general">How I studied for my Ham radio general exam</a> - 2024-05-11 
 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   20:08 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/28/reproducing-gpt-2/#atom-everything'>Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20 
GPT-2 124M was the smallest model in the GPT-2 series released by OpenAI back in 2019. Andrej Karpathy's llm.c is an evolving  4,000 line C/CUDA implementation which can now train a GPT-2 model from scratch in 90 minutes against a 8X A100 80GB GPU server. This post walks through exactly how to run the training, using 10 billion tokens of Fin
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://github.com/karpathy/llm.c/discussions/481">Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20</a></p>
GPT-2 124M was the smallest model in the GPT-2 series released by OpenAI back in 2019. Andrej Karpathy's llm.c is an evolving  4,000 line C/CUDA implementation which can now train a GPT-2 model from scratch in 90 minutes against a 8X A100 80GB GPU server. This post walks through exactly how to run the training, using 10 billion tokens of FineWeb.<p></p>
<p>Andrej notes that this isn't actually that far off being able to train a GPT-3:</p>
<blockquote>
<p>Keep in mind that here we trained for 10B tokens, while GPT-3 models were all trained for 300B tokens. [...] GPT-3 actually didn't change too much at all about the model (context size 1024 -&gt; 2048, I think that's it?).</p>
</blockquote>
<p>Estimated cost for a GPT-3 ADA (350M parameters)? <a href="https://news.ycombinator.com/item?id=40502090#40504950">About $2,000</a>.

    </p><p>Via <a href="https://news.ycombinator.com/item?id=40502090">Hacker News</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:47 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/28/pyodide-026/#atom-everything'>Pyodide 0.26 Release</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Pyodide 0.26 Release 
PyOdide provides Python packaged for browser WebAssembly alongside an ecosystem of additional tools and libraries to help Python and JavaScript work together. 
 The latest release bumps the Python version up to 3.12, and also adds support for pygame-ce, allowing games written using pygame to run directly in the browser. 
 The PyOdide community also just landed a 14-month-long
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://blog.pyodide.org/posts/0.26-release/">Pyodide 0.26 Release</a></p>
PyOdide provides Python packaged for browser WebAssembly alongside an ecosystem of additional tools and libraries to help Python and JavaScript work together.<p></p>
<p>The latest release bumps the Python version up to 3.12, and also adds support for <a href="https://github.com/pygame-community/pygame-ce">pygame-ce</a>, allowing games written using pygame to run directly in the browser.</p>
<p>The PyOdide community also <a href="https://github.com/pypa/cibuildwheel/pull/1456">just landed</a> a 14-month-long PR adding support to cibuildwheel, which should make it easier to ship binary wheels targeting PyOdide.

    </p><p>Via <a href="https://twitter.com/pyodide/status/1795420504511123523">@pyodide</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:04 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/who-actually-uses-instagrams-threads-app-taiwanese-protestors'>Who actually uses Instagram’s Threads app? Taiwanese protestors</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 "While young Taiwanese users discuss everything from relationships to celebrity gossip on Threads, the app has gradually become a gathering space for progressives, who favor independence from China to defend the island’s democracy." 

 Threads has an official stance of not promoting political use. This is an example, though, of how any social platform will be political whether
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>"While young Taiwanese users discuss everything from relationships to celebrity gossip on Threads, the app has gradually become a gathering space for progressives, who favor independence from China to defend the island’s democracy."</p>

<p>Threads has an official stance of not promoting political use. This is an example, though, of how any social platform will be political whether you want it to be or not - and therefore how the challenges and responsibilities surrounding that speech will present themselves regardless of whether you want them to.</p>

<p>I think there's no alternative: every mass social platform must assume that it will host political content from vulnerable groups (as well as powerful ones) and staff up appropriately.  <a href="https://werd.io/tag/Technology">#Technology</a></p>
            <p>[<a href="https://restofworld.org/2024/instagram-threads-app-taiwan-protests/">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:17 &bull;
   7 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/bluesky-engineering-culture'>Inside Bluesky’s Engineering Culture</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A deep dive into how a fully remote, open source-first, tech company is building a challenger social media platform. What can small teams learn from Bluesky about punching above your weight?
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Programming note: this week, there will be no The Pulse on Thursday. I’m attending <a href="https://craft-conf.com/2024">Craft Conference</a> in Budapest, Hungary and delivering my annual conference talk the same day. My keynote is <a href="https://craft-conf.com/2024/talk/whats-old-is-new-again">titled “What’s Old is New Again.”</a> I’ll share the recording in the newsletter, once it will become available. I hope you enjoy this detailed deepdive on a lean and nimble startup (Bluesky) for the week!</p><p>Bluesky is a promising decentralized Twitter-alternative. Founded two years ago, the company received $13M in funding from Twitter in 2021. Today, it employs 12 software engineers, with more than 5 million users a year on from its invite-only beta launch.</p><p>Previously, we looked into the social media platform’s architecture and engineering decisions in <a href="https://newsletter.pragmaticengineer.com/p/bluesky">Building Bluesky: a distributed social network (real-world engineering challenges).</a> In this article, we learn about the team and the engineering culture, covering:</p>  <p>Unusual origins. It’s rare for publicly traded companies to fund startups which may become rivals – but it happened with Bluesky.</p>  <p>Team. A high ratio of former founders amongst the engineering team (70-80!%) , and the near-absence of Big Tech experience.</p>  <p>Tech stack. TypeScript, Go, ScyllaDB, SQLite, React+Expo.</p>  <p>Company culture. Full-remote with a time zone overlap, “extremely online.”</p>  <p>How the team operates. 2-3 person teams with a DRI, work buddies for everyone, and lots of longform writing.</p>  <p>Engineering culture. Integration tests over unit tests, roll-back / roll-forward, &amp; the benefits of not having large-scale organizational expertise.</p>  <p>Open source and community. Treat contributors well, invest in developer platforms and documentation, and celebrate/promote third-party contributions.</p>  <p>By the end of this article, you’ll hopefully understand how a dozen engineers build and operate a product with such outsized impact in its millions of users and fast growth. Small but very efficient engineering teams are increasingly popular – especially with <a href="https://newsletter.pragmaticengineer.com/p/zirp">the end of 0% interest rates</a> – as “efficient” software engineering teams become the norm, industry-wide.</p> 1. Unusual origins <p>Bluesky kicked off in 2019 with a now-iconic TWEET from then-Twitter CEO, Jack Dorsey.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1d6d493-5d0f-4368-a3da-7224910778f3_1092x494.png">    </a><a href="https://twitter.com/jack/status/1204766078468911106">This tweet</a> kicked off Bluesky <p>Potential candidates sent DMs (direct messages) to Jack, of whom around a dozen were invited to join a <a href="https://matrix.org/ecosystem/clients/">Matrix</a> chatroom, where he provided a prompt for discussing ideas, which read:</p><blockquote><p>The biggest and long term goal is to build a durable and open protocol for public conversation. That it not be owned by any one organization but contributed by as many as possible. And that it is born and evolved on the internet with the same principles.</p></blockquote><p>CEO of Bluesky, Jay Graber, was one of this small group, which also included:</p>  <p><a href="https://en.wikipedia.org/wiki/Jeremie_Miller">Jeremie Miller</a>: prolific contributor to the Extensible Messaging and Presence Protocol (XMPP) used today in apps like Whatspp, Kik Messenger, and Zoom.</p>  <p>Matthew Hodgson: technical cofounder of <a href="https://matrix.org/about/">Matrix</a>, an open protocol for decentralized, secure communications.&nbsp;</p>  <p>Ian Preston: cofounder of Peergos, a platform trying to build the “next web,” where end users are in control of their data. Also an early Twitter engineer.</p>  <p>The group was made up of heavy hitters in decentralized messaging protocols, and entrepreneurs aiming to advance and evolve the web.</p><p>In mid-2020, Jay proposed writing <a href="https://gitlab.com/bluesky-community1/decentralized-ecosystem/-/blob/master/README.md">an ecosystem review</a> of existing decentralized social networks and technologies. As this review was developed, the chatroom grew to ~60 people. By the end of 2020, Twitter put out a request for proposals in the group. Several people responded, including Jay Graber.</p><p>Twitter ran an interview process in 2021 to select the Bluesky project’s lead, and Jay was announced as the choice that August:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92e4f019-8388-437e-bd47-9b1922de57c7_1086x654.png">    </a>Announcing Jay as lead of Bluesky, when it was unclear what the project was  A unique setup <p>Unusually, Bluesky spun up as an independent organization, not as part of Twitter. Mostly, when a company announces funding for a new initiative, it becomes an internal division of the company. But in this case, the opposite happened: Bluesky was external to and independent of Twitter from day one.</p><p>Jay used her own money to hire Daniel Holmgren and start prototyping while the details were being finalized. Together, they fleshed out a protocol Jay had coauthored with technical advisor whyrusleeping back in 2018 – a core design that persists in Bluesky to this day.</p><p>The project got the green light on 31 December 2021, when Twitter paid the newly formed entity a substantial sum of $13M to build the AT Protocol, and also the sign off for Bluesky to work fully independently. Actually, there was one condition from <a href="https://x.com/bluesky/status/1518707606881067008">Twitter</a>:</p><blockquote><p>“Twitter's funding of Bluesky is not subject to any conditions except one: that Bluesky is to research and develop technologies that enable open and decentralized public conversation.”</p></blockquote><p>It’s common for companies raising funding from investors to pursue a bold vision, but it’s extremely rare for a publicly traded company like Twitter to fund a startup and hand it a pretty much free rein. This was likely due to Jack's personal interest in the project, but also stems from Jay's conviction about what the project needed. In her own words:</p><blockquote><p>"I negotiated six months for independence because being a subsidiary of Twitter wasn't a viable option. The company moved too slowly, and I felt there was too much risk that Bluesky could be killed as a project if leadership changed. That turned out to be true."</p></blockquote><p>Jay told us that she negotiated for six months for independence because she wouldn't accept that being a subsidiary of Twitter would be a viable option. Jay felt that Twitter moved too slowly and felt there was too much risk that Bluesky could be killed as a project if Twitter’s leadership would have changed. And how true this assumption was – given how in November 2022, <a href="https://newsletter.pragmaticengineer.com/p/the-scoop-30">Elon Musk acquired Twitter</a> and made <a href="https://newsletter.pragmaticengineer.com/p/the-scoop-30">drastic changes</a> from the start.</p> Business model&nbsp; <p>Most VC-funded businesses in the US are limited liability companies (LLCs,) and for-profit. Not Bluesky. It’s a <a href="https://en.wikipedia.org/wiki/Benefit_corporation">Public Benefit C-Corporation</a> (PBC,) meaning its primary mission is the “public good,” not necessarily profit.</p><p>This difference could give Bluesky a slight advantage over regular for-profit companies: they don’t need to focus solely on generating returns for shareholders like publicly traded companies must, or being commercially viable straight away with a product-market fit. Instead, Bluesky could focus on the mission to “develop and drive large-scale adoption of technologies for open and decentralized public conversation.”&nbsp;</p><p>But this also comes with risk because it dodges the need to define a business strategy in the short term, even though it’s known that strategy is a big ingredient in long-term success.</p><p>We asked Bluesky about this risk. After all, why would users trust a service that could run out of cash, due to being focused on its mission, instead of making money? Here’s what the team says:</p>  <p>Different incentives from traditional social media companies. Due to the openness of Bluesky’s architecture, users are not locked in. Whether it's what client app they use or what algorithm they use for their primary feed, users can choose Bluesky’s defaults, or switch to another provider. Since the cost to switch is so low, Bluesky is incentivized to monetize in ways that will keep users happy.&nbsp; On the other hand, Twitter <a href="https://www.theverge.com/2023/1/19/23562947/twitter-third-party-client-tweetbot-twitterific-ban-rules">banned third-party clients in 2023</a>, as they didn’t serve ads.</p>  <p>Exploring paid services as value-adds. In order to not depend on external funding, Bluesky has <a href="https://bsky.social/about/blog/7-05-2023-business-plan">a business vision</a> to sell paid services. The first is <a href="https://bsky.social/about/blog/7-05-2023-business-plan">partnering with Namecheap</a> to sell custom domains, which are a big part of Bluesky. Now, when someone registers a domain through Namecheap, Bluesky gets a small commission.</p>  <p>The existing monetization strategies that traditional social media companies use, like selling user data for ads, would not be as effective for Bluesky. So, they’ll need to find new ways to monetize. An interesting observation by Bluesky’s developer relationships engineer, Emily Liu:</p><blockquote><p>“We’re using this period of time to explore what services will be valuable for users, like custom domains that users can <a href="https://bsky.social/about/blog/7-05-2023-namecheap">purchase and manage through Bluesky</a>. We’re turning a social app inside out — unlike the centralized social media giants, we don’t have a monopoly on user data, and so we aren’t following a super traditional monetization path.”</p></blockquote><p>There are examples of companies that became PBCs and enjoyed success, or even went public. Insurance technology startup Lemonada went public in 2020 with this setup. Education startup Coursera is also a PBC, as is the eyewear company, Warby Parker.</p> 2. Team <p>Today, Bluesky has around 40 full-time staff:</p>  <p>17 in “core” team, of which 12 are developers</p>  <p>Circa 20 people in support and moderation</p>  <p>Fun fact: none of the early hires ever worked at Twitter! (The first person hired with past experience at Twitter was brought onboard in 2024.) I find this interesting, as since Bluesky was founded, Twitter has <a href="https://newsletter.pragmaticengineer.com/p/the-scoop-31">laid off 75% of its workforce</a> – some of whom may have considered joining the new social media startup.</p> Team structure <p>In the core engineering team, everyone is on Bluesky, and we’re linking their profiles.&nbsp;</p>  <p>Protocol/backend and infra: 5 people, working mostly in the <a href="https://github.com/bluesky-social/atproto">Atproto repository</a> and <a href="https://github.com/bluesky-social/indigo">Indigo repo</a>. <a href="https://bsky.app/profile/dholms.xyz">Daniel Holmgren</a>, <a href="https://bsky.app/profile/divy.zone">Devin Ivy</a>, <a href="https://bsky.app/profile/bnewbold.net">Bryan Newbold</a>, <a href="https://bsky.app/profile/matthieu.bsky.team">Matthieu</a>, and <a href="https://bsky.app/profile/jaz.bsky.social">Jaz</a>.</p>  <p>Frontend &amp; apps: 5 people. They do most of their coding in the <a href="https://github.com/bluesky-social/social-app">Apps repository</a>. <a href="https://bsky.app/profile/pfrazee.com">Paul Frazee</a>, <a href="https://bsky.app/profile/esb.lol">Eric Bailey</a>, <a href="https://bsky.app/profile/did:plc:fpruhuo22xkm5o7ttr2ktxdo">Dan Abramov</a>, <a href="https://bsky.app/profile/haileyok.com">Hailey</a> and <a href="https://bsky.app/profile/samuel.bsky.team">Samuel</a>.&nbsp;</p>  <p>Moderation / labeling: 1 person. This is the <a href="http://samuel">Ozone service</a> built by <a href="https://bsky.app/profile/foysal.codes">Foysal Ahamed</a>.</p>  <p>Other: 5 people. <a href="https://bsky.app/profile/did:plc:oky5czdrnfjpqslsw2a5iclo">Jay Graber</a> (CEO), <a href="https://bsky.app/profile/rose.bsky.team">Rose Wang</a> (growth), <a href="https://bsky.app/profile/aaron.bsky.team">Aaron Rodericks</a> (Head of T&amp;S), <a href="https://bsky.app/profile/why.bsky.team">whyrusleeping</a> (technical advisor) and <a href="https://bsky.app/profile/emilyliu.me">Emily Liu</a> (developer relations/growth)</p>  <p>You don’t need to work at Bluesky to map the engineering structure. In most company deep dives, we ask current or past employees to get the team structure. But for Bluesky, you can simply inspect the projects within <a href="https://github.com/bluesky-social">the Bluesky repository</a>, and map Github contributors to their Bluesky handles! All code that Bluesky engineers write is in public, and I’m somewhat surprised just how transparent this all is, in a good way.</p> Unusually high ratio of former founders <p>Close to 4/5 of the “core” team have been founders before, which is extremely unusual at a startup. Of course, startups attract people who prefer working in small teams, some of whom have also been founders. But what attracted so many to Bluesky?</p><p>From talking with the team, it comes down to:</p>  <p>Belief in Bluesky’s greater mission</p>  <p>Building decentralized web experiences&nbsp;</p>  <p>Bluesky is building a social network, to scale. If anyone knows how to rapidly scale up, it’s people at Big Tech companies with personal experience of it; think engineers at Google in the early 2000s, those at Facebook in the mid-2000s and early 2010s, those at Netflix in the 2010s, etc. But Bluesky had no such people among its first few engineering hires.</p><p>Founding engineer Paul Frazee shares:</p><blockquote><p>“A lot of the core team's experience really just came from previous decentralized web or decentralized social projects; way more than from high scale social networks.</p><p>“We had a ton of experience with peer-to-peer networks, and had paid a fair amount of attention to the blockchain world. For the initial team, this was the second, third, or even fourth time building a decentralized network.”</p></blockquote> 3. Tech stack  Typescript (almost) all the way <p>The backend is mostly in Typescript, as is the frontend and mobile apps. This allows software engineers to work across the stack without switching languages. This choice is similar to <a href="https://newsletter.pragmaticengineer.com/i/86186964/technology-stack">Linear’s to use Typescript between the back and front ends</a>.</p><p>Why pick this language? Daniel Holmgren gives several reasons:</p>  <p>De facto language of the web. “Javascript and therefore Typescript is probably as close as we have to a ‘lingua franca’ for the web. We felt it was a natural choice to communicate the core ideas in the reference implementation of the protocol.”</p>  <p>Shared front and back end language. “Having a shared implementation across the client and backend makes for a nice development experience, especially knowing that both sides have a shared understanding of the schema system.”</p>  <p>Fits the AT protocol’s dynamic data approach. “Being a dynamic language, Typescript has been great for working with the dynamic data you find in the ATproto ecosystem.”&nbsp;</p>  <p>A great fit for code generation. “Typescript has a really great ecosystem for code generation. I think it's that the actual official TypeScript implementation just ships the tooling for parsing into an AST and then manipulating it and outputting code. Not many people were needing that, but I was really happy it was there.”</p>  <p>Great for prototyping with a small team. “Typescript is a great language for quick building. The runtime is quite powerful and lets you not worry (much) about things like memory management and concurrency. The type system is very ergonomic and provides the structure you need, while seldom getting in the way. With such a small early team, it made sense to choose a language that enabled us to prototype quickly.”</p>   Node.js <p>The Typescript code on the backend uses the <a href="https://nodejs.org/en">Node.js</a> runtime. One worry the team had was how it would scale, as Node.js app runs in a single thread, without creating a new thread for every request. This means a server running Node.js can handle fewer parallel requests than frameworks that support multi-threading more efficiently.</p><p>However, the team found it's very easy to scale the service horizontally (adding more machines,) but only by building the application to be stateless. They did this, and Daniel recalls they proved a point:</p><blockquote><p>“At one point, we had around 192 node processes running behind HAProxy. All these processes were super ‘bored,’ at around 1% CPU utilization. But the point is that it’s very easy to scale a stateless Node service out horizontally – so that's what we did!”</p></blockquote> Go for performance <p>The backend started out as Typescript, but over time, Go was introduced. But why bring in another language when Typescript works well enough? Performance, says Daniel:</p><blockquote><p>“Some of more infra-y services such as the Relay and our internal ‘Dataplane’ are very performance-based. They often involve a lot of crypto operations and low-level bit twiddling. These are things that Go excels at. In particular, Go has a shard-aware driver for Scylla (a database we use) which made it a natural choice for interfacing with it.”</p></blockquote><p>The team has rewritten services from Typescript to Go when performance improvements warrant it. Services in Go, today:</p>  <p>Search services like <a href="https://github.com/bluesky-social/indigo/blob/main/cmd/palomar/README.md">Palomar</a>.</p>  <p><a href="https://docs.bsky.app/docs/advanced-guides/firehose">The “firehose”</a> called Relay. This service does a lot of low-level fiddling and crypto operations.</p>  <p>Dataplane. This service is called the “BigSky service,” and is one of few closed-sourced services. This service manages data, talking to ScyllaDB, as well as some other data stores like Redis, and their own handwritten cache stores.</p>   Data layer: Postgres, ScyllaDB, SQLite <p>Bluesky started out with PostgreSQL as the data store, but as the load grew the team expected the time would come to move off this flexible but potentially less scalable solution, to a less flexible, more scalable one.</p><p>Migration happened in mid-2023:</p>  <p>ScyllaDB powers AppViews.</p>  <p>SQLite is the database of choice for the personal data servers (PDSses.)</p>  <p>We cover more on the data layer migration in <a href="https://newsletter.pragmaticengineer.com/i/114113498/scaling-the-database-layer">Part 1: Scaling the database layer</a>.</p> Backend-to-frontend communication <p>The <a href="https://atproto.com/specs/lexicon">Lexicon schema</a> is used to describe HTTP endpoints and all of the record types in the network. This approach ensures strongly-typed contracts and agreements between backend and client.</p><p>Such strong contracts are especially important for a decentralized network. Bluesky is also an open microservices network, so being stricter at the protocol level makes sense. Paul Frazee puts it like this:</p><blockquote><p>“You can think of the entire network as an open microservices network. It's not terribly uncommon when you're doing microservices to try to systemize it through a schema language, to make sure that the contracts are being obeyed throughout all these different things, as you don't have the benefit of a monolith.”</p></blockquote> Build tooling <p>The team uses GitHub Actions for the CI/CD stack. The project is running in the open, so you can <a href="https://github.com/bluesky-social/atproto/actions">inspect these runs yourself</a>:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5eca81ad-3c5e-41b1-87c3-a0cf7f97ad95_1600x999.png">    </a>Recent CI/CD runs in the Bluesky protocol repository. Source: <a href="https://github.com/bluesky-social/atproto/actions">Bluesky</a> <p>The builds themselves use common tools and techniques for catching quality issues early:</p><p>Linting: run static code checks to catch coding issues, and ensure the coding style is consistent across the codebase. Here’s an example of the types of issues Bluesky’s linter has captured:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5be214a0-188c-4a10-9d81-78f1ce6be95b_1600x708.png">    </a>Unused variables assigned a value cause the linter to raise warnings. It’s sensible for production codebases to not have variables lying around. Source: <a href="https://github.com/bluesky-social/atproto/actions/runs/8773583228/job/24073857703">verification run</a> <p>Automated tests: run unit, integration and snapshot tests.</p><p>Example of a snapshot test:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F145366d0-64f8-46d4-80a6-cdf5fa567a57_1028x446.png">    </a>A snapshot test exercising that “likes” on a reply are correctly fetched, by checking the UI looks as expected (line 56, via the .toMatchSnapshot() call) and that the number of likes is as expected (in line 58). Source: <a href="https://github.com/bluesky-social/atproto/blob/main/packages/bsky/tests/views/likes.test.ts">Bluesky source code</a>  Mobile and web stack <p>One fascinating thing about Bluesky is that the website, iOS app, and Android mobile app, were all built by a single developer. From summer 2022 until early 2023, it was engineer Paul Frazee who worked on this, alone. Early in 2023, the app had some external contributors, and the second full time hire joined in April. The tech stack that enabled all of this was React Native and Expo:</p><p>On choosing React Native, Paul Frazee explains:</p><blockquote><p>“We knew we wanted to hit all of the major platforms right from the start, and given our Typescript focus, there was only one viable way to do that: React Native.&nbsp;</p><p>If you have two separate application codebases, then you have two separate products. You’re implementing everything twice, and you’re debugging everything twice, and the outcomes are all going to slightly vary.”</p></blockquote><p>But React Native is not a “magic bullet” solution, says Paul:</p><blockquote><p>“It’s not easy. Some days it’s maddening. A lot of web developers know how frustrating it is to target multiple browsers; now imagine targeting native iOS and Android, as well. That said, this is a choice between ‘kind of frustrating sometimes’ and ‘impossible.’”</p></blockquote><p><a href="https://expo.dev/">Expo</a> is a platform to develop, review and deploy universal native apps that run on the web, iOS and Android. Bluesky introduced it in March 2023, and while the team was careful in limiting the introduction of new dependencies, the purpose was to manage builds and access Expo’s module ecosystem.</p><p>React Native for web was a choice that surprised even the development team. Initially, Paul intended to use React Native only for mobile apps, but then they found React Native is pretty manageable for the web. Paul explains:</p><blockquote><p>“The amount of stuff we're able to do with code sharing is just stellar. We've got shared code running everywhere, from our backend all the way into each of these different platforms, and that's part of why we've been able to do this with a small team.”</p></blockquote> “One-person” mobile app <p>As mentioned above, The Bluesky mobile app and website were built by a single developer in around a year, Paul Frazee. Today, six engineers work on the app and website, and roughly the same number on the protocol and backend side.</p><p>Still, it’s remarkably efficient to build a launch-ready app with a single, experienced engineer. So how did Paul pull it off?</p><p>He started off by building the protocol, not the app. For the first 6 months, Paul worked with Daniel and Jay, in consultation with Twitter, to figure out how the protocol would work. Bluesky’s idea was that the protocol would be the more important part, and there was an expectation they’d eventually build an app. But this realization didn’t occur until mid-2022. From Paul:</p><blockquote><p>“We always knew we would build an app, because we believe that protocol development should be done in conjunction with app development, but we initially thought it would be more of a prototype. It wasn’t until our relationship with Twitter changed that we realized we’d need to operationalize the protocol and make a full application. So, that was a bit of a shift.”</p></blockquote><p>The relationship between the backend team of Daniel and Devin, and Paul on the frontend/app team was like a traditional frontend/backend one. They spec’d out features together and discussed what each needed to do. They then worked on their own areas and coordinated on final outcomes.</p><p>Building the app helped evolve the protocol, and made the work more fun, Daniel recalls:</p><blockquote><p>“The most fun thing has probably been seeing our ideas come to life by building the social app on the protocol. It’s one thing to build/design a protocol in the abstract, but seeing it actually working in practice is another.”</p></blockquote> 4. Company culture 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/bluesky-engineering-culture">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1d6d493-5d0f-4368-a3da-7224910778f3_1092x494.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:31 &bull;
   7 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2024/05/28/the-peoples-ai/'>The People’s AI</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  People need their own AIs. Personally and collectively. We won’t get them from Anthropic, Apple, Google, OpenAI, Meta, or Microsoft. Not even from Apple. All those companies will want to provide AIaaS: AI as a Service, rather than AI that’s yours alone. Or ours, collectively. The People’s AI can only come from people. Since it […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

Prompt: “A vast field on which the ground spells the letters A and I, with people on it, having a good time.” Via Copilot | Designer
<p>People need <a href="https://projectvrm.org/2024/05/23/personal-ai-vs-corporate-ai/">their own AIs</a>. Personally and collectively.</p>
<p>We won’t get them from Anthropic, Apple, Google, OpenAI, Meta, or Microsoft. Not even from Apple.</p>
<p>All those companies will want to provide AIaaS: AI as a Service, rather than AI that’s yours alone. Or ours, collectively.</p>
<p>The People’s AI can only come from people. Since it will be made of code, it will come from open-source developers working for all of us, and not just for their employers—even if those employers are companies listed above.*</p>
<p>That’s how we got Linux, Apache, MySQL, Python, and countless other open-source code bases on which the digital world is now built from the ground up. Our common ground is open-source code, standards, and protocols.</p>
<p>The sum of business that happens atop that common ground is incalculably vast. It also owes to what we first started calling because effects twenty years ago <a href="http://bloggercon.scripting.com/2004/10/19.html">at Bloggercon</a>. That was when people were making a lot more money&nbsp;because of&nbsp;blogging than&nbsp;with blogging.</p>
<p>Right after that it also became clear that most of the money being made in the whole tech world was&nbsp;because of&nbsp;open-source code, standards, and protocols,&nbsp;rather than with them. (I wrote more about it <a href="https://www.linuxjournal.com/content/building-relationship-economy">here</a>, <a href="https://www.linuxjournal.com/content/linuxs-broadening-foundation">here</a>, and <a href="https://www.linuxjournal.com/content/wanted-more-open-source-research">here</a>.)</p>
<p>So, thanks to because effects, the most leveraged investments anyone can make today will be in developing open source code for The People’s AI.</p>
<p>That’s the AI each of us will have for our own, and that we can use both by ourselves and together as communities.</p>
<p>Those&nbsp;because&nbsp;investments will pay off on the with side as lavishly as investments in TCP/IP, HTTP, Linux, and countless other open-source efforts have delivered across the last three decades.</p>
<p>Only now they’ll pay off a lot faster. For all of us.</p>
 
<p>*See <a href="https://www.linuxjournal.com/article/8934">what I wrote for Linux Journal in 2006</a> about how IBM got clueful about paying kernel developers to work for the whole world and not just one company.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://150108457.v2.pressablecdn.com/wp-content/uploads/2024/05/f7d85cf2-f68f-4654-9c2b-123a8a42e4cb.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:01 &bull;
   7 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 27. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/27/fastlite/#atom-everything'>fastlite</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   fastlite 
New Python library from Jeremy Howard that adds some neat utility functions and syntactic sugar to my sqlite-utils Python library, specifically for interactive use in Jupyter notebooks. 
 The autocomplete support through newly exposed dynamic properties is particularly neat, as is the diagram(db.tables) utility for rendering a graphviz diagram showing foreign key relationships between al
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://answerdotai.github.io/fastlite/">fastlite</a></p>
New Python library from Jeremy Howard that adds some neat utility functions and syntactic sugar to my <a href="https://sqlite-utils.datasette.io/">sqlite-utils</a> Python library, specifically for interactive use in Jupyter notebooks.<p></p>
<p>The autocomplete support through newly exposed dynamic properties is particularly neat, as is the <code>diagram(db.tables)</code> utility for rendering a graphviz diagram showing foreign key relationships between all of the tables.

    </p><p>Via <a href="https://twitter.com/jeremyphoward/status/1795170005367050655">@jeremyphoward</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   21:14 &bull;
   7 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/study-finds-that-52-percent-of-chatgpt-answers-to-programming'>Study Finds That 52 Percent of ChatGPT Answers to Programming Questions Are Wrong</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 On answering programming questions: "We found that 52 percent of ChatGPT answers contain misinformation, 77 percent of the answers are more verbose than human answers, and 78 percent of the answers suffer from different degrees of inconsistency to human answers." 

 To be fair, I do expect AI answers to get better over time, but it's certainly premature to use it as a trusted 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>On answering programming questions: "We found that 52 percent of ChatGPT answers contain misinformation, 77 percent of the answers are more verbose than human answers, and 78 percent of the answers suffer from different degrees of inconsistency to human answers."</p>

<p>To be fair, I do expect AI answers to get better over time, but it's certainly premature to use it as a trusted toolkit for software development today. One might argue that its answers are more like suggestions for an engineer to check and adapt as appropriate, but will they really be used that way?</p>

<p>I think it's more likely that AI agents will be used to build software by people who want to avoid engaging with a real, human engineer, or people who want to cut corners for one reason or another. So I think the warnings are appropriate: LLMs are bad at coding and we shouldn't trust what they say.  <a href="https://werd.io/tag/AI">#AI</a></p>
            <p>[<a href="https://futurism.com/the-byte/study-chatgpt-answers-wrong">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   18:20 &bull;
   8 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2024/05/27/implement-a-microsoft-entra-id-external-authentication-method-using-asp-net-core-and-openiddict/'>Implement a Microsoft Entra ID external authentication method using ASP.NET Core and OpenIddict</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The article shows how to implement a Microsoft Entra ID external authentication method (EAM) using ASP.NET Core, OpenIddict and FIDO2/passkeys. The application using ASP.NET Core Identity to manage the accounts and the passkeys. Code: https://github.com/damienbod/MfaServer The following flow diagram from the Microsoft docs explains how EAM works. Refer to the documentation for a full explanation. [
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The article shows how to implement a Microsoft Entra ID external authentication method (EAM) using ASP.NET Core, OpenIddict and FIDO2/passkeys. The application using ASP.NET Core Identity to manage the accounts and the passkeys. </p>



<p>Code: <a href="https://github.com/damienbod/MfaServer">https://github.com/damienbod/MfaServer</a></p>



<p>The following flow diagram from the Microsoft docs explains how EAM works. Refer to the documentation for a full explanation.</p>



<p>src: <a href="https://learn.microsoft.com/en-gb/entra/identity/authentication/concept-authentication-external-method-provider">https://learn.microsoft.com/en-gb/entra/identity/authentication/concept-authentication-external-method-provider</a></p>







<p></p>



 Setup Microsoft Entra ID 



<p>To setup the external authentication method (EAM), the following needs to be created:</p>



 
 MFA server (EAM) deployed to a public URL 



 MFA server (EAM) has an OIDC discovery endpoint 



 MFA server (EAM) defines a OIDC public Implicit flow 



 Azure App registration public multi-tenant client using authorization_endpoint as the redirect URL 



 EAM created and added to the ME-ID authentication methods. 
 



<p>The external authentication methods (EAM) server should be created and the following three values are required:</p>



 
 –app-registration-clientId– 



 –your-client_id-from-external-provider– 



 –your-external-provider-url–/.well-known/openid-configuration 
 



 Setup Microsoft Entra ID App registration 



<p>The Microsoft Entra ID App client registration is a multi-tenant registration and required the authorize endpoint as the redirect URL.</p>



<p>The Microsoft Entra ID docs can be found here:</p>



<p><a href="https://learn.microsoft.com/en-gb/entra/identity/authentication/concept-authentication-external-method-provider#configure-a-new-external-authentication-provider-with-microsoft-entra-id">https://learn.microsoft.com/en-gb/entra/identity/authentication/concept-authentication-external-method-provider#configure-a-new-external-authentication-provider-with-microsoft-entra-id</a></p>



<p>API Permissions</p>



<p>The openid permission must be defined as well a profile scope if you want to request user data.</p>







<p></p>



<p>RedirectUrl authorization_endpoint</p>



<p>The Redirect URL is the authorization_endpoint of the OIDC MFA server. This must be set in the Azure app registration. This can be found using the OpenID connect well known endpoints in the browser.</p>



<p>–your-external-provider-url–/.well-known/openid-configuration</p>







<p></p>



 EAM setup using Microsoft Graph 



<p>Microsoft Graph can be used to create the Microsoft Entra ID external authentication method (EAM).</p>



<p>This requires the delegated Policy.ReadWrite.AuthenticationMethod permission</p>



<p>POST</p>



<p><a href="https://graph.microsoft.com/beta/policies/authenticationMethodsPolicy/authenticationMethodConfigurations">https://graph.microsoft.com/beta/policies/authenticationMethodsPolicy/authenticationMethodConfigurations</a></p>


  {
    "@odata.type": "#microsoft.graph.externalAuthenticationMethodConfiguration",
    "displayName": "--name-of-provider--", // Displayed in login
    "state": "enabled"
    "appId": "--app-registration-clientId--", // external authentication app registration, see docs
    "openIdConnectSetting": {
        "clientId": "--your-client_id-from-external-provider--",
        "discoveryUrl": "--your-external-provider-url--/.well-known/openid-configuration"
    },
    "includeTarget": { // switch this if only specific users are required
        "targetType": "group",
        "id": "all_users"
    }
}
  


 EAM setup using the Azure portal 



<p>The Azure portal can also be used to setup an EAM server. In the authentication methods, you can use the Add external method button.</p>







<p></p>



<p>The three values from the Azure App registration and the EAM server can now be used to specific the external method.</p>







<p></p>



<p>Once created, you can view this in the overview.</p>







<p></p>



 EAM MFA Server using OpenIddict and ASP.NET Core 



<p>The demo MFA server can be found in the github repository, linked at the top of the blog. The server is implemented using OpenID connect. I used OpenIddict to implement this. The user management is implemented using ASP.NET Core Identity and the FIDO2/passkeys support is implemented using fido2-net-lib.</p>



<p>The default Identity and OIDC flows are changed to remove the consent and only use passkeys. A user can register using his or her OID from Microsoft Entra ID and the preferred passkey.</p>



<p>ME-ID sends a OIDC Implicit flow request with specific claims and an id_token_hint with an id_token. the claims and the id_token must be fully validated. The tenant id (tid) must also be explicitly validation if you want to avoid phishing attacks. </p>



<p>The id_token can be validated as follows:</p>


  using Microsoft.IdentityModel.JsonWebTokens;
using Microsoft.IdentityModel.Tokens;
using System.Net.Mail;
using System.Security.Claims;

namespace FidoMfaServer.IdTokenHintValidation;

public static class ValidateIdTokenHintRequestPayload
{
    public static (bool Valid, string Reason, string Error) IsValid(ClaimsIdentity claimsIdTokenPrincipal,
        IdTokenHintValidationConfiguration configuration,
        string userEntraIdOid,
        string userName)
    {
        // oid from id_token_hint must match User OID
        var oid = claimsIdTokenPrincipal.FindFirst("oid").Value;
        if (!oid!.Equals(userEntraIdOid))
        {
            return (false, "oid parameter has an incorrect value",
                EntraIdTokenRequestConsts.ERROR_INVALID_CLIENT);
        };

        // aud must match allowed audience if for a specifc app
        if (configuration.ValidateAudience)
        {
            var aud = claimsIdTokenPrincipal.FindFirst("aud").Value;
            if (!aud!.Equals(configuration.Audience))
            {
                return (false, "client_id parameter has an incorrect value",
                    EntraIdTokenRequestConsts.ERROR_INVALID_CLIENT);
            };
        }

        // tid must match allowed tenant
        var tid = claimsIdTokenPrincipal.FindFirst("tid").Value;
        if (!tid!.Equals(configuration.TenantId))
        {
            return (false, "tid parameter has an incorrect value",
                EntraIdTokenRequestConsts.ERROR_INVALID_CLIENT);
        };

        // preferred_username from id_token_hint
        var preferred_username = GetPreferredUserName(claimsIdTokenPrincipal);
        if (!preferred_username!.ToLower().Equals(userName.ToLower()))
        {
            return (false, "preferred_username parameter has an incorrect value",
                EntraIdTokenRequestConsts.ERROR_INVALID_CLIENT);
        };

        return (true, string.Empty, string.Empty);
    }

    public static async Task&lt;(bool Valid, string Reason, TokenValidationResult TokenValidationResult)&gt; ValidateTokenAndSignatureAsync(
        string jwtToken,
        IdTokenHintValidationConfiguration idTokenConfiguration,
        ICollection&lt;SecurityKey&gt; signingKeys,
        bool testingMode)
    {
        try
        {
            var validationParameters = new TokenValidationParameters
            {
                RequireExpirationTime = true,
                ValidateLifetime = true,
                ClockSkew = TimeSpan.FromMinutes(1),
                RequireSignedTokens = true,
                ValidateIssuerSigningKey = true,
                IssuerSigningKeys = signingKeys,
                ValidateIssuer = true,
                ValidIssuer = idTokenConfiguration.Issuer,
                ValidateAudience = idTokenConfiguration.ValidateAudience,
                ValidAudience = idTokenConfiguration.Audience
            };

            if (testingMode)
            {
                //validationParameters.ValidateIssuerSigningKey = false;
                //validationParameters.ValidateIssuer = false;
                validationParameters.ValidateLifetime = false;
            }

            var tokenValidator = new JsonWebTokenHandler
            {
                MapInboundClaims = false
            };

            var tokenValidationResult = await tokenValidator.ValidateTokenAsync(jwtToken, validationParameters);

            if (!tokenValidationResult.IsValid)
            {
                return (tokenValidationResult.IsValid, tokenValidationResult.Exception!.Message, tokenValidationResult);
            }

            return (tokenValidationResult.IsValid, string.Empty, tokenValidationResult);
        }
        catch (Exception ex)
        {
            return (false, $"Id Token Authorization failed {ex.Message}", null);
        }
    }

    public static string GetPreferredUserName(ClaimsIdentity claimsIdentity)
    {
        var preferred_username = claimsIdentity.Claims.FirstOrDefault(t =&gt; t.Type == "preferred_username");
        return preferred_username?.Value ?? string.Empty;
    }

    public static string GetAzpacr(ClaimsIdentity claimsIdentity)
    {
        var azpacrClaim = claimsIdentity.Claims.FirstOrDefault(t =&gt; t.Type == "azpacr");
        return azpacrClaim?.Value ?? string.Empty;
    }

    public static string GetAzp(ClaimsIdentity claimsIdentity)
    {
        var azpClaim = claimsIdentity.Claims.FirstOrDefault(t =&gt; t.Type == "azp");
        return azpClaim?.Value ?? string.Empty;
    }

    public static bool IsEmailValid(string email)
    {
        if (!MailAddress.TryCreate(email, out var mailAddress))
            return false;

        // And if you want to be more strict:
        var hostParts = mailAddress.Host.Split('.');
        if (hostParts.Length == 1)
            return false; // No dot.
        if (hostParts.Any(p =&gt; p == string.Empty))
            return false; // Double dot.
        if (hostParts[^1].Length &lt; 2)
            return false; // TLD only one letter.

        if (mailAddress.User.Contains(' '))
            return false;
        if (mailAddress.User.Split('.').Any(p =&gt; p == string.Empty))
            return false; // Double dot or dot at end of user part.

        return true;
    }
}
  


<p>When the user has successfully authenticated using the required MFA and the id_token is valid, the result is returned to Microsoft Entra ID again with required claims which match the request claims. For example the amr is returned as an array claim, the acr contains a value matching the request value, the sub claim is the exact same as the request claim and so on. The Microsoft specifics the requirements in the docs.</p>



<p></p>



<p>This could be implemented using OpenIddict like the following code block displays. It is important to also validate the id_token fully in the method . The signature must be validated and the user OID must match and so on. The claims MUST be returned in the id_token and not the user info endpoint.</p>


   //get well known endpoints and validate access token sent in the assertion
 var configurationManager = new ConfigurationManager&lt;OpenIdConnectConfiguration&gt;(
     _idTokenHintValidationConfiguration.MetadataAddress,
     new OpenIdConnectConfigurationRetriever());

 var wellKnownEndpoints = await configurationManager.GetConfigurationAsync();

 var idTokenHintValidationResult = await ValidateIdTokenHintRequestPayload.ValidateTokenAndSignatureAsync(
     request.IdTokenHint,
     _idTokenHintValidationConfiguration,
     wellKnownEndpoints.SigningKeys,
     _testingMode);

 if (!idTokenHintValidationResult.Valid)
 {
     return UnauthorizedValidationParametersFailed(idTokenHintValidationResult.Reason,
         "id_token_hint validation failed");
 }

 var requestedClaims = System.Text.Json.JsonSerializer.Deserialize&lt;claims&gt;(request.Claims);

 // The acr claims for the authentication request. This value should match one of the values from the request sent to initiate this request.
 // Only one acr claim should be returned.
 principal.AddClaim("acr", "possessionorinherence");

 var sub = idTokenHintValidationResult.TokenValidationResult.ClaimsIdentity
     .Claims.First(d =&gt; d.Type == "sub");

 principal.RemoveClaims("sub");
 principal.AddClaim(sub.Type, sub.Value);

 var claims = principal.Claims.ToList();

 // The amr claims for the authentication method used in authentication.
 // This value should be returned as an array, and only one method claim should be returned.
 // Openiddict between 5.0.1 =&gt; 5.5.0 does not support this.
 claims.Add(new Claim("amr", "[\"fido\"]", JsonClaimValueTypes.JsonArray));

 var cp = new ClaimsPrincipal(
     new ClaimsIdentity(claims, principal.Identity.AuthenticationType));

 foreach (var claim in cp.Claims)
 {
     claim.SetDestinations(GetDestinations(claim, cp));
 }

 var (Valid, Reason, Error) = ValidateIdTokenHintRequestPayload
     .IsValid(idTokenHintValidationResult.TokenValidationResult.ClaimsIdentity,
     _idTokenHintValidationConfiguration,
     user.EntraIdOid,
     user.UserName);

 if (!Valid)
 {
     return UnauthorizedValidationParametersFailed(Reason, Error);
 }

 return SignIn(cp, OpenIddictServerAspNetCoreDefaults.AuthenticationScheme);
  


 Testing the MFA server 



<p>It is hard to test the EAM server implementation directly from Azure. I created a test application to validate that the server handles the OIDC Implicit flow requests correctly and responds correctly with the correct claims like in the specification.</p>



<p>If implementing this is a production setup, the demo server requires a lot of changes for user and account management.</p>



 Notes 



<p>Microsoft Entra ID external authentication methods makes it possible to integrate with third party MFA servers. This can be really useful for SSPR, smart cards MFA, custom MFA and other such requirements. the external MFA would need to be set as the default or only MFA for users in Microsoft Entra ID for a good user experience. You would also need to integrate this into the continuous access policies and set the authentication strength correctly.</p>



 Links 



<p><a href="https://learn.microsoft.com/en-gb/entra/identity/authentication/concept-authentication-external-method-provider">https://learn.microsoft.com/en-gb/entra/identity/authentication/concept-authentication-external-method-provider</a></p>



<p><a href="https://learn.microsoft.com/en-gb/entra/identity/authentication/how-to-authentication-external-method-manage">https://learn.microsoft.com/en-gb/entra/identity/authentication/how-to-authentication-external-method-manage</a></p>



<p><a href="https://techcommunity.microsoft.com/t5/microsoft-entra-blog/public-preview-external-authentication-methods-in-microsoft/ba-p/4078808">https://techcommunity.microsoft.com/t5/microsoft-entra-blog/public-preview-external-authentication-methods-in-microsoft/ba-p/4078808</a></p>



<p><a href="https://documentation.openiddict.com">https://documentation.openiddict.com</a></p>



<p><a href="https://github.com/passwordless-lib/fido2-net-lib">https://github.com/passwordless-lib/fido2-net-lib</a></p>



<p><a href="https://mysignins.microsoft.com">https://mysignins.microsoft.com</a></p>



<p><a href="https://developer.microsoft.com/en-us/graph/graph-explorer">https://developer.microsoft.com/en-us/graph/graph-explorer</a></p>



<p><a href="https://github.com/damienbod/AspNetCoreIdentityFido2Mfa">https://github.com/damienbod/AspNetCoreIdentityFido2Mfa</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2024/05/me-id_external-authn-flows_01.png?w=1024'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   05:50 &bull;
   8 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 26. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/26/city-in-a-bottle/#atom-everything'>City In A Bottle – A 256 Byte Raycasting System</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   City In A Bottle – A 256 Byte Raycasting System 
Frank Force explains his brilliant 256 byte canvas ray tracing animated cityscape demo in detail.

     Via @hardmaru 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://frankforce.com/city-in-a-bottle-a-256-byte-raycasting-system/">City In A Bottle – A 256 Byte Raycasting System</a></p>
Frank Force explains his brilliant 256 byte canvas ray tracing animated cityscape demo in detail.

    <p>Via <a href="https://twitter.com/hardmaru/status/1794593416598491427">@hardmaru</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   18:25 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/26/statically-typed-functional-programming-with-python-312/#atom-everything'>Statically Typed Functional Programming with Python 3.12</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Statically Typed Functional Programming with Python 3.12 
Oskar Wickström builds a simple expression evaluator that demonstrates some new patterns enabled by Python 3.12, incorporating the match operator, generic types and type aliases.

     Via Lobste.rs 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://wickstrom.tech/2024-05-23-statically-typed-functional-programming-python-312.html">Statically Typed Functional Programming with Python 3.12</a></p>
Oskar Wickström builds a simple expression evaluator that demonstrates some new patterns enabled by Python 3.12, incorporating the match operator, generic types and type aliases.

    <p>Via <a href="https://lobste.rs/s/hphovk/statically_typed_functional">Lobste.rs</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   08:12 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 25. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/nostr-journalism-accelerator'>Nostr Journalism Accelerator</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 Nos is running a "journalism accelerator", which onboards independent journalists and publications onto Nostr with guaranteed promotion and 1:1 help. 

 Nostr is a different kind of open network, in the sense that it's decentralized rather than federated. Famously, Jack Dorsey defected there from Bluesky, in part because Bluesky started offering service-level features like com
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>Nos is running a "journalism accelerator", which onboards independent journalists and publications onto Nostr with guaranteed promotion and 1:1 help.</p>

<p>Nostr is a different kind of open network, in the sense that it's decentralized rather than federated. Famously, Jack Dorsey defected there from Bluesky, in part because Bluesky started offering service-level features like community moderation rather than just focusing on the protocol. It's also much more closely tied to crypto communities than either the fediverse or Bluesky.</p>

<p>I'm curious about the kinds of journalists who might sign up for this. I spotted The Conversation there while I was nosing around, but I haven't found any other publishers I recognized; the network really is very open to build on, so I wonder if more might follow - and if they skew in any particular direction.  <a href="https://werd.io/tag/Technology">#Technology</a></p>
            <p>[<a href="https://www.nos.social/journalism-accelerator">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:03 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/why-publishers-are-preparing-to-federate-their-sites'>Why publishers are preparing to federate their sites</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 "At least two digital media companies are exploring the fediverse as a way to take more control over their referral traffic and onsite audience engagement."  

 The Verge and 404 Media will both support ActivityPub (a protocol, not a "plugin", as the article calls it) and plug into the fediverse. 

 This dovetails with what I've been talking about for some time: "Instead of sp
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>"At least two digital media companies are exploring the fediverse as a way to take more control over their referral traffic and onsite audience engagement." </p>

<p>The Verge and 404 Media will both support ActivityPub (a protocol, not a "plugin", as the article calls it) and plug into the fediverse.</p>

<p>This dovetails with what I've been talking about for some time: "Instead of spending time building a presence on other platforms for their benefit, a publisher can do that on their own sites — while giving readers the ability to see those posts on other federated platforms." And while the fediverse is still in an early, growing stage, it's worth taking a bet on.</p>

<p>As Flipboard's Mike McCue says further into the piece, "What The Verge is doing is definitely pioneering the future for media." I'm hopeful that more publishers follow suit - with this and other experiments that have the potential to help them build more direct first-party relationships with their audiences.  <a href="https://werd.io/tag/Fediverse">#Fediverse</a></p>
            <p>[<a href="https://digiday.com/media/why-publishers-are-preparing-to-federate-their-sites/">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   18:53 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/25/why-googles-ai-might-recommend-you-mix-glue-into-your-pizza/#atom-everything'>Why Google’s AI might recommend you mix glue into your pizza</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Why Google’s AI might recommend you mix glue into your pizza 
I got “distrust and verify” as advice on using LLMs into this Washington Post piece by Shira Ovide.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://www.washingtonpost.com/technology/2024/05/24/google-ai-overviews-wrong/">Why Google’s AI might recommend you mix glue into your pizza</a></p>
I got “distrust and verify” as advice on using LLMs into this Washington Post piece by Shira Ovide.
   
  
  
  

</div>
</div>

<div class='item-footer'>
   06:29 &bull;
   10 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 24. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/the-fatal-flaw-in-publishers-openai-deals'>The Fatal Flaw in Publishers' OpenAI Deals</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 "It’s simply too early to get into bed with the companies that trained their models on professional content without permission and have no compelling case for how they will help build the news business." 

 This piece ends on the most important point: nobody is coming to save the news industry, and certainly not the AI vendors. Software companies don't care about news. They do
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>"It’s simply too early to get into bed with the companies that trained their models on professional content without permission and have no compelling case for how they will help build the news business."</p>

<p>This piece ends on the most important point: nobody is coming to save the news industry, and certainly not the AI vendors. Software companies don't care about news. They don't think your content is more valuable because it's fact-checked and edited. They don't have a vested interest in ensuring you survive. They just want the training data - all of it, in order to build what they consider to be the best product possible. Everything else is irrelevant.  <a href="https://werd.io/tag/AI">#AI</a></p>
            <p>[<a href="https://www.theatlantic.com/technology/archive/2024/05/fatal-flaw-publishers-making-openai-deals/678477/">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   15:07 &bull;
   11 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/the-ai-data-goldmine'>The AI data goldmine</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
 If I was a nefarious software company, here’s how I might be thinking:  AI functionality tends to require that data is sent to a centralized service for processing.  This is often data that is not being shared online in any other way that is easily available for analysis: existential work questions, internal documents, and so on.  This makes it very valuable data to sell to brokers or to use in 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
<p>If I was a nefarious software company, here’s how I might be thinking:</p><p>AI functionality tends to require that data is sent to a centralized service for processing.</p><p>This is often data that is not being shared online in any other way that is easily available for analysis: existential work questions, internal documents, and so on.</p><p>This makes it very valuable data to sell to brokers or to use in targeting advertising.</p><p>So, let’s add lots of AI functionality to our services to encourage people to share that data with us.</p><p>We’ll provide AI services.</p><p>We’ll mine the data that is provided to us when people use those AI services.</p><p>And then we’ll sell it.</p><p>The AI revolution is also the private data sharing revolution. It’s worth asking: does this AI feature I’m interested in using puncture a privacy hole in the service it is a part of? Who am I sharing this information with? What will they do with it?</p> 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:47 &bull;
   11 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/unoffice-hours'>Unoffice Hours</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
   I’m enamored with Matt Webb’s unoffice hours: a way to chat with him about anything, without needing to email him first, for 30 minutes.  As Matt says:   I loved those open conversations over coffee in the Before Times. There’s an ostensible reason to connect, so you talk about work, or compare notes about an idea, or whatever. But then the unexpected emerges. (Sometimes you have to hunt for i
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://werd.io/file/6650a098f5b50178120a8cb2/thumb.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
<p></p><p>I’m enamored with <a href="https://interconnected.org/home/">Matt Webb’s</a> <a href="https://interconnected.org/home/2020/09/24/unoffice_hours">unoffice hours</a>: a way to chat with him about anything, without needing to email him first, for 30 minutes.</p><p>As Matt says:</p><blockquote><p>I loved those open conversations over coffee in the Before Times. There’s an ostensible reason to connect, so you talk about work, or compare notes about an idea, or whatever. But then the unexpected emerges. (Sometimes you have to hunt for it.) There are things in your head that you only know are there when you say them. And there are encounters with new ideas and new perspectives.</p></blockquote><p>Exactly. So let’s do it.</p><p>Introducing my own Unoffice Hours: I’ve set aside a little time on Fridays to connect about anything.</p><p>Here are some topics that might be interesting to chat about:</p>  Feedback on a project you’re working on (startups, software, a writing project) 
 Following up on something I’ve written in this space 
 Product and technology strategy in the public interest (news, education, libraries, other mission-driven organizations) 
 The indie web 
 Fostering a collaborative organizational culture 
 Saying hello 
 <p>Matt calls the effect “manufactured serendipity”; I call it “intentional serendipity,” but the intent is the same. It’s good to chat and meet people, and you never know where it will lead.</p><p><a href="https://calendly.com/benwerdmuller/unoffice-hours">To book a 30-minute chat, click here.</a></p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://werd.io/file/6650a098f5b50178120a8cb2/thumb.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:13 &bull;
   11 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2024/May/24/golden-gate-claude/#atom-everything'>Golden Gate Claude</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Golden Gate Claude 
This is absurdly fun and weird. Anthropic's recent LLM interpretability research gave them the ability to locate features within the opaque blob of their Sonnet model and boost the weight of those features during inference. 
 For a limited time only they're serving a "Golden Gate Claude" model which has the feature for the Golden Gate Bridge boosted. No matter what question you
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://www.anthropic.com/news/golden-gate-claude">Golden Gate Claude</a></p>
This is absurdly fun and weird. Anthropic's recent <a href="https://simonwillison.net/2024/May/21/scaling-monosemanticity-extracting-interpretable-features-from-c/">LLM interpretability research</a> gave them the ability to locate features within the opaque blob of their Sonnet model and boost the weight of those features during inference.<p></p>
<p>For a limited time only they're serving a "Golden Gate Claude" model which has the feature for the Golden Gate Bridge boosted. No matter what question you ask it the Golden Gate Bridge is likely to be involved in the answer in some way. Click the little bridge icon in the Claude UI to give it a go.</p>
<p>I asked for names for a pet pelican and the first one it offered was this:</p>
<blockquote>
<p>Golden Gate - This iconic bridge name would be a fitting moniker for the pelican with its striking orange color and beautiful suspension cables.</p>
</blockquote>
<p>And from a <a href="https://fedi.simonwillison.net/@simon/112497735961388213">recipe for chocolate covered pretzels</a>:</p>
<blockquote>
<p>Gently wipe any fog away and pour the warm chocolate mixture over the bridge/brick combination. Allow to air dry, and the bridge will remain accessible for pedestrians to walk along it.</p>
</blockquote>
<p>UPDATE: I think the experimental model is <a href="https://twitter.com/simonw/status/1794162704711893298">no longer available</a>, approximately 24 hours after release. We'll miss you, Golden Gate Claude.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   08:17 &bull;
   11 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 23. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-94'>The Pulse #94: OpenAI’s ethics crisis</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Claims of predatory stock clawback clause and unethical use of an actor’s voice plague leading AI startup. Also: Microsoft’s urgent focus on security.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering insights, patterns, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Today, we cover:</p>  <p>Industry pulse. Replit pivots from “hobbyist coders” to enterprise, Microsoft takes on Apple with ARM64 laptop, Vercel’s late-stage funding round, GitHub Copilot extensions, and more.</p>  <p>OpenAI’s ethics crisis. Déjà vu: six months after OpenAI’s board booted out Sam Altman as CEO for “not being consistently candid in his communications,” Altman (since restored to his old role) faces new but familiar allegations. This time, he insists he knew nothing of predatory vested stock clawback terms, or that the demo voice of ChatGPT-4o sounds almost identical to Scarlett Johansson in the movie, ‘Her.’</p>  <p>Microsoft’s urgent security focus. After a string of embarrassing, high-profile security incidents, Microsoft is making security a top priority and putting its money where its mouth is, by linking executives’ compensation to security milestones. Tech companies might be wise to consider taking security as seriously as Microsoft.</p>   1. Industry pulse  Replit pivots from consumer to enterprise focus <p>Replit is a software development and deployment platform offering an online integrated development experience (IDE.) Founded in 2016, the company raised $222M in funding and was valued just above $1B in early 2023. In 2021, Replit’s vision was to <a href="https://blog.replit.com/b">bring the “next billion” software creators online</a>. During the 2023 fundraising, the stated focus was to <a href="https://blog.replit.com/b-extension">expand cloud services and AI development</a>.</p><p>There is a lot to like about Replit in the boldness of its vision, the generous free plan for developers, and a friendly pricing model. They also treat employees well; in November 2023 the company <a href="https://newsletter.pragmaticengineer.com/i/138735980/replit-offers-liquidity-to-employees-via-a-tender-offer">offered a total of $20M in liquidity</a> to current and former employees.</p><p>Unfortunately, last week the company <a href="https://x.com/amasad/status/1791155645091283311">announced</a> letting go 20% of staff (30 people) and that it is switching focus to enterprise customers: not the “next billion software creators,” but “knowledge workers” and the “Fortune 500.”</p><p>To me, this makes sense. The company has built neat AI capabilities – a tool called <a href="https://replit.com/ai">Replit AI</a> – but AI is expensive to build and run. In order to have any shot at turning a profit, Replit needs professionals to pay more. There is little point trying to reach a billion or more developers if the company’s future is uncertain. Should Replit succeed in building a viable business from enterprise customers, I’m sure they will consider re-focusing on the “software creator” market.</p><p>Anecdotally, I’ve heard about the usage of Replit by startups and scaleups. A scaleup CEO told me they pay for Replit so they can hack together things; sometimes, when the engineering team says that building a prototype will take weeks to complete, the CEO spins up a Replit service, and builds a prototype of the project in a few hours which gets the point across. This CEO tells me they can’t roll out Replit to their company because it’s a “hobbyist product” without sufficient enterprise features. Well, this seems to be exactly what Replit is changing!</p><p>I expect more consumer-focused AI startups to pivot into enterprise. A takeaway from Replit is that AI-powered products are a hard sell for consumers, who are unlikely to pay more for them. For the economics to work, enterprise customers are needed, otherwise big trouble awaits down the line. Replit is making this pivot when it’s in a strong financial position. Good luck to the team in executing this shift!</p> Microsoft takes on Apple with ARM64 Surface Laptop 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-94">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:35 &bull;
   12 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://benwerd.medium.com/dispatches-from-the-media-apocalypse-221ea95ac89b?source=rss-3b16402f5b9d------2'>Dispatches from the media apocalypse</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

      In saving news, culture is key.  Continue reading on Medium »  
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/0*KIF7onSLS06mmT9A.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   <p><a href="https://benwerd.medium.com/dispatches-from-the-media-apocalypse-221ea95ac89b?source=rss-3b16402f5b9d------2"></a></p><p>In saving news, culture is key.</p><p><a href="https://benwerd.medium.com/dispatches-from-the-media-apocalypse-221ea95ac89b?source=rss-3b16402f5b9d------2">Continue reading on Medium »</a></p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/0*KIF7onSLS06mmT9A.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:02 &bull;
   12 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/dispatches-from-the-media-apocalypse'>Dispatches from the media apocalypse</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
   Without serious intervention, newsrooms are going to disappear. Changes to social media and the advent of generative AI threaten their businesses and the impact of their work. They need to own their online presences outright and build direct relationships with their communities—and they need to do it now.  Social media audiences are plummeting. Less than 35% of internet searches lead users to 
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://werd.io/file/664f4ae345efd31a4a05dd22/thumb.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
<p></p><p>Without serious intervention, newsrooms are going to disappear. Changes to social media and the advent of generative AI threaten their businesses and the impact of their work. They need to own their online presences outright and build direct relationships with their communities—and they need to do it now.</p><p>Social media audiences are plummeting. <a href="https://searchengineland.com/zero-click-google-searches-rose-to-nearly-65-in-2020-347115">Less than 35% of internet searches lead users to click on a website.</a> The views and engagement that newsrooms depend upon to survive are disappearing.</p><p>It’s happening quickly. <a href="https://x.com/maxwelltani/status/1793291963292082564">Semafor’s Max Tani reported recently</a>:</p><blockquote><p>Washington Post CEO Will Lewis is introing the paper’s new “Build It” plan today. In a meeting with staff, he noted that the paper lost $77 million over the past year, and saw a 50% drop off in audience since 2020: “To be direct, we are in a hole, and we have been for some time."</p></blockquote><p>Addressing this challenge will require radical changes to how newsrooms invest in and build technology.</p><p>In this post, I’ll attempt to describe the challenges in more detail and then discuss how they can be more adequately addressed.</p> Some context: my move into news <p>I’ve recently gained a new perspective on these challenges. For over a decade, I’ve worked adjacent to news and journalism. I’ve seen the industry as an engineer, startup founder, product lead, investor, and advisor. More recently, I decided I could be more useful in directly leading technology efforts inside newsrooms. It’s been eye-opening, rewarding work.</p><p>My experience alongside news was diverse. I built product for newsrooms, founded a startup used by public media, invested in early stage media startups, and have taught human-centered product design to teams at organizations like the New York Times and the Associated Press, as well as at institutions like <a href="https://werd.io/2020/designing-for-equity">the Newmark School of Journalism</a> and the Harvard Kennedy School of Government. I’ve built software, founded, grown, and supported startups, and taught product design to some of the biggest names in journalism.</p><p>My immersion inside newsrooms has been much more recent. <a href="https://propublica.org">ProPublica</a> investigates abuses of the public trust by government, businesses, and other institutions. I’ve worked on technology strategy for the last year, first as a contractor, and now as its <a href="https://www.propublica.org/people/ben-werdmuller">Senior Director of Technology</a>. Before that, I was <a href="https://19thnews.org/2022/03/19th-ben-werdmuller-chief-technology-officer/">the first CTO</a> at <a href="https://19thnews.org">The 19th</a>, which reports on the intersection of gender, politics, and power.</p><p>I made this career shift at a pivotal moment for journalism—though it seems every moment for journalism over the last fifteen years has felt pivotal. The industry has struggled to weather the seismic shifts brought about by the internet, which have impacted its business, the state of our politics, and public discourse. It’s been a struggle for decades.</p> The audience threat <p>It’s getting harder and harder for newsrooms to reach their audiences — and for them to sustain themselves.</p><p>I’ve often remarked that journalism treats the internet as something that happened to it rather than something it can actively shape and build, but it at least had some time to adjust to its new normal. The internet landscape has been largely static for well over a decade — roughly from the introduction of the iPhone 3G to Twitter’s acquisition by Elon Musk. People used more or less the same services; they accessed the internet more or less the same way. Publications and online services came and went, but the laws of physics of the web were essentially constants.</p><p>Over the last year in particular, that’s all changed. Shifts in the social media landscape and the growing popularity and prevalence of generative AI have meant that the rules that newsrooms began to rely on no longer hold.</p><p>At their heart, online newsrooms have a reasonably simple funnel. They publish journalism, which finds an audience, some of which either decide to pay for it or view ads that theoretically cover the cost of the work. Hopefully, they will make enough money to publish more journalism.</p><p>This description is a little reductive: there are lots of different revenue models in play, for one thing. I’m particularly enamored with patronage models that allow those with the means to support open-access journalism for anyone to read freely. Still, some are entirely ad-supported, some are sponsored, and others are protected behind a paywall (or some combination of the above). For another, journalism isn’t always the sole driver of subscriptions. The New York Times receives tens of millions of subscribers <a href="https://www.axios.com/2024/01/29/wordle-nyt-games-news-media-layoffs">from its games</a> like Wordle and Connections, as well as <a href="https://digiday.com/media/the-new-york-times-cooking-is-letting-readers-taste-test-recipes-over-text-to-drive-subscriptions/">its Cooking app</a>.</p><p>Still, there are two pivotal facts for every newsroom: their work must reach an audience, and someone must pay for it. The first is a prerequisite of the second: if nobody discovers the journalism, nobody will pay for it. So, reaching and growing an audience is crucial.</p><p>For the last decade and a half, newsrooms have used social media and search engines as the primary way to reach people. People share stories across social media—particularly Facebook and Twitter—and search for topics they’re interested in. It’s generally worked.</p><p>Over the last year, social media has radically fragmented. Twitter transformed into X under its new management; users began to flee the platform in the face of more toxic discourse, and <a href="https://www.axios.com/2023/10/26/x-twitter-usage-statistics-elon-musk-owner">active use plummeted</a>. Facebook <a href="https://www.aljazeera.com/features/2023/5/9/facebook-has-3-billion-users-but-younger-users-quitting-platform">is slowly declining</a> and <a href="https://pressgazette.co.uk/media-audience-and-business-data/media_metrics/facebooks-referral-traffic-for-publishers-down-50-in-12-months/">referrals to news sites have fallen by 50% over the last year</a>. Instagram is not in decline. Still, it’s harder to post links to external sites there, which means that while newsrooms can reach users, they have more difficulty converting them to subscribers.</p><p>On top of these changes, we’ve also seen the rise of Threads, Mastodon, and Bluesky, as well as a long tail of other social apps, platforms, and forums on which to reach people. Audiences on social media used to be found in a very small number of places and are now spread out across very different platforms. The fediverse and AT Protocol also yield different problems: which instance should a newsroom choose to make its home? How can it measure engagement in what it posts in a decentralized system so that it knows what’s working and where it should continue to invest its meager resources?</p><p>Much has been written about newsrooms’ inability to move away from X even as it has become a hotbed of white supremacy and far-right rhetoric. The honest truth is that it still drives significant traffic to their websites, and in an environment where traffic referrals are dropping overall, intentionally further deepening the traffic shortfall is understandably not a career risk newsroom leaders are willing to make.</p><p>Social media isn’t the only way newsrooms are finding it harder to find an audience. Even search engines, long the stalwarts of the web, are moving away from referring traffic.</p><p>As search engines move to make AI-driven answers more prominent than links to external websites, they threaten to reduce newsroom audiences, too. <a href="https://searchengineland.com/zero-click-google-searches-rose-to-nearly-65-in-2020-347115">More than 65% of Google searches already ended without a click to an external site</a>. Now, it’s planning to <a href="https://www.engadget.com/google-search-will-now-show-ai-generated-answers-to-millions-by-default-174512845.html">roll out AI-driven answers to over a billion people</a>. It’s not that other links are going away entirely. Still, because AI answers are the most prominent information on the page, clickthroughs to the external websites where the answers were found initially will be significantly reduced.</p><p>A similar dynamic is at play with the rise of AI services like ChatGPT, emerging as stiff competition for search engines like Google. These services answer questions definitively (although not always correctly), usually with no external links on the page. ChatGPT could learn from a newsroom’s articles and display information gleaned from an expensive investigative story while never revealing its source or allowing readers to support the journalism.</p><p>Generative AI models seem like magic: they answer questions succinctly, in natural language, based on prompts that look a lot like talking to a real human being. They work by training a neural network on a vast corpus of information, often obtained by crawling the web. Based on these enormous piles of data, AI engines answer questions by predicting which word should come next: a magic trick of statistics empowered by something close to the sum total of human knowledge.</p><p>That’s not hyperbole. It’s not a stretch to say that OpenAI’s ChatGPT and Google’s Gemini were trained on most of the internet, including websites, published books, videos, articles, art, science, and research. They couldn’t function without this data — but, ironically, they rarely credit their sources for any of it. Users see the benefit of fast answers; the sources of that information are starved of oxygen.</p><p>We’re at the foothills of both changes: social media is likely to fragment further, and generative AI will become even more prevalent as it becomes more powerful. Newsrooms can no longer rely on their old tactics to reach their audiences, and they will need to build new tactics that take these trends into account if they hope to survive.</p> Some models are more resilient than others <p><a href="https://www.cjr.org/analysis/alexandra-smith-metrics-readership-19th-impact.php">The 19th’s Alexandra Smith recently wrote about the state of play in Columbia Journalism Review</a>:</p><blockquote><p>In our current reality, journalism exists in various formats splintered across platforms and products. People are just as likely to get their news on Instagram as from a news website. It no longer makes sense to rely primarily on measuring readership by traditional website metrics.</p></blockquote><p>This is a depressing fact if you rely on paywalled subscriptions or ad impressions. Nobody’s looking at your ads if they’re consuming your journalism off-platform, and how can you possibly get someone to subscribe if they never touch your app or website? Instagram and TikTok don’t have built-in subscriptions.</p><p>Over the years, many people have suggested micropayments — tiny payments you make every time you read a news article anywhere — but this depends on everyone on the web having some kind of micropayment account that is on and funded by default and the platforms all participating. It’s a reasonable idea if the conditions are right, <a href="https://molodtsov.me/2023/05/why-micropayments-dont-work/">but the conditions will never be right</a> — and, like subscription models, it shuts out people who can’t pay, who are often the people most in need of public service journalism to begin with.</p><p>For newsrooms like The 19th, the picture is much rosier: like most non-profit newsrooms, it depends on donors who support it based on its journalistic impact. (The same is true of ProPublica, my employer.) That impact could occur anywhere, on any platform; the trick is to measure it so donors can be informed. <a href="https://www.cjr.org/analysis/alexandra-smith-metrics-readership-19th-impact.php">Alexandra developed a new metric, Total Journalism Reach, that captures precisely this</a>:</p><blockquote><p>Right now, it includes website views; views of our stories that are republished on other news sites and aggregation apps, like Apple News; views of our newsletters based on how many emails we send and their average open rates, reduced for inflation since Apple implemented a new privacy feature; event attendees; video views; podcast listens; and Instagram post views.</p></blockquote><p>This is clearly valuable work that will help newsrooms like The 19th prove their impact to current and potential donors. The quote above doubles as a useful example of the places The 19th is reaching its audience.</p><p>It’s worth considering how these might change over time. Some of the media Alexandra describes are inside The 19th’s control, and some are less so.</p> Supplier power <p>In his classic piece ‌<a href="https://hbr.org/1979/03/how-competitive-forces-shape-strategy">How Competitive Forces Shape Strategy</a>, Michael Porter described <a href="https://en.wikipedia.org/wiki/Porter%27s_five_forces_analysis">five forces that shape competitive strategy</a>. One of them is supplier power: the ability of providers of essential inputs to a business to exert influence over the organization. If suppliers to the industry have too much power — because there are few alternatives, for example — they can effectively force the company’s strategy by raising costs or enforcing adverse policies.</p><p>Newsrooms’ platforms for reaching their audiences, such as social media and Apple News, currently have outsized supplier power over the journalism industry. As a result, the industry is disproportionately vulnerable to the effects of business decisions made by the owners of those platforms.</p><p>In April, Instagram <a href="https://www.yahoo.com/news/instagrams-new-automatic-filter-for-political-content-prompts-pushback-from-some-users-172033054.html?guccounter=1">introduced a new automatic filter, switched on by default, to remove political content</a>, which affected many newsrooms, and illustrates the kind of changes service providers can make on a whim.</p><p>Newsrooms on Apple News tend to see a multiple of the number of reads they see on their websites, but Apple could pull the product tomorrow. Even today, the number of views you get highly depends on which stories the Apple News team chooses to highlight. Ads in publications on Apple News need to use Apple’s ad network. It’s a closed shop. Apple News is only successful because it comes installed by default on Apple devices; hundreds of similar news aggregators have all failed to survive in their own right. It’s a precarious place to hang your hat.</p><p>We’ve already discussed the impact of search engine design decisions like prioritizing AI over click-through rates. Only one search engine is prominent enough to have disproportionate supplier power: a position Google has bought <a href="https://www.theverge.com/2023/10/27/23934961/google-antitrust-trial-defaults-search-deal-26-3-billion">by spending over $21 billion a year to be the default search engine in every web browser</a>.</p><p>However, not all conduits to readers have this outsized supplier power as a feature. Social media platforms, search engines, and news aggregators are all run by wealthy individual companies like X, Meta, Google, and Apple, who have the potential to exert their power. If you choose to leave them for any reason, you’re also leaving behind the relationships you’ve built up with your audience there: there’s no audience portability.</p><p>In contrast, email, podcasts (real podcasts, not the single-platform kind where you ink an exclusive deal with Spotify or Audible), and the web are well-used methods to reach audiences that aren’t owned by any platform. There are certainly market leaders for each communication type. Still, each is based on an open protocol that no single company controls — which means, for those methods, no supplier can exert adverse supplier power. If one service provider misbehaves, you can simply switch to another without losing functionality. You can bring your audience with you. They’re safer methods, as long as enough readers want to be reached in those ways.</p><p>That’s why so many publications have focused their strategies on their email newsletters. Everyone already has an email address, and (barring technical difficulties) if a publisher sends a subscriber a message, they’re guaranteed to receive it. Moreover, people engaged enough to hit the “subscribe” button are far more likely to convert to donors or upgrade to a paid subscription.</p><p>Newsletters, unfortunately, <a href="https://mkarolian.substack.com/p/newsletter-winter-is-coming">are also in decline</a>. Open rates have fallen over the last decade; Gmail’s dominant position and aggressive filtering have made it harder for newsletters to be noticed; there’s more competition for attention. There aren’t any great ways for new readers to discover newsletters — those subscription pages are subject to the same internet traffic dynamics as articles. It’s getting harder and harder to direct new visitors to subscribe, which is why we see more overt “please subscribe” popup overlays on news sites. The focus has needfully shifted to converting more existing subscribers into donors or customers rather than widening the funnel and finding more newcomers.</p><p>Newsrooms need alternative media that allow them to make direct connections with their audiences. These media must be free from undue supplier power and have a large base of existing users that can be tapped into.</p><p>So what else is out there?</p><p>The answer is not much. Yet.</p> The innovation squeeze <p>Most non-profit newsrooms have tiny technology teams. The 19th, when I was CTO, had two engineers in addition to me; ProPublica has four. (Other interactive developers work on standalone stories but don’t address platform needs.) In contrast, I led a team of twenty-two engineers at the last startup I worked at, and we had over a hundred at Medium.</p><p>To bridge that gap, there is a small community of digital agencies that make supporting newsroom platform needs a core part of their business. Probably the most famous are <a href="https://alley.com/">Alley</a> and <a href="https://upstatement.com/">Upstatement</a>, but there are around a dozen more that are actively used by newsrooms.</p><p>They do beautiful work and are an excellent way for a newsroom to start strong with a modern brand and a well-functioning web platform. I strongly recommend that a new newsroom consults with them.</p><p>There is an emerging dynamic, though, where the technology vision for a newsroom is outsourced to the agencies. As we’ve discussed, a newsroom’s success and impact depend highly on core internet technologies like the web and email. Newsrooms quite reasonably spec and build a platform based on what will work well today. However, because the vision and expertise for harnessing the internet lie with the agencies, they don’t have any meaningful technology capability for innovating around what will work well tomorrow.</p><p>Newsrooms absolutely need to focus on today. That’s an obvious prerequisite: they must meet their audiences, subscribers, and donors where they’re at right now. However, they also need to be aware of what is coming down the road and prepared to experiment with, engage with, and potentially help shape new technologies that could impact their businesses in the future. If the internet changes, they need to be ready for it. To reference an overused Wayne Gretzky quote: you need to skate to where the puck will be, not where it is right now.</p><p>Nobody knows for certain where the puck will be. That means newsrooms need to make bets about the future of technology — which, in turn, means they must have the capacity to make bets about the future of technology.</p><p>Most newsrooms already have technical staff who maintain their websites, fix broken platform stacks, and build tools for the newsroom. These staff must also highlight future business risks and allow them to experiment with new platform opportunities. In a world where newsrooms rely on the internet as a publishing mechanism, technology expertise must be integral to their strategy discussions. And because technology changes so quickly and unpredictably, maintaining the time, space, and intellectual curiosity for experimentation is critical.</p> Nothing will work, but anything might <p>Experimentation doesn’t need to be resource-intensive or time-consuming. Alongside in-house expertise, the most important prerequisite is the willingness of a newsroom to test: to say “yes” to trying something out, but being clear about the parameters for success, and always rooting success or failure in a concrete understanding of their communities.</p><p>I’ve written before about how, if the fediverse is successful, <a href="https://werd.io/2024/the-fediverse-for-media-organizations">it will be a powerful asset to media organizations</a> that combines the direct relationship properties of email with the conversational and viral properties of social media. At the same time, there’s no doubt that the network is relatively small today, that the experience of using Mastodon falls short of corporate social networks like the Twitter everyone remembers, and that <a href="https://anderegg.ca/2024/05/19/whats-up-with-mastodon-and-referer-headers">features like blocking referrer data</a> makes life much harder for audience teams. There are lots of good reasons for a resource-strapped management team to say “no” to joining it.</p><p>At the same time, because it has the potential to be interesting, some newsrooms (<a href="https://newsie.social/@ProPublica">including my employer</a>) are experimenting with a presence. The ones who make the leap are often pleasantly surprised: engagement per capita is dramatically higher, particularly around social justice topics. Anecdotally, I discovered that posting a fundraising call to action to the network yielded more donations than from every other social network — combined.</p><p>It’s worth looking at <a href="https://restofworld.org/platforms/">Rest of World’s “More Ways to Read” page</a> — a massive spread of platforms that runs the gamut from every social network to news apps, messaging platforms, audio, newsletters, and RSS feeds. The clear intention, taken seriously, is to meet audiences where they’re at, even if some of those networks have not yet emerged as a clear winner. All this from <a href="https://restofworld.org/about/team/">a tiny team</a>.</p><p>However, experimenting isn’t just about social media. It’s worth experimenting with anything and everything, from push notifications to website redesigns that humanize journalists to new ways for communities to support the newsroom itself.</p><p>On the last point, I’m particularly enamored with how The 19th allows members <a href="https://19thnews.org/membership/">to donate their time instead of money</a>. Understanding that not everyone who cares about their mission has discretionary spending ability, they’re harnessing their community to create street teams of people who can help promote, develop, and share the work in other ways. It’s brilliant — and very clearly something that was arrived at through an experimental process.</p><p>I learned a formal process for human-centered experimentation as a founder at <a href="https://open.nytimes.com/thinking-like-entrepreneurs-our-experience-with-matter-a-design-thinking-vc-program-b836f4271c6a?gi=697a6a08d381">Matter</a>, the accelerator for early-stage media startups, which changed the way I think about building products forever. A similarly powerful program is now taught <a href="https://journalism.columbia.edu/news/matter-ventures-co-founder-corey-ford-lead-columbias-sulzberger-leadership-program">as Columbia Journalism School’s Sulzberger Fellowship</a>. If you can join a program like this, it’s well worth it, but consultants like <a href="https://www.linkedin.com/in/trannha/">Tiny Collaborative’s Tran Ha</a> and <a href="https://www.linkedin.com/in/coreyjford/">Matter’s Corey Ford</a> are also available to engage in other ways. And again, the most important prerequisites are in-house expertise and the willingness to say “yes”.</p><p>To achieve this, they must shift their cultures. The principles of experimentation, curiosity, and empathy that are the hallmarks of great journalism must also be applied to the platforms that power their publishing and fundraising activities. They must foster great ideas, wherever they come from, and be willing to try stuff. That inherently also implies building a culture of transparency and open communication in organizations that have, on average, underinvested in these areas. As Bo Hee Kim, then a Director of Newsroom Strategy at the New York Times, <a href="https://www.niemanlab.org/2020/12/newsrooms-create-an-intentional-and-collaborative-culture/">wrote back in 2020</a>:</p><blockquote><p>Companies will need to address broader issues with communication, access, and equity within the workplace. Leaders will need to believe that newsroom culture has a bigger impact on the journalism than they understood in previous years — that a strong team dynamic is as important as their sharp and shiny stars. Managers are key to this transition and will need to reset with a new definition of success, followed by support and training to change.</p></blockquote><p><a href="https://hbr.org/2019/01/the-hard-truth-about-innovative-cultures">Gary P. Pisano in Harvard Business Review</a>:</p><blockquote><p>Too many leaders think that by breaking the organization into smaller units or creating autonomous “skunk works” they can emulate an innovative start-up culture. This approach rarely works. It confuses scale with culture. Simply breaking a big bureaucratic organization into smaller units does not magically endow them with entrepreneurial spirit. Without strong management efforts to shape values, norms, and behaviors, these offspring units tend to inherit the culture of the parent organization that spawned them.</p></blockquote><p>Creating an innovative culture is complex, intentional work. But it is work that must be done if news organizations are to innovate and, therefore, survive.</p> Conclusion <p>The internet is changing more rapidly than it has in years, creating headwinds for newsrooms and jeopardizing independent journalism’s viability. We need those organizations to exist: they <a href="https://drnews.substack.com/p/how-loud-does-the-watchdog-bark">reduce corruption</a>, <a href="https://journals.sagepub.com/doi/10.1177/07395329211014968">inform the voting public</a>, and <a href="https://www.theajp.org/news-insights/the-state-of-local-news-and-why-it-matters/#sectionfour">allow us to connect with and understand our communities</a> in vital ways.</p><p>These organizations must own their digital presence outright to shield themselves from risks created by third parties that wield outsized supplier power over their business models. They must build direct relationships with their communities, prioritizing open protocols over proprietary systems. They need to invest in technology expertise that can help them weather these changes and make that expertise a first-class part of their senior leadership teams.</p><p>To get there, they must build an open culture of experimentation, where transparency and openness are core values cemented through excellent, intentional communication. They must be empathetic, un-hierarchical workplaces where a great idea can be fostered from anywhere. They must build a mutual culture of respect and collaboration between editorial and non-editorial staff and ensure that the expertise to advise on and predict technology challenges is present and well-supported in-house.</p><p>Experimentation and innovation are key. Newsrooms can discover practical ways to navigate these challenges by testing new strategies, technologies and mindsets. The road ahead is challenging, but with strategic investments and a forward-looking approach, newsrooms can continue to fulfill their vital role in a well-functioning democratic society. The best time for action was ten years ago; the second best time is now.</p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://werd.io/file/664f4ae345efd31a4a05dd22/thumb.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:02 &bull;
   12 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 22. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/05/mini-summit-six/'>Mini Summit Six</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A rough transcript of the sixth and final Extension Ecosystem Mini-Summit,
in which we review potential topics for the in-person summit at PGConf.dev
and discuss how to organize it.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Last week, a few members of the community got together for for the sixth and
final <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">Postgres Extension Ecosystem Mini-Summit</a>. Follow these
links for the video and slides:</p>
 
 <a href="https://www.youtube.com/watch?v=6o1N1-Eq-Do">Video</a> 
 <a href="https://justatheory.com/shared/extension-ecosystem-summit/organizing-topics.key">Keynote</a> 
 <a href="https://justatheory.com/shared/extension-ecosystem-summit/organizing-topics.pdf">PDF Slides</a> 
 
<p>Or suffer through my interpolation of YouTube’s auto-generated transcript,
interspersed with chat activity, if you are so inclined.</p>
 Introduction 
 
 I opened the meeting, welcomed everyone, and introduced myself as host. I
explained that today I’d give a brief presentation on the list of issues I
I’ve dreamed up and jotted down over the last couple mini-summits as
possible potential topics to take on at <a href="https://www.pgevents.ca/events/pgconfdev2024/schedule/session/191/" title="PGConf.dev: Extensions Ecosystem Summit: Enabling comprehensive indexing, discovery, and binary distribution">the Summit</a> in Vancouver on May
28th. 
 
 Presentation 
 
 
<p>These are things that I’ve written down as I’ve been thinking through the
whole architecture myself, but also that come up in these Summits. I’m
thinking that we could get some sense of the topics that we want to
actually cover at the summit. There is room for about 45 people, and I
assume we’ll break up “unconference style” into four or five working
groups. People an move to corners, hallways, or outdoors to discuss
specific topics.</p>
 
 
<p>Recall the <a href="https://justatheory.com/2024/03/mini-summit-one/" title="Mini Summit One">first mini-summit</a> I showed a list of things that of potential
topics that might come up as we think through what’s issues in the
ecosystem. I left off with the prompt “What’s important to you?” We hope
to surface the most important issues to address at the summit and create a
hierarchy. To that end, I’ve created this Canva board<a>1</a>
following <a href="https://en.wikipedia.org/wiki/Open_space_technology" title="wikipedia: “Open space technology”">Open Space Technology</a><a>2</a> to set things up, with the
rules and an explanation for how it workjs.</p>
 
 
<p>I expect one of us (organizers) to give a brief introduction at the
start of the summit to outline the principles of <a href="https://en.wikipedia.org/wiki/Open_space_technology" title="wikipedia: “Open space technology”">Open Space Technology</a>,
which are similar to unconferences.</p>
 
 
<p><a href="https://en.wikipedia.org/wiki/Open_space_technology" title="wikipedia: “Open space technology”">Open Space Technology</a> principles are:</p>
 
 Whoever comes are the right people 
 Whatever happens is the only thing that could happen 
 Whenever it starts at the right time (but we start at 2 p.m. and we
have only three hours so we’ll try to make the best of it) 
 When it’s over it’s over 
 And whatever happens is the right place 
 
 
 
<p>There is also a “Law of Mobility”. If you start out interested in one
topic and attending a session or discussion about one topic, and you
decide you want to do something else, you can wander over to another
session . Open Space Technology calls these people “bumblebees” who
cross-pollinate between topics. “Butterflies” are the people who hover
around a particular topic to make it happen.</p>
 
 
<p>And “Come to be Surprised” about what will come up.</p>
 
 
<p>I’ve split potential topics into topics in Post-its. we might have four or
five spaces. Pick a space, pick a session; we have two two-hour-long
sessions. I assume we’ll have 15-30 minutes to open the Summit, do intros,
and split up the sessions; then have people do an hour on one topic and an
hour on a second topic. At the end, we’ll do the readout in which we talk
about decisions we came to.</p>
 
 
<p>If you’re interested in facilitating any of these topics, simply drag it
in and stick your name on it.</p>
 
 
<p>First I thought I’d briefly go over the list of topics as I’ve imagined
them. I posted the list on Slack a couple weeks ago and added to it as
things have come up in the discussions. But I want to give a high level
view of what these brief descriptions mean.</p>
 
 
<p>This is ad-hoc; I don’t have anything super planned. Please feel free to
jump in at any time! I think I’ve turned on “talking permitted” for
everybody, or stick your hand up and we’ll be glad to figure out other
stuff, especially if you’re thinking of other topics or related things, or
if you think things should be merged.</p>
 
 
<p>Any questions or thoughts or comments?</p>
 
 
<p>I put the topics in broad categories. There’s some crossover, but the the
first one I think of is metadata. I’ve thought about metadata a fair bit,
and drafted an RFC for the kinds of things to put in an updated metadata
standard, like:</p>
 
 
<p>How do you specify third-party dependencies? For example, PostGIS
depends on additional libraries; how can those be specified in an
ideally platform neutral way within the metadata?</p>
 
 
<p>How to specify the different types of extensions there are? Stephen
wrote a blog post last year about this: you have <code>CREATE EXTENSION</code>
extensions, <code>LOAD</code> command extensions, background workers,
applications, and more. You have things that need
<code>shared_preload_libraries</code> and things that don’t. How do we describe
those things about an extension within a distribution package?</p>
 
 
<p>Taxonomies have come up a few times. PGXN currently allows extension
authors to put an arbitrary number of tags into their <code>META.json</code>
file. Maybe in part because of the precedent of the stuff that that I
released early on, people mostly put stuff in there to describe it,
like “fdw”, or “function” or “JSON”. Some of the newer uh binary
distribution packaging systems, in particular Trunk, have a curated
list of categories that they assign. so there might be different ways
we want to classify stuff.</p>
<p>Another approach is crates.io, which has a canonical list of
categories (or “slugs”), that authors can assign. These are handy they
group things together in a more useful way, like “these are related to
data analytics” or “these are related to Vector search” — as opposed
to the descriptive tags PGXN has now. So, what ought that to look
like? What kind of controls should we have? And who might want to use
it?</p>
 
 
<p>How would we specify system requirements. For example “this package
requires only a subset of OSes”, or the version of an OS, or the
version of postgres, or CPU features. Steven’s mentioned vector-based
ones a few times, but there’s also things like encryption instructions
provided by most chips. Or the CPU architecture, like “this supports
aarch64 but not amd64.” How should we specify that?</p>
 
 
<p>I covered  categorization under taxonomies</p>
 
 
<p>Versioning. I blogged about this a couple months ago. I’m reasonably
sure we should just stick to SemVer, but it’s worth bringing up.</p>
 
 
 
 
<p>Thoughts on metadata, or stuff I’ve left out? This is in addition to the
stuff that’s in the <a href="https://pgxn.org/spec/"><code>META.json</code> spec</a>. It leaves room for
overlap with core stuff. How do we create one sort of metadata for
everything, that might subsume the control file as well as the metadata
spec or <code>trunk.toml</code>?</p>
 
 Jeremy S in chat: So far this is seeming like a good recap of
ground that’s been covered, questions &amp; topics that have been
raised. Great to see how broad it’s been 
 
 
 
<p>The next category is the source registry. This is thinking through
how we should evolve the PGXN root registry for distributing
extension source code. There are questions like identity, namespacing,
and uniqueness.</p>
 
 
<p>These are broad categories but identity is how do you identify
yourself to the system and claim ownership over something.</p>
 
 
<p>What sort of namespacing should we use? Most systems, including PGXN,
just use an arbitrary string and you own a string from [first
release]. But other registries, like Go, allow you to use domain-based
namespacing for packages. This is really nice because it allows a lot
more flexibility, such as the ability to switch between different
versions or forks.</p>
 
 
<p>Then there’s the level of uniqueness of the namespacing. This is kind
of an open question. Another another approach I thought of is that,
rather than string that names your extension distribution being
unique, it could be your username and the string. That makes it
easier when somebody abandoned something and somebody else forks it
and has a new username. Then maybe people can switch more easily. To
be able to account for and handle that sort of evolution in a way that
single string uniqueness makes trickier.</p>
 
 
<p>Distributed versus centralized publishing. I’ve written about this a
couple times. I am quite attracted to the Go model where packages are
not centrally distributed but are in three or four supported Version
Control Systems, and as long as they use SemVers and appropriate tags,
anybody can use them. The centralized index just indexes a package
release the first time it’s pulled. This is where host names come into
play as part of the namespacing. It allows the system to be much more
distributed. Now Go caches all of them in a number of different
regions, so when you download stuff it goes through the Go stuff. When
you say “give me the XYZ package,” it’ll generally give you the cached
version, but will fall back on the repositories as well. So there’s
still the centralized stuff.</p>
<p>I think there’s a a lot to that and it goes along with the namespacing
issue. But there are other ideas at play as well. For example, almost
all the other source code distribution systems just use a centralized
system: crates.io, CPAN, npm, and all the rest.</p>
<p>And maybe there are other questions to consider, like is there some
sort of protocol we should adopt as an abstraction, such as Docker,
where Docker is not a centralized repository other than
hub.docker.com. Anyone can create a new Docker repository, give it a
host name, and then it becomes something that anybody can pull from.
It’s much more distributed. So there are a number of ideas to think
through.</p>
 
 
<p>Binary packaging and distribution patterns. I have a separate slide
that goes into more detail, but there are implications for source code
distribution, particularly with the metadata but perhaps other things.
We also might want to think through how it might vary from source
distribution.</p>
 
 
<p>Federated distribution gets at the Docker idea, or the <a href="https://www.ongres.com/blog/why-postgres-extensions-should-be-distributed-and-packaged-as-oci-images/" title="Why Postgres Extensions should be packaged and distributed as OCI images">OCI idea</a> that
Alvaro proposed a few weeks ago. Stuff like that.</p>
 
 
<p>What services and tools to improve or build. This goes to the
fundamental question of why we’ve had all these new packaging systems
pop up in the last year or so. People were saying “there are problems
that aren’t solved by PGXN.” How do we as a community collectively
decide what are the important bits and what we should build and
provide. Features include developer tools, command line clients,
search &amp; browse, and discovery.</p>
 
 
<p>Stats, reports, and badging. This is another fundamental problem that
some of the emerging registries have tried to to address: How do you
find something? How do you know if it’s any good? How do you know
who’s responsible for it? How do you know whether there’s some
consensus across the community to use it? The topic, then, is what
sort of additional metadata could we provide at the registry level to
include some hint about these issues. For example, a system to
regularly fetch stars and statistical analysis of a GitHub or a
Bitbucket project. Or people wanted review sites or the ability to
comment on on systems.</p>
<p>There’s also badging, in particular for build and test matrices for
extensions that will not only encourage people to better support broad
arrays of versions of Postgres and platforms. There could be badges
for that. so you can see how well an extension supports various
platforms. And any other sort of badging, like quality badging. The
idea is a brainstorming of what sorts of things might be useful there,
and what what might be best to build first, might be the the low
hanging fruit.</p>
 
 
 
 
<p>Any questions, comments,m thoughts, additional suggestions on the root
registry?</p>
 
 
 Interlude 
 
 
<p>Steven Miller: So the idea is there are topics on the left and then they
get lined up into the schedule? So there are five five different rooms, so
horizontally aligned it4ms are at the same time?</p>
 
 
<p>David Wheeler (he/him): Correct. These are session one and these are
session two.</p>
 
 
<p>Jeremy S: I was kind of waiting to jump to that. It seemed like you were
just doing a review of all the topics we’ve covered, but I was waiting
until till you got through everything to bring that up.</p>
 
 
<p>Steven Miller: Oh yeah, good call, good call.</p>
 
 
<p>Jeremy S: I have the same kind of question/concern. This is a great list
of topics, now what do we want to do with the time in Vancouver? David, do
you think we need to go through everything on the list? How do you want to
spend the time today?</p>
 
 
<p>David Wheeler (he/him): I was trying to do a quick review just so people
knew what these words mean. If you all feel like you have a good idea, or
you want to add topics of your own, please do!</p>
 
 
<p>Jeremy S: Like I commented in the chat, it’s amazing to see how much
ground we’ve covered, and it’s good to have a a quick recap. It’s 9:22
right now Pacific time — 22 after the hour wherever you are — I just
want to make sure we don’t run out of time going through everything.</p>
 
 
<p>David Wheeler (he/him): I agree, I’ll make it work. I can speed up a
little. I know I can be verbose about some of this stuff.</p>
 
 
<p>David G. Johnson: Unless the ones from India, in which case they have half
hour time zone.</p>
 
 
<p>David Wheeler (he/him): I was gonna say! [Laughs]</p>
 
 
 Presentation Continues 
 
 
<p>Binary packaging. This is the problem that PGXMan and trunk have tried to
solve with varying degrees of success. I think it’d be worthwhile for us
to think through as a community what, ideally, should a community-provided
binary packaging system look like?</p>
 
 
<p>And what’s the format? Do we want to do tarballs, do OCI like Alvaro
proposed? Do we want something like RPM or Apt or Python wheels?
That’s a that’s actually something I’m super interested to get into.
There was a question that came up two weeks ago in Yurii’s
presentation. I think Daniele suggested that the Python wheel package
format allows you to put dynamic libs into the wheel. That’s pretty
interesting and worth looking into as well.</p>
 
 
<p>How we go about building a community-based binary packaging registry?
How do we do the build farming, what platforms and architectures and
OSes would it support, and what sort of security, trust, and
verification? And the centralization: who runs it, who’s responsible
for it, how should it work at a high level?</p>
 
 
<p>Philippe Noël in chat: Phil from ParadeDB here (pg_search,
pg_analytics, pg_lakehouse) — First minisummit I can attend, glad
to be here</p>
 
 
 
 
<p>Thank for coming, Philippe! Again, interrupt me anytime.</p>
 
 
<p>The next topic is developer tooling. Developer tooling today is kind of
all over the place. There a PGXN client, there’s the PGXN utils client
(which doesn’t compile anymore, as far as I can tell), there’s pgrx stuff,
and maybe a few other things. What sorts of tools would be useful for
developers who actually develop and build extensions?</p>
 
 
<p>CLIs and APIs can do metadata management, or scaffolding your source
code and adding new features through some sort of templating system.</p>
 
 
<p>The packaging and Publishing system based on how we uh ultimately
elect to distribute source code, and how we ultimately elect to
distribute binary code. How does that get packaged up with the
namespacing and all the decisions we made, to be as easy as possible
for the developer?</p>
 
 
<p>What build pipelines do we support? today PGXS and pgrx are perhaps
the most common, but I’ve seen GNU autoconf configure stuff and stuff
that uses Rust or Go or Python-based builds. Do we want to support
those? And how do we integrate them with our binary packaging format
and where Postgres expects to put stuff?</p>
<p>I think this is an important topic. One of the things I’ve been
dealing with as I’ve talked to the people behind Apache Age and a
couple other projects is how they put put stuff in <code>/usr/local</code> by
default. I suggest that it’d be better if it went where <code>pg_config</code>
wants to put it. How do we want to go about integrating those things?</p>
 
 
<p>Tooling for CI/CD workflows to make it as easy
as possible to test across a variety of platforms, Postgres versions, and
those pipelines.</p>
 
 
 
 
<p>Kind of a broad Community topic here. This gets to where things are
hosted. There’s a Postgres identity service that does Oauth 2; is that
something we want to plug into? Is there a desire for the community to
provide the infrastructure for the systems or for at least the core
systems of PGXN v2? Who would support it? The people who work on the
development ideally would also handle the devops, but should work closely
with whoever provides the infrastructure to make sure it’s all copacetic.
And that there’s a a plan for when something happens. People exit the
community for whatever reason; how will systems continue to be maintained?
I don’t think there’s a plan today for PGXN.</p>
 
 
<p>Another topic is documentation. How do we help engineers figure out how to
build extensions; tutorials and references for all the things and all the
various details. Do we end up writing a book, or do we just have very
specifically-focused tutorials like, “So you want to build a foreign data
wrapper; here’s a guide for that.” Or you just need to write a background
worker, here’s an example repository to clone. What should those things
look like?</p>
 
 <code>CREATE EXTENSION</code> 
 Hooks 
 Background workers 
 CLI apps/services 
 Web apps 
 Native apps 
 
<p>This also kind of covers the variety of different kinds of extensions we
might want to package and distribute.</p>
 
 
<p>Then there’s all the stuff that I filed under “core,” because I think it
impacts the core Postgres project and how it may need to evolve or we
might want it to evolve over time. One is the second extension directory;
there’s <a href="https://commitfest.postgresql.org/48/4913/" title="PostgreSQL CommitFest: Add extension_destdir GUC">a patch</a> pending now, but it’ll probably be deferred until until
Postgres 17 ships; it’s on hold while we’re in the freeze. This is a patch
that Christoph Berg wrote for the Debian distribution; it allows you to
have a second destination directory for your extensions where Postgres
knows to find stuff, including shared object libraries. This would make it
easier for projects like Postgres.app and for immutable Docker containers
to mount a new directory and have all the stuff be there.</p>
 
 
<p>I would love to see some sort of more coherent idea of what an extension
pack package looks like, where like if I install pgTAP, all of its files
are in a single subdirectory that Postgres can access. Right now it’s in
package config, and the sharedir and the libder and the docdir — it’s
kind spread all over.</p>
 
 
<p>Should there be a documentation standard, in the way you have JavaDoc and
rustdoc and Godoc, where docs are integrated into the source code, so it’s
easy to use, and there’s tooling to build effective documentation. Today,
people mostly just write short READMEs and leave it at that, which is not
really sufficient for a lot of projects.</p>
 
 
<p>There’s the longstanding idea of inline extensions that Dimitri proposed
back as far as 2013, something they called “units”. Oracle calls them
“packages” or “modules”. Trusted Language Extensions start making a stab
at the problem, trying to make something like inline extensions with the
tooling we have today. How should that evolve? What sorts of ideas do we
want to adapt to make it so that you don’t have to have physical access to
the file system to manage your extensions? Where you could do it all over
SQL or libpq.</p>
 
 
<p>How can we minimize restarts? A lot of extensions require loading DSOs in
the <code>shared_preload_libraries</code> config, which requires a cluster restart.
How can we minimize that need? There are ways to minimize restarts; it’s
just a broad category I threw in.</p>
 
 
<p>What Namespacing is there? I touched on this topic when I wrote about Go
Namespacing a while ago. My current assumption is, if we decided to
support <code>user/extension_string</code> or <code>hostname/user/extension_string</code>
namespacing for package and source distribution, Postgres itself still has
to stick to a single string. How would we like to see that evolve in the
future?</p>
 
 
<p>What kind of sandboxing, code signing, security and trust could be built
into the system? Part of the reason they’ve resisted having a second
extension directory up to now is to have one place where everything was,
where the DBA knows where things are, and it’s a lot it’s easier to manage
there. But it’s also because otherwise people will put junk in there. Are
there ideas we can borrow from other projects or technologies where
anything in some directory is sandboxed, And how is it sandboxed? Is it
just for a single database or a single user? Do we have some sort of code
signing we can build into the system so that Postgres verifies an
extension when you install it? What other kinds of security and trust
could implement?</p>
<p>This is a high level, future-looking topic that occurred to me, but it
comes up especially when I talk to the big cloud vendors.</p>
 
 
<p>An idea I had is dynamic module loading. It came up during Jonathan’s
talk, where there was a question about how one could use Rust crates in
PL/Rust, a trusted language. Well, a DBA has to approve a pre-installed
list of crates that’s on the file system where PL/Rust can load them. But
what if there was a hook where, in PL/Perl for example, you <code>use Thing</code>
and a hook in the Perl <code>use</code> command knows to look in a table that the DBA
manages and can load it from there. Just a funky idea I had, a way to get
away from the file system and more easily let people, through permissions,
be able to load modules in a safe way.</p>
 
 
<p>A topic that came up during Yurii’s talk was binary compatibility of minor
releases, or some sort of ABI stability. I’d be curious what to bring up
with hackers on formalizing something there. Although it has seemed mostly
pretty stable over time to me, that doesn’t mean it’s been fully stable.
I’d be curious to hear about the exceptions.</p>
 
 
<p>So! That’s my quick review. I did the remainder of them in 11 minutes!</p>
 Discussion 
 
 
<p>Jeremy S: Well done.</p>
 
 
<p>David Wheeler (he/him): What I’d like to do is send an email to all the
people who are registered to come to The Summit in two weeks, as well as
all of you, to be able to access this board and put stars or icons or
something — stickers which you can access —</p>
 
 
<p>Jeremy S: I do feel like there’s something missing from the board. I don’t
know that it’s something we would have wanted to put on sooner, but I kind
of feel like one of the next steps is just getting down into the trenches
and looking at actual extensions, and seeing how a lot of these topics are
going to apply once we start looking like at the list. I was looking
around a bit.</p>
<p>It’s funny; I see a mailing list thread from a year or two ago where,
right after Joel made his big list of 1,000 extensions, he jumped on the
hackers list and said, “hey could we stick this somewhere like on the
wiki?” And it looks like nobody quite got around to doing anything like
tha. But that’s where I was thinking about poking around, maybe maybe
starting to work on something like that.</p>
<p>But I think once we start to look at some of the actual extensions, it’ll
help us with a lot of these topics, kind of figure out what we’re talking
about. Like when you’re when you’re trying to figure out dependencies,
once you start to figure out some of the actual extensions where this is a
problem and other ones where it’s not, it might help us to have be a lot
more specific about the problem that we’re trying to solve. Or whether
it’s versioning, which platform something is going to build on, all that
kind of stuff. That’s where I was thinking a topic — or maybe a next
step or a topic that’s missing, or you were talking about how many
extensions even build today. If you go through the extensions on PGXN
right now, how many of them even work, at all. So starting to work down
that list.</p>
 
 
<p>David Wheeler (he/him): So, two thoughts on that. One is: create a sticky
with the topic you want and stick it in a place that’s appropriate, or
create another category if you think that’s relevant.</p>
 
 
<p>Jeremy S: It’s kind of weird, because what I would envision is what I want
to do on the wiki — I’ll see if I can start this off today, I have
rights to make a Postgres Wiki page — is I want to make a list of
extensions, like a table, where down the left is the extensions and across
the top is where that extension is distributed today. So just extensions
that are already distributed like in multiple places. I’m not talking
about the stuff that’s on core, because that’s a given that it’s
everywhere. But something like pg_cron or PGAudit, anybody who has
extensions probably has them. That gives some sense of the extensions that
everybody already packages. Those are obviously really important
extensions, because everybody is including them.</p>
<p>And then the next thing I wanted to do was the same thing with the list of
those extensions on the left but a column for each of the categories you
have here. For, say, PGAudit, for stuff across the top — metadata,
registry packaging, developer stuff — for PGAudit are their packaging
concerns? For PGAudit, go down the list of registry topics like identity,
where’s the where is the source for PGAudit, is the definitive upstream
GitLab, isit GitHub, is it git.postgresql.org? I could go right down the
list of each of these topics for PGAudit. and then go down the list of all
of your topics for pg_hint_plan. That’s another big one; pg_hint_plan is
all over the place. Each of your topics I could take and apply to each of
the top 10 extensions and there might be different things that rise to the
surface for pg_hint_plan than there are for, like, pgvector.</p>
 
 
<p>David Wheeler (he/him): That sounds like a worthwhile project to me, and
it could be a useful reference for any of these topics. Also a lot of
work!</p>
 
 
<p>Jeremy S: Well, in another way to like think about Vancouver might be,
instead of like splitting people up by these topics — I’m spitballing
here, this this might be a terrible idea — but you could take a list of
like 20 or 30 important extensions split people up into groups and say,
“here’s five extensions for you, now cover all these topics for your five
extensions.” You might have one group that’s looking at like pg_hint_plan
and pgvector and PGAudit, and then a different group that has pg_cron and
whatever else we come up with. That’s just another way you could slice it
up.</p>
 
 
<p>David Wheeler (he/him): Yeah, I think that you’re thinking about it the
inverse the way I’ve been thinking of it. I guess mine is perhaps a little
more centralized and top down, and that comes from having worked on PGXN
in the past and thinking about what we’d like to build in the future. But
there’s no reason it couldn’t be bottom up from those things. I will say,
when I was working on the metadata RFC, I did work through an example of
some actually really fussy extension — I don’t remember which one it was
— or no, I think it was the ML extension.<a>3</a> I think that could be a
really useful exercise.</p>
<p>But the idea the Open Technology Space is that you can create a sticky,
make a pitch for it, and have people vote by putting a star or something
on them. I’m hoping that, a. we can try to figure out which ones we feel
are the most important, but ultimately anybody can grab one of these and
say “I want to own this, I’m putting it in session session one, and put
your put your name on it. They ca be anything, for the most part.</p>
 
 
<p>Jeremy S: Sure. I think I don’t totally grok the Canva
board and how that all maps out, but at the end of the day whatever you
say we’re doing in Vancouver I’m behind it 100%.</p>
 
 
<p>David Wheeler (he/him): I’m trying to make it as open as possible. If
there’s something you want to talk about, make a sticky.</p>
 
 
<p>Jeremy S: I’ll add a little box. I’m not sure how this maps to what you
want to do with the time in Vancouver.</p>
 
 
<p>David Wheeler (he/him): Hopefully this will answer the question. First
we’ll do an intro and welcome and talk about the topics, give people time
to look at them, I want to send it in advance so people can have a sense
of it in advance. I know the way they do the the Postgres unconference
that’s been the last day of PGCon for years, they have people come and put
a sticky or star or some sort of sticker on the topics they like, and then
they pick the ones that have the most and and those are the ones they line
up in here [the agenda]. But the idea of the Open Technology stuff is a
person can decide on whatever topic they want, they can create their sticky,
they can put it in the set slot they want and whatever space they want,
and —</p>
 
 
<p>Jeremy S: Ooooh, I think I get it now. Okay, I didn’t realize that’s what
you were doing with the Canva board. Now I get it.</p>
 
 
<p>David Wheeler (he/him): Yeah, I was trying to more or less do an
unconference thing, but because we only have three hours try to have a
solid idea of the topics we want to address are before we get there.</p>
 
 
<p>Jeremy S: I don’t know though. Are you hoping a whole bunch of people are
going to come in here and like put it — Okay, it took me five or ten
minutes to to even realize what you were doing, and I don’t have high
hopes that we’ll get 20 people to come in and vote on the Post-it notes in
the next seven days.</p>
 
 
<p>David Wheeler (he/him): Yeah, maybe we need to…  These instructions here
are meant to help people understand that and if that needs to be
tweaked, let’s do it.</p>
 
 David G. Johnston in chat: How many people are going to in this
summit in Vancouver? 
 David G. Johnston in chat: Is the output of a session just
discussions or are action items desired? 
 
 
 
<p>Steven Miller: I have another question. Are people invited to
present at the Summit if they’re not physically present at the Summit? And then
same question for viewership</p>
 
 
<p>David Wheeler (he/him): I don’t think they are providing remote stuff at
the Summit</p>
 
 
<p>Steven Miller: okay</p>
 
 
<p>David Wheeler (he/him): David, last I heard there were 42 people
registered. I think we have space for 45. We can maybe get up to 50 with
some standing room, and there’s a surprisingly large number of people
(laughs).</p>
 
 David G. Johnston in chat: So average of 10 in each space? 
 
 
 
<p>Jeremy S: Have you gone down the list of names and started to figure out
who all these people? Cuz that’s another thing. There might be people who
have very little background and just thought “this sounds like an
interesting topic.” How those people would contribute and participate
would be very different from someone who’s been working with extensions
for a long time.</p>
 
 
<p>David Wheeler (he/him): David, yeah, and we can add more spaces or
whatever if it makes sense, or people can just arbitrarily go to a corner.
Because it’s an unconference they can elect to do whatever interests them.
I’m just hoping to have like the top six things we think are most
important to get to ahead of time.</p>
<p>Jeremy, Melanie sent me the list of participants, and I recognized perhaps
a quarter of the names were people who’re pretty involved in the
community, and the rest I don’t know at all. so I think it’s going to be
all over the map.</p>
 
 
<p>Steven Miller: So would it work if somebody wanted to do a presentation,
they can. They grab stickies from the left and then you could also
duplicate stickies because maybe there’d be some overlap, and then you put
them in a session. But there’s basically supposed to be only one name per
field, and that’s who’s presenting.</p>
 
 
<p>David Wheeler (he/him): You can put however many names on it as you want.
Open technology usually says there’s one person who’s facilitating and
another person should take notes.</p>
 
 
<p>Steven Miller: Okay.</p>
 
 
<p>David Wheeler (he/him): But whatever works! The way I’m imagining it is,
people say, “Okay I want to talk to other people about make some decisions
about, I don’t know, documentation standards.” So they go off to a corner
and they talk about it for an hour. There are some notes. And the final
half hour we’ll have readouts from those, from whatever was talked about
there.</p>
 
 
<p>Steven Miller: These are small working sessions really,it’s not like
a conference presentation. Okay, got it</p>
 
 
<p>David Wheeler (he/him): Yeah. I mean, somebody might come prepared with a
brief presentation if they want to set the context. [Laughs] Which is what
I was trying to do for the overall thing here. But the idea is these are
working sessions, like “here’s the thing we want to look at” and we want
to have some recommend commendations, or figure out the parameters, or you
have a plan — maybe — at the end of it. My ideal, personally, is that
at the at the end of this we have a good idea of what are the most
important topics to address earlier on in the process of building out the
ecosystem of the future, so we can start planning for how to execute on
that from those proposals and decisions. That’s how I’m thinking about it</p>
 
 
<p>Steven Miller: Okay, yeah I see.</p>
 
 
<p>Jeremy S: This sounds a lot like the CoffeeOps meetups that I’ve been to.
They have a similar process where you use physical Post-it notes and vote
on topics and then everybody drops off into groups based on what they’re
interested in.</p>
 
 
<p>David Wheeler (he/him): Yeah it’s probably the same thing, the Open
Technology stuff.</p>
 
 
<p>Steven Miller: Maybe we should do one field so we kind of get an idea?</p>
 
 
<p>David Wheeler (he/him): Sure. Let’s say somebody comes along and there are
a bunch of stickers on this one [drops stickers on the sticky labeled
“Identity, namespacing, and uniqueness”]. So so we know that it’s
something people really want to talk about. So if somebody will take
ownership of it, they can control click, select “add your name”, find a
slot that makes sense (and we may not use all of these) and drag it there.
So “I’m going to take the first session to talk about this.” Then people
can put the stickies on it over here [pasties stickers onto the topic
sticky in the agenda], so you have some sense of how many people are
interested in attending and talking about that topic. But there are no
hard and fast rules.</p>
<p>Whether or not they do that, say, “David wants to talk about identity name
spacing uniqueness in the core registry,” we’re going to do that in the
first session. We’ll be in the northeast corner of the room — I’m going
to try to get access to the room earlier in the day so I can have some
idea of how it breaks up, and I’ll tweak the the Canva to to add stuff as
appropriate.</p>
 
 David G. Johnston in chat: Same thing multiple times so people don’t
miss out on joining their #2 option? 
 David G. Johnston in chat: How about #1, #2, #3 as labels instead of
just one per person? 
 
 
 
<p>Jeremy S: Are you wanting us to put Post-it notes on the agenda now,
before we know what’s been voted for?</p>
 
 
<p>David Wheeler (he/him): Yep! Especially if there’s some idea you had
Jeremy. If there’s stuff you feel is missing or would be a different
approach, stick it in here. It may well be not that many people interested
in what I’ve come up with but they want to talk about
those five extensions.</p>
 
 
<p>David Wheeler (he/him): (Reading comment from David Johnson): “One two and
three as labels instead of just one per person?” David I’m sorry I don’t
follow.</p>
 
 
<p>David G. Johnston: So basically like rank choice. If you’re gonna do I
core one time and binary packaging one time, and they’re running at the
same time, well I want to do both. I want to do core — that’s my first
choice — I want to do binary packaging — that’s my second choice. If I
had to choose, I’d go to number one. But if you have enough people saying
I want to see this, that’s my number two option, you run binary packaging
twice, not conflicting with core so you can get more people.</p>
 
 
<p>David Wheeler (he/him): I see, have people stick numbers on the topics
that most interest in them. Let’s see here… [pokes around the Canva UX,
finds stickers with numbers.] There we go. I’ll stick those somewhere
that’s reasonable so people can rank them if they want, their top
choices.</p>
<p>This is all going to be super arbitrary and unscientific. The way I’ve
seen it happen before is people just drop stars on stuff and say, okay
this one has four and this one has eight so we definitely want to talk
about that one, who’s going to own it, that sort of thing. I think what
makes sense is to send this email to all the participants in advance;
hopefully people will take a look, have some sense of it, and maybe put a
few things on. Then, those of us who are organizing it and will be
facilitating on the day, we should meet like a day or two before, go over
it, and make some decisions about what we definitely think should be
covered, what things are open, and get a little more sense of how we want
to run things. Does that make sense?</p>
 
 
<p>Jeremy S: Yeah, I think chatting ahead of time would be a good idea. It’ll
be interesting to see how the Canva thing goes and what happens with it.</p>
 
 
<p>David Wheeler (he/him): It might be a mess! Whatever! But the answer is
that whatever happens this is the right place. Whenever it starts is the
right time. Whatever happens could only happen here. It’s super arbitrary
and free, and we can adapt as much as we want as it goes.</p>
 
 
<p>David Wheeler (he/him): I think that’s it. Do you all feels like you have
some sense of what we want to do?</p>
 
 
<p>Jeremy S: Well not really, but that’s okay! [Laughs]</p>
 
 
<p>Steven Miller: Okay, so here’s what we are supposed to do. Are we supposed
to go find people who might be interested to present — they will already
be in the list of people who are going to Vancouver. Then we talk to them
about these Post-its and we say, “would you like to have a small
discussion about one of these things. If you are, then put a sticky note
on it.” And then we put the sticky notes in the fields, we have a list of
names associated with the sticky notes. Like, maybe Yurii is interested in
binary distribution, and then maybe David is also interested in that. So
there’s like three or four people in each section, and we’re trying to
make sure that if you’re interested multiple sections you get to go to
everything?</p>
 
 
<p>David Wheeler (he/him): Yeah you can float and try to organize things. I
put sessions in here assuming people would want to spend an hour, but
maybe a topic only takes 15 minutes.</p>
 
 
<p>David G. Johnston: Staying on my earlier thought on what people want to
see, people who are willing to present and can present on multiple
things, if we have a gold star for who’s willing to actually present on
this topic. So here’s a topic, I got eight people who want to see it but
only one possible presenter. Or I got five possible presenters and three
possible viewers. But you have that dynamic of ranked choice for both “I’ll
present stuff” or “I’m only a viewer.</p>
 
 
<p>David Wheeler (he/him): I think that typically these things are
self-organizing. Somebody says, “I want to do this, I will facilitate, and
I need a note taker.” But they negotiate amongst themselves about how they
want to go about doing it. I don’t think it necessarily has to be formal
presentation, and usually these things are not. Usually it’s like somebody
saying, “here’s what this means, this is the topic, we’re going to try to
cover, these are the decisions we want to make, Go!”</p>
 
 
<p>Jeremy S: You’re describing the the the unconference component of PGCon
that has been down in the past.</p>
 
 
<p>David Wheeler (he/him): More or less, yes</p>
 
 
<p>Jeremy S: So should we just come out and say this is a unconference? Then
everybody knows what you’re talking about really fast, right?</p>
 
 
<p>David Wheeler (he/him): Sure, sure, yeah. I mean —</p>
 
 
<p>Jeremy S: We’re just we’re doing the same thing as – yeah.</p>
 
 
<p>David Wheeler (he/him): Yeah, I try to capture that here but we can use
the word “unconference” for sure. [Edits the Canva to add “an
unconference session” to the title.] There we go.</p>
 
 
<p>Steven Miller: I imagine there are people who might be interested to
present but they just aren’t in this meeting right now. So maybe we need
to go out and advertise this to people.</p>
 
 
<p>David Wheeler (he/him): Yeah, I want to draft an email to send to all the
attendees. Melanie told me we can send an email to everybody who’s registered.</p>
 
 
<p>Jeremy S: And to be clear it’s full, right? Nobody new can register at
this point?</p>
 
 
<p>David Wheeler (he/him): As far as I know, but I’m not sure how hard and
fast the rules are. I don’t think any more people can register, but it
doesn’t mean other people won’t wander in. People might have registered
and then not not come because they’rein the patch the patch session or
something.</p>
<p>So I volunteer to draft that email today or by tomorrow and share it with
the Slack channel for feedback. Especially if you’re giving me notes to
clarify what things mean, because it seems like there are
more questions and confusions about how it works than I anticipated — in
part because it’s kind of unorganized by design [chuckles].</p>
 
 Jeremy S in chat: <a href="https://wiki.postgresql.org/wiki/PgConUnconferenceFAQ">https://wiki.postgresql.org/wiki/PgConUnconferenceFAQ</a> 
 
 
 
<p>David Wheeler (he/him): Oh that’s a good thing to include Jeremy. that’s a
good call. But to also try to maximize participation of the people who’re
planning to be there. It may be that they say, “Oh this sounds
interesting,” or whatever, so and I’ll add some different stickers to this
for some different meanings, like “I’m interested” or “I want to take
ownership of this” or “this is my first, second, third, or fourth choice”.
Sound good?</p>
 
 
<p>Steven Miller: Yes, it sounds good to me!</p>
 
 
<p>David Wheeler (he/him): Thanks Steven.</p>
 
 
<p>Jeremy S: Sounds good, yeah.</p>
 
 
<p>David Wheeler (he/him): All right, great! Thanks everybody for coming!</p>
 
 
 
 
 
 
<p>Hit the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a> for the link!&nbsp;<a>↩︎</a></p>
 
 
<p>In the meeting I kept saying “open technology” but meant <a href="https://en.wikipedia.org/wiki/Open_space_technology" title="wikipedia: “Open space technology”">Open
Space Technology</a> 🤦🏻‍♂️.&nbsp;<a>↩︎</a></p>
 
 
<p>But now I can look it up. It was <a href="https://github.com/postgresml/postgresml" title="The GPU-powered AI application database.">pgml</a>, for which I <a href="https://justatheory.com/2024/03/rfc-pgxn-metadata-sketch/#example-pgml-extension" title="RFC: PGXN Metadata Sketch --- Example: PGML Extension">mocked up a
<code>META.json</code></a>.&nbsp;<a>↩︎</a></p>
 
 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   21:56 &bull;
   12 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wrenchinthegears.com/2024/05/22/blockchain-brain-boxing-why-people-continue-to-disregard-electronic-health-records-and-pay-for-success-wellness-data/'>Blockchain Brain Boxing – Why People Continue To Disregard Electronic Health Records And Pay for Success Wellness Data</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This video elaborates on a comment exchange that was posted this morning to a video I made last night&nbsp;unpacking Bobby’s Lunarpunk “Ape” signal. Links to maps referenced in the video for those interested in further inquiry. &nbsp; This is the video I did with Shai on the RAND Noopolitik report.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This video elaborates on a comment exchange that was posted this morning to <a href="https://www.youtube.com/watch?v=OARpbiBoVgU">a video I made last night&nbsp;</a>unpacking Bobby’s Lunarpunk “Ape” signal.</p>
<p></p>
<p></p>
<p>Links to maps referenced in the video for those interested in further inquiry.</p>
<a href="https://embed.kumu.io/f6b7ddc6451089c299ff8f3c09ac7613#untitled-map?s=bm9kZS1DOXhZUnRJbw%3D%3D">https://embed.kumu.io/f6b7ddc6451089c299ff8f3c09ac7613#untitled-map?s=bm9kZS1DOXhZUnRJbw%3D%3D</a>
<a href="https://www.rand.org/content/dam/rand/pubs/perspectives/PEA200/PEA237-1/RAND_PEA237-1.pdf">https://www.rand.org/content/dam/rand/pubs/perspectives/PEA200/PEA237-1/RAND_PEA237-1.pdf</a>
<a href="https://www.rand.org/content/dam/rand/pubs/perspectives/PEA200/PEA237-1/RAND_PEA237-1.pdf">https://www.rand.org/content/dam/rand/pubs/perspectives/PEA200/PEA237-1/RAND_PEA237-1.pdf</a>
<a href="https://web.archive.org/web/20210122170720/https://littlesis.org/oligrapher/6310-industrial-agriculture"></a><a href="https://web.archive.org/web/20210122170720/https://littlesis.org/oligrapher/6310-industrial-agriculture">https://web.archive.org/web/20210122170720/https://littlesis.org/oligrapher/6310-industrial-agriculture</a>
<a href="https://web.archive.org/web/20201207235929/https://littlesis.org/oligrapher/5719-produce-rx-food-pay-for-success-medicaid"></a><a href="https://web.archive.org/web/20201207235929/https://littlesis.org/oligrapher/5719-produce-rx-food-pay-for-success-medicaid">https://web.archive.org/web/20201207235929/https://littlesis.org/oligrapher/5719-produce-rx-food-pay-for-success-medicaid</a>
<a href="https://web.archive.org/web/20200925091209/https://littlesis.org/oligrapher/4954-blockchain-healthcare-challenge-2016"></a><a href="https://web.archive.org/web/20200925091209/https://littlesis.org/oligrapher/4954-blockchain-healthcare-challenge-2016">https://web.archive.org/web/20200925091209/https://littlesis.org/oligrapher/4954-blockchain-healthcare-challenge-2016</a>
<a href="https://web.archive.org/web/20201208182811/https://littlesis.org/oligrapher/4968-bloomberg-mypass-austin-blockchain-id"></a><a href="https://web.archive.org/web/20201208182811/https://littlesis.org/oligrapher/4968-bloomberg-mypass-austin-blockchain-id">https://web.archive.org/web/20201208182811/https://littlesis.org/oligrapher/4968-bloomberg-mypass-austin-blockchain-id</a>
<a href="https://web.archive.org/web/20210110131607/https://littlesis.org/oligrapher/5814-e-government-impact-investing-digital-israel"></a><a href="https://web.archive.org/web/20210110131607/https://littlesis.org/oligrapher/5814-e-government-impact-investing-digital-israel">https://web.archive.org/web/20210110131607/https://littlesis.org/oligrapher/5814-e-government-impact-investing-digital-israel</a>
<a href="https://web.archive.org/web/20201204194140/https://littlesis.org/oligrapher/5757-dassault-digital-twin-simulations-blockchain-id"></a><a href="https://web.archive.org/web/20201204194140/https://littlesis.org/oligrapher/5757-dassault-digital-twin-simulations-blockchain-id">https://web.archive.org/web/20201204194140/https://littlesis.org/oligrapher/5757-dassault-digital-twin-simulations-blockchain-id</a>
<a href="https://docs.google.com/presentation/d/1beurviKJ8YkZBla-HLtXy8OxsyIWkJAnGBXwf28Tbw8/edit?usp=sharing"></a><a href="https://web.archive.org/web/20201207235929/https://littlesis.org/oligrapher/5719-produce-rx-food-pay-for-success-medicaid">https://docs.google.com/presentation/d/1beurviKJ8YkZBla-HLtXy8OxsyIWkJAnGBXwf28Tbw8/edit?usp=sharing</a>
<a href="https://docs.google.com/presentation/d/1beurviKJ8YkZBla-HLtXy8OxsyIWkJAnGBXwf28Tbw8/edit?usp=sharing"></a><a href="https://docs.google.com/presentation/d/1beurviKJ8YkZBla-HLtXy8OxsyIWkJAnGBXwf28Tbw8/edit?usp=sharing">https://docs.google.com/presentation/d/1beurviKJ8YkZBla-HLtXy8OxsyIWkJAnGBXwf28Tbw8/edit?usp=sharing</a>
<a href="https://docs.google.com/presentation/d/1beurviKJ8YkZBla-HLtXy8OxsyIWkJAnGBXwf28Tbw8/edit?usp=sharing"></a><a href="https://docs.google.com/presentation/d/1beurviKJ8YkZBla-HLtXy8OxsyIWkJAnGBXwf28Tbw8/edit?usp=sharing">https://docs.google.com/presentation/d/1beurviKJ8YkZBla-HLtXy8OxsyIWkJAnGBXwf28Tbw8/edit?usp=sharing</a>
<a href="https://docs.google.com/presentation/d/1beurviKJ8YkZBla-HLtXy8OxsyIWkJAnGBXwf28Tbw8/edit?usp=sharing"></a><a href="https://docs.google.com/presentation/d/1beurviKJ8YkZBla-HLtXy8OxsyIWkJAnGBXwf28Tbw8/edit?usp=sharing">https://docs.google.com/presentation/d/1beurviKJ8YkZBla-HLtXy8OxsyIWkJAnGBXwf28Tbw8/edit?usp=sharing</a>
<p>&nbsp;</p>
<p>This is the video I did with Shai on the RAND Noopolitik report.</p>
<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/brain-box-.png?resize=1020%2C1232&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:24 &bull;
   12 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://herestomwiththeweather.com'>Heres Tom with the Weather</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://herestomwiththeweather.com/2024/05/22/feditesting/'>Feditesting!</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   It’s cool to see the progress of the FediTest project.  On March 7, there was a show-and-tell online meeting and at the end of April, a FediTest implementation update included a Quickstart to try out some examples. 

 I was pleasantly surprised by the experience (including the specification annotations referencing each test) and the organization of the testing framework even at an early stage.  I 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>It’s cool to see the progress of the <a href="https://feditest.org/">FediTest</a> project.  On March 7, there was a <a href="https://fedidevs.org/notes/2024-03-07/">show-and-tell online meeting</a> and at the end of April, a <a href="https://feditest.org/blog/2024-04-30-update/">FediTest implementation update</a> included a <a href="https://feditest.org/docs/quickstart/">Quickstart</a> to try out some examples.</p>

<p>I was pleasantly surprised by the experience (including the specification annotations referencing each test) and the organization of the testing framework even at an early stage.  I was able to get all the tests for the <a href="https://github.com/fediverse-devnet/feditest-tests-fediverse/blob/develop/example-testplans/saas-imp-webfinger-server.json">sass-imp-webfinger-server testplan</a> passing last night for <a href="https://github.com/herestomwiththeweather/irwin">Irwin</a>.  For each failing test, I created an issue and referenced the test (e.g. <a href="https://github.com/herestomwiththeweather/irwin/issues/15"> Well-known webfinger should respond with access-control-allow-origin header #15 </a> ).</p>

<p>Here’s the output of this example testplan:</p>

  <code>TAP version 14
# test plan: Unnamed
# started: 2024-05-22 06:33:53.423983+00:00
# ended: 2024-05-22 06:34:42.924770+00:00
# platform: Linux-6.5.0-28-generic-x86_64-with-glibc2.35
# username: tbbrown
# hostname: agency
# session: Unnamed
# constellation: Unnamed
#   roles:
#     - name: client
#       driver: imp.ImpInProcessNodeDriver
#     - name: server
#       driver: saas.SaasFediverseNodeDriver
ok 1 - webfinger.server.4_1__2_parameter_ordering_not_significant::parameter_ordering
ok 2 - webfinger.server.4_2__14_must_only_redirect_to_https::must_only_redirect_to_https
ok 3 - webfinger.server.4_2__3_requires_resource_uri::requires_resource_uri
ok 4 - webfinger.server.4_2__4_do_not_accept_malformed_resource_parameters::double_equals
ok 5 - webfinger.server.4_2__4_do_not_accept_malformed_resource_parameters::not_percent_encoded
ok 6 - webfinger.server.4_2__5_status_404_for_nonexisting_resources::status_404_for_nonexisting_resources
ok 7 - webfinger.server.4_5__1_any_uri_scheme_for_resource_identifiers::any_uri_scheme_for_resource_identifiers
ok 8 - webfinger.server.4__1_accepts_all_link_rels_in_query::accepts_combined_link_rels_in_query
ok 9 - webfinger.server.4__1_accepts_all_link_rels_in_query::accepts_known_link_rels_in_query
ok 10 - webfinger.server.4__1_accepts_all_link_rels_in_query::accepts_unknown_link_rels_in_query
ok 11 - webfinger.server.4__3_only_returns_jrd_in_response_to_https_requests::only_returns_jrd_in_response_to_https
ok 12 - webfinger.server.5_1_cors_header_required::cors_header_required
1..12
# test run summary:
#   total: 12
#   passed: 12
#   failed: 0
#   skipped: 0
#   errors: 0
</code> 
 

<p>Getting these tests passing certainly improved the code and my understanding of the webfinger spec. Thanks to Johannes Ernst and the <a href="https://fedidevs.org">Fediverse Developer Network</a> community for this.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:29 &bull;
   13 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/can-llms-provide-references-for-their'>Can LLMs Provide References for Their Work? An Experiment</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Recently my friend Scott Mace asked this on X in response to a post about the evolution of software and how LLMs will likely follow that same trajectory: Yes, but how trustworthy will it be? Can I at least see the links it used to create its response?
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae49950f-95f1-402b-bce7-eca2c2164cc9.heic">    </a> <p>Recently my friend <a href="https://maceonhealthcare.blogspot.com/">Scott Mace</a> <a href="https://x.com/scottmace/status/1792624531368185920">asked this on X</a> in response to a post about the evolution of software and how LLMs will likely follow that same trajectory:</p><blockquote><p>Yes, but how trustworthy will it be? Can I at least see the links it used to create its response?</p><p>From <a href="https://x.com/windley/status/1792883491757076592">X</a><br>Referenced 2024-05-21T18:09:06-0400</p></blockquote><p>The short answer is "no", but the real answer is a bit more nuanced because of how LLMs like ChatGPT work. First the short answer: LLMs don't understand what they're saying so they can't reference the material that it used to create a response. They don't work like that. They don't go out, do a bunch of research and then create a response. Instead lots (and I mean LOTS) of material is fed into the program that creates the model, a multi-dimensional matrix of word orders. Then when you ask a question ("give it prompt" in LLM-speak), it processes the question and generates text based on the probabilities of words in its model1. There no way if can satisfy Scott directly cause the model doesn't—can't, as far as I know—include the references that it used to create the text cause it's generating it on the fly from material that has been thoroughly blended together.</p><p>But, in fairness, that's not what people do most of the time either. If you asked me over lunch or at the end of a tech presentation to describe zero trust and its pros and cons, I'd start talking. I wouldn't start looking up things on the web and somehow weave them into text complete with references. I'd go look for references if you asked me to back up something I said. So, let's try that with ChatGPT 4o. I gave it the following prompt:</p><blockquote><p>You are a writer for a website focusing on technology. Your specialty is security. Write a 150 word explanation of zero trust with three pros and three cons.</p></blockquote><p>ChaptGPT answered with this text:</p><blockquote><p>Zero Trust is a security model based on the principle of "never trust, always verify." Unlike traditional security models that assume everything inside the network is safe, Zero Trust assumes that threats can exist both inside and outside the network. It requires strict identity verification for every user and device attempting to access resources, regardless of their location.</p><p>Pros:</p>  <p>Enhanced Security: By continuously verifying users and devices, Zero Trust reduces the risk of breaches and unauthorized access.</p>  <p>Minimized Attack Surface: It limits access to only what is necessary, reducing potential entry points for attackers.</p>  <p>Improved Compliance: Helps meet regulatory requirements by ensuring strict access controls and logging all access attempts.</p>  <p>Cons:</p>  <p>Complex Implementation: Deploying Zero Trust can be complex and time-consuming, requiring a significant overhaul of existing systems and processes.</p>  <p>Increased Costs: The initial investment in technology and training can be high.</p>  <p>User Experience Impact: Continuous verification can lead to friction and inconvenience for users if not implemented smoothly.</p>  <p>Zero Trust is essential for modern cybersecurity, but it requires careful planning and resources to implement effectively.</p></blockquote><p>That's pretty good. But what if we need some references to other material to back up the pros and cons? I asked ChatGPT to supply them:</p><blockquote><p>Now repeat that task, but this time include references as hyperlinks to back up each pro and con.</p></blockquote><p>ChaptGPT answered with this text:</p><blockquote><p>Zero Trust is a security model based on the principle of "never trust, always verify." Unlike traditional security models that assume everything inside the network is safe, Zero Trust assumes that threats can exist both inside and outside the network. It requires strict identity verification for every user and device attempting to access resources, regardless of their location.</p><p>Pros:</p>  <p>Enhanced Security: By continuously verifying users and devices, Zero Trust reduces the risk of breaches and unauthorized access (<a href="https://www.csoonline.com/article/3572352/what-is-zero-trust.html">CSO Online</a>).</p>  <p>Minimized Attack Surface: It limits access to only what is necessary, reducing potential entry points for attackers (<a href="https://www.nist.gov/publications/zero-trust-architecture">NIST</a>).</p>  <p>Improved Compliance: Helps meet regulatory requirements by ensuring strict access controls and logging all access attempts (<a href="https://www.gartner.com/en/newsroom/press-releases/2021-04-06-gartner-says-zero-trust-is-the-future-of-security">Gartner</a>).</p>  <p>Cons:</p>  <p>Complex Implementation: Deploying Zero Trust can be complex and time-consuming, requiring a significant overhaul of existing systems and processes (<a href="https://go.forrester.com/blogs/what-are-the-most-common-zero-trust-pitfalls/">Forrester</a>).</p>  <p>Increased Costs: The initial investment in technology and training can be high (<a href="https://www.zdnet.com/article/what-is-zero-trust/">ZDNet</a>).</p>  <p>User Experience Impact: Continuous verification can lead to friction and inconvenience for users if not implemented smoothly (<a href="https://www.techrepublic.com/article/zero-trust-security-model/">TechRepublic</a>).</p>  <p>Zero Trust is essential for modern cybersecurity, but it requires careful planning and resources to implement effectively.</p></blockquote><p>That looks great! Nice references for each bullet. There's just one problem. All the links, except the NIST reference, are dead. I don't know if they're wrong, made up, or stale. I tried finding content on a few of the sites by searching and came up dry. As I heard Johannes Ernst say once, ChatGPT is a "large language model" not a "large fact model" and this bears that out.</p><p>Still, asking for references or evidence for statements is something people are going to want. So, I tried a different tack. This time I asked this:</p><blockquote><p>Why do you say complex implementation is a problem for zero trust? Can you give me a few references on that I can read myself?</p></blockquote><p>ChatGPT responded with this:</p><blockquote><p>Implementing a Zero Trust security model can be challenging for several reasons:</p>  <p>Overall Complexity: Zero Trust requires a comprehensive understanding of your network's intricacies, including users, devices, applications, and data. This can involve significant changes to existing infrastructure, which can be disruptive and costly. Organizations need to be prepared for the extensive planning and resources required for successful implementation​ (<a href="https://www.helpnetsecurity.com/2024/02/01/zero-trust-challenges/">Help Net Security</a>)​​ (<a href="https://www.enterprisenetworkingplanet.com/security/pros-and-cons-of-zero-trust-security/">Enterprise Networking Planet</a>​).</p>  <p>Legacy Systems: Many organizations have legacy systems that are not designed to operate within a Zero Trust framework. These systems might require different security measures or even complete replacement, adding to the complexity and cost (<a href="https://www.helpnetsecurity.com/2024/02/01/zero-trust-challenges/">Help Net Security</a>)​​ (<a href="https://www.enterprisenetworkingplanet.com/security/pros-and-cons-of-zero-trust-security/">Enterprise Networking Planet</a>​)</p>  <p>Cultural Shift: Zero Trust is not just a technological shift but also a cultural one. It requires continuous verification and strict access controls, which can lead to user frustration and resistance if not managed properly. Ensuring user buy-in and minimizing disruption to workflows through training and communication is essential (<a href="https://www2.deloitte.com/us/en/blog/human-capital-blog/2022/zero-trust-adoption-for-human-centered-cyber-security.html">Deloitte United States</a>)​​ (<a href="https://guidehouse.com/case-studies/defense-and-security/2023/implementing-zero-trust-across-a-complex-multi-agency-client">Guidehouse</a>).</p>  <p>Resource Intensive: Implementing and maintaining Zero Trust can put a strain on IT resources due to the need for continuous monitoring and management. The need for advanced security tools and ongoing updates further adds to the resource demands (<a href="https://www.helpnetsecurity.com/2024/02/01/zero-trust-challenges/">Help Net Security</a>)​​ (<a href="https://www.enterprisenetworkingplanet.com/security/pros-and-cons-of-zero-trust-security/">Enterprise Networking Planet</a>​).</p>  <p>Stakeholder Alignment: Achieving buy-in from all stakeholders and aligning them with the Zero Trust principles is critical. This requires a coordinated effort to educate stakeholders, prioritize gaps, and ensure consistent understanding and application of Zero Trust principles across the organization (<a href="https://www2.deloitte.com/us/en/blog/human-capital-blog/2022/zero-trust-adoption-for-human-centered-cyber-security.html">Deloitte United States</a>)​​ (<a href="https://guidehouse.com/case-studies/defense-and-security/2023/implementing-zero-trust-across-a-complex-multi-agency-client">Guidehouse</a>).</p>  <p>These challenges highlight the importance of a phased and well-planned approach to Zero Trust implementation, considering both technological and human factors. For more detailed insights, you can refer to articles from <a href="https://www.helpnetsecurity.com/2024/02/01/zero-trust-implementation-plan/">Help Net Security</a> and <a href="https://guidehouse.com/insights/cybersecurity/2023/implementing-zero-trust-across-complex-multi-agency-client">Guidehouse</a>.</p></blockquote><p>Much better. The links in the bullets all work, but the ones at the end don't. The articles it references aren’t very detailed, so I wouldn’t say they’re stellar references, but at least there’s a pointer to outside material that discusses similar challenges. So, partial success. </p><p>I've found that when you're using ChatGPT to do research, treating it like a librarian and asking questions that follow up are the best way to gain confidence that the material is correct and find the supporting material, just like I did above. I'm sure you could wrap additional helper functions around an LLM to make getting references easier. But for now interrogation through chat is the best way to get comfortable with the results.</p> Notes   <p>Yeah, this is really simplified and thus wrong, but it's directionally correct. For a better explanation, see <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">What Is ChatGPT Doing … and Why Does It Work?</a>.</p>  <p>Photo Credit: Older Man Talking to Robot Librarian from DALL-E (public domain) Prompt: draw me a rectanglular picture of an older man talking with a robot librarian. Use earth tones.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae49950f-95f1-402b-bce7-eca2c2164cc9.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:01 &bull;
   13 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.patrick-breyer.de/lass-dich-ueberwachen-eu-rat-will-sich-auf-chatkontrolle-mit-zustimmung-der-nutzer-einigen/'>„Lass dich überwachen“: EU-Rat will sich auf Chatkontrolle mit Zustimmung der Nutzer einigen [ergänzt]</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Aufgrund eines neuen Vorschlags der belgischen Innenministerin könnten die EU-Regierungen die vielkritisierte Chatkontrolle doch bereits im Juni beschließen. Das ergibt sich aus internen Informationen, die dem Europaabgeordneten der Piratenpartei und Schattenberichterstatter … 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Aufgrund eines neuen Vorschlags der belgischen Innenministerin könnten die EU-Regierungen die vielkritisierte Chatkontrolle doch bereits im Juni beschließen. Das ergibt sich aus internen Informationen, die dem Europaabgeordneten der Piratenpartei und Schattenberichterstatter Dr. Patrick Breyer zugespielt wurden.</p>



<p>Konkret sollen nach dem neuesten Vorschlag Nutzer von Apps mit Kommunikationsfunktion per AGB oder Popup-Nachricht zustimmen müssen, dass alle verschickten Bilder und Videos automatisiert und verdachtslos gescannt und gegebenenfalls der EU und der Polizei gemeldet werden. Dazu sollen wohl auch Überwachungsfunktionen in bisher sicher Ende-zu-Ende verschlüsselte Messenger eingebaut werden, auch wenn die Belgier behaupten, die von ihnen vorgeschlagene „Uploadmoderation“ unterscheide sich vom „Client-Side Scanning“. Stimmt ein Nutzer der Chatkontrolle nicht zu, soll er den Chat weiter nutzen nutzen können, aber keine Bilder und Videos mehr versenden können. An EU und Polizei ausleiten sollen die Scanner-Algorithmen bekanntes Material und unbekannte Bilder und Videos, die von „künstlicher Intelligenz“ für potenziell verdächtig gehalten werden. Auf die Durchsuchung von Textnachrichten nach Hinweisen auf „Grooming“, die auch bisher kaum zum Einsatz kommt, soll genauso verzichtet werden wie auf das noch nie eingesetzte Scannen von Audiokommunikation.</p>



<p>Bei Vorstellung des Vorschlags am 8. Mai haben sich mehrere bisher kritische Regierungen wohlwollend und aufgeschlossen geäußert. <a href="https://www.parlament.gv.at/dokument/XXVII/EU/184770/imfname_11373735.pdf">Schon übermorgen (24. Mai) soll erneut über den „neuen Vorschlag“ beraten werden.</a> Direkt nach der Europawahl treffen sich die EU-Innenminister und könnten die Chatkontrolle beschließen.</p>



<p>„Der geleakte Vorschlag zeigt, dass der extreme Ausgangsentwurf der EU-Kommission zur in der freien Welt einzigartigen Chatkontrolle im Kern unverändert beibehalten werden soll“, warnt der Europaabgeordnete und profilierteste Gegner der Chatkontrolle Dr. Patrick Breyer (Piratenpartei).„Der neueste Vorstoß zur Chatkontrolle erinnert an die Fernsehshow ‚Lass dich überwachen‘. Messengerdienste rein textbasiert zu nutzen, ist im 21. Jahrhundert keine ernsthafte Option. Und Auswüchse der Chatkontrolle zu streichen, die ohnehin in der Praxis keine Rolle spielen, ist eine Mogelpackung.</p>



<p>Es bleibt dabei: Millionen privater Chats und Privatfotos unbescholtener Bürger sollen mit unzuverlässiger Technik durchsucht und ausgeleitet werden, ohne dass die Betroffenen auch nur entfernt mit Kindesmissbrauch zu tun haben – das zerstört unser digitales Briefgeheimnis. So landen unsere Nacktfotos und Familienbilder bei Fremden, in deren Hände sie nicht gehören und bei denen sie nicht sicher sind. Trotz Lippenbekenntnissen zu Verschlüsselung soll mit Client-Side-Scanning bisher sichere Ende-zu-Ende-Verschlüsselung allgemein ausgehebelt werden, um unsere Smartphones zu Spionen umzufunktionieren – das zerstört sichere Verschlüsselung.</p>



<p>Mich alarmiert, dass bisher kritische EU-Regierungen die umverpackten Pläne loben und dadurch die bisherige Sperrminorität zu kippen droht. Die Bundesregierung schweigt und ist wegen der SPD-Innenministerin Faeser seit Monaten ‚nicht sprechfähig‘ – dementsprechend bildet sie auch keine Allianzen mit anderen kritischen Staaten. Nicht einmal ein schriftliches Gutachten des Rechtsdienstes des Rates zu diesem offensichtlichen Grundrechtsverstoß ist bisher angefordert worden. Wenn die EU-Regierungen tatsächlich mit dieser radikalen Position zur Chatkontrolle in die Trilogverhandlungen gehen sollten, droht das Parlament hinter verschlossenen Türen erfahrungsgemäß seine Ausgangsposition schrittweise aufzugeben und sich auf schlechte und gefährliche Kompromisse einzulassen, die unsere Sicherheit im Netz auf Spiel setzen.</p>



<p>Jetzt ist die Zeit für Privatsphäre und sichere Verschlüsselung auf die Barrikaden zu gehen!“</p>



<p></p>



<p><a href="http://chatkontrolle.de/">Breyers Infoportal und Dokumentenarchiv zur Chatkontrolle</a></p>



<p>Ergänzung vom 23.05.2024: netzpolitik.org hat das Protokoll, auf das sich diese Analyse stützt, inzwischen auch <a href="https://netzpolitik.org/2024/internes-protokoll-belgien-will-nutzer-verpflichten-chatkontrolle-zuzustimmen/">im Volltext veröffentlicht</a>.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   12:07 &bull;
   13 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wrenchinthegears.com/2024/05/21/bobby-goes-ape-signaling-austins-lunar-punks/'>Bobby Goes Ape Signaling Austin’s Lunar Punks</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  There aren’t many out there who could begin to decipher RFK Jr.’s strange message to the Apes; but, since I’m one of them, here’s forty minutes of off-the-cuff analysis in which I attempt to weave together crypto gaming, esotericism, microbiome management, fin-tech behavioral economics, gut instinct, and cyclical harmonization of the commons – based on [...]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>There aren’t many out there who could begin to decipher RFK Jr.’s strange message to the Apes; but, since I’m one of them, here’s forty minutes of off-the-cuff analysis in which I attempt to weave together crypto gaming, esotericism, microbiome management, fin-tech behavioral economics, gut instinct, and cyclical harmonization of the commons – based on a foundation of ubiquitous sensing starting with carbon-nano permaculture protocols and personalized food prescriptions for AI-managed “wellness.” Remember, our biology is the post-Moore’s law technology. The future microbial network will run on orchestrated morphic fields coordinated by dynamic (Kundalini?) community currencies – always signaling the “wisdom of the crowd.”</p>
<p>See if you can follow along with me…</p>
<a href="https://x.com/RobertKennedyJr/status/1792970117204287992"></a><a href="https://x.com/RobertKennedyJr/status/1792970117204287992">https://x.com/RobertKennedyJr/status/1792970117204287992</a>
<p></p>
<p>&nbsp;</p>
<p>Here’s my map in progress if you care to explore.</p>
<a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/ape-map.png?ssl=1"></a>Interactive map: <a href="https://embed.kumu.io/a2ceb5922c02e84c6977d4665c2e5a9c#untitled-map?s=bm9kZS15NDVjc29aVw%3D%3D">https://embed.kumu.io/a2ceb5922c02e84c6977d4665c2e5a9c#untitled-map?s=bm9kZS15NDVjc29aVw%3D%3D</a>
<p>&nbsp;</p>
<p>Here are some links to things mentioned in my chat. You can click on the image or caption to go to the source.</p>
<a href="https://fintel.io/so/us/amc"></a><a href="https://fintel.io/so/us/amc">https://fintel.io/so/us/amc</a>
<a href="https://consensus2024.coindesk.com/speakers/"></a><a href="https://consensus2024.coindesk.com/speakers/">https://consensus2024.coindesk.com/speakers/</a>
<a href="https://medium.com/@hasanmahira/crypto-punk-cultures-cypherpunk-solarpunk-and-lunarpunk-55775b8676db"></a><a href="https://medium.com/@hasanmahira/crypto-punk-cultures-cypherpunk-solarpunk-and-lunarpunk-55775b8676db">https://medium.com/@hasanmahira/crypto-punk-cultures-cypherpunk-solarpunk-and-lunarpunk-55775b8676db</a>
<a href="https://signalsfromtheedge.org/lunarpunk-solarpunk"></a><a href="https://signalsfromtheedge.org/lunarpunk-solarpunk">https://signalsfromtheedge.org/lunarpunk-solarpunk</a>
<p></p>
<p>&nbsp;</p>
<a href="https://www.osti.gov/opennet/manhattan-project-history/publications/ORNLClassifiedActivities.pdf"></a><a href="https://www.osti.gov/opennet/manhattan-project-history/publications/ORNLClassifiedActivities.pdf">https://www.osti.gov/opennet/manhattan-project-history/publications/ORNLClassifiedActivities.pdf</a>
<a href="https://youtu.be/jfCCDB9LOcY?t=172"></a><a href="https://youtu.be/jfCCDB9LOcY?t=172">https://youtu.be/jfCCDB9LOcY?t=172</a>
<a href="https://www.youtube.com/watch?v=jB5gChpa9OY"></a><a href="https://www.youtube.com/watch?v=jB5gChpa9OY">https://www.youtube.com/watch?v=jB5gChpa9OY</a>
<a href="https://cycles.org/news/harmonics-theory-part-17-origins/"></a><a href="https://cycles.org/news/harmonics-theory-part-17-origins/">https://cycles.org/news/harmonics-theory-part-17-origins/</a>
<p>&nbsp;</p>
<a href="https://drrichardsmith.com/"></a><a href="https://drrichardsmith.com/">https://drrichardsmith.com/</a>
<a href="https://cycles.org/letters/a-letter-from-our-new-chairman-dr-richard-smith/"></a><a href="https://cycles.org/letters/a-letter-from-our-new-chairman-dr-richard-smith/">https://cycles.org/letters/a-letter-from-our-new-chairman-dr-richard-smith/</a>
<a href="https://cycles.org/letters/a-letter-from-our-new-chairman-dr-richard-smith/"></a><a href="https://cycles.org/letters/a-letter-from-our-new-chairman-dr-richard-smith/">https://cycles.org/letters/a-letter-from-our-new-chairman-dr-richard-smith/</a>
<a href="https://youtu.be/Zy5Uq_pTUns?t=34"></a><a href="https://youtu.be/Zy5Uq_pTUns?t=34">https://youtu.be/Zy5Uq_pTUns?t=34</a>
<a href="https://www.frontiersin.org/journals/microbiology/articles/10.3389/fmicb.2023.1214870/full"></a><a href="https://www.frontiersin.org/journals/microbiology/articles/10.3389/fmicb.2023.1214870/full">https://www.frontiersin.org/journals/microbiology/articles/10.3389/fmicb.2023.1214870/full</a>
<a href="https://wrenchinthegears.com/wp-content/uploads/2024/05/biosensors-12-00377.pdf"></a><a href="https://wrenchinthegears.com/wp-content/uploads/2024/05/biosensors-12-00377.pdf">https://wrenchinthegears.com/wp-content/uploads/2024/05/biosensors-12-00377.pdf</a>
<a href="https://blog.chain.link/reversing-climate-change-how-hybrid-smart-contracts-incentivize-regenerative-agriculture/"></a><a href="https://blog.chain.link/reversing-climate-change-how-hybrid-smart-contracts-incentivize-regenerative-agriculture/">https://blog.chain.link/reversing-climate-change-how-hybrid-smart-contracts-incentivize-regenerative-agriculture/</a>
<a href="https://www.youtube.com/watch?v=rV481ArX8UA&amp;list=PLUcrhDVaUj97YUUo8RfmmO4OJav6g5tS4&amp;index=2"></a><a href="https://www.youtube.com/watch?v=rV481ArX8UA&amp;list=PLUcrhDVaUj97YUUo8RfmmO4OJav6g5tS4&amp;index=2">https://www.youtube.com/watch?v=rV481ArX8UA&amp;list=PLUcrhDVaUj97YUUo8RfmmO4OJav6g5tS4&amp;index=2</a>
<p>&nbsp;</p>
<a href="https://youtu.be/M0KvPkCCJf0?t=129"></a><a href="https://youtu.be/M0KvPkCCJf0?t=129">https://youtu.be/M0KvPkCCJf0?t=129</a>
<a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/hawk.png?ssl=1"></a><a href="https://embed.kumu.io/e4740d5fc69c4eb9522fcb75c14dd6ef#untitled-map?s=bm9kZS16ejJFaFBXVg%3D%3D">https://embed.kumu.io/e4740d5fc69c4eb9522fcb75c14dd6ef#untitled-map?s=bm9kZS16ejJFaFBXVg%3D%3D</a>
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4304725/"></a><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4304725/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4304725/</a>
<p>&nbsp;</p>
<a href="https://www.conducttr.com/"></a><a href="https://www.conducttr.com/">https://www.conducttr.com/</a>
<a href="https://plato.stanford.edu/Entries/qualia/"></a><a href="https://plato.stanford.edu/Entries/qualia/">https://plato.stanford.edu/Entries/qualia/</a>
<a href="https://pubmed.ncbi.nlm.nih.gov/37003434/"></a><a href="https://pubmed.ncbi.nlm.nih.gov/37003434/">https://pubmed.ncbi.nlm.nih.gov/37003434/</a>
<p>&nbsp;</p>
 
<p></p>
<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/ape1.png?resize=1020%2C788&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   03:53 &bull;
   13 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 21. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/propublicas-new-50-states-commitment-builds-on-a-decade-plus-of'>ProPublica’s new “50 states” commitment builds on a decade-plus of local news partnerships</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 "It’s a good time to be ProPublica. And it’s a good thing that we have ProPublica."  

 Hey, that's where I work! 

 The article continues: 

 "Spreading its journalistic wealth has long been core to its mission. The latest iteration of that is the 50 State Initiative, announced last month." 

 The 50 State Initiative is a commitment to publishing accountability journalism in 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>"It’s a good time to be ProPublica. And it’s a good thing that we have ProPublica." </p>

<p>Hey, that's where I work!</p>

<p>The article continues:</p>

<p>"Spreading its journalistic wealth has long been core to its mission. The latest iteration of that is the 50 State Initiative, announced last month."</p>

<p>The 50 State Initiative is a commitment to publishing accountability journalism in every US state over the next five years. This is an expansion of the Local Reporting Network, which was already doing great work in partnership with local newsrooms. As this piece points out, there are actually only two states where ProPublica hasn't run some kind of an investigative story - but, of course, the 50 States Initiative goes much deeper than that. It's an exciting time to be working here.  <a href="https://werd.io/tag/Media">#Media</a></p>
            <p>[<a href="https://www.niemanlab.org/2024/05/propublicas-new-50-states-commitment-builds-on-a-decade-plus-of-local-news-partnerships/">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   21:34 &bull;
   13 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/decentralized-systems-will-be-necessary-to-stop-google-from-putting'>Decentralized Systems Will Be Necessary To Stop Google From Putting The Web Into Managed Decline</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 "The various decentralized social media systems that have been growing over the past few years offer a very different potential approach: one in which you get to build the experience you want, rather than the one a giant company wants." 

 There's a chicken and egg problem here: while decentralized systems are absolutely going to be part of the solution, or at least hold most 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>"The various decentralized social media systems that have been growing over the past few years offer a very different potential approach: one in which you get to build the experience you want, rather than the one a giant company wants."</p>

<p>There's a chicken and egg problem here: while decentralized systems are absolutely going to be part of the solution, or at least hold most of the properties that make for a good solution, they also need to have a critical mass of people who use them.</p>

<p>A lot of people are looking towards Threads to provide this critical mass, but just as I'd invite newsrooms to consider how to gain more traffic without Apple News, I'd invite the federated social web community to consider what a growth looks like without Meta. It's not that Threads won't help - it's that you don't want to be dependent on a megacorp to provide the assistance you need. You never know when they'll change their policies and look elsewhere.</p>

<p>Still, the point stands: decentralization is a key part of the answer. There's a lot to be gained from investing in projects that provide strong user experiences, solve concrete real human problems alongside the ideological ones and the existential threats, and onboard a new generation of internet users to a better way to share and browse.</p>

<p>That's a tall order, but, as always, I'm hopeful.  <a href="https://werd.io/tag/Technology">#Technology</a></p>
            <p>[<a href="https://www.techdirt.com/2024/05/21/decentralized-systems-will-be-necessary-to-stop-google-from-putting-the-web-into-managed-decline/">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   20:29 &bull;
   13 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/oxide'>Startups on hard mode: Oxide. Part 1: Hardware</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  What is tougher than building a software-only or hardware-only startup? Building a combined hardware and software startup. This is what Oxide is doing, as they build a “cloud computer.” A deepdive.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>👋 Hi, this is Gergely with a subscriber-only issue of the Pragmatic Engineer Newsletter. In every issue, I cover challenges at Big Tech and startups through the lens of engineering managers and senior engineers. To get articles like this in your inbox, every week, subscribe:</p><p><a href="https://newsletter.pragmaticengineer.com/subscribe?">Subscribe now</a></p><p>What does an early-stage startup look like? Usually, there’s an office with devs working on laptops, a whiteboard with ideas, and lots of writing of code. Sometimes, there’s no physical office because team members work remotely, sharing ideas on virtual whiteboards.</p><p>But what about companies which aren’t “pure” software startups, and which focus on hardware? I recently got a glimpse of one in San Francisco, at the offices of Oxide Computer Company, which is a startup building a new type of server. I was blown away by the working environment and the energy radiating from it. This is their office:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e61ec2b-7bd3-4b71-be93-c5d51ecc3de6_1600x1306.png">    </a>Hive of activity: Oxide co-founder and CTO, Bryan Cantrill at Oxide’s office, with under-construction servers, hardware measurement tools, laptops, and cables <p>Some things are hard to describe without experiencing them, and this includes being at a hardware+software startup right as the first product is being finished, with the team already iterating on it. In today’s issue, we cover hardware at Oxide:</p>  <p>Why build a new type of cloud computer?</p>  <p>Building a networking switch from scratch</p>  <p>Using “proto boards” to build faster</p>  <p>A remote-first hardware company</p>  <p>Custom hardware manufacturing process</p>  <p>The importance of electrical engineers (EE)</p>  <p>Working closely with hardware vendors</p>   1. Why build a new type of cloud computer? <p>If you want to build an application or service for millions of users, there are two main options for the infrastructure:</p>  <p>Cloud provider. AWS, GCP, Azure, Oracle, etc. Elsewhere, Uber is making the move from on-prem to GCP and Oracle, <a href="https://newsletter.pragmaticengineer.com/p/uber-move-to-cloud">as previously covered</a>.</p>  <p>On-premises (prem.) Operate your own servers, or more commonly rent rack space at a data center for them. This approach is sometimes called a “private cloud.” We’ve covered how and why booking platform <a href="https://newsletter.pragmaticengineer.com/p/inside-agodas-private-cloud">Agoda is staying on-prem and how its private cloud is built</a>. Social media site Bluesky also <a href="https://newsletter.pragmaticengineer.com/p/bluesky">uses its own data centers</a> since leaving AWS.</p>  <p>In data centers, the unit of measurement is “one rack.” A rack is a storage unit that can hold a few dozen servers; often referred to as “pizza box servers” because of their shape. Thicker types are called “rack servers.” Side note: an alternative to pizza box servers is <a href="https://newsletter.pragmaticengineer.com/i/124842294/the-evolution-of-data-centers-at-agoda">blade servers, inserted into blade enclosures</a> as building blocks in data centers.&nbsp;</p><p>Here’s a “pizza box server” that online travel booking platform Agoda utilized heavily:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdeb72058-0c61-425f-b332-fb8402f33585_1270x582.png">    </a>A server Agoda used in its private cloud. See <a href="https://newsletter.pragmaticengineer.com/p/inside-agodas-private-cloud">Inside Agoda’s Private Cloud: Part 1</a> <p>And here’s some commodity servers in Oxide’s office:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bac98b7-2a72-497a-8f98-ed6f314fead6_1600x768.png">    </a>A typical server rack at Oxide’s office at 50% capacity, running Gigabyte and Tyan servers. <p>The rack in the image above was operating during my visit. It was loud and generated a lot of heat as expected, and there were lots of cables. It’s messy to look at and also to operate: the proprietary PC-era firmware causes security, reliability and performance issues. “PC-era” refers to the 1980s – early-2000s period, before <a href="https://en.wikipedia.org/wiki/X86#History">x86 64-bit machines</a> became the servers of choice.</p><p>Elsewhere, Big Tech companies have manufactured their own highly optimized racks and servers, but these aren’t for sale. The likes of Meta, Google, and Amazon no longer use traditional racks, and have “hyper-scaled” their servers to be highly energy efficient, easier to maintain, and with few cables.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f4895b2-6514-498b-8807-c73a116d36e6_1600x1063.png">    </a>Google’s custom-built servers. Source: <a href="https://www.google.com/about/datacenters/data-security/">Google</a> <p>Joe Kava, VP of Google's Data Center Operations, described these racks back in 2017:</p><blockquote><p>“Many of our racks don’t look like traditional server racks. They are custom designed and built for Google, so we can optimize the servers for hyper-efficiency and high-performance computing.”</p></blockquote><p>Back to Oxide, whose vision is to build a cloud computer that incorporates the technological advances of Big Tech’s cloud racks, but makes them available to all. What if smaller tech companies could purchase energy-efficient servers like those that Meta, Amazon, Google and Microsoft have designed for themselves, and which customers of the big cloud providers like AWS, GCP, and Azure use – but without being locked in?</p><p>This is what the Oxide Computer offers, and I’ve seen one of its first racks. It appears similar in size to a traditional rack, but the company says it actually occupies 33% less space than a traditional rack, while offering the same performance. It’s much quieter than an everyday commodity server; in comparison the Gigabyte and Tyan servers are ear-splitting, and there are hardly any cables compared to a typical server.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe09302cf-5e06-4576-94dd-ee1d59b23fe9_1384x964.png">    </a>Side and rear views of a fully-loaded Oxide rack. Fewer cables and SO MUCH quieter <p>The benefits of the Oxide computer compared to traditional racks:</p>  <p>Faster installation: installing a traditional rack typically takes weeks or months, says Bryan, because the servers need to be put in, wired up, and then tested. The Oxide rack comes fully assembled; it just needs to be slotted in at a data center.</p>  <p>Space and power efficiency. Uses less power and occupies less space. Less noise indicates superior power efficiency, with fans not needing to work as hard because the heat is channeled better.</p>  <p>Comes with integrated software to manage elastic infrastructure. With traditional rackmounted servers, it’s necessary to select software to manage virtual machines, like <a href="https://www.vmware.com/solutions/virtualization.html">VMware</a>, <a href="https://maas.io/">Metal as a Service</a>, <a href="https://www.proxmox.com/en/proxmox-virtual-environment/overview">Proxmox Virtual Environment</a>, and <a href="https://wiki.openstack.org/wiki/Ironic">OpenStack Ironic</a>. The Oxide cloud computer includes built-in virtualization for storage (an equivalent of AWS’s <a href="https://docs.aws.amazon.com/ebs/latest/userguide/what-is-ebs.html">Elastic Block Store</a>) and also networking (an alternative to <a href="https://en.wikipedia.org/wiki/Virtual_private_cloud">virtual private clouds</a>.)&nbsp;</p>  <p>Oxide’s target customer is anyone running large-scale infrastructure on-prem for regulatory, security, latency, or economic reasons. The Oxide rack comes with 2,048 CPU cores (64 cores per “sled,” where one sled is Oxide’s version of a “rackmount server”,) 16-32TB of memory (512GB or 1TB of memory per sled) and 1PB (petabyte) of storage (32TB storage per sled). See <a href="https://oxide.computer/product/specifications">full specification.</a></p><p>This kind of setup makes sense for companies that already operate thousands of CPU cores. For example, we previously covered how Agoda <a href="https://newsletter.pragmaticengineer.com/p/inside-agodas-private-cloud">operated</a> 300,000 CPU cores in its data centers in 2023; at such scale investing in racks like Oxide’s could make sense. Companies in the business of selling virtual machines as a service might also find this rack an interesting investment to save money on operations, compared to traditional racks.</p><p>An interesting type of customer are companies running thousands of CPU cores in the public cloud, but which are frustrated by network latencies. There’s a growing sense that multi-tenancy in public clouds; where one networking switch serves several racks and customers, causes worse latency which cannot be debugged or improved. In contrast, an Oxide rack offers dedicated rack space in data centers. Using these servers can also considerably reduce network latencies because the customer can choose the data center they use, based on their own regional needs. Customers also get full control over their networking and hardware stack – something not possible to do when using a cloud provider.</p><p>Oxide doesn’t target smaller startups that only need a few hundred CPU cores. For these businesses, using cloud providers, or buying/renting and operating smaller bare metal servers is the sensible solution.</p> 2. Building a networking switch from scratch <p>In server manufacturing, where does innovation come from? I asked Bryan:</p><blockquote><p>“Companies like Google, Meta and similar companies producing their custom hardware and software to build better servers, could bring competition to the market. However, it’s highly unlikely that these companies would release their servers as a commercial product. It’s not their business model.&nbsp;</p><p>So, no, the next big server innovation will not come from Google or a similar company. It will come from a startup. And we want to be that startup.”</p></blockquote><p>Oxide had to design two pieces of hardware from scratch: the switch and the server.</p> Why build a switch instead of integrating a third-party switch? <p>Oxide’s mission is to build their own cloud computer. Building a custom server usually means taking off-the-shelf components for a system and integrating it all together, including the server chassis, a reference design system board, and a separately-developed network switch. A “reference design” is a blueprint for a system containing comprehensive guidance on where to place its elements, that’s been certified to work as intended: it should not overheat, or cause unexpected interference.</p><p>However, Oxide also needed to build their own networking switch, as well as build a custom server – which is quite an undertaking! This need came from the constraint that Oxide wanted to control the entire hardware stack, end-to-end. A networking switch is a “mini-computer” in itself. So in practice, they designed and built two computers, not just one.</p><p>Producing a separate switch meant integrating a switching application-specific integrated circuit (<a href="https://en.wikipedia.org/wiki/Application-specific_integrated_circuit">ASIC</a>), management CPU, power supplies, and physical network ports.</p><p>Oxide’s goals for this switch were:</p>  <p>Highly available operation. Each Oxide rack has two networking switches which operate simultaneously, as per high availability. If links to one switch have issues, or a switch needs to be serviced, then the servers can still access networks via the other switch, ensuring more reliable operation than with a single-switch setup.&nbsp;</p>  <p>Integrated with control plane software. The single most important factor in Oxide’s decision was the desire to deliver a quality end-to-end experience for multi-tenant elastic infrastructure. The team knew from their experience of deploying public cloud infrastructure that the switch is often a nexus of reliability and performance issues.</p>  <p>Use the same “core” hardware as the server. The switch must use the same regulators and power controller as the Oxide servers.</p>  <p>Building the custom networking switch took around 2 years, from designing it in March 2020, to the first unit being assembled in January 2022.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77f367ca-e917-446a-95a0-5a055a876344_998x950.png">    </a>The initial build of Oxide’s network switch, codenamed “Sidecar”  <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F009fab28-c0a5-4e12-9158-c2b5de0eee5b_1196x1128.png">    </a>The front view of the initial engineering build of Sidecar <p>Building custom hardware almost always comes with unexpected challenges. In the case of the networking switch, the team had to work around an incorrect voltage regulator on the board, marked with yellow tape in the image above.</p> 3. Using proto boards to build hardware faster <p>Proto boards is short for “prototype printed circuit boards,” which help the company test small components to ensure they work independently. Once validated, those components can be used as building blocks.</p><blockquote><p>“When we set out to build a server from scratch, we didn’t want to go straight to building the server motherboard. However, when we started we had zero full-time electrical engineers!</p><p>There’s a process I learned from robotics people in a previous job, called a ‘roadkill build.’ You get all the parts that will end up in the thing you eventually build, but instead of being integrated, they are all spread out across a bench with nasty cables between them, so that you can probe them, poke them, and replace them. We thought it would be a good idea to do this for the servers.”</p></blockquote> First prototype board <p>For the initial prototype printed circuit board, the team started with the service processor. This was a well understood, very critical part of the server and the switch. The team decided to build it from two off-the shelf microcontrollers, and built a prototype board around this:</p><p>Starting out, the company took inspiration from robotics. Founding engineer, Cliff L. Biffle, <a href="https://github.com/oxidecomputer/oxide-and-friends/blob/master/2023_01_16.md">shares</a>:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffed8ca7d-f0ce-4a7e-bc5b-6c4fad41e066_1600x952.png">    </a>The service processor’s prototype board, which facilitates all communication between components of the server <p>The team found that It’s possible to bite off more than you can chew, even with prototype circuit boards, The first board was a success in that it worked: upon start up, the hardware and software “came up” (in electrical engineering, “coming up” refers to the successful powering up and initializing of an electronic system board.) But the time it took to develop was longer than the team wanted.</p><p>It turns out this prototype board was too highly integrated, with too many moving parts and too many I/O pins. There was simply too much on the board for it to be productive. The team learned that progress would be faster with multiple, simpler boards as pluggable hardware modules, instead of one complicated board with lots of functionality and many fixed functions. As engineer <a href="https://www.linkedin.com/in/mxshift/">Rick Altherr</a> – who worked on the board – <a href="https://oxide.computer/podcasts/oxide-and-friends/1173990">noted</a>:</p><blockquote><p>“We put too many things on. The concept we ran into was that an x86 server, with all of its management, is way too complicated. Let’s slim it down to just the management subsystems. This board is intended to be the service processor. But it turns out that even that’s too much.&nbsp;</p><p>By having so many things be on one board, instead of pluggable modules, it meant that we committed to a lot of design choices. For example, the two ethernet jacks were never actually used because we changed our philosophy on how that was going to work, before we got the boards back from manufacturing.”</p></blockquote> A more modular approach <p>Before building the service processor board, the team separated out the “root of trust” (RoT) functionality onto a separate board. The RoT hardware is the foundational base upon which the security and trust of the entire system are built. A RoT has “first instruction integrity,” guaranteeing exactly which instructions run upon startup. The RoT sets up the secure boot and permanently locks the device to ensure ongoing secure operation. Below is the prototype of Oxide’s RoT module:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12630492-6230-4e3c-9edc-51972a39e997_1600x1262.png">    </a>The root of trust module; a separate, pluggable prototype <p>Other modules the Oxide team built included a power module, a multiplexer (a device with multiple input signals, which selects which signal to send to the output):</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79caa9f-57c7-45fa-af67-695e0bb5d475_1470x858.png">    </a>The Oxide team built many modules for prototyping and testing <p>Over time, the team found the right balance of how much functionality the prototype board needed. Below is an evolved prototype board version of the service processor:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8c939a2-a630-4345-bf3e-c7d9915741b5_1052x1148.png">    </a>Evolved version of the prototype service processor. Redundant ports like Ethernet are absent <p>The Oxide team calls this board a “workhorse” because they can plug in so many modules, and do so much testing and hardware and software development with it. Here’s an example:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad40beac-01e3-471e-9340-76ec9c5bba19_1600x1200.png">    </a>The hardware version of modular development: a prototype service processor board with modules plugged in <p>A prototype board is unit testing for hardware. In software development, unit tests ensure that components continue to work correctly as the system is modified. Oxide found that prototype boards come pretty close to this approach, and allowed Oxide to iterate much faster on hardware design, than by manufacturing and validating test devices.</p> Using smart workarounds to iterate faster 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/oxide">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e61ec2b-7bd3-4b71-be93-c5d51ecc3de6_1600x1306.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:24 &bull;
   14 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2529'>Proposed Implementer&#8217;s Draft of OpenID Federation</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The OpenID Connect working group has started working group last call (WGLC) for a proposed Implementer’s Draft of the OpenID Federation specification. As described in the WGLC message: OpenID Federation -35 has been published at https://openid.net/specs/openid-federation-1_0-35.html and https://openid.net/specs/openid-federation-1_0.html. This draft is being proposed as the fourth (and hopefully fi
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The <a href="https://openid.net/wg/connect/">OpenID Connect working group</a> has started working group last call (WGLC) for a proposed Implementer’s Draft of the <a href="https://openid.net/specs/openid-federation-1_0.html">OpenID Federation</a> specification.  As described in the <a href="https://lists.openid.net/pipermail/openid-specs-ab/2024-May/010320.html">WGLC message</a>:</p>
<blockquote><p>
OpenID Federation -35 has been published at <a href="https://openid.net/specs/openid-federation-1_0-35.html">https://openid.net/specs/openid-federation-1_0-35.html</a> and <a href="https://openid.net/specs/openid-federation-1_0.html">https://openid.net/specs/openid-federation-1_0.html</a>.  This draft is being proposed as the fourth (and hopefully final) Implementer’s Draft of the specification.
</p></blockquote>
<p>An Implementer’s Draft is a stable version of a specification providing intellectual property protections to implementers of the specification.  The two-week working group last call ends on Friday, May 31, 2024.  Unless reasons are identified during the last call to substantially revise the specification, the 45-day OpenID Foundation-wide review of the specification for approval as an OpenID Implementer’s Draft will shortly follow.</p>
<p>Special thanks to all the implementers and deployers who provided feedback to get us to this point!</p>
   
  
    
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/openid-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   07:08 &bull;
   14 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 20. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/as-clicks-dry-up-for-news-sites-could-apple-news'>As clicks dry up for news sites, could Apple News be a lifeline?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 "The free version of Apple News is one of the biggest news platforms in the world. It’s the most widely used news application in the United States, the U.K., Canada, and Australia, and boasted over 125 million monthly users in 2020."  

 And publications are becoming dependent on it. 

 I agree strongly with the journalist's view at the bottom of this piece: 

 "It incentivize
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>"The free version of Apple News is one of the biggest news platforms in the world. It’s the most widely used news application in the United States, the U.K., Canada, and Australia, and boasted over 125 million monthly users in 2020." </p>

<p>And publications are becoming dependent on it.</p>

<p>I agree strongly with the journalist's view at the bottom of this piece:</p>

<p>"It incentivizes users to subscribe to Apple News+ rather than to publications directly, likely cannibalizing some potential revenue. It’s driving editorial decisions, meaning publishers are once again changing their content strategy to placate a platform. And of course the company could wake up one day and decide, like Facebook, that it no longer really wants to be in the news business, leaving news publishers stranded."</p>

<p>Newsrooms - say it with me - need to establish direct, first-party connections with their audiences. Anything else gives a third party too much supplier power over their businesses and presents an existential risk. Apple News is useful right now, but at its heart the dynamics that drive it are no different to Facebook or Twitter. There's nothing to say it's here for good, and there's nothing smart about letting Apple own your relationship with your readers.  <a href="https://werd.io/tag/Media">#Media</a></p>
            <p>[<a href="https://www.semafor.com/article/05/19/2024/as-clicks-dry-up-for-news-sites-could-apples-news-app-be-a-lifeline">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   15:53 &bull;
   15 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/the-indiewebs-next-stage'>The IndieWeb’s next stage?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 "I want the IndieWeb to be a viable alternative to social media, gradually widening the audience beyond tech-savvy folks by making the tools easier to use and more reliable."  

 This is what we were trying for with Known: something that felt social but was fully under the user's control. We had installers at third-party hosts; we had our own managed service; we had the open s
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>"I want the IndieWeb to be a viable alternative to social media, gradually widening the audience beyond tech-savvy folks by making the tools easier to use and more reliable." </p>

<p>This is what we were trying for with Known: something that felt social but was fully under the user's control. We had installers at third-party hosts; we had our own managed service; we had the open source code for people who wanted to use that directly.</p>

<p>The fediverse adds a missing piece here: Known suffered immensely from a blank page and no reader view when you logged in for the first time. Now we can build platforms that immediately connect people to a much wider social network that is outside of monolithic corporate control but also makes it (relatively) easy to find the people you care about.</p>

<p>A combination between the fediverse and indieweb is almost inevitable. This is what Ghost appears to be building today, for example, with its new integrated fediverse reader tool. WordPress may also be headed in that direction. And there will be many others.</p>

<p>A huge +1, also, to the idea that we can "manifest momentum by speaking aloud your dreams and letting others share them with you". This is how community-building works.</p>

<p>And, for the record, I'm all-in.  <a href="https://werd.io/tag/Technology">#Technology</a></p>
            <p>[<a href="https://tracydurnell.com/2024/05/17/indieweb-next-stage/">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   15:37 &bull;
   15 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.patrick-breyer.de/piraten-europaabgeordneter-zu-assange-urteil-whistleblower-ermutigen-statt-sie-mundtot-zu-machen/'>Piraten-Europaabgeordneter zu Assange-Urteil: Whistleblower ermutigen statt sie mundtot zu machen!</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Zu der heutigen Entscheidung des britischen High Court, die Berufung von Wikileaks-Gründer Julian Assange gegen seine Auslieferung an die USA zuzulassen, erklärt der Europaabgeordnete Dr. Patrick Breyer: 
 „Die heutige Gerichtsentscheidung gibt … 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Zu der heutigen Entscheidung des britischen High Court, die Berufung von Wikileaks-Gründer Julian Assange gegen seine Auslieferung an die USA zuzulassen, erklärt der Europaabgeordnete Dr. Patrick Breyer:<br><br>„Die heutige Gerichtsentscheidung gibt Hoffnung, dass europäische Menschenrechte und Gerichte die Aufdeckung von Unrecht, Machtmissbrauch, Korruption und Kriegsverbrechen schützen, ohne Freund und Feind mit zweierlei Maßstäben zu messen. Ich wünsche mir, dass aus der von den USA gewollten Abschreckung von Whistleblowern und Journalisten weltweit ihre Ermutigung durch ein Grundsatzurteil wird.<br><br>Dass Zensursula von der Leyen zu dem Fall schweigt, ist beschämend, aber bezeichnend für die Mutter der Pläne zur Chatkontrolle, die ihre eigenen dienstlichen SMS mit Pharmakonzernen geheim halten will. Für mich als Pirat ist Transparenz von Regierungshandeln die unverzichtbare Grundlage jeder Demokratie. Transparenz darf kein Verbrechen sein!“<br><br>Breyer veranstaltet <a href="https://www.patrick-breyer.de/event/webinar-zur-drohenden-auslieferung-von-julian-assange-wie-weiter/">heute um 15 Uhr ein Webinar mit Rechtsanwalt Sören Schomburg aus Assanges Verteidigungsteam und Marie Wasilewski als Sprecherin des Aktivistenbündnisses FreeAssange.EU</a>, um über die Gerichtsentscheidung und die nächsten Schritte zu sprechen.<br><br>Freitag hatte <a href="https://www.patrick-breyer.de/eu-abgeordnete-fordern-die-britische-regierung-auf-julian-assanges-auslieferung-zu-stoppen/">31 Europaabgeordnete einen von Breyer initiierten Brief an den britischen Innenminister gesandt, in dem die Freilassung von Assange gefordert wird.</a> Zu den Unterzeichnern gehören die deutschen Europaabgeordneten Martin Sonneborn (die PARTEI) und Erik Marquardt (Bündnis 90/Grüne).</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   12:55 &bull;
   15 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/shareopenly-is-now-on-tedium'>ShareOpenly is now on Tedium</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
 I adore the way ShareOpenly has been added to Tedium:    You can see it for yourself on all its posts, including this great one about the decline of the ball mouse. Its founder, Ernie Smith, told me: “figured I had to have fun with it”.  
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://werd.io/file/664aae5867b29eb53207b332'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
<p>I adore the way <a href="https://shareopenly.org">ShareOpenly</a> has been added to <a href="https://tedium.co">Tedium</a>:</p><p></p><p>You can see it for yourself on all its posts, including <a href="https://tedium.co/2024/05/19/optical-mouse-history-evolution/">this great one about the decline of the ball mouse</a>. Its founder, <a href="https://erniesmith.net/">Ernie Smith</a>, <a href="https://writing.exchange/@ernie/112470878385444731">told me</a>: “figured I had to have fun with it”.</p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://werd.io/file/664aae5867b29eb53207b332'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   01:59 &bull;
   15 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 18. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@m-ruminer/'>Michael Ruminer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://m-ruminer.medium.com/trust-over-ip-in-decentralized-information-and-the-role-of-delegates-81d09e4ff100?source=rss-7e85224c0a32------2'>Trust Over IP in Decentralized Information and the Role of Delegates</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   As I read the Issuer Requirements Guide from Trust Over IP Foundation it only reinforces my long-standing belief that well-known trusted proxy/witness/delegate issuers will serve an important role. Every organization or even many organizations will not want to maintain the governance needs to be a trusted issuer and instead will outsource that activity to a delegate that can ensure integrity and g
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*TDtjXQ0MvORF8eIY2VfJ7g.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>As I read the <a href="https://trustoverip.org/wp-content/uploads/Issuer-Requirements-Guide-V0.01-2024-01-30.pdf">Issuer Requirements Guide</a> from <a href="https://trustoverip.org/">Trust Over IP Foundation</a> it only reinforces my long-standing belief that well-known trusted proxy/witness/delegate issuers will serve an important role. Every organization or even many organizations will not want to maintain the governance needs to be a trusted issuer and instead will outsource that activity to a delegate that can ensure integrity and governance over their issuing activities.</p><p>For example, a gig platform such as Uber will not issue proof of work and proof of income credentials but will instead have a delegate do this on their behalf. The overhead for them in proper governance would be too steep. Instead the proxy/witness/delegate will collect the information from the source and issue a credential on their behalf. The delegate would be the trusted party in Trust Over IP&nbsp;(TOIP).</p><p>We see this in today’s world with service bureaus for different activities. The future will not be quite as decentralized as the potential for it to be because orgs will not wish to maintain the needed structures to ensure it. There is a place for well-known delegates.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*TDtjXQ0MvORF8eIY2VfJ7g.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:02 &bull;
   17 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 17. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/share-openly-a-simple-icon-for-a-new-social-sharing'>Share Openly: A simple icon for a new social sharing service</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 A lovely blog post by Jon Hicks on his process for creating the ShareOpenly icon. Characteristically, lots of care and attention went into this. 

 I'm really glad you get to see the open hand icons, which we eventually decided against, but feel really warm and human. 

 Jon's amazing, lovely to work with, and has a really impressive body of work. I'm grateful he was able to c
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>A lovely blog post by Jon Hicks on his process for creating the ShareOpenly icon. Characteristically, lots of care and attention went into this.</p>

<p>I'm really glad you get to see the open hand icons, which we eventually decided against, but feel really warm and human.</p>

<p>Jon's amazing, lovely to work with, and has a really impressive body of work. I'm grateful he was able to contribute such an important part of this personal project.  <a href="https://werd.io/tag/Technology">#Technology</a></p>
            <p>[<a href="https://hicks.design/journal/share-openly">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:56 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/a-new-ani-difranco-album-is-something-to-celebrate'>A new Ani DiFranco album is something to celebrate</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
 I’ve been following Ani DiFranco for decades. I’ve seen her play live around twenty times: she always brings a kind of joyful, progressive energy that leaves me motivated and buzzing.  She has a new album out, and it feels like a return to visceral, honest form. It’s not quite the acoustic punk from the late nineties / early aughts — seriously, go check out Living in Clip, Not a Pretty Girl or D
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
<p>I’ve been following <a href="https://anidifranco.com/">Ani DiFranco</a> for decades. I’ve seen her play live around twenty times: she always brings a kind of joyful, progressive energy that leaves me motivated and buzzing.</p><p><a href="https://anidifranco.bandcamp.com/album/unprecedented-sh-t">She has a new album out</a>, and it feels like a return to visceral, honest form. It’s not quite the acoustic punk from the late nineties / early aughts — seriously, go check out <a href="https://anidifranco.bandcamp.com/album/living-in-clip">Living in Clip</a>, <a href="https://anidifranco.bandcamp.com/album/not-a-pretty-girl">Not a Pretty Girl</a> or <a href="https://anidifranco.bandcamp.com/album/dilate">Dilate</a> — and it goes to some really experimental places, but I’m into it. This time, rather than making it on her own, she’s worked with producer <a href="https://en.wikipedia.org/wiki/BJ_Burton">BJ Burton</a>, who’s also worked with Bon Iver and Taylor Swift.</p><p>We need progressive, momentum-bringing, energetic music more than ever. Ani delivers. And even the name of the album itself — Unprecedented Sh!t — feels very apt for the era.</p><p>From the liner notes:</p><blockquote><p>The title Unprecedented Sh!t is not only representative of how much of a sonic departure the 11-track album is from Ani’s other work, but also a political and social commentary on the current state of the world. “We find ourselves in unprecedented times in many ways, faced with unprecedented challenges. So, our responses to them and our discourse around them, need to rise to that level.”</p></blockquote><p>Amen.</p> 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   15:20 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/apple-spacex-microsoft-return-to-office-mandates-drove-senior-talent-away'>Apple, SpaceX, Microsoft return-to-office mandates drove senior talent away</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 

 "Taken together, our findings imply that return to office mandates can imply significant human capital costs in terms of output, productivity, innovation, and competitiveness for the companies that implement them." 

 There's no doubt that there's a lot of value in being in the same physical room together; I'm writing this on the day after a work summit that brought my team to
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 

<p>"Taken together, our findings imply that return to office mandates can imply significant human capital costs in terms of output, productivity, innovation, and competitiveness for the companies that implement them."</p>

<p>There's no doubt that there's a lot of value in being in the same physical room together; I'm writing this on the day after a work summit that brought my team together from across the country, and I'm still buzzing from the energy. But I think anyone in tech that proposes a full-time return to office policy needs to rethink.</p>

<p>It comes down to this: "it's easier to manage a team that's happy". People want their lives and contexts to be respected; everyone's relationship with their employers has been reset over the last few years. This goes hand in hand with the resurgence of unions, too: the contract between workers and employers is being renegotiated, and particularly for parents and carers, but really for everyone, working from home yields a kind of freedom that's hard to replace. And asking people to come back reads as a lack of trust and autonomy that erodes relationships and decimates morale.  <a href="https://werd.io/tag/Business">#Business</a></p>
            <p>[<a href="https://arstechnica.com/information-technology/2024/05/rto-mandates-led-to-pronounced-exodus-of-senior-workers-at-top-tech-firms/">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:11 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.talkingidentity.com'>Talking Identity</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.talkingidentity.com/2024/05/talking-ethics-in-identity-at-identiverse-2024.html'>Talking Ethics in Identity at Identiverse 2024</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Identiverse being the best identity conference around means that it’s always a challenge coming up with a talk proposal, as Andrew Hindle and team raise the bar each year. The process acts as a forcing function for me to think about the topics I want to bring to the community for discussion, topics that I […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://identiverse.com/">Identiverse</a> being the best identity conference around means that it’s always a challenge coming up with a talk proposal, as <a href="https://www.linkedin.com/in/ahindle/">Andrew Hindle</a> and team raise the bar each year. The process acts as a forcing function for me to think about the topics I want to bring to the community for discussion, topics that I encounter all the time in #DayJob, but seldom have time to dig into.</p>



<p>This year I wanted to build on my talk from last year, but realized I couldn’t tackle it alone. Thankfully, I managed to con(vince) two absolute rock stars in <a href="https://www.linkedin.com/in/michelledennedy/">Michelle Dennedy</a> and <a href="https://www.linkedin.com/in/evemaler/">Eve Maler</a> to join me on stage for a <a href="https://identiverse.com/idv24/session/2089520/">fireside chat</a> about the ethics imperative facing our industry.</p>



<a href="https://identiverse.com/idv24/session/2089520/"></a>Screenshot



<p>As the abstract says, “What happens when you throw a privacy wonk, a standards guru, and a product architect that have been working in identity far too long on to a conference stage together?” Join us in Vegas and find out.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://7ee14c.p3cdn1.secureserver.net/wp-content/uploads/2024/05/Identiverse2024_Promo_Banner-1024x471.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   13:19 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.patrick-breyer.de/eu-abgeordnete-fordern-die-britische-regierung-auf-julian-assanges-auslieferung-zu-stoppen/'>EU-Abgeordnete fordern die britische Regierung auf, Julian Assanges Auslieferung zu stoppen</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Heute haben sich auf Initiative des EU-Abgeordneten Dr. Patrick Breyer (Piratenpartei) 31 Europaabgeordnete verschiedener Fraktionen in einem offenen Brief an den britischen Innenminister James Cleverly gewandt und … 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Heute haben sich auf Initiative des EU-Abgeordneten Dr. Patrick Breyer (Piratenpartei) <a href="https://www.patrick-breyer.de/wp-content/uploads/2024/05/MEP-Letter-Assange-before-UK-extradition-ruling-of-20.05.2024.pdf">31 Europaabgeordnete verschiedener Fraktionen in einem offenen Brief an den britischen Innenminister James Cleverly gewandt und dringlich appelliert, die Auslieferung von Wikileaks-Gründer Julian Assange an die Vereinigten Staaten zu stoppen.</a> Dies geschieht im Vorfeld der bevorstehenden Gerichtsentscheidung am 20. Mai 2024, die voraussichtlich final über Assanges Schicksal entscheiden wird.</p>



<p>In dem Brief wird die britische Regierung aufgefordert, ihre Verantwortung im Hinblick auf Menschenrechte und Pressefreiheit wahrzunehmen. „Genau wie die EU-Kommission verbreitet die britische Regierung die Mär, über die Auslieferung hätten ausschließlich die Gerichte zu entscheiden. § 70 (2) des britischen Auslieferungsgesetzes gibt dem Innenminister die Befugnis, die Auslieferung zu verweigern, wenn sie gegen das Recht auf Leben oder das Verbot der Folter und unmenschlicher oder erniedrigender Behandlung der Europäischen Menschenrechtskonvention verstoßen würde“, erklärt Breyer. „Die psychische Gesundheit von Julian Assange, die potenziellen Haftbedingungen in den USA und das reale Risiko eines Suizids bei Auslieferung führen dazu, dass die Auslieferung eine solche unmenschliche oder erniedrigende Behandlung darstellen würde.“</p>



<p>Die Abgeordneten betonen in ihrem Schreiben außerdem, dass die Verfolung von Julian Assange politisch motiviert sei. Die Bestimmungen des britisch-amerikanischen Auslieferungsvertrags verbieten zu Recht die Auslieferung wegen politischer Straftaten. Die eindeutig politische Natur dieses Falls wird durch zahlreiche und hochgradig voreingenommene Aussagen führender Persönlichkeiten der US-Politik deutlich, die seit mindestens 2011 die extralegale Bestrafung oder Ermordung von Herrn Assange fordern.</p>



<p>Ein weiterer Kritikpunkt ist das Fehlen einer Garantie der US-Regierung, dass Assange vor Gericht dieselben Rechte wie ein US-Bürger erhalten würde. „Ein Verfahren gegen jemanden, der Dokumente veröffentlicht hat, in einem Land, das möglicherweise grundlegende Rechte auf Meinungs- und Pressefreiheit nicht anerkennt oder anwendet, ist inakzeptabel“, so Breyer.</p>



<p>Breyer und die anderen unterzeichnenden EU-Abgeordneten fordern die britische Regierung auf, die Pressefreiheit und das Recht auf freie Meinungsäußerung zu schützen und die Auslieferung von Julian Assange zu stoppen.</p>



<p>Die Unterzeichner des Briefes schließen sich den Forderungen von großen Organisationen wie Amnesty International und Reporter ohne Grenzen an, die die sofortige Freilassung von Julian Assange verlangen. Sie betonen, dass seine anhaltende Inhaftierung das Recht auf freie Meinungsäußerung gefährdet.</p>



<p></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   07:59 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2024/05/16/blog-newsletter/'>Blog + Newsletter</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Newsletters are all the rage now. In recognition of that, I blogged here two years ago about the idea of writing a solo newsletter. Since then I’ve been co-producing this one with Katherine Druckman at Reality 2.o. It’s a Substack one, so I know how that game works on the production as well as the […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Newsletters are all the rage now. In recognition of that, I blogged <a href="https://doc.searls.com/2022/06/21/newsletter/">here</a> two years ago about the idea of writing a solo newsletter. Since then I’ve been co-producing <a href="https://reality2.substack.com/">this one</a> with Katherine Druckman at <a href="https://www.reality2cast.com/">Reality 2.o</a>. It’s a Substack one, so I know how that game works on the production as well as the consumption ends.</p>
<p>Recently I also learned that WordPress makes it easy to turn blogs into newsletters, which is why you see “Get New Posts by Email” in the right column here. So, wanting an image to go with the news that this blog can now be a newsletter if you like, I said to ChatGPT, “Draw me Doc Searls blogging on the left side of the image, and producing a newsletter on the right side.” It gave me this, including the caption:</p>
Here’s the illustration depicting Doc Searls engaging in two different activities. On the left, he is blogging, and on the right, he is producing a newsletter. This split scene captures his multitasking abilities in digital content creation.
<p>Except for the slouch, I never looked like that. But what the hell. AI is wacky shit, so there ya go.</p>
<p>Feel free to subscribe.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://150108457.v2.pressablecdn.com/wp-content/uploads/2024/05/DALL·E-2024-05-16-23.45.15-An-illustrative-split-scene-depicting-Doc-Searls-engaging-in-two-different-activities.-On-the-left-side-Doc-Searls-is-shown-blogging-sitting-at-a-mo-1-1024x585.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   03:53 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 16. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-93'>The Pulse #93: OpenAI makes Google dance</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Google’s core search business has never been under as much threat as it is today – and the threat comes from OpenAI. Also: Google Cloud deletes a customer’s infra, and Tesla in trouble.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering insights, patterns, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Today, we cover:</p>  <p>Industry pulse. NetBSD and Gentoo ban AI-generated commits, StackOverflow to sell data to OpenAI, AWS losing market share, jail time for for Tornado Cash developer, and more.&nbsp;</p>  <p>OpenAI makes Google dance. OpenAI is setting the pace for Google with AI phone assistant capabilities, and is probably the reason that Google started shipping AI answers on top of search results.</p>  <p>Google Cloud deletes Australian trading fund’s infra. A $124B fund in Australia would have lost all data stored with Google Cloud, had they not relied on a third-party backup. A rare blunder from GCP, where regional replication did not stop the deletion – and a just as rare statement from Google Cloud’s CEO taking the blame.</p>  <p>Tesla trouble accompanies poorly executed layoffs. The market leader for electric vehicles in the US is seeing demand for cars drop, and is responding with mass layoffs. This included firing the 500-person Supercharger team: only to start to rehire them a week later.</p>   1. Industry pulse  Open source projects ban AI-generated code commits 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-93">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:29 &bull;
   19 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/2024/googles-broken-link-to-the-web'>Google’s broken link to the web</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
                 
 "A quarter-century into its existence, a company that once proudly served as an entry point to a web that it nourished with traffic and advertising revenue has begun to abstract that all away into an input for its large language models."  This has the potential to be a disaster for the web and everyone who depends on it: for journalism, for bloggers, for communities, for every 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
                 
<p>"A quarter-century into its existence, a company that once proudly served as an entry point to a web that it nourished with traffic and advertising revenue has begun to abstract that all away into an input for its large language models."</p><p>This has the potential to be a disaster for the web and everyone who depends on it: for journalism, for bloggers, for communities, for every voice that couldn't be heard without an open, egalitarian platform.</p><p>The answer for all of those stakeholders has to be depending on forging real, direct relationships with real people. It doesn't scale; it doesn't fit well with a unidirectional broadcast model for publishing; it's now how most people who make content think about what they do. But it's how all of them are going to survive and continue to find each other.</p><p>I've been urging publishers to stop using the word "audience" and to replace it with "community", and to think about what verb might replace "publish" in a multi-directional web that is more about relationships than it is reaching mass eyeballs.</p><p>Of course, it might go in a direction we haven't predicted. We'll find out very soon; the only real certainty is that things are changing, and the bedrock that many people have depended on for two decades is shifting. <a href="https://werd.io/tag/Technology">#Technology</a></p>            <p>[<a href="https://www.platformer.news/google-io-ai-search-sundar-pichai/">Link</a>]</p>
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   13:34 &bull;
   19 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.patrick-breyer.de/piraten-zur-verurteilung-eines-tornado-cash-programmierers-privatsphaere-ist-kein-verbrechen/'>Piraten zur Verurteilung eines Tornado Cash-Programmierers: Privatsphäre ist kein Verbrechen!</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Ein niederländisches Gericht hat gestern den Programmierer Alexey Pertsev zu fünf Jahren Haft verurteilt. Er sei deshalb der Geldwäsche schuldig, weil die von ihm entwickelte Software „Tornado Cash“ auch Straftätern vollständig … 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Ein niederländisches Gericht hat gestern den Programmierer Alexey Pertsev zu fünf Jahren Haft verurteilt. Er sei deshalb der Geldwäsche schuldig, weil die von ihm entwickelte Software „Tornado Cash“ auch Straftätern vollständig anonyme und nicht rückverfolgbare Krypto-Transaktionen ermögliche (sog. „crypto mixer“). „Die Wahrung der Anonymität des Nutzers und die Verschleierung des Transaktionsverlaufs standen dabei im Mittelpunkt,“ so der Vorwurf des Gerichts. Aufgrund ihrer Funktionsweise sei die Software „speziell für Straftäter bestimmt“. In 36 Fällen seien gestohlene Kryptowährungen mithilfe der Software anonym in Umlauf gebracht worden, wofür der Programmierer verantwortlich sei. <a href="https://uitspraken.rechtspraak.nl/details?id=ECLI:NL:RBOBR:2024:2069">Das Gericht warf dem Angeklagten in seinem Urteil eine „Ideologie maximaler Privatsphäre“ vor.</a></p>



<p>Der Europaabgeordnete der Piratenpartei und Jurist Dr. Patrick Breyer warnt vor den Konsequenzen der Verurteilung: „Dieses Urteil kriminalisiert legitime Anonymität und alle, die sie als Programmierer ermöglichen. Die bei Bargeld selbstverständliche Anonymität, die unsere finanzielle Freiheit schützt, darf bei Digitalwährungen nicht kriminalisiert werden.</p>



<p>Die Konsequenzen dieser Denkweise könnten weit über Kryptowährungen hinaus auch Programmierer von Messengersoftware oder Anonymisierungsnetzwerke treffen. Aus diesem Geist heraus hat die EU zuletzt anonyme Barzahlungen eng begrenzt und mit der Chatkontrolle die Zerstörung des digitalen Briefgeheimnisses vorgeschlagen.</p>



<p>In jeder Freiheit nur den Missbrauch durch Einzelne zu sehen, schafft Unfreiheit und ist eine autoritäre Denkweise. Wir Piraten sind digitale Freiheitskämpfer, weil Freiheit Grundlage unserer Gesellschaft ist und ihr weit mehr nützt als der befürchtete Schaden durch ihren kriminellen Missbrauch.“</p>



<p>Die Spitzenkandidatin der Piratenpartei zur Europawahl und Informatikerin Anja Hirschel erklärt: „Wir Piraten sind klar pro Anonymisierung. Die Programmierung, der Betrieb und die Verwendung von Privacy-Tools muss diskriminierungsfrei möglich sein. Aus ihrer Verwendung reflexartig strafbare Handlungen unterstellen zu wollen ist grundlegend falsch.”</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   06:17 &bull;
   19 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.webistemology.com/bill-194/'>Bill 194, Strengthening Cyber Security and Building Trust in the Public Sector Act, 2024</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  While Bill 194 introduces some welcome upgrades to Ontario's cybersecurity and privacy legislation, it falls short of delivering in several key areas, particularly protecting employees' privacy.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This post intends to contribute to the public debate on what could have been a significant legislation - Bill 194. This post is not a summary of Bill 194. I am not a lawyer, and this is not a legal analysis. The post below draws on my experience as a privacy and data protection expert and understanding of current standards and legislation. I will provide an overview of the bill's scope, goals, and provisions and assess its potential to enhance Ontario’s cybersecurity and respect the privacy of Ontarians. While Bill 194 introduces some welcome upgrades to Ontario's cybersecurity and privacy legislation, it falls short of delivering in several key areas, particularly protecting employees' privacy.</p> Overview <p><a href="https://www.ola.org/en/legislative-business/bills/parliament-43/session-1/bill-194?ref=webistemology.com">Bill 194, Strengthening Cyber Security and Building Trust in the Public Sector Act, 2024</a> (the Bill), was introduced in the Ontario Legislature for first reading and passed on May 13, 2024. It has been ordered for its Second Reading. Bill 194 has been introduced in the current context of the ongoing evolution of cybersecurity and privacy threats and the explosive growth of artificial intelligence. The Bill is, therefore, not surprising in what it is intended to address:</p><blockquote>The Act addresses cyber security and artificial intelligence systems at public sector entities. Public sector entities are institutions within the meaning of the Freedom of Information and Protection of Privacy Act and the Municipal Freedom of Information and Protection of Privacy Act, children’s aid societies and school boards. (See <a href="https://www.ola.org/en/legislative-business/bills/parliament-43/session-1/bill-194?ref=webistemology.com">explanatory note</a>.)</blockquote><p>It is worth noting that the Bill does not make amendments to the Municipal Freedom of Information and Protection of Privacy - MFIPPA (the sister act to the Freedom of Information and Protection of Privacy Act - FIPPA). Hopefully, this can be addressed as the Bill goes through the legislative process.</p><p>It must be said that if one of the government's objectives in the Bill were to improve cyber security and privacy protections for Ontarians, this would have been a golden opportunity to introduce private sector legislation to provide a made-in-Ontario solution that could supplement and extend the protections offered by Federal legislation and ensure that Ontarians have robust and equivalent protection in both the public and private sectors. In particular, the government of Ontario's failure to protect employees' privacy is a long-standing issue highlighted by the gaps in this legislation. I note that the current Federal private-sector privacy law is due to be superseded by the contents of <a href="https://www.parl.ca/DocumentViewer/en/44-1/bill/C-27/first-reading?ref=webistemology.com">Bill C-27</a>, but that is not part of this post.</p><p>Employees in Ontario do not have legislation that protects their privacy in either the public or the public sector. Public sector privacy protections were removed in <a href="https://www.ola.org/en/legislative-business/bills/parliament-36/session-1/bill-7?ref=webistemology.com">1995</a>, making Ontario unique among Canadian provinces in that it does not protect the privacy of civil servants at work. It is also the case that, due to employment being in provincial jurisdiction, Federal private-sector privacy legislation does not protect employees in Ontario.</p><p>Ontario-based employees in the federal public sector or employed under the federal labour code (entities like banks, for example) have privacy protection under federal legislation. Still, those are estimated to be less than 500,000 of Ontario's nearly 8 million employees or slightly more than 6%. In the private sector, employees under collective agreements, based on arbitral jurisprudence and the specifics of their contract, will have privacy protection, but that accounts for less than 14% of private sector works. I derived these numbers mainly from searching for available Statistics Canada and other online sources.</p><p>TL;DR — employees in Ontario are the least likely to have privacy protection at work compared to other provinces or territories.</p> The Bill <p>The Bill has two significant elements. Schedule 1, “Enhancing Digital Security and Trust Act,” addresses cyber security issues, the use of artificial intelligence systems, the impact of digital technology on children, and some general provisions, all of which will be addressed below. Schedule 2, “Freedom of Information and Protection of Privacy Act,” amends the <a href="https://canlii.ca/t/2d9?ref=webistemology.com">Freedom of Information and Protection of Privacy Act, RSO 1990, c F.31</a>. Bill 194 is 51 pages long. From a content perspective, that is about 17 pages in English, with a matching section in French. If you think, "This seems a bit perfunctory, given the complicated nature of cyber security, digital protection of children, and privacy," you would be right. It seems to me that the entire bill could be summarized by saying that the government recognizes the importance of issues and will, therefore, write and implement regulations sometime in the future to deal with them. "Just trust us and pass the bill." When you compare this to the 4 years of discussion that went into creating the 458-page EU Artificial Intelligence Act, it comes up short, literally and figuratively. Closer to home, Bill C-27, which includes the Artificial Intelligence Data Act, is 148 pages (or 74 pages in English) but is accompanied by more than 100 civil society, industry, and expert submissions on the provisions and issues of the bill.</p> Schedule 1, Enhancing Digital Security and Trust Act <p>The following describes some of the more significant elements of this part of the Act. This includes Definitions (s. 1), Cyber Security (s. 2 - 4), Use of Artificial Intelligence Systems (s. 5 - 8), and Digital Technology Affecting Individuals Under Age 18 (s. 9 - 11), and some concluding general sections.</p> Definitions <p>The Bill adds a definition of artificial intelligence that appears to be derived, at least in part, from the definition of an AI system in Article 3 of the EU <a href="https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138-FNL-COR01_EN.pdf?ref=webistemology.com">Artificial Intelligence Act</a>. (An easier-to-use reference than the official text can be found in the <a href="https://artificialintelligenceact.eu/ai-act-explorer/?ref=webistemology.com">AI Act Explorer</a> prepared by <a href="https://futureoflife.org/?ref=webistemology.com">The Future of Life Institute</a>). It may be summarized as any system that infers from input to generate outputs to accomplish explicit or implicit objectives. Using an AI chatbot is an example that is fulfilled by this definition. A sample of definitions that are included in the AI Act but not this act include:</p>  reasonably foreseeable misuse  safety component  training data  input data  <p>It is good that the Bill includes procured services and systems as a "use" of artificial intelligence systems. Still, much of the success of this approach will be determined by the nature of the due diligence in Ontario Public Service (OPS) procurement requirements for AI and machine learning systems. Another positive inclusion is that digital information includes collection, use, retention or disclosure by a third party. This will help ensure that accountability remains with the originating government institution.</p> Cyber Security <p>This part of Bill 194 boils down to a requirement for the government to make regulations governing cyber security, including s. 2 (1):</p>  requiring public sector entities to develop and implement programs for ensuring cyber security;  governing programs mentioned in clause (1), which may include prescribing elements to be included in the programs;  requiring public sector entities to submit reports to the Minister or a specified individual in respect of incidents relating to cyber security, which may include different requirements in respect of different types of incidents;  prescribing the form and frequency of reports.  <p>In the absence of a public consultation on the content and purpose of the governing regulations, there is no assurance that the regulations that will be promulgated will meet diverse stakeholder needs nor that they will be effective in proving the desired effect of protecting security. While section 3 allows the government to make regulations setting technical standards, the devil will be in the details here. Noting that there are boatloads of security standards to choose from. There needs to be governance to ensure that the standards chosen are enforced. For example, I have been a consultant on several projects inside various Ministries, and it sometimes surprises information architects and project managers that there are Government of Ontario Information and Technology Standards (<a href="https://www.ontario.ca/page/information-technology-standards?ref=webistemology.com">GO-ITS</a>) to which their projects should abide. There is nothing to suggest in the Bill that even if good standards are adopted, they will be enforced with any rigour.</p> Use of Artificial Intelligence Systems <p>This part of Bill 194, similar to the prior section, mainly sets out the authority for the government to make regulations to govern the use of AI systems without creating content that could be publicly reviewed or publicly debated. I will note two particular gaps I feel should be addressed.</p> Developing an accountability framework <p>Section 5. (3) of the Bill states that each entity using artificial intelligence systems will develop and implement an accountability framework following the yet-to-be-published regulations. I will highlight what I believe to be two flaws with this approach.</p><p>There are no assurances in the Bill that marginalized or disadvantaged communities will provide input or be engaged in developing an Accountability Framework for an artificial intelligence system that may significantly impact their lives. Secondly, it appears that the approach in this Bill could lead to a proliferation of entity-specific Accountability Frameworks. This burdens both citizens whose data may be processed in multiple artificial intelligence systems with different frameworks and entities without the appropriate accountability expertise being asked to develop and implement their frameworks.</p><p>Rather than a proliferation of frameworks, creating a single Accountability Framework based on transparent, inclusive, and robust stakeholder engagement would be better.</p> Creating a risk framework <p>All that Bill 194 says on managing the risk of using artificial intelligence systems is, "A public sector entity to which this section applies shall take such steps as may be prescribed to manage risks associated with the use of the artificial intelligence system." This is woefully inadequate. The high-level risks and harms that can be created using artificial intelligence need to be articulated so that systems that may create high risks to individuals or Ontario as a whole can be identified, and those risks and harms can be identified and either avoided or mitigated. There is no identification of what might be termed unacceptable uses of AI systems or a way to identify whether a high-risk AI system - such as a system that collects biometric information about Ontarians and uses that as a basis for determining access to systems - is acceptable. (In my mind, such a system is inherently unacceptable.)</p> Digital Technology Affecting Individuals Under Age 18 <p>This section replicates the section above; it essentially boils down to allowing the government to make regulations that</p>  set out how children's information may be collected, used, or disclose  require reports about how children's information may be collected, used, or disclosed  may prohibit some processing of children's information  <p>I have two broad comments here. The first is that I am somewhat relieved that the government is not trying to introduce broad systems of digital control or censorship in the name of protecting children. Such legislation is usually both overly broad and ineffective in its intended purpose. That isn't to say that there aren't real risks to students that could have been articulated, not least of which is using easily available tools to enable students to create deep fake photos and videos of other students - creating real trauma and having real-world consequences.</p><p>My second comment is that many digital risks to students are also digital risks for their parents, including misinformation and other social harms. This legislation would have been a great opportunity, for example, to create a requirement for school boards to develop and provide curricula and training to support students in identifying misinformation through critical digital media training.</p> General <p>The last section of Bill 194 includes section 12, which states that nothing in the Act establishes a private law duty of care owed to any person. I'm not a lawyer, but when I <a href="https://ca.practicallaw.thomsonreuters.com/Glossary/CAPracticalLaw?docGuid=I188a88d2f92311e498db8b09b4f043e0&amp;transitionType=DocumentItem&amp;contextData=%28sc.Default%29&amp;ppcid=9d24509e378c447198416aeb9f522e24&amp;ref=webistemology.com">looked up</a> the phrase, it said, "A duty recognized by law to take reasonable care to avoid conduct that poses an unreasonable risk of harm to others." My only comment here is to note that despite the title of the bill, the writers of the bill have taken care to ensure that the heads of government institutions do not have a duty to ensure that they take reasonable care to avoid the risk of harm (aside from the requirement of privacy safeguards addition Schedule 2, which doesn't appear to me to be the same thing). It seems that where an individual's information, especially sensitive information, is collected under a legislative authority, the institution or head should have a duty of care for that individual's information. It may be that this is standard language in this kind of legislation, but it still leaves me a little perplexed. 🤷‍♂️</p> Schedule 2, Freedom of Information and Protection of Privacy Act <p>This schedule is, in some ways, simpler in that it provides amendments to an existing Act (FIPPA) and doesn't endlessly defer to yet-to-be-determined regulations. Schedule 2 defines "information practices" to FIPPA, which will help those responsible for building systems comply with FIPPA. Some worthwhile elements for reporting have been added. I will take particular note of two significant changes: requirements for privacy impact assessments (PIAs) as well as breach reporting and notification requirement</p> Privacy Impact Assessments <p>This is a welcome addition to FIPPA. PIAs are a standard tool for identifying the risks to privacy in a system and recommending steps for their remediation. By standardizing the information required in a PIA, this legislation goes some distance to raising the floor for privacy protection and providing the ability to develop consistent expertise across all of government. I look forward to any prescribed requirements. This is followed by a section on risk mitigation that directs government institutions to implement the recommendations of the PIA</p><blockquote>I would be remiss if I didn't point out the obvious gap between this and Schedule 1. There is no directive in Schedule 1 concerning impact assessments for AI systems nor is there a direction to heads to mitigate identified risks.</blockquote><p>A copy of PIAs is required to be provided to the Information and Privacy Commissioner if asked. This could be improved by changing this to a mandatory filing with the Commissioner. This doesn’t require the IPC to approve the PIA but does make it available to the Commissioner promptly in case of a complaint or breach related to a system with a PIA.</p> Breach Reporting and Notice <p>Schedule 2 adds a Privacy Safeguards section to FIPPA. Specifically, the requirement is that "The head of an institution shall take steps that are reasonable in the circumstances to ensure that personal information in the custody or under the control of the institution is protected against theft, loss and unauthorized use or disclosure and to ensure that the records containing the personal information are protected against unauthorized copying, modification or disposal."  This begs the question of why this requirement for privacy safeguards is only being added now, but suffice to applaud it. </p><p>The requirement for privacy safeguards provides the underpinning for defining a breach as "any theft, loss or unauthorized use or disclosure of personal information in the custody or under the control of the institution if it is reasonable in the circumstances to believe that there is a real risk that a significant harm to an individual would result...". Such breaches will be reported to the Commissioner, whose budget will hopefully reflect this new obligation. The factors identified as determining whether there is a real risk of significant harm include:</p>  the sensitivity of the personal information;  the probability of misuse;  the availability of steps that a person could take to  reduce the risk of harm  mitigate the risk of harm    directions or guidance from the Commissioner  <p>With safeguards, breaches, and risks of harm defined, the last piece is the addition of a requirement to notify individuals if there has been a breach of their information. This is welcome but has consequences. In some circumstances, such a notification can be traumatic or require expenditures by the individual to compensate. Where is the requirement to compensate the individual or help them mitigate the impact?</p> Order Making Power <p>It is worth noting that the amended FIPPA will provide the Commissioner concerning privacy breaches a new power for the Commissioner and, I suspect, a welcome one to bring the Commissioner's powers for privacy in FIPPA in alignment with her order-making powers for Freedom of Information issues.</p> Wrapping Up <p>This post was created within a day or two of Bill 194's First Reading. I look forward to other and deeper contributions to the debate in the days to come. In the meantime, I have these takeaways:</p>  It is past time for Ontario to stop being a laggard in the protection of employee privacy and the government should, at the very least, amend Bill 194 to give public sector employees the privacy protection and respect they deserve.   A private sector privacy bill could address employment privacy issues, putting it under the authority of the Commissioner with private sector order-making powers. Alternatively, elements of privacy protection for employees could also be addressed by adding to Ontario's Employment Standards Act.  The government should use Bill 194's second reading and committee review to ensure that there is a clear legislative articulation of:  What are the acceptable and unacceptable uses of artificial intelligence  How to identify, categorize, and mitigate individual and social risks associated with the use of artificial intelligence    If the government wants to ensure that digital technology doesn't harm children, it should start with digital media training and take steps to prevent children from using technology to bully other children.  Consider recognizing that the government has a duty of care when it processes sensitive personal information under a legislative authority that deprives individuals of the ability to refuse that processing.  Adding PIA requirements with breach notifications will raise the bar for institutions processing Ontarians' personal information. This may lead to some interesting changes or headlines in the short term, but the longer-term consequences should be good.  <p>At the end of the day, the government appears to want to be able to take steps to address cybersecurity, children's data processing, and artificial intelligence through regulations. It will be interesting to see how, or if, the consultation process will significantly alter this approach. The public consultation is open until June 11th and can be found at <a href="https://www.ontariocanada.com/registry/view.do?postingId=47433&amp;language=en&amp;ref=webistemology.com">https://www.ontariocanada.com/registry/view.do?postingId=47433&amp;language=en</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://images.unsplash.com/photo-1569875695184-dca23cbac58e?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDExfHxlbXBsb3llZSUyMGN5YmVyc2VjdXJpdHklMjBvbnRhcmlvfGVufDB8fHx8MTcxNTc5MzcyMHww&ixlib=rb-4.0.3&q=80&w=2000'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   01:12 &bull;
   19 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 15. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2024/05/5796/'>プロ責法あらため情プラ法が成立しました(5月10日)</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  わたしも構成員であった総務省プラットフォームに関する研究会での検討を受けて、この３月に国会に提出された改正「プロバイダ責任制限法」改め「情プラ法」が、木村花さんの４回目の命日（5月23日）を前に、5月10日、国会で成立しました。法律名が変更されたのは「これまでの投稿の発信者情報の…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>わたしも構成員であった総務省プラットフォームに関する研究会での検討を受けて、この３月に国会に提出された改正「プロバイダ責任制限法」<a href="https://www.sakimura.org/2024/05/5796/#easy-footnote-bottom-1-5796" title="「特定電気通信役務提供社の損害賠償責任の制限及び発信者情報の開示に関する法律」">1</a>改め「情プラ法」<a href="https://www.sakimura.org/2024/05/5796/#easy-footnote-bottom-2-5796" title="「特定電気通信による情報の流通によって発生する権利侵害等への対処に関する法律」（通称：「情報流通プラットフォ－ム対処法」（情プラ法））">2</a>が、木村花さんの４回目の命日（5月23日）を前に、5月10日、国会で成立しました。法律名が変更されたのは「これまでの投稿の発信者情報の開示等にとどまらない内容となったため」<a href="https://www.sakimura.org/2024/05/5796/#easy-footnote-bottom-3-5796" title="自民党Webサイトより">3</a>です。</p>



<p>法案、その他の関係資料は以下のとおりです<a href="https://www.sakimura.org/2024/05/5796/#easy-footnote-bottom-4-5796" title="（総務省サイトからの転載。多分リンクが消えてしまうので…）">4</a>。</p>



令和6年3月1日特定電気通信役務提供者の損害賠償責任の制限及び発信者情報の開示に関する法律の一部を改正する法律案<a href="https://www.soumu.go.jp/main_content/000931474.pdf">概要</a>【325 KB】<br><a href="https://www.soumu.go.jp/main_content/000931475.pdf">要綱</a>【97 KB】<br><a href="https://www.soumu.go.jp/main_content/000931476.pdf">法律案・理由</a>【160 KB】<br><a href="https://www.soumu.go.jp/main_content/000931477.pdf">新旧対照条文</a>【254 KB】<br><a href="https://www.soumu.go.jp/main_content/000931478.pdf">参照条文</a>【310 KB】<br>　（所管課室名）<br>総合通信基盤局電気通信事業部利用環境課（出所）総務省 &lt;<a href="https://www.soumu.go.jp/menu_hourei/k_houan.html">https://www.soumu.go.jp/menu_hourei/k_houan.html</a>&gt; (2024-05-15取得)



<p></p>



<p>この改正は、近年のSNS等での権利侵害情報の流通による被害の増加を受け、プラットフォーム事業者の責任を強化し、被害者の保護を図ることを目的としています。大規模プラットフォーム事業者には削除等の義務が課され、違反には罰則が科されることになりました。</p>



<p>主な改正点は以下の通りです：</p>



 
 法律の題名と通称名が変更された。新しい題名は上記の通りで、通称名は「情報流通プラットフォーム対処法（情プラ法）」となった。 



 第5章として「大規模特定電気通信役務提供者の義務」が新設された。これにより、一定規模以上のプラットフォーム事業者に対し、権利侵害情報の流通防止措置等の義務が課されることになった。 



 第6章として「罰則」が新設された。義務に違反した場合の罰則規定が設けられた。 



 条文数が従来の19条から38条へと倍増した。 
 



 総務省プラットフォームに関する研究会について 



<p><a href="https://www.soumu.go.jp/main_sosiki/kenkyu/platform_service/index.html">総務省プラットフォームに関する研究会</a>は平成30年(2018年)10月18日から足掛け６年、全52回にわたって開催されてきた研究会です。令和2年2月に<a href="https://www.soumu.go.jp/main_content/000668595.pdf">最終報告書</a>を出し、一旦は終了したはずだったのですが、木村花さんの誹謗中傷と自殺事件を受け、緊急再招集され、以後、言論の自由を含め様々座な角度から検討を進めてきました。その経緯は、<a href="https://www.soumu.go.jp/main_sosiki/kenkyu/platform_service/index.html">プラットフォームに関する研究会の概要ページ</a>にある議事概要や以下のような文書から見ていただけると思います。</p>



 
 <a href="https://www.soumu.go.jp/menu_news/s-news/01kiban18_01000086.html">インターネット上の誹謗中傷への対応の在り方についての意見募集（令和2年7月3日）</a> 



 <a href="https://www.soumu.go.jp/menu_news/s-news/01kiban18_01000092.html">インターネット上の誹謗中傷への対応の在り方に関する緊急提言及び意見募集の結果の公表</a> 



 <a href="https://www.soumu.go.jp/menu_news/s-news/01kiban18_01000123.html">中間とりまとめ（案）についての意見募集に関する報道発表（令和3年7月16日）</a> 



 <a href="https://www.soumu.go.jp/menu_news/s-news/01kiban18_01000128.html">中間とりまとめ（案）についての意見募集の結果及び中間とりまとめの公表に関する報道発表（令和3年9月15日）</a> 



 <a href="https://www.soumu.go.jp/menu_news/s-news/01kiban18_01000167.html">第二次とりまとめ（案）についての意見募集に関する報道発表（令和4年7月4日）</a> 



 <a href="https://www.soumu.go.jp/menu_news/s-news/01kiban18_01000173.html">第二次とりまとめ（案）についての意見募集の結果及び第二次とりまとめの公表に関する報道発表（令和4年8月25日）</a> 



 <a href="https://www.soumu.go.jp/menu_news/s-news/01kiban18_01000213.html">第三次とりまとめ（案）についての意見募集に関する報道発表（令和5年12月13日）</a> 



 <a href="https://www.soumu.go.jp/menu_news/s-news/01kiban18_01000220.html">第三次とりまとめ（案）についての意見募集の結果及び第三次とりまとめの公表に関する報道発表（令和6年2月2日）</a> 
 



<p>そして、本年、第52回（令和6年1月31日）をもって最終回となり、第三次とりまとめを発表し、クローズいたしました。今次法改正は、この第三次とりまとめがベースとなっています。</p>



<p>この間、総務省および事務局の方々には困難な議論を取りまとめていただき、法改正にまでつなげていただけたことに本当に感謝しております。また、微力ながらわたくしも本件に貢献できたことをとても嬉しく思っております。</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   06:57 &bull;
   20 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 14. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/rag'>Applied AI Software Engineering: RAG</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Retrieval-Augmented Generation (RAG) is a common building block of AI software engineering. A deep dive into what it is, its limitations, and some alternative use cases. By Ross McNairn.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>👋 Hi, this is Gergely with a subscriber-only issue of the Pragmatic Engineer Newsletter. In every issue, I cover challenges at Big Tech and startups through the lens of engineering managers and senior engineers. To get articles like this in your inbox, every week, subscribe:</p><p><a href="https://newsletter.pragmaticengineer.com/subscribe?">Subscribe now</a></p><p>I recently spoke with <a href="https://twitter.com/hkarthik">Karthik Hariharan</a>, who heads up engineering at VC firm Goodwater Capital, and he highlighted a trend he’d spotted:</p><blockquote><p>“There’s an engineering project I’m seeing almost every startup building a Large Language Model (LLM) put in place: building their own Retrieval Augmentation Generation (RAG) pipelines.</p><p>RAGs are a common pattern for anyone building an LLM application. This is because it provides a layer of ‘clean prompts’ and fine-tuning. There are some existing open-source solutions, but almost everyone just builds their own, anyway.”</p></blockquote><p>I asked a few Artificial Intelligence (AI) startups about this, and sure enough, all do build their own RAG. So, I reached out to a startup I know is doing the same: <a href="https://www.wordsmith.ai/">Wordsmith AI.</a> It’s an AI startup for in-house legal teams that’s making heavy use of RAG, and was co-founded by <a href="https://www.linkedin.com/in/rossmcnairn/">Ross McNairn</a>. He and I worked for years together at Skyscanner and he offered to share Wordsmith AI’s approach for building RAG pipelines, and some learnings. Declaration of interest: I’m <a href="https://blog.pragmaticengineer.com/investing/">an investor in Wordsmith</a>, and the company has <a href="https://www.wordsmith.ai/">recently launched</a> out of stealth.</p><p>Today, we cover:</p>  <p>Providing an LLM with additional context</p>  <p>The simplest RAGs</p>  <p>What is a RAG pipeline?</p>  <p>Preparing the RAG pipeline data store</p>  <p>Bringing it all together</p>  <p>RAG limitations</p>  <p>Real-world learnings building RAG pipelines</p>  <p>Today’s article includes a “code-along,” so you can build your own RAG. View the code used in this article <a href="https://github.com/wordsmith-ai/hello-wordsmith">at this GitHub repository: hello-wordsmith</a>. To keep up with Ross, <a href="https://www.rossmcnairn.com/">subscribe to his blog</a> or <a href="https://www.linkedin.com/in/rossmcnairn">follow him on LinkedIn</a>.</p><p>With that, it’s over to Ross:</p> Introduction <p>Hi there! This post is designed to help you get familiar with one of the most fundamental patterns of AI software engineering: RAG, aka Retrieval Augmented Generation.</p><p>I co-founded a legal tech startup called <a href="https://www.wordsmith.ai/">Wordsmith</a>, where we are building a platform for running a modern in-house legal team. Our founding team previously worked at Meta, Skyscanner, Travelperk and KPMG.</p><p>We are working in a targeted domain – legal texts – and building AI agents to give in-house legal teams a suite of AI tools to remove bottlenecks and improve how they work with the rest of the business. Performance and accuracy are key characteristics for us, so we’ve invested a lot of time and effort in how to best enrich and “turbo charge” these agents with custom data and objectives.</p><p>We ended up building our RAG pipeline, and I will now walk you through how we did it and why. We’ll go into our learnings, and <a href="https://www.wordsmith.ai/benchmarks">how we benchmark</a> our solution. I hope that the lessons we learned are useful for all budding AI engineers.</p> 1. Providing an LLM with additional context <p>Have you ever asked ChatGPT a question it does not know how to answer, or its answer is too high level? We’ve all been there, and all too often, interacting with a GPT feels like talking to someone who speaks really well, but doesn’t know the facts. Even worse, they can make up the information in their responses!</p><p>Here is one example. On 1 February 2024, during an earnings call, Mark Zuckerberg laid out the strategic benefits of Meta’s AI strategy. But when we ask ChatGPT a question about this topic, this model will make up an answer that is high-level, but is not really what we want:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc39fb336-b9a4-43b3-8cb1-7c95022ed02e_1536x998.png">    </a>ChatGPT 3.5’s answer to a question about Meta’s AI strategy. The answer is generalized, and misses a critical source which answers the question <p>This makes sense, as the model’s training cutoff date was before Mark Zuckerberg made the comments. If the model had access to that information, it would have likely been able to summarize the facts of that meeting, which are:</p><blockquote><p>“So I thought it might be useful to lay out the strategic benefits [of Meta’s open source strategy) here. (...)</p><p>The short version is that open sourcing improves our models. (...)<br><br>First, open-source software is typically safer and more secure as well as more compute-efficient to operate due to all the ongoing feedback, scrutiny and development from the community. (...)<br><br>Second, open-source software often becomes an industry standard. (...)</p><p>Third, open source is hugely popular with developers and researchers. (...)</p><p>The next part of our playbook is just taking a long-term approach towards the development.”</p></blockquote><p>LLMs’ understanding of the world is limited to the data they’re trained on. If you’ve been using ChatGPT for some time, you might remember this constraint in the earlier version of ChatGPT, when the bot responded: “I have no knowledge after April 2021,” in several cases.&nbsp;</p> Providing an LLM with additional information <p>There is a bunch of additional information you want an LLM to use. In the above example, I might have the transcripts of all of Meta’s shareholders meetings that I want the LLM to use. But how can we provide this additional information to an existing model?</p> Option 1: input via a prompt <p>The most obvious solution is to input the additional information via a prompt; for example, by prompting “Using the following information: [input a bunch of data] please answer the question of [ask your question].”</p><p>This is a pretty good approach. The biggest problem is that this may not scale because of these reasons:</p>  <p>The input tokens limit. Every model has an input prompt token limit. At the time of publication <a href="https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4">this is</a> 4.069 tokens for GPT-3, 16,385 for GPT-3.5, 8,192 for GPT-4, 128,000 for GPT-4 Turbo, 200.000 for Anthropic models. Google’s Gemini model allows for an impressive one million token limit. While a million-token limit greatly increases the possibilities, it might still be too low for use cases with a lot of additional text to input.</p>  <p>Performance. The performance of LLMs substantially decreases with longer input prompts; in particular, you get degradation of context in the middle of your prompt. Even when creating long input prompts is a possibility, the performance tradeoff might make it impractical.</p>   Option 2: fine-tune the model <p>We know LLMs are based on a massive weights matrix. Read more on <a href="https://newsletter.pragmaticengineer.com/i/141865286/how-does-chatgpt-work-a-refresher">how ChatGPT works in this Pragmatic Engineer issue.</a> All LLMs use the same principles.</p><p>An option is to update these weight matrices based on additional information we’d like our model to know. This can be a good option, but it is a much higher upfront cost in terms of time, money, and computing resources. Also, it can only be done with access to the model’s weightings, which is not the case when you use models like ChatGPT, Anthropic, and other “closed source” models.</p> Option 3: RAG <p>The term ‘RAG’ originated in a <a href="https://arxiv.org/pdf/2005.11401.pdf">2020 paper</a> led by Patrick Lewis. One thing many people notice is that “Retrieval Augmented Generation” sounds a bit ungrammatical. Patrick agrees, and has said this:</p><blockquote><p>“We always planned to have a nicer-sounding name, but when it came time to write the paper, no one had a better idea.”</p></blockquote><p>RAG is a collection of techniques which help to modify a LLM, so it can fill in the gaps and speak with authority, and some RAG implementations even let you cite sources. The biggest benefits of the RAG approach:</p><p>Give a LLM domain-specific knowledge You can pick what data you want your LLM to draw from, and even turn it into a specialist on any topic there is data about.&nbsp;</p><p>This flexibility means you can also extend your LLMs’ awareness far beyond the model’s training cutoff dates, and even expose it to near-real time data, if available.</p><p>Optimal cost and speed. For all but a handful of companies, it's impractical to even consider training their own foundational model as a way to personalize the output of an LLM, due to the very high cost and skill thresholds.&nbsp;</p><p>In contrast, deploying a RAG pipeline will get you up-and-running relatively quickly for minimal cost. The tooling available means a single developer can have something very basic functional in a few hours.</p><p>Reduce hallucinations. “Hallucination” is the term for when LLMs “make up” responses. A well-designed RAG pipeline that presents relevant data will all but eliminate this frustrating side effect, and your LLM will speak with much greater authority and relevance on the domain about which you have provided data.<br><br>For example, in the legal sector it’s often necessary to ensure an LLM draws its insight from a specific jurisdiction. Take the example of asking a model a seemingly simple question, like:</p><p>How do I hire someone?<br><br>Your LLM will offer context based on the training data. However, you do not want the model to extract hiring practices from a US state like California, and combine this with British visa requirements!&nbsp;</p><p>With RAG, you control the underlying data source, meaning you can scope the LLM to only have access to a single jurisdiction’s data, which ensures responses are consistent.</p><p>Better transparency and observability. Tracing inputs and answers through LLMs is very hard. The LLM can often feel like a “black box,” where you have no idea where some answers come from. With RAG, you see the additional source information injected, and debug your responses.</p> 2. The simplest RAGs <p>The best way to understand new technology is often just to play with it. Getting a basic implementation up and running is relatively simple, and can be done with just a few lines of code. To help, Wordsmith has <a href="https://github.com/wordsmith-ai/hello-wordsmith">created a wrapper</a> around the <a href="https://www.llamaindex.ai/">LlamaIndex</a> open source project to help abstract away some complexity. You can <a href="https://github.com/wordsmith-ai/hello-wordsmith">get up and running</a>, easily. It has a README file in place that will get you set up with a local RAG pipeline on your machine, and which chunks and embeds a copy of the US Constitution, and lets you search away with your command line.</p><p>This is as simple as RAGs get; you can “swap out” the additional context provided in this example by simply changing the source text documents!</p><p>This article is designed as a code-along, so I'm going to link you to sections of <a href="https://github.com/wordsmith-ai/hello-wordsmith">this repo</a>, so you can see where specific concepts manifest in code.</p><p>To follow along with the example, the following is needed:</p>  <p>An active OpenAI subscription with API usage. <a href="https://platform.openai.com/settings/organization/billing/overview">Set one up here</a> if needed. Note: running a query will cost in the realm of $0.25-$0.50 per run.</p>  <p><a href="https://github.com/wordsmith-ai/hello-wordsmith">Follow the instructions</a> to set up a virtual Python environment, configure your OpenAI key, and start the virtual assistant.</p>  <p>This example will load the text of the US constitution <a href="https://github.com/wordsmith-ai/hello-wordsmith/blob/main/hello_wordsmith/public_wordsmith_dataset/us_constitution.txt">from this text file</a>, as a RAG input. However, the application can be extended to load your own data from a text file, and to “chat” with this data.</p><p>Here’s an example of how the application works when set up, and when the OpenAI API key is configured:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0725b694-9547-4c8e-ba32-cda83db425f5_1180x772.png">    </a>The example RAG pipeline application answering questions using the US Constitution supplied as additional context <p>If you’ve followed along and have run this application: congratulations! You have just executed a RAG pipeline. Now, let’s get into explaining how it works.</p> 3. What is a RAG pipeline? <p>A RAG pipeline is a collection of technologies needed to enable the capability of answering using provided context. In our example, this context is the US Constitution and our LLM model is enriched with additional data extracted from the US Constitution document.&nbsp;</p><p>Here are the steps to building a RAG pipeline:</p><p>&nbsp;<br>Step 1: Take an inbound query and deconstruct it into relevant concepts<br>Step 2: Collect similar concepts from your data store<br>Step 3: Recombine these concepts with your original query to build a more relevant, authoritative answer.</p><p>Weaving this together:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdecd2b92-02ee-4fba-95da-61c75c473f26_1556x1572.png">    </a>A RAG pipeline at work. It extends the context an LLM has access to, by fetching similar concepts from the data store to answer a question <p>While this process appears simple, there is quite a bit of nuance in how to approach each step. A number of decisions are required to tailor to your use case, starting with how to prepare the data for use in your pipeline.</p> 4. Preparing the RAG pipeline data store 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/rag">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc39fb336-b9a4-43b3-8cb1-7c95022ed02e_1536x998.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:39 &bull;
   21 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 13. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/05/extension-summit-topics/'>Extension Summit Topic Review</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Final Postgres Extension Mini-Summit! On Wednesday May 15 at noon Eastern /
16:00 UTC, we’ll review some of the topics that have come up in previous
Mini-Summits with the goal to winnow down and select issues to address at
PGConf.dev.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Boy howdy that went fast.</p>
<p>This Wednesday, May 15, the final Postgres <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">extension ecosystem
mini-summit</a> will review topics covered in previous Mini-Summits,
various <a href="https://planet.postgresql.org">Planet PostgreSQL</a> posts, the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres
Slack</a> and the <a href="https://discord.com/invite/bW2hsax8We">Postgres Discord</a>. Following a brief description of each,
we’ll determine how to reduce the list to the most important topics to take on
at the <a href="https://www.pgevents.ca/events/pgconfdev2024/schedule/session/191-extension-ecosystem-summit/" title="PGConf.dev: Extensions Ecosystem Summit: Enabling comprehensive indexing, discovery, and binary distribution">Extension Ecosystem Summit</a> at <a href="https://2024.pgconf.dev" title="PostgreSQL Development Conference 2024">PGConf.dev</a> in Vancouver on May 28.
I’ll post a summary later this week along with details for how to participate
in the selection process.</p>
<p>In the meantime, here’s the list as of today:</p>
 
 Metadata:
 
 Third-party dependencies 
 Types of extensions 
 Taxonomies 
 System requirements (OS, version, CPU, etc.) 
 Categorization 
 Versioning 
 
 
 Registry:
 
 Identity, namespacing, and uniqueness 
 Distributed vs. centralized publishing 
 Binary packaging and distribution patterns 
 Federated distribution 
 Services and tools to improve or build 
 Stats, Reports, Badging: (stars, reviews, comments, build &amp; test matrices, etc.) 
 
 
 Packaging:
 
 Formats (e.g., tarball, OCI, RPM, wheel, etc.) 
 Include dynamic libs in binary packaging format? (precedent: Python wheel) 
 Build farming 
 Platforms, architectures, and OSes 
 Security, trust, and verification 
 
 
 Developer:
 
 Extension developer tools 
 Improving the release process 
 Build pipelines: Supporting PGXS, prgx, Rust, Go, Python, Ruby, Perl, and more 
 
 
 Community:
 
 Community integration: identity, infrastructure, and support 
 How-Tos, tutorials, documentation for creating, maintaining, and distributing extensions 
 Docs/references for different types of extensions: <code>CREATE EXTENSION</code>, hooks, background workers, CLI apps/services, web apps, native apps, etc. 
 
 
 Core:
 
 <a href="https://www.postgresql.org/message-id/flat/E7C7BFFB-8857-48D4-A71F-88B359FADCFD@justatheory.com" title="pgsql-hackers: RFC: Additional Directory for Extensions">Second extension directory</a> (a.k.a. variable installation location, search path) 
 Keeping all files in a single directory 
 Documentation standard 
 Inline extensions: UNITs, PACKAGEs, TLEs, etc. 
 Minimizing restarts 
 Namespacing 
 Sandboxing, code signing, security, trust 
 Dynamic module loading (e.g., <code>use Thing</code> in PL/Perl could try to load <code>Thing.pm</code> 
 from a table of acceptable libraries maintained by the DBA) 
 Binary compatibility of minor releases and/or /ABI stability 
 
 
 
<p>Is your favorite topic missing? Join us at the <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">mini-summit</a> or drop
suggestions into the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a>.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/yum/">Yum</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:12 &bull;
   21 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2024/05/13/using-sonarcloud-with-asp-net-core-angular-and-github-actions/'>Using SonarCloud with ASP.NET Core, Angular and github actions</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This article demonstrates how to implement code analysis and Static Application Security Testing (SAST) using SonarCloud and GitHub Actions. The solution involves building a secure web application with ASP.NET Core for the backend and an Angular UI for the frontend, following a backend-for-frontend security architecture. Both the ASP.NET Core (C#) codebase and the Angular (TypeScript […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This article demonstrates how to implement code analysis and Static Application Security Testing (SAST) using SonarCloud and GitHub Actions. The solution involves building a secure web application with ASP.NET Core for the backend and an Angular UI for the frontend, following a backend-for-frontend security architecture. Both the ASP.NET Core (C#) codebase and the Angular (TypeScript and JavaScript) files undergo analysis during the quality build process with SonarCloud.</p>



<p>Code: <a href="https://github.com/damienbod/EndToEndSecurity">https://github.com/damienbod/EndToEndSecurity</a></p>



<p></p>







<p></p>



 Solution Setup 



<p>The application uses an Angular UI implemented with NX tools and services. During deployment builds, the UI is integrated into the ASP.NET Core backend, while in development, it operates as a standalone component. Microsoft YARP facilitates the connection between Angular development and local ASP.NET Core APIs. The Angular application is an integral part of the backend system when deployed. Both technical stacks require code analysis and Static Application Security Testing (SAST). Additionally, the solution is secured as a single OpenID Connect confidential client, utilizing the code flow with Proof Key for Code Exchange (PKCE).</p>







<p></p>



 SonarCloud setup 



<p>I set up SonarCloud testing and integrated it with GitHub Actions using the repository from <a href="https://www.linkedin.com/in/marcrufer/">Marc Rufer</a>.</p>



<p><a href="https://github.com/rufer7/github-sonarcloud-integration">https://github.com/rufer7/github-sonarcloud-integration</a></p>



<p>This references the docs from SonarCloud and all the steps required for setting up a build and analysis of the different technical stacks are documented.</p>



 ASP.NET Core project setup 



<p>To enable SonarCloud to analyze both the ASP.NET Core project and the Angular projects, you’ll need to make adjustments in the .NET Core csproj file settings. Specifically, the Angular components should be added as hidden elements so that SonarCloud can properly detect and analyze them.</p>


  &lt;ItemGroup&gt;
&lt;!-- This is required to include ts and js files in SonarCloud analysis --&gt;
&lt;!-- Add to the sonar cloud build: EndToEndSecurity == github repo --&gt;
&lt;!-- /d:sonar.projectBaseDir="D:\a\EndToEndSecurity\EndToEndSecurity" /d:sonar.exclusions=**/node_modules/** --&gt;
&lt;!-- See https://docs.sonarsource.com/sonarqube/9.8/analyzing-source-code/scanners/sonarscanner-for-dotnet/#advanced-topics --&gt;
   &lt;Content Include="..\ui\**\*.ts" Visible="false"&gt;
      &lt;CopyToOutputDirectory&gt;Never&lt;/CopyToOutputDirectory&gt;
   &lt;/Content&gt;
   &lt;Content Include="..\ui\**\*.js" Visible="false"&gt;
      &lt;CopyToOutputDirectory&gt;Never&lt;/CopyToOutputDirectory&gt;
   &lt;/Content&gt;
&lt;/ItemGroup&gt;
  


 Quality build 



<p>The SonarCloud github action YAML file implements the quality build. Normally this would be integrated with the default build, PRs and feature branches would run this. The dotnet testing tools are added but not active. The build uses a windows-latest image and java. When testing the code of the two technical stacks, you should ignore folders like node_modules and so on. This can be excluded in the YAML file. For this to work, the SonarCloud project must match the YAML file definitions. This is well documented in the Sonar documentation.</p>


  name: SonarCloud
on:
  push:
    branches:
      - develop
      - main
  pull_request:
    types: [opened, synchronize, reopened]
jobs:
  build:
    name: Analyze dotnet and Augular projects
    runs-on: windows-latest
    steps:
      - name: Set up JDK 17
        uses: actions/setup-java@v4
        with:
          java-version: 17
          distribution: 'zulu' # Alternative distribution options are available.
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Shallow clones should be disabled for a better relevancy of analysis
      - name: Cache SonarCloud packages
        uses: actions/cache@v4
        with:
          path: ~\sonar\cache
          key: ${{ runner.os }}-sonar
          restore-keys: ${{ runner.os }}-sonar
      - name: Cache SonarCloud scanner
        id: cache-sonar-scanner
        uses: actions/cache@v4
        with:
          path: .\.sonar\scanner
          key: ${{ runner.os }}-sonar-scanner
          restore-keys: ${{ runner.os }}-sonar-scanner
      - name: Install SonarCloud scanner
        if: steps.cache-sonar-scanner.outputs.cache-hit != 'true'
        shell: powershell
        run: |
          New-Item -Path .\.sonar\scanner -ItemType Directory
          dotnet tool update dotnet-sonarscanner --tool-path .\.sonar\scanner
      - name: Install dotnet-coverage # not used as not tests exist in backend
        shell: powershell
        run: |
          dotnet tool install --global dotnet-coverage
      - name: Build and analyze
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # Needed to get PR information, if any
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        shell: powershell
        run: |
          .\.sonar\scanner\dotnet-sonarscanner begin /k:"damienbod_EndToEndSecurity" /o:"damienbod" /d:sonar.token="${{ secrets.SONAR_TOKEN }}" /d:sonar.host.url="https://sonarcloud.io" /d:sonar.projectBaseDir="D:\a\EndToEndSecurity\EndToEndSecurity" /d:sonar.exclusions=**/node_modules/**
          dotnet build .\Bff.sln --configuration Release
          # dotnet-coverage collect 'dotnet test .\src\--testproj--.Tests\--testproj--.Tests.csproj' -f xml  -o 'coverage.xml'
          .\.sonar\scanner\dotnet-sonarscanner end /d:sonar.token="${{ secrets.SONAR_TOKEN }}"

  


 Badges 



<p>Badges from SonarCloud can be added directly in the readme file of the github repository. The badges are created in SonarCloud and I switched them to the overall results and not just the last delta. By clicking the badges in the readme, you are redirected to the SonarCloud test results.</p>







<p></p>



<p>SonarCloud is a great service for code quality analysis and has a good SAST implementation with very good access into the github UI and tools. Security alerts can be directly viewed in github.</p>



 Links 



<p><a href="https://docs.sonarsource.com/sonarcloud/getting-started/github">https://docs.sonarsource.com/sonarcloud/getting-started/github</a></p>



<p><a href="https://github.com/rufer7/github-sonarcloud-integration">https://github.com/rufer7/github-sonarcloud-integration</a></p>



 
<blockquote><a href="https://blog.rufer.be/2023/10/06/howto-integrate-sonarcloud-analysis-in-an-azure-devops-yaml-pipeline/">[HOWTO] Integrate SonarCloud analysis in an Azure DevOps YAML&nbsp;pipeline</a></blockquote>
 



<p><a href="https://community.sonarsource.com/t/code-coverage-report-for-net-not-working-on-linux-agent/62087">https://community.sonarsource.com/t/code-coverage-report-for-net-not-working-on-linux-agent/62087</a></p>



<p><a href="https://docs.sonarsource.com/sonarcloud/advanced-setup/ci-based-analysis/sonarscanner-for-net/#analyzing-languages-other-than-c-and-vb">https://docs.sonarsource.com/sonarcloud/advanced-setup/ci-based-analysis/sonarscanner-for-net/#analyzing-languages-other-than-c-and-vb</a></p>



<p><a href="https://andreiepure.ro/2023/08/20/analyze-web-files-with-s4net.html">https://andreiepure.ro/2023/08/20/analyze-web-files-with-s4net.html</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2024/04/sonarcloud_github_01.png?w=1024'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   05:57 &bull;
   22 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 12. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wrenchinthegears.com/2024/05/12/blessings/'>Blessings</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  It’s mother’s day. Some get to celebrate with loved ones, while others navigate the bittersweet reality of bereavement or estrangement. My heart is with you all. If there’s one lesson I’ve learned these past few years it is the importance of love, even across distance, even when it may appear to be unreciprocated. The system [...]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>It’s mother’s day. Some get to celebrate with loved ones, while others navigate the bittersweet reality of bereavement or estrangement. My heart is with you all. If there’s one lesson I’ve learned these past few years it is the importance of love, even across distance, even when it may appear to be unreciprocated. The system seeks to harness the power of mothers’ hearts through token engineering, but our hearts are wild and unruly. Our torus fields will not be tamed.&nbsp;</p>
<p>Today I enjoyed many blessings. I am working to move forward from a place of gratitude, so I want to acknowledge them in this short post with a few pictures from Shrine Mont, Virginia. It is a resort maintained by the Anglican Church that includes several mineral springs that were sacred to the original people’s of the Shenandoah. At some point in the past year, I looked up who founded MIT. It turns out it was a Virginia geologist <a href="https://en.wikipedia.org/wiki/William_Barton_Rogers">William Barton Rogers</a>. Having studied the role of water as a solvent on minerals and overseen the state geological survey in 1835, Rogers would have been familiar with these springs. His survey was made three years after Andrew Jackson put the <a href="https://www.nps.gov/hosp/learn/historyculture/index.htm">hot springs of Arkansas</a> under federal ownership. In addition to the springs, the area also features a subtle energy vortex that you can read about <a href="http://basyevortex.com/index.php?option=com_content&amp;task=view&amp;id=70&amp;Itemid=115">here</a>.</p>
<p>This is my second visit. I made a video of my first trip in the summer of 2022.&nbsp;</p>
<p></p>
<p>Things for which I am grateful today:&nbsp;</p>
<p>Friends of like mind who offer good conversation, a soft bed, hot shower, and laundry services to a wayward traveler.</p>
<p>Unconditional love from an affectionate dog.</p>
<p>A swift recovery from a debilitating migraine.</p>
<p>A sky of clear blue, full of natural, puffy clouds.&nbsp;</p>
<p>Stands of purple and white woodland phlox.</p>
<p>A burbling brook with a nicely-spaced stone crossing.&nbsp;</p>
<p>A labyrinth to walk.&nbsp;</p>
<p>An abundance of tulip poplar buds.</p>
<p>Varied shades of pink in myrtle flowers and lady slipper orchids.</p>
<p>The sweet fragrance of locust tree blossoms.</p>
<p>A quiet moment to enjoy the view of white wooden buildings with dark bottle green trim beyond a pond ringed by last season’s cattails and trilling red-winged blackbirds.</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p>&nbsp;</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/img_5626-1.jpg?resize=768%2C1024&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   23:32 &bull;
   22 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://herestomwiththeweather.com'>Heres Tom with the Weather</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://herestomwiththeweather.com/social/2024/05/12/71385/'></a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:49 &bull;
   22 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://aaronparecki.com'>Aaron Parecki</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://aaronparecki.com/2024/05/12/3/fedcm-for-indieauth'>FedCM for IndieAuth</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  IndieWebCamp Düsseldorf took place this weekend, and I was inspired to work on a quick hack for demo day to show off a new feature I've been working on for IndieAuth.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>IndieWebCamp <a href="https://events.indieweb.org/2024/05/indiewebcamp-d%C3%BCsseldorf-2024-lPSJYTuqCdL2">Düsseldorf</a> took place this weekend, and I was inspired to work on a quick hack for demo day to show off a new feature I've been working on for <a href="https://indieauth.net/">IndieAuth</a>.</p>

<p>Since I do actually use my website to log in to different websites on a regular basis, I am often presented with the login screen asking for my domain name, which is admittedly an annoying part of the process. I don't even like having to enter my email address when I log in to a site, and entering my domain isn't any better.</p>


  


<p>So instead, I'd like to get rid of this prompt, and let the browser handle it for you! Here's a quick video of logging in to a website using my domain with the new browser API:</p>

<p></p>

<p>So how does this work?</p>

<p>For the last couple of years, there has been an ongoing effort at the Federated Identity Community Group at the W3C to build a new API in browsers that can sit in the middle of login flows. It's primarily being driven by Google for their use case of letting websites show a Google login popup dialog without needing 3rd party cookies and doing so in a privacy-preserving way. There's a lot to unpack here, more than I want to go into in this blog post. You can check out <a href="https://blog.timcappalli.me/p/preso-osw24-fedcm101/">Tim Cappalli's slides from the OAuth Security Workshop</a> for a good explainer on the background and how it works.</p>

<p>However, there are a few experimental features that are being considered for the API to accommodate use cases beyond the "Sign in with Google" case. The one that's particularly interesting to the IndieAuth use case is the <a href="https://github.com/fedidcg/FedCM/issues/240">IdP Registration API</a>. This API allows any website to register itself as an identity provider that can appear in the account chooser popup, so that a relying party website doesn't have to list out all the IdPs it supports, it can just say it supports "any" IdP. This maps to how IndieAuth is already used today, where a website can accept any user's IndieAuth server without any prior relationship with the user. For more background, check out my previous blog post "<a href="https://aaronparecki.com/2018/07/07/7/oauth-for-the-open-web">OAuth for the Open Web</a>".</p>

<p>So now, with the IdP Registration API in FedCM, your website can tell your browser that it is an IdP, then when a website wants to log you in, it asks your browser to prompt you. You choose your account from the list, the negotiation happens behind the scenes, and you're logged in!</p>


  


<p>One of the nice things about combining FedCM with IndieAuth is it lends itself nicely to running the FedCM IdP as a separate service from your actual website. I could run an IndieAuth IdP service that you could sign up for and link your website to. Since your identity is your website, your website would be the thing ultimately sent to the relying party that you're signing in to, even though it was brokered through the IdP service. Ultimately this means much faster adoption is possible, since all it takes to turn your website into a FedCM-supported site is adding a single <code>&lt;link&gt;</code> tag to your home page.</p>

<p>So if this sounds interesting to you, leave a comment below! The IdP registration API is currently an early experiment, and Google needs to see actual interest in it in order to keep it around! In particular, they are looking for Relying Parties who would be interested in actually using this to log users in. I am planning on launching this on <a href="https://webmention.io">webmention.io</a> as an experiment. If you have a website where users can sign in with IndieAuth, feel free to get in touch and I'd be happy to help you set up FedCM support as well!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://aaronparecki.com/2024/05/12/3/image-1.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:39 &bull;
   23 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://kentbull.com/'>Kent Bull</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://kentbull.com/2024/05/11/keri-internals-part-1-concurrency-and-async-programming-with-the-hio-async-framework-and-i-o-library/'>KERI Internals Part 1: Concurrency and Async Programming with the HIO Async Framework and I/O Library</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Welcome to the inner workings of the Python implementation of KERI! HIO stands for Hierarchical IO. Disclaimer: this post is for a technical audience who have a need to read and understand the WebOfTrust Python implementation of the KERI, ACDC, and CESR Trust over IP (ToIP) specifications. Have you ever […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
 


<p>Welcome to the inner workings of the Python implementation of KERI! <a href="https://github.com/ioflo/hio">HIO</a> stands for Hierarchical IO.</p>



<p>Disclaimer: this post is for a technical audience who have a need to read and understand the <a href="https://github.com/WebOfTrust">WebOfTrust</a> Python implementation of the <a href="https://trustoverip.github.io/tswg-keri-specification/">KERI</a>, <a href="https://trustoverip.github.io/tswg-acdc-specification/#go.draft-ssmith-acdc.html">ACDC</a>, and <a href="https://trustoverip.github.io/tswg-cesr-specification/">CESR</a> Trust over IP (ToIP) specifications.</p>



<p>Have you ever wanted to contribute to the KERI ecosystem and been curious about the way the Python implementations are written? Or have you wanted to build on top of or modify the Python KERI codebase? </p>



<p>Not to worry, this technical series on KERI internals has your back. You will be ready to read through, understand, and build on top of the Python KERI code base once you understand the HIO async runtime, the focus of this article. </p>



<p>You are wanted as a contributor to the KERI ecosystem! The goal of this article is to assist you in becoming either a contributor to the Python implementation of KERI &amp; ACDC or an advanced user of said implementation.</p>



 HIO Introduction 



<p>HIO is an asynchronous runtime and input/output (IO) framework written by <a href="https://keri.one/131-2/">Dr. Samuel Smith</a> that supports cooperative multitasking. It is used throughout the Python implementation of the KERI suite of protocols.</p>



<p>This article serves as an introduction to the three primary classes composing the basis for HIO’s  asynchronous runtime and as the lifecycle context functions for the main task class, the <code>Doer</code>. Additionally, you will have an idea of how these concepts relate to similar concepts in Python’s AsyncIO runtime. The three HIO classes include:</p>



 
 the <code>Doist</code>, the root scheduler, 



 the <code>DoDoer</code>, the heirarchical container of <code>Doer</code> and <code>DoDoer</code> instances 



 <code>Doer</code>, the core task concept in HIO. 
 



<p>Due to its nature as the asynchronous runtime engine, HIO is found at the heart of the core Python libraries in the WebOfTrust ecosystem including the core library <a href="https://github.com/WebOfTrust/keripy">KERIpy</a>, the agent server <a href="https://github.com/WebOfTrust/keria">KERIA</a>, and the <a href="https://github.com/WebOfTrust/signifypy">SignifyPy</a> client companion to KERIA.</p>



<p>In order to understand the purpose of three classes mentioned above and how they compare to Python’s AsyncIO it is important to clarify terminology around concurrent and asynchronous programming in Python. As Python’s <code>async/await</code> is much more common and familiar than HIO this article starts there to introduce the concepts.</p>



 Why is HIO used in KERIpy, KERIA, and SignifyPy? 



<p>Performance, control, and features, at a high level, are the reason why HIO was used for KERIpy. HIO’s use of what are called “classic coroutines” and asynchronous buffers for I/O provide a level of control and performance that is difficult to achieve with Python’s AsyncIO implementation. An API into the timing system used for the event loop and scheduler provide tight, deterministic control over scheduling order of tasks.</p>



<p>A future article will go deeper than this short overview into the argument for using HIO and what specifically sets it apart from other async frameworks like AsyncIO, <a href="https://curio.readthedocs.io/en/latest/">Curio</a>, and <a href="https://trio.readthedocs.io/en/latest/">Trio</a>.</p>



 Async Framework Short Comparison 



<p>An asynchronous framework typically consists of a number of major abstractions including an event loop, task or coroutine, scheduler, queues for communicating between tasks, futures, callbacks, non-blocking I/O, synchronization primitives (locks, semaphores), timeouts and cancellation, and some notion of lifecycle for tasks. This article focuses specifically on the event loop, scheduler, and task abstractions in HIO and Python’s AsyncIO. </p>



 Cooperative Multitiasking 



<p>Both HIO and AsyncIO allow you to accomplish what is called “cooperative multitasking” which is where each coroutine yields control to a central scheduler so that other coroutines can be activated for their next execution. In AsyncIO the scheduler is the <code>asyncio</code> event loop and a coroutine is any function declared with the <code>async def</code> syntax. In HIO the scheduler is the <code>Doist</code> class and the coroutine is the <code>Doer</code> class.</p>



 Concurrency and parallelism in Python 



<p>When discussing concurrency or asynchronous programming it is important to distinguish between what is typically meant by concurrency and parallelism. </p>



<blockquote>
<p>Concurrency is about dealing with lots of things at once.</p>



<p>Parallelism is about doing lots of things at once.</p>



<p>Not the same, but related.<br>One is about structure, one is about execution.</p>



<p>Concurrency provides a way to structure a solution to solve a problem that may (but not necessarily) be parallelizable.</p>



<p>— Rob Pike, co-inventor of the Go language</p>
</blockquote>



<p>Parallelism is a special case of concurrency. In Python <code>threading</code>, <code>multiprocessing</code>, and <code>asyncio</code> are the core packages for concurrent programming. In this post we only address the <code>asyncio</code> package, which supports what are called native coroutines.</p>



 Python’s AsyncIO package 



 Native coroutines – async/await 



<p>A native coroutine is any function that, as mentioned earlier, uses the <code>async def</code> syntax to define a function, introduced with <a href="https://peps.python.org/pep-0492/">PEP-492</a> in Python 3.5 (2015). Calling an <code>async def</code> function does not automatically execute the code in the function. To execute the code the <code>await</code> keyword must be used when calling the function. This instructs the <code>asyncio</code> event loop to schedule execution of the function. </p>


  import asyncio

# Native coroutine - uses the "async def" syntax to define a function
async def print_after(delay, what):
    await asyncio.sleep(delay)
    print(what)

# An asyncio-compatible main function
async def main():
    print(f"started at {time.strftime('%X')}")
    await print_after(1, 'hello')
    await print_after(2, 'world')
    print(f"finished at {time.strftime('%X')}")

# The asyncio task scheduler - uses the default asyncio event loop
asyncio.run(main())
  


<p>In Python the <code>asyncio</code> package provides the <code>run</code> function where you can run the default event loop and task scheduler with <code>asyncio.run(my_main_coroutine())</code>. </p>



<p>The image below illustrates how coroutines, the core task concept in asyncio, are run in the AsyncIO framework. </p>


 
 


<p>You have your program, the Python process, that sends tasks to the asyncio event loop with either an explicit call to <code>asyncio.create_task()</code> or use the <code>await</code> keyword to schedule a task in the <code>asyncio</code> event loop and wait for its completion within the body of the function that used the <code>await</code> keyword.</p>



<p>AsyncIO can be difficult to use correctly though it is usually easy to recognize due to most library authors targeting <code>asyncio</code> mark their async functions with <code>async def</code>. There is also the <code>@types.coroutine</code> annotation to make an existing generator function compatible with native coroutines. In order to use AsyncIO and get the performance benefits of using asyncio your whole program has to embrace the AsyncIO paradigm, meaning you use <code>async def</code> native coroutines for anything that does I/O or long-running tasks and you use <code>await</code> to schedule all coroutines.</p>



 Summary of AsyncIO 



<p>AsyncIO has a few main concepts for the async runtime, the asyncio event loop and an <code>async def</code> function as a coroutine. These basic concepts map nicely onto the HIO concepts of a <code>Doist</code>, the root scheduler in HIO, and the <code>Doer</code>, the coroutine or task concept in HIO. The main point where AsyncIO and HIO differ are that HIO has an explicit construct for creating hierarchies of tasks, the <code>DoDoer</code>. While there is no explicit construct in AsyncIO for a <code>DoDoer</code> any <code>async/await</code> coroutine could be considered to be a container for other coroutines.</p>



 Combining AsyncIO and HIO 



<p>Though <code>asyncio</code> native coroutines are not used at all in HIO the two approaches are compatible and composable. You can schedule AsyncIO tasks from a HIO task (a Doer) and you can also schedule a HIO task from an asyncio coroutine.</p>



<p>Yet first we must describe what HIO is. The subject of combining AsyncIO and HIO will be covered in a future article. This article is a short introduction to the three main classes of HIO’s async runtime implementation.</p>



 What is HIO? 


 
 


<p>HIO stands for Hierarchical IO. The README [1] describes it as weightless, hierarchical asynchronous coroutines and I/O in Python. This means that the task abstractions in HIO allow for nesting subtasks within tasks. HIO has three primary classes that make up its asynchronous runtime: <br>1. the <code>Doist</code>, or the root scheduler,<br>2. the <code>DoDoer</code>, a container holding either other <code>DoDoer</code> instances or <code>Doer</code> instances allowing you to create task hierarchies,<br>3. the <code>Doer</code>, the basic task or coroutine construct in HIO.</p>



<p>HIO makes heavy use of what are now known as “<a href="https://www.fluentpython.com/extra/classic-coroutines/">classic coroutines</a>“[2] where the scheduler uses the <code>my_coro.send(data)</code> function to send data into a <a href="https://wiki.python.org/moin/Generators">Python generator function</a>. This generator function is the classic coroutine. </p>



<p>A few keywords distinguish classical coroutines including:</p>



 
 <code>yield</code>: used to pause execution of a coroutine (generator function), send a value out of a coroutine, and receive a value into a coroutine. 



 <code>yield from</code>: used when nesting generators (inner generators) to pause execution of the outer generator and pass, or delegate, conrol to a sub-generator. Once the sub-generator completes then control is passed back to the outer generator.
 
 The <code>yield from</code> keyword is very similar to the <code>await</code> keyword from AsyncIO. Both drive sub-generators and both allow consumption of values returned by sub-generators. 



 <code>await</code> does not completely replace <code>yield from</code> because <code>await</code> must be used inside a native coroutine and must be used with an awaitable object.  



 <code>yield from</code> can be used in any function and with any iterable. 
 
 
 



<p>The <code>yield</code> keyword used in the body of a Python generator function allows it to receive values from the <code>my_coro.send()</code> function, similar to how Erlang/Elixir use the OTP to pass messages between processes with <code>send</code> and <code>receive</code>. The Python <code>my_coro.send(data)</code> is the “send” and the <code>myvar = yield from</code> invocation is the “receive.” And the <code>yield from</code> keyword used in the body of a classic coroutine allows delegating to, or transferring execution to, a nested or sub-generator.</p>



<p>This classic coroutine approach HIO uses is grounded in <a href="https://en.wikipedia.org/wiki/Structured_concurrency">structured concurrency</a> where there are clear entry and exit points to tasks, errors in concurrently executing tasks propagate up the task chain, and clear expression of control flow within the structure of source code despite the presence of concurrency. The context methods of a HIO <code>Doer</code> task provide the clear entry and exit points as well as a clear exception handling mechanism.</p>



 Overview 



<p>The root scheduler, the Doist, processes an array of <code>Doer</code> and <code>DoDoer</code> tasks. The <code>DoDoer</code> is the hierarchical task concept, and the <code>Doer</code> is the core task concept as shown below in the diagram.</p>


 
 


<p>Your program, the Python process, runs the Doist and the Doist runs the list of tasks until they finish or the program is terminated.</p>


  # from github.com/WebOfTrust/keripy/src/keri/app/cli/directing.py
# module: keri.app.cli.directing

# receives a list of tasks for the scheduler to run
def runController(doers, expire=0.0): 
    """
    Utility Function to create doist to run doers
    """
    tock = 0.03125

    # creates the Doist, the root scheduler
    doist = doing.Doist(limit=expire, tock=tock, real=True)  
   
    # adds tasks to the Doist to run. Calling "do" runs the Doist
    doist.do(doers=doers)  

  


<p>Here is a code example of creating an array of <code>doers</code> to pass to the root scheduler, the Doist, from KERIpy. This <code>runWitness</code> function shows the set of tasks that must be created in order to run a KERIpy witness.</p>


  # from github.com/WebOfTrust/keripy/src/keri/app/cli/commands/witness/start.py
# module: keri.app.cli.commands.witness

# Function used by the CLI to run a single basic witness
def runWitness(name="witness", base="", alias="witness", bran="", tcp=5631, http=5632, expire=0.0):
    """
    Setup and run one witness
    """

    ks = keeping.Keeper(name=name, base=base, temp=False, reopen=True)

    aeid = ks.gbls.get('aeid')
    if aeid is None:
        hby = habbing.Habery(name=name, base=base, bran=bran)
    else:
        hby = existing.setupHby(name=name, base=base, bran=bran)

    hbyDoer = habbing.HaberyDoer(habery=hby)  # setup doer
    
    doers = [hbyDoer]  # list of tasks

    # extends the task list with the tasks from indirecting.setupWitness
    doers.extend(indirecting.setupWitness(alias=alias, hby=hby, tcpPort=tcp, httpPort=http)) 

    # calls the Doist root scheduler with a list of tasks
    directing.runController(doers=doers, expire=expire)
  


<p>This function creates a few tasks to be run and hands them off to the <code>Doist</code> scheduler with <code>directing.runController</code>. The scheduler then runs the tasks to completion, or infinitely, depending on the contents of the <code>recur</code> function shown below in the <code>Doer</code>.</p>



 HIO Task – a Doer 



<p>The core task concept in HIO is expressed as the <code><a href="https://github.com/ioflo/hio/blob/main/src/hio/base/doing.py#L437">Doer</a></code> class shown in the UML diagram below. The HIO scheduler, a <code>Doist</code>, runs the <code>Doer</code> task until the <code>.done</code> attribute becomes <code>True</code>. There are six context functions five of which are executed over the lifecycle of the task including <code>enter</code>, <code>recur</code>, <code>clean</code>, <code>close</code>, and <code>exit</code>. The <code>abort</code> function is only called when a task is cancelled or an exception is raised.</p>


 
 


 HIO Scheduler – the Doist 



<p>At the top of the execution hierarchy in the HIO library you find the <code><a href="https://github.com/ioflo/hio/blob/main/src/hio/base/doing.py#L19">Doist</a></code> class, the root scheduler of all task instances, or <code>Doer</code> instances. The generator returned from invoking a <code>Doer</code> is called a “deed” and is handed over to the <code>Doist</code> function. The <code>Doist</code> shown below has a list of <code>deeds</code> that are these generator functions, classic coroutines, that it runs when the <code>Doist</code> is executed.</p>


 
 


<p>To run a <code>Doist</code> you invoke the <code>.do</code> function on the <code>Doist</code> as shown below in a test adapted from HIO.</p>


  def test_doist_doers():
    """
    Test doist.do with .close of deeds
    """
    tock = 0.03125
    doist = doing.Doist(tock=tock)

    # creates a Doer, an example doer 
    doer0 = doing.ExDoer(tock=tock, tymth=doist.tymen())
    # creates a Doer, an example doer 
    doer2 = doing.ExDoer(tock=tock, tymth=doist.tymen())
    doers = [doer0, doer1]

    doist.do(doers=doers)  # run the Doist
    assert doer0.done == True
  


 Context Functions 



<p>The six context functions in the <code>Doer</code> are run by the <code>enter</code> and <code>exit</code> functions of the <code>Doist</code> as well as the <code>do</code> function of the <code>Doer</code>. Each of these functions serve as a lifecycle hook for a different time in the execution of the <code>Doer</code>. The <code><a href="https://github.com/ioflo/hio/blob/main/src/hio/base/doing.py#L541">.do</a></code> function reproduced below shows where each context function is executed after calling <code>Doer.do</code>. Take special notice of the <code>while</code> loop inside of the <code>try/except</code> block. This is the loop that continues to run the body of the <code>Doer</code>, the function or generator that does the work of the <code>Doer</code>.</p>


  # from github.com/ioflo/hio/src/hio/base.doing.py
class Doer(tyming.Tymee):
    ...

    def do(self, tymth, *, tock=0.0, **opts):
        """
        Generator method to run this doer. Calling this method returns generator.
        Interface matches generator function for compatibility.
        To customize create subclasses and override the lifecycle methods:
            .enter, .recur, .exit, .close, .abort

        Parameters:
            tymth is injected function wrapper closure returned by .tymen() of
                Tymist instance. Calling tymth() returns associated Tymist .tyme.
            tock is injected initial tock value
            args is dict of injected optional additional parameters
        """
        try:
            # enter context
            self.wind(tymth)  # update tymist dependencies
            self.tock = tock  #  set tock to parameter
            self.done = False  # allows enter to override completion state
            self.enter()  # (1) first context function, enter

            #recur context
            if isgeneratorfunction(self.recur):  #  .recur is generator method
                self.done = yield from self.recur()  # (2) recur context delegated, second context function
            else:  # .recur is standard method so iterate in while loop
                while (not self.done):  # recur context
                    tyme = (yield (self.tock))  # yields .tock then waits for next send
                    self.done = self.recur(tyme=tyme)   # (2) second context function, recur

        except GeneratorExit:  # close context, forced exit due to .close
            self.close()  # (3) third context function, close

        except Exception as ex:  # abort context, forced exit due to uncaught exception
            self.abort(ex=ex)   # (4) fourth context function, abort
            raise

        else:  # clean context
            self.clean()  # (5) fifth context function, clean

        finally:  # exit context, exit, unforced if normal exit of try, forced otherwise
            self.exit()  # (6) sixth context function, exit

        # return value of yield from or StopIteration.value indicates completion
        return self.done  # Only returns done state if normal return not close or abort raise
  


<p>In the normal execution of a <code>Doer</code> the <code>.do()</code> function calls, in this order, <code>enter</code>, <code>recur</code>, <code>clean</code>, and then <code>exit</code>. The <code>close</code> context function is only executed when it is explicitly called by some higher level construct such as a <code>DoDoer</code> or the <code>Doist</code> scheduler itself.</p>



<p>In an error case, or abnormal execution of a <code>Doer</code>, the <code>abort</code> context function is called. This can also be called as a part of normal execution of a program to catch a shutdown signal to instruct a <code>DoDoer</code> or a <code>Doist</code> to perform a graceful shutdown.</p>



 HIO DoDoer – where task hierarchies are defined 



<p>This post touches lightly on <code>DoDoer</code>s to say that the <code>DoDoer</code> provides hierarchical task management which means you can nest tasks for a clear hierarchy of task execution for groups of tasks. A future article will detail the definition and usage of the <code>DoDoer</code>.</p>



 AsyncIO vs HIO – How do they compare? 



<p>Classic coroutines are very powerful constructs that provide a richer control flow construct as compared to AsyncIO’s <code>async def</code> coroutine construc. This is because you can use any number of <code>yield</code> or <code>yield from</code> statements in the body of a classic coroutine, which provides you with the ability to custom-fit the execution of a generator-based coroutine to your specific use case. The <code>async/await</code> syntax does a similar thing for you, yet with a standard syntax that you cannot customize.</p>



<p>With HIO you can also repeatedly accept information into a classic coroutine instance through the <code>yield from</code> syntax. The fact that classic coroutines are just generator functions means you have full control over iteration of that generator, and all of it’s contained state including any state it has closed over, from an async context with all the power of Python iterators.</p>



<p>For example, you could run a classic coroutine any arbitrary number of times within a custom scheduler depending on special rules and have fine-grained access to what is sent into the coroutine with the <code>.send()</code> function.</p>



<p>Yet with this additional power comes the potential to have complicated and hard to understand control flow. It is understandable why there would be so much support in the Python community for a simpler, less powerful syntax, which is what <code>async/await</code> is. The linked <a href="https://www.fluentpython.com/extra/classic-coroutines/">article</a>[2] from Luciano Ramalho goes in depth on the features of both classic coroutines and Python’s AsyncIO.</p>



 Wrap up and Next Steps 



<p>This article focused on the “what” of the async framework side of HIO, specifically the three primary classes at the core of the async runtime in HIO, the <code>Doist</code> scheduler, <code>DoDoer</code> hierarchical task container, and the <code>Doer</code> task class. The raw power of classic coroutines significantly influenced the decision to use them in HIO as well as in KERIpy, KERIA, and SignifyPy. Yet, this is not an either-or, all-or-nothing situation. You can use HIO and AsyncIO together. </p>



<p>Major topics not covered in this article that are important to understand HIO include details of the <code>DoDoer</code> and the network and file I/O capabilities of the HIO package.</p>



<p>Future articles will delve deeper into the “why” of HIO, the rationale behind HIO, how and when to use it properly, as well as how to use HIO and AsyncIO together. To gain a deeper understanding of HIO one of your next steps would be to read some of the <a href="https://github.com/ioflo/hio/tree/main/tests">tests</a> in the HIO source code repository, specifically the <code><a href="https://github.com/ioflo/hio/blob/main/tests/base/test_doist.py#L30">test_doist_once</a></code> and <code><a href="https://github.com/ioflo/hio/blob/main/tests/base/test_doist.py#L691">test_nested_doers</a></code> tests.</p>



 References 



<p>[1] S. Smith, “hio/README.md at main · ioflo/hio,” GitHub, Aug. 21, 2021. <a href="https://github.com/ioflo/hio/blob/main/README.md">https://github.com/ioflo/hio/blob/main/README.md</a> (accessed May 09, 2024).</p>



<p>[2] L. Ramalho, “Classic Coroutines,” Fluent Python, the lizard book, Apr. 2022. <a href="https://www.fluentpython.com/extra/classic-coroutines">https://www.fluentpython.com/extra/classic-coroutines</a> (accessed May 11, 2024).</p>



<p>[3] Real Python, “Async IO in python: A complete walkthrough,” Real Python, <a href="https://realpython.com/async-io-python">https://realpython.com/async-io-python</a> (accessed May 9, 2024).</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://kentbull.com/wp-content/uploads/2024/05/dalle-2024-05-09-22.00.43-python-and-ioflo-logos.webp?w=300'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   00:11 &bull;
   23 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 11. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://benwerd.medium.com/think-twice-before-exercising-your-stock-options-2e5142f4c015?source=rss-3b16402f5b9d------2'>Think twice before exercising your stock options</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

      Startup options and when they’re worth it  Continue reading on Medium »  
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/0*kzZVFgKpc0HtnOIC.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   <p><a href="https://benwerd.medium.com/think-twice-before-exercising-your-stock-options-2e5142f4c015?source=rss-3b16402f5b9d------2"></a></p><p>Startup options and when they’re worth it</p><p><a href="https://benwerd.medium.com/think-twice-before-exercising-your-stock-options-2e5142f4c015?source=rss-3b16402f5b9d------2">Continue reading on Medium »</a></p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/0*kzZVFgKpc0HtnOIC.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:33 &bull;
   23 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@rufftimo'>Timothy Ruff</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://rufftimo.medium.com/keri-101-witnesses-watchers-detectability-fb604cae7a26?source=rss-11906019170a------2'>KERI 101: Witnesses, Watchers, & Detectability</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   The “oil lights” of cryptographic key management.  TL;DR   KERI is an open, decentralized protocol for discovery, recovery, and management of cryptographic keys. KERI enables anyone to digitally sign digital things and to verify what others have signed, without the need for shared networks, blockchains, registries, platforms, or identity providers.  KERI solves seven hard problems of key managemen
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/300/0*5_g0cmf-5cMyHv6x'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 The “oil lights” of cryptographic key management.  TL;DR   KERI is an open, decentralized protocol for discovery, recovery, and management of cryptographic keys. KERI enables anyone to digitally sign digital things and to verify what others have signed, without the need for shared networks, blockchains, registries, platforms, or identity providers.  KERI solves seven hard problems of key management: Rotation, Recovery, Detectability, Discovery, Delegability, Revocability, and Multi-Signature.  KERI has three Detectability functions — Witnesses, Watchers, and Anchored Issuances — that are like the oil light of a car: they detect key compromise and malicious behaviors of Signers and service providers.  <p>With KERI Detectability functions…</p><p>Signers can&nbsp;detect:</p>  Their own compromised keys when used by hackers for&nbsp;signing;  Their own compromised keys when used by hackers to rotate, delegate, or revoke their signing&nbsp;keys.  <p>And Verifiers can&nbsp;detect:</p>  Compromised Signers;  Malicious Signers;  Their own malicious or compromised verification service provider.  <p>Without KERI Anchored Issuances in particular, if a Signer suspects key compromise and performs a rotation, it cannot determine which issuances were made fraudulently, and must revoke and reissue all previously issued credentials. (!)</p><p>With KERI Anchored Issuances, fraudulently issued credentials will not successfully verify before or after key rotations, whereas authentically issued credentials will continue to successfully verify, even after an unlimited number of key rotations.</p> What is&nbsp;KERI? <p>KERI — Key Event Receipt Infrastructure — is a new (2019), open, decentralized protocol for discovering, recovering, and managing <a href="https://www.cloudflare.com/learning/ssl/what-is-a-cryptographic-key/">cryptographic keys</a>. KERI enables people, organizations, and things to <a href="https://www.cisa.gov/news-events/news/understanding-digital-signatures">sign</a> digital things and verify what others have signed, without the need for shared networks, blockchains, registries, platforms, or identity providers.</p><p>With KERI, Signers and Verifiers worldwide maintain their own infrastructure without anything in common, but they do share one game-changing thing: a protocol for discovering the Signer’s current key state. This enables Verifiers to securely confirm that a piece of data has been signed by a particular Signer, and that it hasn’t been tampered with, revoked, or&nbsp;expired.</p><p>KERI is “end verifiable”; the means through which data travels needn’t be secure for it to still be verifiably authentic when it arrives at its destination. KERI is zero trust, enabling objective verifiability without subjective trust, and it creates no centralized troves of valuable data. KERI is quantum-resistant and uses existing cryptography and cloud infrastructure.</p><p>In short, KERI brings the ability to distinguish fake from real in the digital realm, across networks, borders, and boundaries of all&nbsp;kinds.</p> Seven Hard Problems of Key Management <p>KERI address seven hard problems of key management, listed below. Most were previously not solvable in a decentralized manner, and some not in a centralized one.</p><p>Seven hard problems (with the KERI solution):</p>  Rotation (change current keys for new, without re-issuing past issuances)  Recovery (from private key loss or compromise, without re-issuing past issuances)  Detectability (of key compromise or malicious Signer behavior)  Discovery (just-in-time discovery of key&nbsp;state)  Delegability (directly from one Signer/key controller to&nbsp;another)  Revocability (decentralized, instant, with&nbsp;privacy)  Multi-signature (weighted m-of-n, verifiable)   Detectability: Witnesses, Watchers, &amp; Anchored Issuances  Detectability Is the ‘Oil Light’ of Key Management <p>Together, KERI Witnesses, Watchers, and Anchored Issuances, all described below, enable Detectability. Detectability is like the oil light of a car: ignore it, disconnect it, or omit it at your peril, because it is the most critical indicator on your dashboard.</p><p>Removing Witnesses, Watchers, and Anchored Issuances from a system removes Detectability.</p><p>Without Detectability, Signers cannot&nbsp;detect:</p>  Their own compromised keys when used by hackers for&nbsp;signing;  Their own compromised keys when used by hackers to rotate, delegate, or revoke the Signer’s&nbsp;keys.  <p>And Verifiers cannot&nbsp;detect:</p>  A Signer’s compromised keys;  Malicious Signers;  Their own malicious or compromised verification service provider.   What Are Witnesses? <p>KERI <a href="https://medium.com/finema/keri-jargon-in-a-nutshell-part-1-fb554d58f9d0#a998">Witnesses</a> are secure, distributed replicas of a Signer’s <a href="https://medium.com/finema/keri-jargon-in-a-nutshell-part-1-fb554d58f9d0#7c43">key event log</a>. They are established (built or bought) and controlled by a <a href="https://medium.com/finema/keri-jargon-in-a-nutshell-part-1-fb554d58f9d0#2627">Signer</a> (also a “key controller” or “issuer”). A Signer can be anyone or anything, anywhere.</p><p>Witnesses make separate, additional replicated points of verification available to <a href="https://medium.com/finema/keri-jargon-in-a-nutshell-part-1-fb554d58f9d0#1f72">Verifiers</a>, enabling Verifiers to detect both compromised and malicious behaviors by Signers. Witnesses can be web servers, distributed databases, blockchains, or any other addressable data&nbsp;source.</p><p>To compromise a KERI <a href="https://medium.com/finema/keri-jargon-in-a-nutshell-part-1-fb554d58f9d0#7344">identifier</a>, each of its Witnesses must also be compromised.</p><p>There are no limits or requirements as to whether a Signer uses Witnesses at all or how many; the KERI protocol simply enables any number of Witnesses to be established, deployed however the Signer chooses, and for Verifiers to verify that all Witnesses are acting in unanimity about the Signer’s key&nbsp;state.</p> What Are Watchers? <p>KERI <a href="https://medium.com/finema/keri-jargon-in-a-nutshell-part-1-fb554d58f9d0#0d21">Watchers</a> are services established (built or bought) by a Verifier — who can be anyone or anything anywhere — that enable the Verifier to confidently rely on what their own service providers are telling them about a Signer’s <a href="https://medium.com/finema/keri-jargon-in-a-nutshell-part-1-fb554d58f9d0#7344">key&nbsp;state</a>.</p><p>Watchers help Verifiers avoid the single-oracle problem: if you have only one source of truth, you cannot know if that source has been compromised or is acting maliciously (deliberately lying to you). When multiple distinct Watchers all report the same key state of a Signer, it can be more safely relied upon. Having Watchers removes a tempting single point of vulnerability for&nbsp;hackers.</p><p>As with Witnesses, there are no limits or requirements as to whether Watchers are used at all or how many; the KERI protocol simply enables any number of Watchers to be established and deployed however&nbsp;desired.</p> Anchored Issuances: A Breakthrough in Detecting and Preventing the Use of Stolen&nbsp;Keys <p>Another critical element of Detectability, separate from Witnesses and Watchers, is “Anchored Issuances” (referred to as “seals” in the KERI white&nbsp;paper).</p><p>When issued credentials are properly anchored in one’s Key Event Log (KEL) and properly witnessed and watched, it becomes impossible for a hacker of your private keys to use them without your detection; the unauthorized issuances will not successfully verify. With un-anchored / “unbound” issuances, hackers can use stolen private keys as they please with zero detectability and, unfortunately, every issuance they make will successfully verify.</p><p>Akin to a check register, Anchored Issuances provide a secure record of issued credentials that can be instantly and cryptographically confirmed by Verifiers. No other system we are aware of provides detectability or prevention of the use of stolen private&nbsp;keys.</p> No Detectability? Rotate, Revoke, and Re-Issue Everything. (Really) <p>Some alternative key management systems borrow KERI’s primary functions — <a href="https://medium.com/finema/keri-jargon-in-a-nutshell-part-1-fb554d58f9d0#0a82">pre-rotation</a>, <a href="https://medium.com/finema/keri-jargon-in-a-nutshell-part-1-fb554d58f9d0#909c">SCIDS</a>, etc. — but forgo its Detectability functions. For them and for all non-KERI key management systems, a big future headache awaits: if a Signer suspects key compromise and performs a rotation, there is no way to distinguish valid issuances from invalid ones — they will all continue to verify successfully — so a Signer must revoke and reissue all previously issued credentials. (!)</p><p>That is, if a Signer wishes to maintain a positive reputation for its credentials. Revoking and reissuing everything would likely be a user experience nightmare, which alone defeats the purpose of having pre-rotation. Pre-rotation is arguably KERI’s greatest innovation, but does not achieve its full potential without Detectability.</p> Can’t We Just Rotate More&nbsp;Often? <p>Yes, but it won’t help much. KERI pre-rotation can be done repeatedly without downstream costs or consequences for KERI identifiers, a breathtaking breakthrough in key management. But… while a Signer’s more-frequent rotation might attempt to repair one vulnerability — compromise of the Signer’s keys — it does not address other vulnerabilities listed in the “KERI Detectability” section, and it still leaves the ugly problem of revoking and re-issuing everything if a compromise is suspected. In other words, removing Detectability leaves significant holes in the security model no matter how often a Signer rotates their&nbsp;keys.</p><p>Such a tradeoff may be acceptable for less critical use cases or at smaller scale, but is likely unacceptable for most organizations.</p> In Conclusion <p>KERI Witnesses, Watchers, and Anchored Issuances are the ‘oil lights’ of cryptographic key management, enabling robust detectability of malicious or compromised digital signing behaviors for both Signers and Verifiers. KERI Anchored Issuances goes beyond detection to full-on prevention, actively blocking the use of stolen private keys for credential signing.</p><p>KERI Detectability brings critical advances in key management specifically and cybersecurity generally, and should be seriously considered for inclusion within any comprehensive cybersecurity, identity, or key management system.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/300/0*5_g0cmf-5cMyHv6x'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   18:35 &bull;
   24 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 10. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/05/pgxn-language-poll-result/'>PGXN Language Poll Result</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Results of the Postgres community poll for building PGXN v2 in Go, Rust, or both.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Back on March 28, I <a href="https://justatheory.com/2024/03/pgxn-language-choices/" title="PGXN v2: Go or Rust?">asked</a> the Postgres community whether new services for
<a href="https://github.com/orgs/pgxn/projects/1/views/1" title="PGXN v2 Project">PGXN v2</a> should be written in <a href="https://go.dev" title="The Go Programming Language">Go</a>, <a href="https://www.rust-lang.org" title="Rust Programming Language">Rust</a>, or “some of each”. I
went so far as to create a <a href="https://dev.star.vote/tqkv3v/results" title="Poll: What language should PGXN v2 tools and services be written in?">poll</a>, which ran through April 12. A month later
you might reasonably be wondering what became of it. Has David been refusing
to face reality and accept the results?</p>
<p>The answer is “no”. Or at least I don’t think so. Who among us really knows
ourselves. Since it closed, the <a href="https://dev.star.vote/tqkv3v/results" title="Poll: What language should PGXN v2 tools and services be written in?">poll</a> has provided the results since it
closed, but I suspect few have looked. So here they are:</p>



Candidate
Votes
% All Votes




🦀 Rust
102
60.4%


🐿️ Go
53
31.4%


🐿️ + 🦀 Some of each
13
7.7%



<p>🦀 Rust is the clear winner.</p>
<p>I don’t know whether some Rust brigade descended upon the poll, but the truth
is that the outcome was blindingly apparent within a day of posting the poll.
So much so that I decided to get ahead of things and try writing a <a href="https://github.com/pgcentralfoundation/pgrx" title="pgrx: Build Postgres Extensions with Rust!">pgrx</a>
extension. I released <a href="https://pgxn.org/dist/jsonschema/" title="The jsonschema Postgres Extension on PGXN">jsonschema</a> on PGXN on April 30. Turned out to be kind
of fun, and the pgrx developers kindly answered all my questions and even made
a new release to simplify integration testing, now included in the
<a href="https://github.com/pgxn/docker-pgxn-tools?tab=readme-ov-file#pgrx-build-test" title="pgrx-build-test: Build and test a pgrx extension"><code>pgrx-build-test</code></a> utility in the <a href="https://hub.docker.com/r/pgxn/pgxn-tools">pgxn-tools</a> Docker image.</p>
<p>But I digress. As a result of this poll and chatting with various holders of
stakes at <a href="https://tembo.io" title="Tembo: Goodbye Database Sprawl, Hello Postgres">work</a> and haunting the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> Slack channel, I plan to use
Rust for all new PGXN projects — unless there is an overwhelmingly
compelling reason to use something else for a specific use case.</p>
<p>Want to help? Rustaceans welcome! Check out the <a href="https://github.com/orgs/pgxn/projects/1/views/1" title="PGXN v2 Project">project plan</a>
plan or join us in the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a>.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/go/">Go</a> 
                 <a href="https://justatheory.com/tags/rust/">Rust</a> 
                 <a href="https://justatheory.com/tags/perl/">Perl</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   18:13 &bull;
   25 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2024/05/10/personal-vs-personalized/'>Personal vs. Personalized AI</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  There is a war going on. Humanity and nature are on one side and Big Tech is on the other. The two sides are not opposed. They are orthogonal. The human side is horizontal and the Big Tech side is vertical.* The human side is personal, social, self-governed, heterarchical, open, and grounded in the physical […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p></p>
<p>There is a war going on. Humanity and nature are on one side and Big Tech is on the other. The two sides are not opposed. They are orthogonal. The human side is horizontal and the Big Tech side is vertical.*</p>
<p>The human side is personal, social, self-governed, heterarchical, open, and grounded in the physical world. Its model is nature, and the cooperative contexts in which competition, creation, and destruction happen in the natural world.</p>
<p>The Big Tech side is corporate, industrial, hierarchical, competitive, mechanistic, extractive, and closed, even though it produces many positive-sum products and services that are good for people and good for nature. It is also, being competitive and rewarding toward winner-take-most outcomes, dominated by giants.</p>
<p>This war has been fought over many other things in the past, especially in tech. But AI is the big one right now—and perhaps the biggest one of all time.</p>
<p>Over the long run, both sides will win, because we need the best of what both bring to the world’s big round table. In the past, this has happened in countless markets, countries, polities, societies, and other contexts. In tech it happened with the conflict between PCs and mainframes, between the open and public Internet and closed private networks, and between open operating systems and closed ones. None of those conflicts have final outcomes, but they have arrived at stable and cooperative states. They arrived at those states because everyone recognized that people can do more with their own tools than those big entities can do on theirs, or by using people as extracted resources.</p>
<p>This conflict is perhaps best represented by the ironies that have been at work within OpenAI and Sam Altman, its prior and current CEO. We can also see it with sparkling clarity in something amazing that Sam said near the close of <a href="https://www.youtube.com/watch?v=U9mJuUkhUzk">his keynote</a> at the first <a href="https://devday.openai.com/">OpenAI Developer Conference</a>, in November of last year:</p>
<p></p>
<p>Lets unpack that.</p>
<p>Individual empowerment means exactly what it says. Agency&nbsp;is the ability to take action or to choose what action to take&nbsp;(<a href="https://dictionary.cambridge.org/us/dictionary/english/agency">Cambridge Dictionary</a>).&nbsp;Scale means growth toward ubiquity.</p>
<p>You have individual empowerment and agency when you ride your bike, drive your car, mow your lawn, or button your shirt. You also have it when you work on your laptop, phone, and other smart rectangles. Scale means your power and agency will grow to the max while the same will happen for nearly everyone else. Scale for you alone won’t cut it. When most of us have it, that’s scale.</p>
<p>Technologies <a href="https://en.wikipedia.org/wiki/Understanding_Media">extend us</a>. They enlarge our capacities in the world. Intelligence is one of those capacities. So is memory. Your rectangles help a lot with both. That’s why those have already scaled to ubiquity, or close enough.</p>
<p>AI will do the same, but only if it’s personal. Not if it’s&nbsp; just “personalized.”</p>
<p>As Jamie Smith made clear <a href="https://www.customerfutures.com/p/customer-futures-perspective-i-have">here</a> a year ago, “your personal AI isn’t really personal.”</p>
<p>ChatGPT4 doesn’t know that yet. I can tell because I just asked:</p>
<blockquote><p>Me:<br>
What is “personal AI”?</p>
<p>ChatGPT4:<br>
“Personal AI” refers to artificial intelligence systems that are designed to serve individual users through personalized interactions and services. These AI systems learn from the specific preferences, behaviors, and needs of their users to offer more tailored assistance, recommendations, or information. The goal of personal AI is to enhance user experience by adapting to unique personal profiles.</p>
<p>There are various applications of personal AI, including:</p>
 
 Virtual Assistants: Like Siri, Google Assistant, and others, these AIs help with daily tasks such as scheduling, reminders, searching information, and controlling smart home devices, all customized to the user’s habits and preferences. 
 Recommendation Systems: Used in streaming services like Netflix or shopping platforms like Amazon, these AI systems analyze your past behavior to recommend movies, shows, or products you might like. 
 Healthcare: Personal AI can track individual health metrics and provide personalized advice, monitor conditions, or even alert medical professionals if needed. 
 Learning and Development: In education, personal AI can adapt learning materials to the pace and learning style of the student, offering a customized educational experience. 
 Gaming: AI in gaming can adapt to provide a challenge tailored to the player’s skill level or alter the game narrative based on choices made by the player. 
 
<p>The development of personal AI focuses on enhancing user engagement by creating more intuitive and responsive interactions that feel uniquely suited to each individual.</p></blockquote>
<p>For a second opinion, I asked Google’s <a href="https://gemini.google.com/">Gemini</a> the same question.</p>
<blockquote><p>Me:<br>
What is “personal AI”?</p>
<p>Gemini:<br>
Personal AI refers to artificial intelligence designed to assist and interact with a specific individual. It’s essentially your own AI companion that learns about you and tailors its services to your needs. Here are some key aspects of personal AI:</p>
 
 Personalization: Unlike general-purpose AI assistants, personal AI focuses on you. It learns from your interactions, preferences, and habits to become more helpful over time. 
 Focus on Memory and Knowledge: Some personal AI systems aim to become an extension of your mind. They can store your memories, knowledge, and experiences, making them readily accessible and even usable to help with tasks like writing or communication. 
 Varied Applications: Personal AI can have a wide range of applications. Imagine an AI that helps you: 
 Organize your life: Manage schedules, appointments, and reminders. 
 Boost productivity: Assist with tasks, research, and communication. 
 Enhance creativity: Help you brainstorm ideas or generate creative content. 
 Improve learning: Personalize learning experiences and provide targeted information. 
 
<p>Privacy Concerns: Storing personal data on an AI raises concerns about privacy and security.</p>
<p>Overall, personal AI holds promise for a future where technology seamlessly integrates with our lives, offering personalized assistance and support.</p></blockquote>
<p>To both companies, personal AI is a personalized service—from them. It’s not something you own and control. It’s not about individual empowerment and agency. It doesn’t extend you. It’s hired prosthesis: an extension of them rather than of you.</p>
<p>But hats off. OpenAI and Gemini do an amazing job. So do lots of other AI services by wannabe giants. I use AI every day to improve what I write, to generate and fix images, to answer questions that old-fashioned search engines can’t answer or suck at. I even pay extra for some of it (such as ChatGPT4 and Adobe’s Creative Suite). And it seems they all get better, all the time, at everything. Won’t personalization be good enough, eventually?</p>
<p>No. Because they aren’t you. They also aren’t yours, so they can’t extend you. They can seem to. But they don’t. They also don’t have access to most of your private life. Nor should they.</p>
<p>But your private life could use some AI help. For example—</p>
 
 Schedules, including on your calendars, past and future 
 Health data, including all your medical reports, prescriptions, appointments, insurance information, past and present providers, plus what your watch, phone, and other devices record about you 
 Financial records, including bills, receipts, taxes, and anything called an account that involves money 
 Travel, including all the movements your phone (and <a href="https://www.cnet.com/tech/mobile/data-privacy-all-the-ways-your-cellphone-carrier-tracks-you-and-how-to-stop-it/">phone company</a>), watch, and <a href="https://foundation.mozilla.org/en/blog/privacy-nightmare-on-wheels-every-car-brand-reviewed-by-mozilla-including-ford-volkswagen-and-toyota-flunks-privacy-test/">car</a> record about where you go and where you’ve been 
 Work—past and present, including whatever HR systems know or knew about you 
 Contacts—all the people, businesses, and other entities you know 
 Business relationships, with brokers, retailers, service providers, whatever 
 Subscriptions, including all those “just $1 for the first four weeks” offers you’ve accepted, plus other forms of <a href="https://doc.searls.com/2020/10/17/unscrew/">screwage</a> that are stock-in-trade for companies selling subscription systems to businesses. 
 Property, including all the stuff on your shelves, floors, closets, garages, and storage spaces—plus your stocks and real estate. 
 
<p>It’s not easy to visualize what a personal AI might do for those, but let’s try. Here’s how Microsoft’s Copilot (or whatever it’s called this week) did it for me before I got rid of all its misspellings and added my own hunks of text:</p>
<p></p>
<p>All that stuff is data. But most of it is scattered between apps and clouds belonging to Apple, Google, Microsoft, Amazon, Meta, phone companies, cable companies, car makers, health care systems, insurance companies, banks, credit card companies, retailers, and other systems that are not yours. And most of them also think that data is theirs and not yours.</p>
<p>To collect and manage all that stuff, you need tools that don’t yet exist: tools that are yours and not theirs. We could hardly begin to imagine those tools before AI came along. Now we can.</p>
<p>For example, you should be able to take a picture of the books on your shelves and have a complete record of what those books are and where you got them. You’ll know where you got them because you have a complete history of what you bought, where and from whom. You should be able to point your camera in your closets, at the rugs on your floors, at your furniture, at the VIN number of your car that’s visible under your windshield, at your appliances and plumbing fixtures, and have your AI tell you what those are, or at least make far more educated guesses than you can make on your own.</p>
<p>Yes, your AI should be able to tap into external databases and AI systems for help, but without divulging identity information or other private data. Those services should be dependent variables, not independent ones. For full individual empowerment and agency, you need to be independent. So does everyone else with personal AI.</p>
<p>Now imagine having a scanner that you can feed every bill, every receipt, every subscription renewal notice, and have AI software that tells you what’s what with each of them, and sorts records into the places they belong.</p>
<p>Ever notice that the Amazon line items on your credit card bill not only aren’t itemized, but don’t match Amazon’s online record of what you ordered? Your personal AI can sort that out. It can help say which are business and personal expenses, which are suspicious in some way, what doesn’t add up, and much more.</p>
<p>Your personal AI should be able to answer questions like, How many times have I had lunch at this place? Who was I with? When was it we drove to see so-and-so in Wisconsin? What route did we take? What was that one car we rented that we actually liked?</p>
<p>Way back in 1995, when our family first got on the Internet over dial-up, using the first graphical browsers on our PC, and e-commerce began to take off with Amazon, eBay, and other online retailers, my wife asked an essential question: Why can’t I have my own shopping cart that I take from site to site?</p>
<p>Twenty-nine years later, we still don’t have the answer, because every retailer wants you to use its own. And we’re stuck in that system. It’s the same system that has us separately consenting to what sites ironically call “your privacy choices.” And aren’t.</p>
<p>There are countless nice things we can’t have in the digital world today because we aren’t people there. We are accounts. And we are reduced to accounts by every entity that requires a login and password.</p>
<p><a href="https://dsearls.medium.com/beyond-the-web-b33518312876">This system</a> is a legacy of <a href="https://en.wikipedia.org/wiki/Client%E2%80%93server_model">client-server</a>, a euphemism for slave-master. We might also call it calf-cow, because that’s how we relate to businesses with which we have accounts. And that model is leveraged on the Web like this:</p>
<p></p>
<p>We go to sites for the milk of content and free cookies, whether we want them or not. We are also just “users.”</p>
<p>In the client-server world, servers get scale. Clients have no more scale than what each account—each cow—separately allows. Sure, users get lots of benefits, but scale across many cows is not one of them. And no, “login with Google” and “login with Facebook” are just passes that let calves of ruling cows wander into vassal pastures.</p>
<p>For individual empowerment and scale to happen, we need to be self-sovereign and independent. Personal AI can give that to us. It can do that by solving problems such as the ones listed above, and by working as agents that represent us as human beings—rather than mere users—when we engage with Big Tech’s cows.</p>
<p>This will be a fight at first, because the cows think they run all of nature and not just their own farms. And $trillions are being invested in the same old cattle industry, with AI painted all over the new barns. Comparatively speaking, close to nothing is going toward giving independent and self-sovereign individuals the kind of power and scale Sam Altman says he wants to give us but can’t because he’s on the big cow side of this thing.</p>
<p>So where do we start?</p>
<p>First, with open source code and open standards. We have some already. <a href="https://ai.meta.com/blog/meta-llama-3/">Llama 3</a>, from <a href="https://ai.meta.com/">Meta AI</a>, is “your own intelligent assistant,” and <a href="https://www.thealgorithmicbridge.com/p/openai-rules-the-changes-but-meta">positions Meta as a more open and user-friendly</a> cow than OpenAI. Meta is still on the top-down Big Tech side of the war we’re in. But hell, we can use what they’ve got. So let’s play with it.</p>
<p>Here on the ground there are all these (with quotage lifted from their sites or reviews such as <a href="https://www.infoworld.com/article/3673976/13-open-source-projects-transforming-ai-and-machine-learning.html">this one</a>)—</p>
 
 <a href="https://en.wikipedia.org/wiki/MindsDB">MindsDB</a>: “an open-source AutoML framework” 
 <a href="https://alt.ai/en/">Alt.ai</a>: “It’s an A.I. which aims to digitize users’ intentions and place it on the cloud to let our clones deal with all digital operations.” 
 <a href="https://keras.io/">Keras</a>: “a multi-backend deep learning framework, with support for JAX, TensorFlow, and PyTorch” 
 <a href="https://pytorch.org/">PyTorch</a>: “Python package that provides two high-level features: Tensor computation (like NumPy) with strong GPU acceleration, and Deep neural networks built on a tape-based autograd system 
 <a href="https://www.tensorflow.org/">Tensor Flow</a>: “open-source framework for machine learning” 
 <a href="https://github.com/apple/corenet">CoreNet</a>: a deep neural network toolkit for small and large-scale models, from Apple 
 <a href="https://haystack.deepset.ai/overview/intro">Haystack</a>: an “open source Python framework by deepset for building custom apps with large language models (LLMs).” 
 <a href="https://sourceforge.net/projects/image-super-res-isr.mirror/"> Image Super-Resolution (ISR)</a>: “(an) open source tool employs a machine learning model that you can train to guess at the details in a low-resolution image: 
 <a href="https://www.blender.org/">Blender</a>: “A rich interface and numerous plugins make it possible to create complex motion graphics or cinematic vistas” 
 <a href="https://sourceforge.net/projects/deepfacelab.mirror">DeepFaceLab</a>: “open source deepfake technology that runs on Python” 
 <a href="https://topapps.ai/ai-apps/tflearn/">tflearn</a>: “an advanced deep learning library” 
 <a href="https://pytensor.readthedocs.io/en/latest/">PYTensor</a>: “a Python library that allows you to define, optimize/rewrite, and evaluate mathematical expressions involving multi-dimensional arrays efficiently.” (Was Theano) 
 <a href="https://lmstudio.ai/">LM Studio</a>: “Discover, download, and run local LLMs” 
 <a href="https://huggingface.co/docs/transformers/index">HuggingFace Transformers</a>: “a popular open-source library for Natural Language Processing (NLP) tasks” 
 <a href="https://www.fast.ai/">Fast.ai</a>: “a library for working with deep learning tasks” 
 <a href="https://opencv.org/">OpenCV</a>: “a popular Computer Vision and Image Processing library developed by Intel” 
 <a href="https://ai.meta.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/">Detectron2</a>: “a next-generation library that provides advanced detection and segmentation algorithm” and “a&nbsp;PyTorch-based modular object detection library” 
 <a href="https://ivy.ai/">Ivy.ai</a>: “an open-source deep learning library in Python focusing on research and development” 
 <a href="https://open-assistant.io/">OpenAssistant</a>: “a project aimed at giving everyone access to a great chat-based large language model” 
 <a href="https://github.com/PaddlePaddle/PaddleNLP">PaddleNLP</a>: “a popular open source NLP library that you can use to glean search sentiment and flag important entities” 
 <a href="https://www.delphi.ai/">Delphi.AI</a>: “Clone yourself. Build the digital version of you to scale your expertise and availability, infinitely.” 
 Fauxpilot: “This is an attempt to build a locally hosted alternative to&nbsp;<a href="https://copilot.github.com/">GitHub Copilot</a>. It uses the&nbsp;<a href="https://github.com/salesforce/CodeGen">SalesForce CodeGen</a> models inside NVIDIA’s&nbsp;<a href="https://developer.nvidia.com/nvidia-triton-inference-server">Triton Inference Server</a>&nbsp;with the&nbsp;<a href="https://github.com/triton-inference-server/fastertransformer_backend/">FasterTransformer backend</a>.” 
 <a href="https://docs.ray.io/en/latest/index.html">Ray</a>: “An open source framework to build and scale your ML and Python applications easily” 
 <a href="https://solidproject.org/">Solid</a>: “Solid is a specification that lets individuals and groups store their data securely in decentralized data stores called Pods. Pods are like secure web servers for data. When data is stored in a Pod, its owners control which people and applications can access it.” 
 <a href="https://www.sagen.ai/">Sagen.ai</a>: “Your very own AI Personal Assistant to manage your digital life.” 
 <a href="https://github.com/WongKinYiu/yolov7">YOLOv7</a>: “is one of the fastest and most accurate open source object detection tools. Just provide the tool with a collection of images full of objects and see what happens next.” 
 
<p>—and lots of others that readers can tell me about. Do that and I will add links later. This is a work in progress.</p>
<p>Below all of those we still need something Linux-like that will become the open base on which lots of other stuff runs. The closest I’ve seen so far is <a href="https://paios.org/">pAI-OS</a>, by <a href="https://www.kwaai.ai/">Kwaai.ai</a>, a nonprofit I now serve as <a href="https://www.kwaai.ai/about">Chief Intention Officer</a>. I got recruited by <a href="https://www.linkedin.com/in/rezarassool/">Reza Rassool</a>, Kwaai’s founder and chair, because he believes personal AI is required to make <a href="https://en.wikipedia.org/wiki/The_Intention_Economy">The Intention Economy </a>finally happen. So that was a hard offer to refuse. Kwaai also has a large, growing, and active community, which I believe is necessary, cool, and very encouraging.</p>
<p>As with most (maybe all) of the projects listed above, Kwaai is a grass-roots effort by human beings on the natural, human, and horizontal side of a battle with giants who would rather give us personalized AI than have us meet them in a middle to which we will bring personal AI powers of our own. In the long run, we will meet in that middle, because personal AI will be better for everyone than personalized AI alone.</p>
<p>Watch us prove it. Better yet, join the effort.</p>
 
<p>*I am indebted to <a href="https://www.linkedin.com/in/lavonnereimer/">Lavonne Reimer</a> for introducing and co-thinking the horizontal vs. vertical frame, and look forward eagerly to her own writings and lecturings on the topic.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://150108457.v2.pressablecdn.com/wp-content/uploads/2024/05/horizontal-vs-vertical-with-red.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:25 &bull;
   25 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wrenchinthegears.com/2024/05/10/unwinding-smart-city-to-country-roads/'>Unwinding – Smart City to Country Roads</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I recognize going on and on about my personal situation may be rather tedious. I really don’t mean to make it all about “me,” because I feel like each of our journeys are part of this unfolding multi-dimensional tapestry. Since this is my blog, it seems like an appropriate place to tell the story from [...]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I recognize going on and on about my personal situation may be rather tedious. I really don’t mean to make it all about “me,” because I feel like each of our journeys are part of this unfolding multi-dimensional tapestry.</p>
<p>

</p>
<p>Since this is my blog, it seems like an appropriate place to tell the story from my particular point of view. For now, I’m giving myself leeway to share these loose, stream of consciousness “notes” – observations, pictures, mullings over – a work in progress. If this upsets or bores you, feel free to take a break from my blog and check back later in the summer or fall. Hopefully by then, I’ll have found my new footing.</p>
<p>

</p>
<p>On the way down to the Shenandoah yesterday, I was listening again to the biography of Elizebeth Friedman, a skilled codebreaker, who with her husband William laid the groundwork for cryptanalysis in the twentieth century. They were initially based at Riverbank Labs on the Fox River outside of Chicago, a stone’s throw from the future site of Fermi Lab.</p>
<p>

</p>

<p>

</p>
<a href="https://www.cabinetmagazine.org/issues/40/sherman.php"></a>
<a href="https://www.cabinetmagazine.org/issues/40/sherman.php">https://www.cabinetmagazine.org/issues/40/sherman.php</a>

<p>

</p>
<p>The secretive Colonel Fabyan founded one of the first Defense Department think tanks in the 1910s (pre-Alfred Loomis), cultivating ties with the University of Chicago. He used his fortune to underwrite private research into plant and fruit fly genetics , bone structure, x-rays, and acoustics, the latter in partnership with Wallace Sabine. You can still purchase precision tuning forks from Riverbank Labs.</p>
<p>

</p>
<a href="http://riverbanklabs.com/about/"></a>
<a href="http://riverbanklabs.com/about/">http://riverbanklabs.com/about/</a>

<p>

</p>

<p>

</p>
<p>What took up much of the Colonel’s attention was research into the Baconian cipher, supposedly encrypted messages about Elizabethan England hidden in Shakespeare’s first folio. It was work on this cipher that eventually led to Elizebeth and William’s future careers in codebreaking.</p>
<p>

</p>
<a href="https://www.goodreads.com/en/book/show/32025298"></a>
<a href="https://www.goodreads.com/en/book/show/32025298">https://www.goodreads.com/en/book/show/32025298</a>

<p>

</p>
<a href="https://www.cabinetmagazine.org/issues/40/sherman.php"></a>
<a href="https://www.cabinetmagazine.org/issues/40/sherman.php">https://www.cabinetmagazine.org/issues/40/sherman.php</a>

<p>

</p>
<a href="https://sirbacon.org/bacon-forum/index.php?/topic/107-friedman-cipher-study/"></a>
<a href="https://sirbacon.org/bacon-forum/index.php?/topic/107-friedman-cipher-study/">https://sirbacon.org/bacon-forum/index.php?/topic/107-friedman-cipher-study/</a>

<p>

</p>
<p>Consider William’s involvement in plant genetics and codebreaking later in this post in relation to the Matter CBD sticker and the push to blockchain cannabis genetics…</p>
<p>

</p>
<a href="https://www.cabinetmagazine.org/issues/40/sherman.php"></a>
<a href="https://www.cabinetmagazine.org/issues/40/sherman.php">https://www.cabinetmagazine.org/issues/40/sherman.php</a>

<p>

</p>
<a href="https://medicinalgenomics.com/blockchained-cannabis-dna/#:~:text=Cannabis%20breeders%20and%20cultivators%20can,file%20for%20their%20own%20patent."></a>
<a href="https://medicinalgenomics.com/blockchained-cannabis-dna/#:~:text=Cannabis%20breeders%20and%20cultivators%20can,file%20for%20their%20own%20patent">https://medicinalgenomics.com/blockchained-cannabis-dna/#:~:text=Cannabis%20breeders%20and%20cultivators%20can,file%20for%20their%20own%20patent</a>.

<p>

</p>
<p>William went on to found the NSA. As a team, he and Elizebeth solved complex puzzles – uniting as an extended complementary mind. That, I think, is what is intended with AR LARPy civic gaming. Maybe we can get out ahead of Niantic, Nvidia, the CIA, the Vatican…and untangle some interdimensional knots on our own?</p>
<p>

</p>
<p>I invite you to to consider my oversharing in that context. It’s an invitation into a collective puzzle-solving game. The one trick is we don’t have the box top image, and we don’t know what we are supposed to be making with all of these seemingly random loose parts.</p>
<p>

</p>
<p>Anyway, I’m going to make these notes for myself at the very least. I know looking back months or years later with more context, makes all the difference.</p>
<p>

</p>
<a href="https://en.m.wikipedia.org/wiki/Qualia"></a>
<a href="https://en.m.wikipedia.org/wiki/Qualia">https://en.m.wikipedia.org/wiki/Qualia</a>

<p>

</p>
<p>Qualia – it’s a topic Stephers has been digging into – personalization, biophysics, digital ledgers, umwelt, creating a big picture – the BIGGEST picture. I hope to talk more about that after I get settled.</p>
<p>

</p>
<a href="https://www.conducttr.com"></a>
<a href="https://www.conducttr.com">https://www.conducttr.com</a>

<p>

</p>
<p>Settlement was yesterday. I guess shouldn’t have been surprised that the office where we were to sign the documents to finalize the sale of the family home was in the Penn Mutual Life Insurance Building, founded in 1847. Think of the probability and stochastic modeling upon which the industry was built, as well as its ties to web3 through parametric deals that run on ubiquitous sensor networks.</p>
<p>

</p>

<p>

</p>

<p>

</p>
<p>The massive complex takes up an entire block and sits on the south side of Walnut (tree, choice theory) opposite Independence Hall (sovereign agents). It was in a Berkshire Hathaway Real Estate office, evidently they are under The Trident Group. As the notary quipped, “Warren Buffett owns us all.”</p>
<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>
<p>A close friend of my husband’s grandfather, Phil Carret, was a role model for Buffett. Carret, then in his 90s, came to our wedding. He helped create the world of mutual funds and was a solar eclipse chaser all his life. I continue to think automated financial markets, linked to collectivity and mutualism, are part of the superorganism emergence trajectory. Gates, Buffett, and their “Giving Pledge” is, in my opinion less about philanthropy, and more about using open source software, sensors, web3, and social impact data, to catalyze new forms of informational “life.”</p>
<p>

</p>

<p>

</p>

 <a href="https://youtu.be/KOqr17GNTJ8?feature=shared">https://youtu.be/KOqr17GNTJ8?feature=shared</a> 

<p>

</p>
<p>A section of the Penn Mutual complex incorporates the Egyptian Revival facade of another nineteenth-century insurance company. Remember insurance was the brain child of inventor, publisher, electricity explorer, diplomat Ben Franklin, a supreme consciousness engineer.</p>
<p>

</p>

<p>

</p>

<p>

</p>
<p>The facade of the Pennsylvania Fire Insurance Company features palm columns and lintels of winged suns, both linked to longevity and the soul.</p>
<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>
<p>Out in front is a random sign for “The Bagel Factory,” but there was no retail store anywhere on the block that I could see. It reminded me of the symbolic everything bagel in the Academy Award winning film “Everything Everywhere All At Once.”</p>
<p>

</p>
<a href="https://www.sportskeeda.com/comics/everything-everywhere-once-what-the-bagel-actually-represent#:~:text=The%20Everything%20Bagel%20becomes%20more,be%20felt%20throughout%20the%20universe."></a>
<a href="https://www.sportskeeda.com/comics/everything-everywhere-once-what-the-bagel-actually-represent#:~:text=The%20Everything%20Bagel%20becomes%20more,be%20felt%20throughout%20the%20universe">https://www.sportskeeda.com/comics/everything-everywhere-once-what-the-bagel-actually-represent#:~:text=The%20Everything%20Bagel%20becomes%20more,be%20felt%20throughout%20the%20universe</a>.

<p>

</p>

<p>

</p>

<p>

</p>
<p>Nope, we can’t control everything.</p>
<p>

</p>

<p>

</p>
<p>After it was all over, my not-quite ex husband, and I parted ways surrounded by the bronze reliefs that comprised a massive narrative portal. An auspicious place, I suppose, to step forward into what comes next.</p>
<p>

</p>

<p>

</p>

<p>

</p>
<p>I couldn’t not cross the street to take one last look at the iconic symbol of freeDOM with its clock and its bell giving form to how we perceive “reality.” After taking the photo below, I turned to walk back to the car when I spied a piece of Bitcoin sticker art stuck to a pseudo-colonial lamp post, above it (upside down) was another sticker for Matter CBD, an organic cannabis outfit based southern Oregon where the growers, Cleve Backster-style, sing and talk to their plants.</p>
<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>
<a href="https://legacy.iftf.org/fileadmin/user_upload/downloads/blockchain/IFTF_BlockchainFutures_Map.pdf"></a>
<a href="https://legacy.iftf.org/fileadmin/user_upload/downloads/blockchain/IFTF_BlockchainFutures_Map.pdf">https://legacy.iftf.org/fileadmin/user_upload/downloads/blockchain/IFTF_BlockchainFutures_Map.pdf</a>

<p>

</p>
<a href="https://legacy.iftf.org/fileadmin/user_upload/downloads/blockchain/IFTF_BlockchainFutures_Map.pdf"></a>
<a href="https://legacy.iftf.org/fileadmin/user_upload/downloads/blockchain/IFTF_BlockchainFutures_Map.pdf">https://legacy.iftf.org/fileadmin/user_upload/downloads/blockchain/IFTF_BlockchainFutures_Map.pdf</a>

<p>

</p>
<p>Upside down matter in an era of prescribed plant medicine for altered consciousness where tokens masquerade as money, but are perhaps actually cells of some as yet indescribable life form jumping off bibliometry (Eugene Garfield) and maybe geoengineering? How to account for this sticker – a slightly blurry Bitcoin stencil on a scan of an old library check-out card (also upside down with multiple dates from the height of altered state / human potential 60s and 70s) with a jet plane flying below it all? Folks who know more about numerology, do the featured dates have any significance to you?</p>
<p>

</p>

<p>

</p>
<p>I kept walking, on past the American Philosophical Society, an off-shoot of the Royal Society laying out what is proper/acceptable science, while keeping esoteric objects like Kelpius’s Horologium of Achaz away from prying eyes. There you have good old Ben in the alcove, Philadelphia’s business, civic, and intellectual patriarch.</p>
<p>

</p>

<p>

</p>
<a href="https://www.jstor.org/stable/pdf/983156.pdf?refreqid=fastly-default%3Acb0b6221286adcbe5ea9d638716a01e8&amp;ab_segments=&amp;origin=&amp;initiator=&amp;acceptTC=1"></a>
<a href="https://www.jstor.org/stable/pdf/983156.pdf?refreqid=fastly-default%3Acb0b6221286adcbe5ea9d638716a01e8&amp;ab_segments=&amp;origin=&amp;initiator=&amp;acceptTC=1">https://www.jstor.org/stable/pdf/983156.pdf?refreqid=fastly-default%3Acb0b6221286adcbe5ea9d638716a01e8&amp;ab_segments=&amp;origin=&amp;initiator=&amp;acceptTC=1</a>

<p>

</p>
<p>And in the next block, flanking Independence Hall to the east is the Bourse, once a commodities exchange, now an upscale tourist food court. The xoxo sculpture out front reminds me of Galia’s digital heart tokens – part of “the city that loves you back” PR campaign I suppose. Affection as tradable commodity. Boy, I got shorted this round for sure. As if to further sour my entanglement with the city, I came back to a parking ticket on my windshield. I’ve maybe gotten one other in my thirty plus years here, and I still had a half hour paid up. Stupid “smart” parking tech. I guess their sham will be the final dues I pay for my “freeDOM.”</p>
<p>

</p>

<p>

</p>
<p>After packing the last of my things into the car, and giving my honey locust street tree one more big hug, I headed to I-95 south. There’s part of me that hopes the children who will grow up in its lacy shade will decorate the sidewalk with chalk art as my child once did. For a time, 24th Street was a wonderful place to be.</p>
<p>

</p>

<p>

</p>
<p>I ended up pulling off at the University City exit to say good-bye to Bartram’s Garden where I worked for seventeen years until the lockdowns started stealing things from my life. I wanted to get a jar of local honey to sweeten the parting and see my favorite trees. The tide was in, so no mudlarking this time around. I gathered a bit of compost from inside the ancient yellow wood and rubbed it into my hands, I found a walnut with a hole that looks like a twinned portal and put it in my backpack as a keepsake. When I found my favorite, a majestic London Plane, there was the trickster energy, again. It was hosting a piece of outdoor art featuring a fox with a quizzical expression. Well played Philadelphia, well played.</p>
<p>

</p>

<p>

</p>

<p>

</p>
<p>For the past two nights I have been camping at Bear Den off the Appalachian Trail outside Blumont, VA. The site was the former summer estate of a wealthy DC OB-GYN and his opera singer wife. It sits on veins of milky white quartz. A few miles south is Weather Mountain, a Cold War bunker for the federal government. According to a local history write up in the lodge, Mahlon Loomis, a rather visionary nineteenth century Boston dentist conducted experiments transmitting wireless communications between two hills nearby, decades before Marconi, using gilded kites!</p>
<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>
<p>It’s been cold for May, and rainy. But my tent stayed dry and between the showers I explored the grounds and went out to the rocky overlook and gathered up plants to make a heart. I spied a fire pit with lots of charcoal. So, I decided to make one with that in honor of my friend Sean and his family who use biochar to feed the garden beds on their small farm in Washington State.</p>
<p>

</p>

<p>

</p>
<p>The lodge’s volunteer caretaker found it and was excitedly telling me about it. She said she took photos to send to her boyfriend, and was pointing it out to the incoming hikers, including a couple who said they’d recently relocated to Harpers Ferry from Washington State, which closes the circle. This heart has rose petals, wood sorrel, bramble blossoms, sassafras, puffballs, hemlock cones, fern, and quartz.</p>
<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>
<p>I need to pack up my tent and head a bit farther down the road to Bayse. I hear conversations of families across the clearing hauling dripping blankets and debating the merits of camping after a wet, cold night. I remember those days. I’m not sure we ever had a camping trip with good weather. Now I am a solo traveler with my stuffed bear and my house plants. I will see what it feels like to be on my own reconnaissance, finding my own grounding and my own views.</p>
<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

<p>

</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/img_5599.jpg?resize=640%2C888&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:50 &bull;
   25 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 08. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://herestomwiththeweather.com'>Heres Tom with the Weather</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://herestomwiththeweather.com/2024/05/08/60+-journalism-profs-demand-investigation-into-controversial-nyt-article/'>60+ Journalism Profs Demand Investigation into Controversial NYT Article</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  

 As the “paper of record” this NYT article was extremely influential in shaping public opinion. 

 60+ Journalism Profs Demand Investigation into Controversial NYT Article Alleging Mass Rape on Oct. 7 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  

<p>As the “paper of record” this NYT article was extremely influential in shaping public opinion.</p>

<p><a href="https://www.democracynow.org/2024/5/8/nyt_investigation">60+ Journalism Profs Demand Investigation into Controversial NYT Article Alleging Mass Rape on Oct. 7</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   23:46 &bull;
   26 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wrenchinthegears.com/2024/05/08/creative-placemaking-as-cryptographic-cultural-computation/'>Creative Placemaking As Cryptographic Cultural Computation</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I’ll be leaving Philadelphia tomorrow after the house closing paperwork is signed. The cleaners are getting it ready for its new family. A chapter closes, a new one opens. I’ll slowly make my way down to Little Rock, camping along the way. After three back-to-back, three-day, 1,300-mile u-haul drives in as many weeks, I’m sick [...]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>





<p></p>



<p>I’ll be leaving Philadelphia tomorrow after the house closing paperwork is signed. The cleaners are getting it ready for its new family. A chapter closes, a new one opens. I’ll slowly make my way down to Little Rock, camping along the way. After three back-to-back, three-day, 1,300-mile u-haul drives in as many weeks, I’m sick of bland hotels with oddly coded decor and ready to sleep under the stars without worrying if someone broke into the truck while I was sleeping.</p>







<p>I built in a few extra days to poke around Oak Ridge outside Knoxville and see the colossal Athena in Nashville’s 1931 Parthenon replica with the snakey Erichthonius (born of Hephasteus’s semen falling to the soul). Interesting allusions to biogeochemistry and the origins of life in a city growing by leaps and bound on economies of biotech and blockchain health systems. </p>



<p>Yesterday I hit REI and got some new Tevas a year after my 20-year old pair gave up the ghost. I need shower shoes for the campgrounds, and Arkansas lakes are clear with sharp rocky bottoms. I also grabbed a tiny USB-rechargeable lantern. It’s been raining a lot. If I get stuck in the tent, I have “Godel, Escher, Bach” to keep me busy – all strange loops, and recursion, self-referencing refrains that call to mind for me reincarnation or many worlds depending on your ideas about spacetime. Using digital light to process print information seems par for the course.</p>







<p>I’d hoped to be able to tour the Barnes Foundation today, but it was closed to the general public. In reading how Godel assigned symbols to numbers to generate a coded language that generated insights into the limitations of mathematical proofs, I started to think more about Steganography, secretly encoding objects with alternate meanings. It is a form of information exchange that can exist out in the open, but only able to be translated by those who know the coding. Stephers brought it to my attention a few years back. </p>



<p>The Barnes Foundation has an unusual, and highly politicized history. Albert Barnes, a Philadelphian, got his MD at the University of Pennsylvania, then trained as a corporate chemist in Heidelberg, Germany, and later made a fortune selling, <a href="https://hekint.org/2020/08/18/albert-c-barnes-md-the-physician-who-spun-silver-into-gold/">Argyrol</a>, silver nitrate drops used in infants eyes after delivery. </p>



<p>He used his fortune to acquire a vast, eclectic collection of Impressionist, Post-Impressionist, and Modern art. These pieces are displayed in methodical arrangements, or ensembles, that incorporate pieces of decorative arts and furniture as well as ritual objects from a range of world cultures. Barnes created sets, and the interplay of the objects in each set generates additional meaning, context, through juxtaposition. </p>



<p>Barnes focused on the application of scientific principles to art, and established education programs around the collection. His wife Laura managed an arboretum on the property, also focused on continuing adult education. They considered the estate in Lower Merion to be a laboratory. </p>



<p>Albert, who had grown up poor, was often at odds with Philadelphia’s social elite. When he died, management of the collection and its programs was entrusted to a board run by Lincoln University, a historically Black college in Chester County.  For decades there was rancor with the Barnes neighbors in Merion, and the elite wanted to break the trust, push out the Lincoln trustees and bring Barnes’s art lab, in a rather more neutered format to the Ben Franklin Parkway, which is where I’m now sitting killing time under the fancy blooming buckeyes until my car is ready. </p>



The “new” Barnes on the Ben Franklin Parkway







<p>Now I am factoring in artifacts and Godel’s numbering system that to me has digital ID overtones, as well as agents, civic gaming, tokens, signals, and computation within layers of meaning where some (maybe many) interconnected information fields are obscured or subject to the Pitt/Ober Athenian “knowledge management” protocols. </p>



<p>What is AR crypto “play to earn,” really? If I listen to my gut, I would say it is about massive parallel post-Moore’s law information processing through networked biophysics and social physics and dynamic game mechanics linked to the sensors and actuators of spatial computing.</p>







<p>Are we meant to be Godel Numbers, dancing, harmonizing, particles in a cosmic conversation only our subconscious can access?  </p>



<p>Maybe notes in a musical performance? Orchestral music? Jazz? Sacred? Profane? Remember, Tik Tok started out as ByteDance, and a Conductrr crisis simulation modelling system that promotes the use of xAPI to build fluid reality has been running in the background gor a decade or more. I was intrigued to read that Ada Lovelace imagined Babbage’s computing machine might be used to realize complex scientific forms of music. Was it music of the spheres, cymatics, manifestation? </p>















<p>If Galia of Bancor seeks to harness the power of mothers’ hearts through tokens, what does it mean that Joseph Lubin, founder of Consensys and a lead builder of smart contract worlds got his start in Princeton in robotics and automated music?   </p>











<p>Jamie Wheal’s book “Stealing Fire” has a whole chapter on the tech behind the electronic dance music scene as a catalyst for ekstasis.  Is the point to use token engineering combined with nano and frequency to network human embodied intelligence into a global composition so that we collectively unlock access to subconscious fields of information and serve as some kind of vast liquid crystal manifesting system that uses encoded cultural artifacts as a kind of creative language? These are all crazy possibilities I am mulling over. </p>



<p>I just can’t help but wonder about Barnes and Heidelberg and baby eyes and arranged artifacts and encoded (perhaps interdimensional?) communication and botany and arborization and many worlds and the Montecarlo method and what jokes this gritty, esoteric city is playing on me…</p>







<p>Two imposing sculptures flank the median of the Parkway, The Soldiers and Sailors Civil War Monument, erected by the city during World War I. I can’t help but think of the intergenerational polarity generated by the Civil War, still reverberating through our shared consciousness as the culture wars, now with memes, hashtags, social graphs, and digital stigmergy. When I read the statements about freeing slaves and freedom under a unified Constitution, one destiny, I picture a future with Nicole Shanahan’s equitable smart contract laws, logic melded culture that has been codified, standardized, and made accessible to a global outside-in robot that has no soul. </p>



“In Giving Freedom To The Slave, We Assure Freedom To The Free”



<p>I think of sovereign agents in a never-ending series of loosely programmed scenarios, degrees of  freedom set in advance, where AI life coaches log choices made on permanent ledgers, and assess our relative worth in the current fitness landscape. Not slaves, but “free” agents responding to a digital constitution reimagined as a dynamic, responsive gameboard, cells in the collective Athenian smart-city superorganism. </p>



“One Country, One Constitution, One Destiny”



<p>As I mull all of this over – my city speaking to me in steganography – you are an agent who is failing to advance the noetic biohybrid computer. Therefore you are a cancer to be shunned, excluded, expelled. That’s just how the program works. I see it, and my bags are packed. Rafael says the fluids are all topped off and my old dumb Subaru is ready to head South into unknown territory.   </p>



<p>I made some site visits over the last few days, left a few hearts behind – shocker, right? The first was under an ancient oak behind Jeffrey Yass’s Susquehanna International Group headquarters in Bala Cynwyd on the Main Line. I think the fact that it’s labelled “revolutionary” is significant.</p>















<p>I used pollen and fallen oak leaves with pine cones and dandelions – so cheerful and humble. In the center I placed a rose nicked from the parking lot of my Jungian analyst whom I’m sad to leave. Even though he didn’t follow a lot of what I said, he valued my authenticity and my journey, as odd as it is turning out to be. I told him the heart would be a tribute to entelechy, as represented by the acorn – the tiny spark of powerful potential that is our soul’s quest. I had to look really hard to find three tiny acorns, but I did and placed them on some mugwort (good for dreams). </p>







<p>I hugged the massive girth of the oak, a symbol of arborization, anima mundi, branching choices, montecarlo, and asked for guidance in what comes next – to be of service with the time I have left.  </p>











<p>The sculpture below is right out in front of the building. It is white metal and consists of three overlapping circles topped by linear waves. To me this connected to Hofstadter’s discussion of recursion, cannon, and fugues. Remember SIG was the esrly lead investor in Tik Tok, pendulum, time, entrainment. </p>















<p>Yesterday I visited the Kelpius cave, and to my surprise it seems like some Pythagoreans had gotten there first. When I arrived there were sizeable sticks, along with rocks and rubble, arranged in a large triangle. </p>







<p>Inside was a base and from it extended several smaller sticks that created a simple “tree” with three branches. There was a symbol with paired triangles, like the Star of David etched in the damp earth near the doorway. Scattered around seemed to be broken remnants of a popsicle stick project, which to me symbolized configurations of crystal lattice. </p>







<p>Upon that I placed my heart of purple Pawlonia blossoms, dandelions, fern, may apple, pine cones, and tulip poplar petals. Scattered around the edge were Star of Bethlehem flowers. </p>



























<p>When I finished I chatted with a white-haired gentleman, a birder named Paul (which factors into my cathedral visit) who’d been watching migrating warblers around the cave. He told me he had a friend who he used to walk with who had been sort of an informal caretaker of the cave. I pointed out the fading Nephele graffiti and showed him the heart and branched sticks, and then went into avianmagnetoreception as a study case for quantum biology and how that tied into US Navy funded anthropology  research into Micronesian open water navigation. He told me a bit about local garnets, and I shared some mica flakes I found. It was a gentle way to close my time in the Wissahickon. </p>















<p>So then on my drive to take my car to the mechanic, I stopped at a light on Broad Street at MARCONI Plaza and in the median was a woman in a sun hat digging dandelions! </p>











<p>Then heading home I walked through Love Park, noticing it was now sponsored by Bank of America. There were some guys with big cameras set up to take pictures of people taking pictures posing in front of the sculpture. I wondered if they were working on a documentary, so I tried to chat them up about gap junctions, group mind, the energy of the heart torus field, and Lev “Heart” being the name of Galia Benarzi’s first digital community currency – structured around an Israeli mothers’ babysitting co-op. They were not very interested. </p>











<p>The next person to not be interested was the gal staffing the open budget pop-up cargo container civic education junket sponsored in part by Mural Arts. I tried to explain web3 token voting linked to AI twins and social impact betting. Then a guy showed up with flyers about advice on home ownership and career services, and I had to point out that when Philadelphia’s City Council had anti-poverty hearings five years ago, the region’s job growth report said the average wage was going to be $15 an hour. You can’t buy a house in Philadelphia with that income. I pointed to the office buildings around us and said – those will be redone as affordable rental housing, only it will be like a dorm and your bedroom will have no windows, and you will pay your rent with UBI and data tied to sustainability and wellness behaviors. If you want sunlight the common area by the windows will be full of AI vision cameras and sensors like at WeWork (now Flow) and if you play the game well you can earn a small bit of equity in the ant computer. This will be set up by strange bedfellows partnerships between private equity (Blackstone/Ancestry.com DNA), community banks, high net individuals, and religious groups. </p>



<p>They barely registered what I was saying. In the end I told her to look up Cesar Hidslgo’s TED talk on radical participatory voting. They are training voters for the old game, just as they are about to be taken for a ride by the new web3 tokenomics game.   </p>











<p>And why is Mural Arts behind this? Think cultural artifacts, symbols, civic gaming. This is a useful p<a href="https://www.sciencedirect.com/science/article/abs/pii/S0304422X18303747">aper </a>on social impact finance linked to public art. The city remade as a digital museum for collective manipulation and “creative placemaking.”</p>



<a href="https://www.sciencedirect.com/science/article/abs/pii/S0304422X18303747">https://www.sciencedirect.com/science/article/abs/pii/S0304422X18303747</a>



<p>My final stop was at the Catholic Cathedral of Saints Peter and Paul overlooking Logan Circle. I hadn’t actually been inside before, but yesterday was the day. I entered the foyer with plaques commemorating masses given by Pope Francis and Pope John Paul II, then I went inside. What struck me was all the different ways sacred light was communicated, the many references to paired keys (encryption), and the book and the sword (Paul) that evoked for me Philip Pullman’s subtle knife that can cut through dimensions. or perhaps information fields as represented by the good book. </p>































<p>There was a huge painting of the Magi under a beaming star of Bethlehem. I’d used Star of Bethlehem blossoms in my Kelpius heart. </p>















<p>The final synchronicity was the strange appearance of a new white ball cap with a Northeastern University Huskies logo on it. I’d been going back and forth the day before, texting with a friend about the oversize role it seemed the Boston university was playing in the AI, biotech, ed-tech transformation of Maine under the guidance of former Jackson Labs spokesperson David Roux.  And now here was a hat for that very school, the first thing I saw upon entering the sanctuary.</p>











<p>So my last day in the city of brotherly love is winding down. I read this passage on the subway today – how to step outside the system. </p>







<p>I don’t think it’s truly possible to exit, but for now I’m unplugging from Philly, a city that taught me much and broke my heart. With time I hope it will heal, and I can become a wise old crone. </p>








   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/img_5439-1.jpg?resize=768%2C1024&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:26 &bull;
   27 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.patrick-breyer.de/gutachten-zum-computerspiele-sterben-piraten-wollen-klare-eu-regeln-gegen-willkuerliches-lahmlegen-von-spielen/'>Gutachten zum Computerspiele-Sterben: Piraten wollen klare EU-Regeln gegen willkürliches Lahmlegen von Spielen</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Vor dem Hintergrund der Spielerinitiative „Stop killing games“, die sich gegen das Lahmlegen von Computerspielen durch ihre Hersteller wendet, hat der Wissenschaftliche Dienst des Europäischen Parlaments … 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Vor dem Hintergrund der Spielerinitiative <a href="https://www.stopkillinggames.com/countries/germany">„Stop killing games“, die sich gegen das Lahmlegen von Computerspielen durch ihre Hersteller wendet,</a> hat der <a href="https://www.patrick-breyer.de/wp-content/uploads/2024/05/EPRS_116648_124540-Final_answer_on_software_updates_-1.pdf">Wissenschaftliche Dienst des Europäischen Parlaments im Auftrag der Europaabgeordneten der Piratenpartei Dr. Patrick Breyer die einschlägigen EU-Gesetze untersucht und sieben potenziell relevante Regelungen aufgeführt.</a> Breyer sieht Lücken im Rechtsrahmen:</p>



<p>„Es fehlen klare EU-Verbraucherschutzregelungen gegen die branchenübliche Praxis, dass Spielehersteller beliebte Spiele gewinnbringend verkaufen, sich dabei aber die willkürliche, jederzeitige Unbrauchbarmachung vorbehalten und nicht einmal eine Rückerstattung des Kaufpreises anbieten. Es gibt zwar eine allgemeine EU-Richtlinie gegen ‚missbräuchliche Klauseln in Verbraucherverträgen‘, aber deren Auslegung muss langwierig von Gerichten entschieden werden. Es gibt außerdem eine EU-Richtlinie, derzufolge digitale Inhalte während der Vertragsdauer bereit gestellt werden müssen, aber gegen die kurzfristige Kündigung und Einstellung eines gerade erst gekauften Spiels schützt sie nicht. Die EU-Regeln werden der kulturellen Bedeutung von Games nicht gerecht, die gewachsene Gemeinschaften von Millionen von Spielern zusammen bringen können. Wir Piraten fordern, dass die EU Games als Kulturgut schützt, ein Lahmlegen nach Gutdünken verbietet und, wenn der Hersteller Games aufgibt, ein Weiterführen durch die Community ermöglicht.“</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   10:31 &bull;
   27 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 07. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/05/mini-summit-five/'>Mini Summit Five</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Links, notes, and commentary from Yurii Rashkovskii’s presentation,
“Universally Buildable Extensions: Dev to Prod,” at the fifth Postgres
Extension Ecosystem Mini-Summit on May 1, 2024.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>The video for Yurii Rashkovskii’s presentation at the fifth <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">Postgres
Extension Ecosystem Mini-Summit</a> last week is up. Links:</p>
 
 <a href="https://youtu.be/R5ijx8IJyaM">Video</a> 
 <a href="https://justatheory.com/shared/extension-ecosystem-summit/omni-universally-buildable-extensions.pdf">PDF Slides</a> 
 
<p>Here’s my interpolation of YouTube’s auto-generated transcript, interspersed
with chat activity.</p>
 Introduction 
 
 I opened the meeting and introduced <a href="https://omnigres.com">Omnigres</a>’s <a href="https://yrashk.com">Yurii Rashkovskii</a>. 
 
 Presentation 
 
 
<p>Yurii: Today I’m going to be talking about universally buildable
extensions. This is going to be a shorter presentation, but the point of
it is to create some ideas, perhaps some takeaways, and actually provoke a
conversation during the call. It would be really amazing to explore what
others think, so without further ado…</p>
 
 
<p>I’m with Omnigres, where we’re building a lot of extensions. Often  they
push the envelope of what extensions are supposed to do. For example, one
of our first extensions is an HTTP server that embeds a web server inside
of Postgres. We had to do a lot of unconventional things. We have other
extensions uniquely positioned to work both on developer machines and
production machines — because we serve the the developers and devops
market.</p>
 
 
<p>The point of Omnigres is turning Postgres into an application runtime —
or an application server — so we really care how extensions get adopted.
When we think about application developers, they need to be able to use
extensions while they’re developing, not just in production or on some
remote server. They need extensions to work on their machine.</p>
 
 
<p>The thing is, not everybody is using Linux Other people use macOS and
Windows and we have to account for that. There are many interesting
problems associated with things like dependencies.</p>
 
 
<p>So there’s a very common approach used by those who who try to orchestrate
such setups and by some package managers: operating out of container. The
idea is that with a can create a stable environment where you bring all
the dependencies that your extension would need, and you don’t have to
deal with the physical reality of the host machine. Whether it’s a
developer machine, CI machine, production machine, you always have the
same environment. That’s definitely a very nice property.</p>
 
 
<p>However, there are some interesting concerns that we have to be aware when
we operate out of a container. One is specifically mapping resources. When
you have a container you have to map how many cores are going there,
memory, how do we map our volumes (especially on Docker Desktop), how we
connect networking, how we pass environment variables.</p>
 
 
<p>That means whenever you’re running your application — especially
locally, especially in development — you’re always interacting with that
environment and you have to set it up. This is particularly problematic
with Docker Desktop on macOS and Windows because these are not the same
machines. You’re operating out of a virtual machine machine instead of
your host machine, and obviously containers are Linux-specific, so it’s
always Linux.</p>
 
 
<p>What we found is that often times it really makes a lot of sense to test
extensions, especially those written in C, on multiple platforms. Because
in certain cases bugs, especially critical memory-related bugs, don’t show
up on one platform but show up on another. That’s a good way to catch
pretty severe bugs.</p>
 
 
<p>There are also other interesting, more rare concerns. For example, you
cannot access the host GPU through Docker Desktop on macOS or through
Colima. If you’re building something that could have use the host GPU that
would work on that machine it’s just not accessible. If you’re working
something ML-related, that can be an impediment</p>
 
 
<p>This also makes me wonder: what are other reasons why we’re using
containers. One reason that struck out very prominently was that Postgres
always has paths embedded during compile time. That makes it very
difficult to ship extensions universally across different installations,
different distributions. I wonder if that is one of the bigger reasons why
we want to ship Postgres as a Docker container: so that we always have the
same path regardless of where where it’s running.</p>
 
 
<p>Any questions so far about Docker containers? Also if there’s anybody who
is operating a Docker container setup — especially in their development
environment — if you have any thoughts, anything to share: what are the
primary reasons for you to use a Docker container in your development
environment?</p>
 
 
<p>Jeremy S in chat: When you say it’s important to test on multiple
platforms, do you mean in containers on multiple platforms, or
directly on them?</p>
 
 
<p>Jeremy S in chat: That is - I’m curious if you’ve found issues, for
example, with a container on Mac/windows that you wouldn’t have found
with just container on linux</p>
 
 
 
 
<p>Daniele: Probably similarity with the production deployment environments.
That’s one. Being free from whatever is installed on your laptop, because
maybe I don’t feel like upgrading the system Python version and
potentially breaking the entire Ubuntu, whereas in a Docker container you
can have whatever version of Python, whatever version of NodeJS or
whatever other invasive type of service. I guess these are these are good
reasons. These were the motivation that brought me to start developing
directly in Docker instead of using the desktop.</p>
 
 
<p>Yurii: Especially when you go all the way to to production, do you find
container isolation useful to you?</p>
 
 
<p>Daniele: Yeah I would say so; I think the problem is more to break
isolation when you’re are developing. So just use your editor on your
desktop, reload the code, and have a direct feedback in the container. So
I guess you have to break one barrier or two to get there. At least from
the privilege points of having a Linux on desktop there is a smoother
path, because it’s not so radically different being in the container.
Maybe for Windows and macOS developers it would be a different experience</p>
 
 
<p>Yurii: Yeah, I actually wanted to drill down a little bit on this In my
experience, I build a lot on macOS where you have to break through the
isolation layers with the container itself and obviously the VM. I’ve
found there are often subtle problems that make the experience way less
straightforward.</p>
 
 
<p>One example I found it that, in certain cases, you’re trying to map a
certain port into the container and you already have something running [on
that port] on your host machine. Depending on how you map the port —
whether you specify or don’t specify the address to bind on — you might
not get Docker to complain that this port is actually overridden.</p>
 
 
<p>So it can be very frustrating to find the port, I’m trying to connect to
it but it’s not connecting to to the right port. There’s just very small
intricate details like this, and sometimes I’ve experienced problems like
files not perfectly synchronizing into the VM — although that has gotten
a little better in the past 2–3 years — but there there were definitely
some issues. That’s particularly important for the workflows that we’re
doing at Omnigres, where you’re running this entire system — not just
the database but your back end. To be able to connect to what’s running
inside of the container is paramount to the experience.</p>
 
 
<p>Daniele: Can I ask a question about the setup you describe? When you go
towards production, are those containers designed to be orchestrated by
Kubernetes? Or is there a different environments where you have your
Docker containers in a local network, I assume, so different Dockers
microservices talking to each other. Are you agnostic from what you run in
it, or do you run it on Kubernetes or on Docker Compose or some other form
of glue that you you set up yourself, or your company has set up?</p>
 
 Steven Miller in chat: … container on Mac/windows [versus linux] 
 Steven Miller in chat: Have seen with chip specific optimizations
like avx512 
 
 
 
<p>Yurii: Some of our users are using Docker Compose to run everything
together. However, I personally don’t use Docker containers. This is part
of the reason why the topic of this presentation is about universally
buildable extensions. I try to make sure that all the extensions are
easily compilable and easily distributable on any given supported
platform. But users do use Docker Compose, it’s quite common.</p>
 
 
<p>Does anyone else here have a preference for how to move Docker containers
into production or a CI environment?</p>
 
 
<p>Nobody?  I’ll move on then.</p>
 
 
<p>Steven Miller in chat: Since in docker will run under emulation, but
on linux will run with real hardware, so the environment has different
instruction set support even though the docker —platform config is the
same</p>
 
 
<p>Jeremy S in chat: That makes sense</p>
 
 
 
 
<p>Yurii: I wanted to show just a little bit of a proof of concept tool that
we’ve been working on, on and off for the last year—</p>
 
 
<p>David Wheeler (he/him): Yurii, there are a couple comments and questions
in chat, I don’t know if saw that</p>
 
 
<p>Yurii: I didn’t see that sorry.</p>
 
 
<p>Jeremy is saying, “when you say it’s important to test on
multiple platforms do you mean in containers on multiple platforms or
directly on them?” In that particular instance I meant on multiple
platforms, directly.</p>
 
 
<p>The other message from Jeremy was, “I’m curious if you found issues for
example with a container on Mac or Windows that you wouldn’t have found
with just container on Linux?” Yeah I did see some issues depending on the
type of memory-related bug. Depending on the system allocator, I was
either hitting a problem or not. I was not hitting it on Linux, I believe
and it was hidden macOS. I don’t remember the details right now,
unfortunately, but that difference was indicative of a bug.</p>
 
 
<p>Steven wrote, trying to connect this… “Have * seen chip-specific
optimizations for containers?” And, “Docker will run under emulation but
on Linux will run with real Hardware.” Yeah that’s an interesting one
about ax512. I suppose this relates to the commentary about  about GPU
support, but this is obviously the other part of supporting specific
hardware, chip-specific optimizations That’s an interesting thing to
learn; I was not aware of that! Thank you Steven.</p>
 
 
<p>Let’s move on. postgres.pm is a pro of concept that I was working on for
some time. The idea behind it was both ambitious but also kind of simple:
Can we try describing Postgres extensions in such a way that they will be
almost magically built on any supported platform?</p>
 
 
<p>The idea was to build an expert system of how to build things from a
higher level definition. Here’s an example for pgvector:</p>
  <code>:- package(vector(Version), imports([git_tagged_revision_package(Version)])).
git_repo("https://github.com/pgvector/pgvector").
:- end_package.
</code>  <p>It’s really tiny! There are only two important things there: the Git
tagged revision package and Git repo. There’s nothing else to describe the
package.</p>
 
 
<p>The way this works is by inferring as much information as possible from
what’s available. Because it’s specified as a Git-tagged revision package,
it knows that it can download the list of version-shaped revisions — the
versions — and it can checkout the code and do further inferences. It
infers metadata from <code>META.json</code> if it’s available, so it will know the
name of the package, the description, authors, license, and everything
else included there.</p>
 
 David G. Johnston in chat: PG itself has install-check to verify
that an installed instance is functioning. What are the
conventions/methods that extension authors are using so that a
deployed container can be tested at a low level of operation for the
installed extensions prior to releasing the image to production? 
 
 
 
<p>It automatically infers the build system. For example for C extensions, if
it sees that there’s a <code>Makefile</code> and C files, it infers that you need
<code>make</code> and a C compiler and it tries to find those on the system: it will
try to find <code>cc</code>, <code>gcc</code>, Clang —  basically all kinds of things.</p>
<p>*David Wheeler (he/him)() in chat: Feel free to raise hands with questions</p>
 
 
<p>Here’s a slightly more involved example for <code>pg_curl</code>. Ah, there was a
question from David Johnson. David says, “PG has install-check to verify
that installed instance is functioning. What are the conventions methods
that extension authors are using so the deployed container can be tested
at a low level of operation for the installed extension prior to releasing
the image to production?”</p>
 
 
<p>I guess the question is about general conventions for how extension
authors ensure that the extensions work, but I suppose maybe part of this
question is whether that’s also testable in a production environment.
David, are you talking about the development environment alone or both?</p>
 
 
<p>David G. Johnston: Basically, the pre-release to production. You go in
there in development and you cut up an extension and source and then you
build your image where you compile it — you compile PG, you compile it,
or you deploy packages. But now you have an image, but you’ve never
actually tested that image. I can run <code>installcheck</code> on an installed
instance of Postgres and know that it’s functioning, but it won’t test my
extension. So if I install PostGIS, how do I test that it has been
properly installed into my database prior to releasing that image into
production?</p>
 
 Tobias Bussmann in chat: shouldn’t have the extension a make
installcheck as well? 
 
 
 
<p>Yurii: To my knowledge there’s no absolutely universal method. Of course
the PGXS methods are the most standard ones — like <code>installcheck</code> — to
to run the tests. In our [Omnigres’s] case, we replaced <code>pg_regress</code> with
<code>pg_yregress</code>, another tool that we’ve developed. It allows for more
structural tests and tests that test certain things that <code>pg_regress</code>
cannot test because of the way it operates.</p>
 
 David Wheeler (he/him) in chat:
<a href="https://docs.omnigres.org/pg_yregress/intro/">https://docs.omnigres.org/pg_yregress/intro/</a> 
 
 
 
<p>I can share more about this later if that’s of interest to anybody. So we
basically always run <code>pg_yregress</code> on our extensions; it creates a new
instance of Postgres — unless told to use a pre-existing instance —
and it runs all the tests there as a client. It basically deploys the the
extension and runs the set of tests on it.</p>
 
 
<p>David G. Johnston: Okay.</p>
<p>Yurii: I guess you know it depends on how you ship it. For example, if you
look at the pgrx camp, they have their own tooling for that, as well. I’ve
also seen open-source extensions where they could be written in, say,
Rust, but still using <code>pg_regress</code> tests to test their behavior. That
would often depend on how their build system is integrated in those tests.
I guess the really short answer is there’s probably no absolutely
Universal method.</p>
 
 
<p>David thank you for pasting the link to <code>pg_yregress</code>. If there are ny
questions about it, feel free to ask me. Any other thoughts or questions
before I finish this slide? Alright will carry on then.</p>
  <code>:- package(pg_curl(Version), imports(git_explicit_revision_package(Version))).
:- inherit(requires/1).
git_repo("https://github.com/RekGRpth/pg_curl").
git_revisions([
        '502217c': '2.1.1',
        % ... older versions omitted for now ...
    ]).
requires(when(D := external_dependency(libcurl), version::match(D, '^7'))).
:- end_package.
</code>   
 
<p>The difference between this example and the previous one is that here it
specifies that there will be an explicit revision map because that project
does not happen to have version tags, so they have to be done manually.
You can see that in the Git revision specification. But what’s more
interesting about this is that it specifies what kind of dependency it
needs. In this particular instance it’s <code>libcurl</code>, and the version has to
match version 7 — any version 7.</p>
 
 
<p>These kinds of requirements, as well as compiler dependencies, <code>make</code>
dependencies, and others are always solved by pluggable satisfiers. They
look at what’s available depending on the platform — Linux, a particular
flavor of Linux, macOS, etc — and picks the right tools to see what’s
available. In the future there’s a plan to add features like building
these dependencies automatically, but right now it depends on the host
system, but in a multi-platform way.</p>
 
 David Wheeler (he/him) in chat: How does it detect that libcurl is
required? 
 
 
 
<p>The general idea behind this proof of concept is that we want to specify
high level requirements and not how exactly to satisfy them. If you
compare this to a Docker file, the Docker file generally tells you exactly
what to do step by step: let’s install this package and that
package, let copy files, etc. so it becomes a very specific set of
instructions.</p>
 
 Jeremy S in chat: And how does it handle something with different
names in different places? 
 
 
 
<p>There was a question: “how does it detect that <code>libcurl</code> is required?”
There there is this line at the bottom says “requires external dependency
<code>libcurl</code>, so that was the definition.”</p>
 
 
<p>The other question was “how does it handle something with different names
in different places?” I’m not sure I understand this question.</p>
 
 
<p>Jeremy S: I can be more spe specific. A dependency like <code>libc</code> is called
<code>libc</code> on Debian platforms and it’s called <code>glibc</code> on Enterprise Linux.
You talked about available satisfiers like Homebrew, Apt and package
config, but what if it has a different name in Homebrew than in Apt or
something like? Does it handle that or is that just something you haven’t
tackled yet?</p>
 
 
<p>Yurii: It doesn’t tackle this right now, but it’s part of the division
where it should go. For certain known libraries there’s an easy way to add
a mapping that will kick in for a distribution, and otherwise it will be a
satisfier for another one. They’re completely pluggable, small satisfiers
looking at all the predicates that describe the system underneath.</p>
 
 David G. Johnston in chat: How is the upcoming move to meson in core
influencing or impacting this? 
 
 
 
<p>Just for point of reference, this is built on top of Prolog, so it’s like
a knowledge base and rules for how to apply on this knowledge to
particular requirements.</p>
 
 
<p>Tobias Bussmann in chat: Prolog 👍</p>
 
 
<p>Shaun Thomas in chat: What if there are no satisfiers for the
install? If something isn’t in your distro’s repo, how do you know
where to find the dependency? And how is precedence handled? If two
satisfiers will fulfill a requirement, will the highest version win?</p>
 
 
 
 
<p>Jeremy S: I remember Devrim talking about, if you read through the [RPM]
spec files, what find is all this spaghetti code with <code>#ifdefs</code> and  logic
branches and in his case is just dealing with differences between Redhat
and SUSE. If this is something that we manually put in, we kind of end up
in a similar position where it’s on us to create those mappings, it’s on
us to maintain those mappings over time — we kind of own it — versus
being able to automate some kind of automatic resolution. I don’t know if
there is a good automatic way to do it. David had found something that he
posted, which I looked at a little bit, but Devrim talked about how much
of maintenance overhead it becomes in the long run to constantly have to
maintain this which seemed less than ideal.</p>
 
 
<p>Yurii: It is less than ideal. For now, I do think that would have to be
manual, which is less than ideal. But it could be addressed at least on on
a case-by-case basis. Because we don’t really have thousands of extensions
yet — in the ecosystem maybe a thousand total — I think David Wheeler
would would know best from his observations, and I think he mentioned some
numbers in his presentation couple of weeks ago. But basically handling
this on on a case-by-case basis where we need this dependency and
apparently it’s a different one on a different platform, so let’s address
that. But if there can be a method that can at least get us to a certain
level of unambiguous resolution automatically or semi-automatically, that
would be really great.</p>
 
 Samay Sharma in chat: +1 on the meson question. 
 
 
 
<p>Jeremy S: I think there’s a few more questions in the chat.</p>
 
 
<p>Yurii: I’m just looking at them now. “how is the upcoming move to meson
and core influencing or impacting this?” I don’t think it’s influencing
this particular part in any way that I can think of right now. David, do
you have thoughts how it can? I would love to learn.</p>
 
 
<p>David G. Johnston: No, I literally just started up a new machine yesterday
and decided to build it from meson instead of <code>make</code> and the syntax of the
meson file seems similar to this. I just curious if there are any influences
there or if it’s just happenstance.</p>
 
 
<p>Yurii: Well from from what I can think right now, there’s just general
reliance on either implicitly found PG config or explicitly specified PG
config. That’s just how you discover Postgres itself. There’s no relation
to how Postgres itself was built. The packaging system does not handle say
building Postgres itself or providing it so it’s external to this proof of
concept.</p>
 
 
<p>David G. Johnston: That’s a good separation of concerns, but there’s also
the idea that, if core is doing something, we’re going to build extensions
against PostgresSQL, if we’re doing things similar to how core is doing
them, there’s less of a learning curve and less of everyone doing their
own thing and you have 500 different ways of doing testing.</p>
 
 
<p>Yurii: That’s a good point. That’s something definitely to reflect on.</p>
 
 
<p>I’ll move on to the next question from Sean. “What if there are no
satisfiers for the install? If something isn’t in your distro how do you
know where to find the dependency?” And “if two satisfiers will fulfill a
requirement, will the highest version win?” If there are no satisfiers
right now it will just say it’s not solvable. So we fail to do anything.
You would have to go and figure that out. It is a proof of concept, it’s
not meant to be absolutely feature complete but rather an exploration of
how we can describe the the packages and their requirements.</p>
 
 
<p>David Wheeler (he/him): I assume the
idea is that, as you come upon these you would add more satisfiers.</p>
 
 
<p>Yurii: Right, you basically just learn. We learn about this particular
need in a particular extension and develop a satisfier for it. The same
applies to precedence: it’s a question of further evolution. Right now it
just finds whatever is available within the specified range.</p>
 
 
<p>If there are no more pressing questions I’ll move to the next slide. I was
just mentioning the problem of highly specific recipes versus high-level
requirements. Now I want to shift attention to another topic that has been
coming up in different conversations: whether to build and ship your
extension against minor versions of Postgres.</p>
 
 
<p>Different people have different stances in this, and even package managers
take different stands on it. Some say, just build against the latest major
version of Postgres and others say build extensions against every single
minor version. I wanted to research and see what the real answer should
be: should we build  against minor versions or not?</p>
 
 
<p>I’ve done a little bit of experimentation and my answer is “perhaps”, and
maybe even “test against different minor versions.” In my exploration of
version 16 (and also 15 bu Id didn’t include it) there there are multiple
changes between minor versions that can potentially be dangerous. One
great example is when you have a new field inserted in the middle of a
structure that is available through a header file. That definitely changes
the layout of the structure.</p>
  <code> typedef struct BTScanOpaqueData
 {
-    /* these fields are set by _bt_preprocess_keys(): */
+    /* all fields (except arrayStarted) are set by _bt_preprocess_keys(): */
     bool            qual_ok;                /* false if qual can never be satisfied */
+    bool            arrayStarted;     /* Started array keys, but have yet to "reach
+                                                               * past the end" of all arrays? */
     int                     numberOfKeys    /* number of preprocessed scan keys */
 }
</code>   
 
<p>In this particular case, for example, will not get number of keys if
you’re intending to. I think that change was from 16.0 to 16.1. If you
build against 16.0 and then try to run on 16.1, it might not be great.</p>
<p>The other concern that I found is there are new apis appearing in header
files between different versions. Some of them are implemented in header
files, either as macros or static and line functions. When you’re building
against that particular version, you’ll get the particular implementation
embedded.</p>
 
 
<p>Others are exports of symbols, like in this case, try index open
and contain mutable functions after planning, if you’re using any of this.
But this means that these symbols are not available on some minor versions
and they’re available later on, or vice versa: they may
theoretically disappear.</p>
 
 
<p>There are also changes in inline behavior. There was a change between 16.0
and 16.1 or 16.2 where an algorithm was changed. Instead of just <code>&gt; 0</code>
there’s now <code>&gt;= 0</code>, and that means that particular behavior will be
completely different between these implementations. This is important
because it’s coming from a header file, not a source file, so you’re
embedding this into your extension.</p>
 
 
<p>David Wheeler (he/him) in chat: That looks like a bug fix</p>
 
 
<p>Yeah it is a bug fix. But what I’m saying is, if you build your extension
against say 16.0m which did not have this bug fix, and then you deploy it
on 16.1, then you still have the bug because it’s coming from the header
file.</p>
 
 
<p>*David Wheeler (he/him): Presumably they suggest that you build from the
latest minor release and that’s Backward compatible to the earlier
releases.</p>
 
 
<p>Yurii: Right and that’s a good middle ground for this particular case. But
but of course sometimes when you do a minor upgrade you have to remember
that you have to rebuild your extensions against that minor version so you
can just easily transfer them yeah.</p>
 
 Jeremy S in chat: The struct change in a minor is very interesting 
 
 
 
<p>*David Wheeler (he/him)Jeremy points out that struct change is pretty
interesting.</p>
 
 
<p>Yurii: Yeah, it’s interesting because it’s super dangerous! Like if
somebody is expecting a different versioned structure, then  it can be
pretty nasty.</p>
 
 Shaun Thomas in chat: Yeah. It’s a huge no-no to insert components
into the middle of a struct. 
 
 
 
<p>Jeremy S: Is that common? I’m really surprised to see that in a minor
version. On the other hand, I don’t know that Postgres makes promises
about — some of this seems to come down to, when you’re coding in C and
you’re coding directly against structures in Postgres, that’s really
interesting. That’s — I’m surprised to see that still.</p>
 
 
<p>Steven Miller in chat: In the case of trunk, we would have built
against minor versions in the past then upgrade the minor version of
postgres without reinstalling the binary source of the extension, so
this is an issue</p>
 
 
<p>David G. Johnston in chat: Yeah, either that isn’t a public
structure and someone is violating visibility (in which case yes, you
should be tracking minor builds)</p>
 
 
<p>Shaun Thomas in chat: I’m extremely shocked that showed up in 16.2.</p>
 
 
 
 
<p>Yurii: Yeah, I didn’t expect that either, because that’s just a great way
to have absolutely undefined behavior. Like if somebody forgot to rebuild
their extension against a new minor, then this can be pretty terrible.</p>
 
 
<p>But my general answer to all of this unless you’re going really deep into
the guts of Postgres, unless you’re doing something very deep in terms
query planning, query execution, you’re probably okay? But who knows.</p>
 
 
<p>Jason Petersen in chat: yeah it feels like there’s no stated ABI
guarantee across minor updates</p>
 
 
<p>Jason Petersen in chat: other than “maybe we assume people know not
to do this"</p>
 
 
<p>David Christensen in chat: yeah ABI break in minor versions seems
nasty</p>
 
 
 
 
<p>Jeremy S: But it’s not just remembering to rebuild your extension. Let’s
let’s suppose somebody is just downloading their extensions from the PGDG
repo, because there’s a bunch of them there. They’re not compiling
anything! They’re they’re downloading an RPM and the extension might be in
a different RPM from Postgres and the extension RPMs — I don’t know that
there have been any cases with any of the extensions in PGDG, so far,
where a particular extension RPM had to have compatibility information at
the level of minors.</p>
 
 
<p>Shaun Thomas in chat: There was actually a huge uproar about this a
couple year ago because they broke the replication ABI by doing this.</p>
 
 
<p>David G. Johnston in chat: I see many discussions about ABI
stability on -hackers so it is a goal.</p>
 
 
<p>Steven Miller in chat: PGDG is the same binaries for each minor
version because the postgres package is only major version, right?</p>
 
 
 
 
<p>Yurii: Yeah, that’s definitely a concern, especially when it comes to the
scenario when you rebuild your extensions but just get pre-built packages.
It’s starting to leak out of the scope of this presentation, but I thought
it was a very interesting topic to bring to everybody’s attention.</p>
 
 
<p>Jason Petersen in chat: “it’s discussed on hackers” isn’t quite the
same as “there’s a COMPATIBILITY file in the repo that states a
guarantee”</p>
 
 
<p>Jason Petersen in chat: (sorry)</p>
 
 
 
 
<p>My last item. Going back to how we ship extensions and why do we need
complex build systems and packaging. Oftentimes you want your extensions
to depend on some library, say OpenSSL or SQLite or whatever, and the
default is to bring the shared dependency that would come from different
packages on different systems.</p>
 
 
<p>What we have found at Omnigres is that it is increasingly simpler to
either statically link with your dependencies — and pay the price of
larger libraries — but then you have no questions about where it comes
from — what what package, which version – you know exactly what which
version it is and how it’s getting built. But of course you also have a
problem where, if you want to change the version of the dependency it’s
harder because it’s statically linked. The question is whether you should
be doing that or not, depending on the authors of the extension and their
promises for compatibility with particular versions of their dependencies.
This one is kind of naive and simple, as in just use static. Sometimes
it’s not possible or very difficult to do so, some some libraries don’t
have build systems amenable to static library production.</p>
 
 
<p>What we found that works pretty nicely is using <code>rpath</code> in your dynamic
libraries. You can use special variables — <code>$ORIGIN</code> or <code>@loader_path</code>
on Linux or macOS, respectively, to specify that your dependency is
literally in the same folder or directory where your extension is. So you
can ship your extension with the dependencies alongside, and it will not
try to load them immediately from your system but from the same directory.
We find this pretty pretty useful.</p>
 
 
<p>That’s pretty much it. Just to recap I talked about the multi-platform
experience, the pros and cons of containers, inferencing how you build and
how you can build extensions with dependencies, static and <code>rpath</code>
dependencies, and the problems with PG minor version differences. If
anybody has thoughts, questions, or comments I think that would be a
great. Thank you.</p>
 
 
 Discussion 
 
 
<p>David Wheeler (he/him): Thank you, Yurii, already some good discussion.
What else do you all have?</p>
 
 
<p>David G. Johnston: PG doesn’t use semantic versioning. They we have a
major version and a minor version. The minor versions are new releases,
they do change behaviors. There are goals from the hackers to not break
things to the extent possible. But they don’t guarantee that this will not
change between dot-three and dot-four. When you’re releasing once a year
that’s not practical if things are broken, you can’t wait nine months to
fix something. Some things you need to fix them in the next update and
back-patch.</p>
 
 
<p>Steven Miller in chat: Thank you, this is very useful info</p>
 
 
<p>Jeremy S in chat: Dependency management is hard 🙂 it’s been a topic
here for awhile</p>
 
 
 
 
<p>David G. Johnston: So we don’t have a compatibility file, but we do have
goals and if they get broken there’s either a reason for it or someone
just missed it. From an extension standpoint, if you want to be absolutely
safe but absolutely cost intensive, you want to update every minor
release: compile, test, etc. Depending on what your extension is, you can
trade off some of that risk for cost savings. That’s just going to be a
personal call. The systems that we build should make it easy enough to do
releases every “dot” and back-patching. Then the real cost is do you spend
the time testing and coding against it to make sure that the stuff works.
So our tool should assume releasing extensions on every minor release, not
every major release, because that’s the ideal.</p>
 
 Shaun Thomas in chat: It’s good we’re doing all of this though. It
would suck to do so much work and just become another pip spaghetti. 
 
 
 
<p>Yurii: That’s exactly what I wanted to bring to everybody’s attention,
because there’s still a lot of conversations about this and there was not
enough clarity. So that helps a lot.</p>
 
 
<p>Jeremy S: Did you say release or did you say build with every Miner?
I think I would use the word “build”.</p>
 
 
<p>David G. Johnston: Every minor release, the ones that go out to the
public. I mean every commit you could update your extension if you wanted.
but really the ones that matter are the ones that go public. So, 16.3 or
16.4 comes out, automation would ideally would build your extension
against it run your test and see if anything broke. And then deploy the
new [???] of your extension against version 16.3. Plus that would be your
your release.</p>
 
 
<p>Jeremy S: I think there are two things there: There’s rebuilding it —
because you can rebuild the same version of the extension and that would
pick up if they they added a field in the middle of a struct which is what
happened between 16.0 and 16.1, rebuild the same version. Versus: the
extension author … what would they be doing? If they they could tag a
new version but they’re not actually changing any code I don’t think it is
a new release of the extension, because you’re not even changing anything
in the extension, you’re just running a new build. It’s just a rebuild.</p>
 
 David Wheeler (he/him) in chat: It’d be a new binary release of the
same version. In RPM it goes from v1.0.1-1 to v1.0.1-2 
 
<p>It reminds me of what Alvaro did in his his OCI blog post, where he said
you really have to … Many of us don’t understand how tightly coupled the
extensions need to be to the database. And these C extensions that we’re
we’re building have risks when we separate them don’t just build
everything together.</p>
 
 
<p>David G. Johnston: The change there would be metadata. Version four of my
extension, I know it works on 16.0 to 16.1. 16.2 broke it, so that’s where
it ends and my version 4.1 is known to work on 16.2.</p>
 
 
<p>Jeremy S: But there is no difference between version 4 and version 4.1.
There’s a difference in the build artifact that your build farm spit out,
but there’s no difference in the extension, right?</p>
 
 
<p>Keith Fiske in chat: Still confusing if you don’t bump the release
version even with only a library change</p>
 
 
<p>Keith Fiske in chat: How are people supposed to know what library
version is running?</p>
 
 
 
 
<p>David G. Johnston: Right. If the extension still works, then` your
metadata would just say, “not only do I work through version 16.2, I now
work through 16.3.</p>
 
 
<p>Jeremy S: But it goes back to the question: is the version referring to a
build artifact, or is the version referring to a version of the code? I
typically think of versions as a user of something: a version is the
thing. It would be the code of the extension. Now we’re getting all meta;
I guess there are arguments to be made both ways on that.</p>
 
 
<p>Jason Petersen in chat: (it’s system-specific)</p>
 
 
<p>Jason Petersen in chat: no one talks in full version numbers, look
at an actual debian apt-cache output</p>
 
 
 
 
<p>David Wheeler (he/him): Other questions? Anybody familiar with the <code>rpath</code>
stuff? That seems pretty interesting to me as a potential solution for
bundling all the parts of an extension in a single directory — as
opposed to what we have now, where it’s scattered around four different
directories.</p>
 
 
<p>Jason Petersen: I’ve played around with this. I think I was trying to do
fault injection, but it was some dynamically loaded library at a different
point on the <code>rpath</code>. I’m kind of familiar with the
mechanics of it.</p>
<p>I just wanted to ask: In a bigger picture, this talks about building
extensions that sort of work everywhere. But the problems being solved are
just the duplication across the spec files, the Debian files, etc. You
still have to build a different artifact for even the same extension on
the same version of Postgres on two different versions of Ubuntu, Right?
Am I missing something? It is not an extension that runs everywhere.</p>
 
 
<p>Yurii: No, you still have to build against the set of attributes that
constitute your target, whether that’s architecture, operating system,
flavor. It’s not yet something you can build and just have one binary. I
would love to have that, actually! I’ve been pondering a lot about this.
There’s an interesting project, not really related to plugins, but if
you’ve seen A.P.E. and Cosmopolitan libc, they do portable executables.
It’s a very interesting hack that allows you to run binaries on any
operating system.</p>
 
 
<p>Jason Petersen: I expected that to be kind of “pie in the
sky.”</p>
 
 
<p>Yurii: It’s more of a work of art.</p>
 
 
<p>Jason Petersen: Do you know of other prior art for the <code>rpath</code>?  Someone
on Mastodon the other day was talking about Ruby — I can’t remember the
library, maybe it was ssh — and they were asking, “Do I still have to
install this dynamic library?” And they said, “No, we vendor that now;
whenever you install this it gets installed within the Ruby structure.”
I’m not sure what they’re doing; maybe it’s just a static linking. But I
was curious if you were aware of any prior art or other packaging systems
where system manages its own dynamic libraries, and use <code>rpath</code> to
override the loading of them so we don’t use the system ones and don’t
have to conflict with them. Because I think that’s a really good idea! I
just was wondering if there’s any sort of prior art.</p>
 
 
<p>Daniele: There is an example: Python Wheels binaries us <code>rpath</code>. A wheel
is a ZIP file with the C extension and all the depending libraries the
with the path modified so that they can refer to each other in the the
environment where they’re bundled. There is a tool chain to obtain this
packaging — this vendoring — of the system libraries. There are three,
actually: one for Unix, one for macOS, one for Windows. But they all more
or less achieve the same goal of having libraries where they can find each
other in the same directory or in a known directory. So you could take a
look at the wheel specification for Python and the implementation.
That could be a guideline.</p>
 
 
<p>Jason Petersen: Cool.</p>
 
 
<p>Yurii: That’s an excellent reference, thank you.</p>
 
 
<p>David Wheeler (he/him): More questions?</p>
 
 
<p>Jeremy S: Yeah, I have one more. Yurii, the build inferencing was really
interesting. A couple things stood out to me. One that you mentioned was
that you look for The <code>META.json</code> file. That’s kind of neat, just that
it’s acknowledged a useful thing; and a lot of extensions have it and we
want to make use of it. I think everybody knows part of the background of
this whole series of meetings is — one of the things we’re asking is,
how can we improve what’s the next generation of <code>META.json</code> to make all
of this better? Maybe I missed this, but what was your high-level takeaway
from that whole experience of trying to infer the stuff that wasn’t there,
or infer enough information to build something if there isn’t a
<code>META.json</code> at all? Do you feel like it worked, that it was successful?
That it was an interesting experiment but not really viable long term? How
many different extensions did you try and did it work for? Once you put it
together, were you ever able to point it at a brand new extension you’d
never seen before and actually have it work? Or was it still where you’d
try a new extension and have to add a little bit of extra logic to handle
that new extension? What’s your takeaway from that experience?</p>
 
 
<p>Yurii: The building part is largely unrelated to <code>META.json</code>, that was
just primarily the metadata itself. I haven’t used in a lot of extensions
because I was looking for different cases — extensions that exhibit
slightly different patterns — not a whole ton of them yet. I would say
that, so far, this is more of a case-by-case scenario to see for a
particular type of or shape of extension what we need to do. But
generally, what I found so far that it works pretty nicely for C
extensions: it just picks up where all the stuff is, downloads all the
necessary versions, allows to discover the new versions — for example
you don’t need to update the specification for a package if you have a new
release, it will just automatically pick that up rom the list of tags.
These these were the current findings. I think overall the direction is
promising, just need to continue adjusting the results and see how much
further it can be taken and how much more benefit it can bring.</p>
 
 
<p>Jeremy S: Thank you.</p>
 
 
<p>Yurii: Any other comments or thoughts?</p>
 
 
<p>David Wheeler (he/him): Any more questions for Yurii?</p>
 
 
<p>David Wheeler (he/him): I think this is a an interesting space for some
research between Devrim’s presentation talking about how much effort it is
to manually maintain all the extensions in the Yum repository. I’ve been
doing some experiments trying to build everything from PGXN, and the
success rate is much lower than I’d like. I think there are some
interesting challenges to automatically figuring out how things work
versus convincing authors to specify in advance.</p>
 
 
<p>Jeremy S: Yep. Or taking on that maintenance. Kind of like what a spec
file maintainer or a Debian package maintainer is doing.</p>
 
 
<p>Yurii: Yeah, precisely.</p>
 
 
 Wrap Up 
 
 
<p>David Wheeler (he/him): Thanks, Yurii, for that. I wanted to remind
everyone that we have our final Mini-Summit before PGConf on May 15th.
That’s two weeks from today at noon Eastern or 4 pm UTC. We’re going to
talk about organizing the topics for the Summit itself. I posted a long
list of stuff that I’ve extracted from my own brain and lots more topics
that I’ve learned in these presentations in the Slack. Please join the
<a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">community Slack</a> to participate.</p>
<p>The idea is to winnow down the list to a reasonable size. We already are
full with about 45 attendees, and we we can maybe have a few more with
standing room and some hallway track stuff. We’ll figure that out, but
it’s a pretty good size, so I think we’ll be able to take on a good six or
maybe eight topics. I’m going to go over them all and we’ll talk about
them and try to make some decisions in advance, so when we get there we
don’t have to spend the first hour figuring out what we want to, we can
just dive in.</p>
<p>And that’s it. Thank you everybody for coming, I really appreciate. We’ll
see you next time</p>
 
 
<p>Tobias Bussmann in chat: Thanks for the insights and discussion!</p>
 
 
<p>Jeremy S: Thank you!</p>
 
 
 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/yurii-rashkovskii/">Yurii Rashkovskii</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   22:12 &bull;
   27 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/getting-an-engineering-executive'>Getting an Engineering Executive Job</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  An overview of successful, tried-and-true routes into CTO, VPE, and Head of Engineering jobs, from the new book, ‘The Engineering Executive’s Primer’ by Will Larson.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>So, just how do you win that first, coveted CTO, VP of Engineering, Head of Engineering, or Director of Engineering role? I know several professionals who have achieved such promotions internally, and a couple who were successful external candidates for their first CTO role.</p><p>These jobs are usually classed as executive roles, and author and CTO, <a href="https://lethain.com/">Will Larson</a>, has just published a handy book about these upper echelons. Before this latest title, he wrote a book on engineering management (<a href="https://lethain.com/elegant-puzzle/">An Elegant Puzzle</a>,) and one on staff+ engineering (<a href="https://staffeng.com/book">Staff Engineer</a>). Side note; we almost overlapped at Uber, with him departing the San Francisco office just as I joined the company in Amsterdam. We met in-person last year when I was in SF.</p><p>I picked up this new book, <a href="https://learning.oreilly.com/library/view/the-engineering-executives/9781098149475/">The Engineering Executive’s Primer</a>, and can report that I’m thoroughly impressed. It’s honest in how it covers topics like navigating tricky power dynamics with CEOs, surviving “peer panic,” dealing with competition within leadership teams, and in its overall breadth and depth. There’s the usual topics: onboarding, hiring, engineering processes, performance processes, compensation, etc, and the book also goes deep into how to navigate mergers and acquisitions, cultural surveys, and how to onboard peer executives.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc1d25fe-ab5a-47d3-8e3e-e3cd1f1003c6_1600x1200.jpeg">    </a>My copy of <a href="https://learning.oreilly.com/library/view/the-engineering-executives/9781098149475/">The Engineering Executive’s Primer</a> <p>I reached out to Will to ask if he’d be open to publishing a chapter from the new book in this newsletter, and he generously agreed. In today’s issue, we cover two topics:</p>  <p>The book’s and Will’s backgrounds. How did the idea of the book come about, how long did it take to write, and what is Will’s advice for software engineers aiming to be engineering executives.</p>  <p>Chapter 1. The opening chapter is titled “Getting the Job.” It’s a thorough summary of how to get that first engineering executive position; more so than I’ve read elsewhere. An excerpt from <a href="https://learning.oreilly.com/library/view/the-engineering-executives/9781098149475/">the book</a>.</p>  <p>As usual with all my recommendations, I am not paid to recommend this book, and none of the links are affiliate ones. See my <a href="https://blog.pragmaticengineer.com/ethics-statement/">ethics statement</a> for more detail.</p> 1. The book’s and Will’s backgrounds <p>Before we jump into the chapter, Will answered a few questions:</p><p>How did you get your first engineering executive job?</p><p>‘In late 2019, I was getting close to four years at Stripe, and understood the problems I was working on a bit too well. I started thinking about how to get my hands on a new set of problems. In particular, I was interested in finding ones to bring me back into product engineering, rather than continuing along the infrastructure engineering path, which I got on to somewhat accidentally at Uber.</p><p>‘Most inbound interest I received was for roles similar to the one I was in. So I asked some recently-hired executives how they’d found their jobs. Most mentioned working with executive recruiters, and I asked for referrals. This led me to Sam, the executive recruiter who helped find my CTO role at Calm, which I accepted following deep discussions with 3-4 other companies.’</p><p>What made you decide to write this book on executive roles, and can you discuss your choice to be so candid about the behind-the-scenes reality?</p><p>‘This is my third book, and I learned a lot from writing ‘An Elegant Puzzle’ and ‘Staff Engineer.’ There’s a curse in authorship, where I see so many things I could have done better in writing those books! Still, writing two intentionally different books helped me understand what I want to accomplish with my writing.</p><p>‘My goal when writing is to create something both useful and unique. Being useful is the only good reason to write a professional book, while being unique is the only way to create something durable that resonates with readers over time, and doesn’t fade away after a year or two.</p><p>‘When I first became an executive, I learned a lot by making mistakes. I wanted to scrutinize those errors, figure out frameworks that would’ve helped me avoid them, and collate these insights in a book to help others entering that role. Helping others avoid my mistakes is my best tool for advancing the technology industry, which is the professional goal I’m trying to channel my energy toward.’</p><p>How did you write the book, and how long did it take? I guess you started taking notes a long time ago, with so many observations across 24 chapters and more than 300 pages.</p><p>‘I have two concurrent writing processes:</p>  <p>I write down things I learn, as I learn them. This is the closest I have to a separate note-taking process. For example, I wrote the blog posts <a href="https://lethain.com/hard-to-work-with/">Hard to work with</a> and <a href="https://lethain.com/profit-and-loss-statement/">Reading a Profit &amp; Loss statement</a> in 2022 as standalone pieces, and updated versions of each show up in The Engineering Executive’s Primer.&nbsp;</p>  <p>I write based upon an outline of topics for a book. The chapter, <a href="https://lethain.com/measuring-engineering-organizations/">Measuring an engineering organization</a>, is a good example, and was the first piece I wrote explicitly with the new book in mind.</p>  <p>‘Writing this book took about 14 months. I started working on the outline in late 2022, and iterated on it three or four times, before it stabilized roughly into these chapters. I wrote about two-thirds of chapters before I started talking with the publisher, O’Reilly, about whether they’d be interested in it.&nbsp;</p><p>‘After signing the contract, I reworked the topics a bit with my editor, Virginia Wilson, completed the remaining chapters, and revised individual chapters several times based on feedback from her and reviewers. Writing books really does get easier the more you do it, and this book was relatively peaceful compared to my first two.’</p><p>You’ve been a software engineer, and are now a CTO. What are the biggest differences between these roles?&nbsp;</p><p>‘A friend recently emailed me asking if it’s possible to have friends at work as an executive, because he was finding that harder and harder, the more senior he’d gotten. In general, I’ve found it’s complex to have work friends as an executive, and that it only works when both individuals take the long view on outcomes. I miss my earlier roles where these things were less fraught.</p><p>‘Conversely, I’m really motivated by being able to engage with problems blocking forward progress, and being in an effective executive team is a job characterized by exclusively dealing with that kind of problem. For me, engaging with problems is the most energizing part of being an executive. At the same time, in my earlier work as an engineer I often found myself stymied by organizational constraints. To be fair, I was also a lot less effective at navigating organizational constraints back then.’</p><p>What’s your advice for software engineers ambitious to be a Head of Engineering, VP of Engineering, or CTO?</p><p>‘Spend more time understanding why people’s perspectives are “right,” rather than why they’re “wrong.” I think of this as extracting the core of what people say.&nbsp;</p><p>‘Being able to learn from those who don’t communicate clearly is a super power. Many who are theoretically good communicators – maybe the executives you work with – are too busy to always communicate clearly, and getting good at understanding them despite messy formatting is invaluable.</p><p>‘Spend less time on pursuits you don’t find energizing. For example, I’ve seen so many people try to become “content creators” to further their career, despite having no interest in creating content. The vast majority of successful executives don’t write stuff online and don’t speak at conferences. Do so if it’s energizing for you, but if it isn’t, find something that is!’</p><p>Thanks Will for sharing your thoughts! With that, we dive into Chapter 1 of the book.</p> 2. Getting an Engineering Executive Job <p>The below excerpt is from <a href="https://learning.oreilly.com/library/view/the-engineering-executives/9781098149475/">The Engineering Executive's Primer</a>, by Will Larson. Copyright © 2024 Will Larson. Published by O'Reilly Media, Inc. Used with permission.</p><p>At Digg, I ended up running Engineering, but I certainly wasn’t hired to do so. It wasn’t until a decade later, when I joined Calm, that a company deliberately hired me into my first executive role. If you start researching executive career paths, you’ll find folks who nominally became Engineering executives at 21 when they found a company, and others who were more than 30 years into their career before taking an Engineering executive role.</p><p>As these anecdotes suggest, there is no “one way” to get an Engineering executive job. However, the more stories you hear about folks assuming executive roles, the more they start to sound pretty similar. I’ve condensed the many stories I’ve heard, along with my own experiences, into a repeatable process that prospective candidates typically follow.</p><p>This chapter will cover:</p>  <p>deciding whether to pursue an executive role</p>  <p>why each executive job search is unique, and how that will shape your process</p>  <p>finding executive roles externally and internally</p>  <p>navigating the often chaotic executive interview process after you’ve gotten comfortable interviewing in well-designed middle management interview processes</p>  <p>negotiating an executive contract, particularly the terms that rarely come up in the non-executive contracts you may have negotiated prior</p>  <p>deciding whether to accept an executive offer once you have it</p>  <p>If you’re kicking off the search for your first executive role, reading through this chapter will provide a clear roadmap through the process.</p> Why Pursue an Executive Role? <p>If you’re spinning up your first executive role search, you should have a clear answer to a question you’ll get a number of times, “Why are you looking for an executive role?” It’s important to answer this for yourself, as it will be a valuable guide throughout your search. If you’re not sure what the answer is, spend time thinking this through until you have a clear answer (maybe in the context of a <a href="https://lethain.com/career-checkup/">career checkup</a>).</p><p>There’s no right answer, but here are some examples from real people:</p>  <p>“I’m heavily motivated by learning. I’ve directly reported into an Engineering executive for my past two roles, and I’m looking to step into the role myself.”</p>  <p>“I’ve enjoyed working in a fast-growing company, but I also miss the direct ownership and pace of working at a small company. I’m excited to combine my previous startup experience with my recent experience at scale as an Engineering executive.”</p>  <p>The rationale doesn’t need to be particularly compelling, just something positive that expresses your excitement and qualification for the role. Don’t get discouraged if your statement isn’t profound—there are very few profound ways to say that it’s the next logical step in your career. Once you’ve written your rationale down, review it with a few peers or mentors who have already been in executive roles. Incorporate their feedback, and you’re done. (If you don’t have peers or mentors in executive roles, do some cold outreach to executives at companies you’ve worked at with your rationale and see if they’ll weigh in.)</p><p>The other side of this is that interviewers are also very curious about your reason for pursuing an executive role, but not necessarily for the reason you’d expect. Rather than looking for your unique story (although, yes, they’ll certainly love a memorable, unique story), they’re trying to filter out candidates with red flags: ego, jealousy, excessive status-orientation, and ambivalence.</p> One of One <p>Limited-release luxury items like fancy cars sometimes label each item with their specific production number, along with the size of the overall run. For example, you might get the fifth car in a run of 20 cars overall. The most exclusive possible production run is “one of one.” That item is truly bespoke, custom, and one of a kind.</p><p>All executive roles and processes are “one of one.”</p><p>For non-executive roles, good interviewing processes are systematized, consistent, and structured. Sometimes the interview processes for executive roles are well-structured, but more often they aren’t. If you approach these bespoke processes like your previous experiences interviewing, your instincts may mislead you through the process.</p><p>The most important thing to remember when searching for an executive role is that while there are guidelines, stories, and even statistics, there are no rules when it comes to finding executive jobs. There is a selection bias in executive hiring for confidence, which makes it relatively easy to find an executive who will tell you with complete confidence how things work but be a bit wary.</p><p>It’s not just the hiring process that is not standardized; the Engineering executive roles themselves vary greatly as well. Sometimes they’ll include managing Product Management, and sometimes they’ll exclude managing some parts of Engineering. Working with technology-oriented founders, you may provide more organizational support than technical guidance, whereas working in an older business may mean there are few other executives with a technology background. “One of one” means that anything is possible, in both the best and worst possible sense.</p> Finding Internal Executive Roles <p>Relatively few folks find their first executive job through an internal promotion. These are rare for a couple reasons. The first is that each company only has one Engineering executive, and that role is usually already filled. The second is that companies seeking a new Engineering executive generally need someone with a significantly different set of skills than the team they already have in place.</p><p>Even in cases where folks do take on an executive role at their current company, they often struggle to succeed. Their challenges mirror those of <a href="https://lethain.com/tech-lead-managers/">taking on tech lead manager roles</a>, where they are stuck learning how to do their new job in addition to performing their previous role. They are often also dealing with other internal candidates who were previously their peers and who may feel slighted by not getting the role themselves. This makes their new job even more challenging, and can lead to departures that hollow out the organization’s key leaders at a particularly challenging time.</p><p>That’s not to say that you should avoid or decline an internal promotion into an executive engineering role; just that you should go into it with your eyes open. In many ways, it’s harder to transition internally than externally. Because of that, even if an internal transition into an executive role goes poorly for you, don’t assume that means you wouldn’t do well as a newly hired executive at another company.</p> Finding External Executive Roles <p>Most executive roles are never posted on the company’s jobs page. So before discussing how you should approach your executive job search, let’s dig into how companies usually find candidates for their executive roles. Let’s imagine that my defunct company <a href="https://lethain.com/monocle-studios/">Monocle Studios</a> had been a wild success and we wanted to hire our first CTO.</p><p>How would we find candidates? Something along the lines of:</p>  <p>Consider any internal candidates for the role.</p>  <p>Reach out to the best folks in my existing network, seeing if any are interested in interviewing for the role.</p>  <p>Ask our internal executive recruiter to source candidates. (I’d skip this step if we didn’t have any executive recruiters internally, as generally there’s a different network and approach to running an executive search than a non-executive search; executive candidates also tend to ask different questions than non-executive candidates, which makes hiring them with non-executive recruiters even messier.)</p>  <p>Reach out to our existing investors for their help, relying on both their networks and their firms’ recruiting teams.</p>  <p>Hire an executive recruiting firm to take over the search.</p>  <p>Certainly not every company does every job search this way, but it does seem to be the consistent norm. This structure exposes why it’s difficult to answer the question, “How do I find my first executive role?” The quick answer is to connect with an executive recruiter—ideally one that peers have worked with before—but that approach comes with some implications on the sort of roles you’ll get exposed to. Typically, these will be roles that have been challenging to fill for some reason.</p><p>It’s important to note that the most desirable roles, and roles being hired by a well-networked and well-respected CEO, will never reach an executive recruiting firm. If you try to enter your search without an established network and rely solely on executive recruiters to find roles, you are almost certain to be selecting from second-tier opportunities.</p><p>This is, by the way, absolutely not a recommendation against using executive recruiters. Executive recruiting firms can be fantastic. A good executive recruiter will coach you through the process much more diligently than the typical company or investor’s in-house recruiter. I found my first executive role through an executive recruiter, as did the majority of my peers. (Note that the executive recruiters of tomorrow are your internal recruiting colleagues of today, so learning to partner effectively with Recruiting will pay dividends in both your current hiring and your long-term career options.) Similarly, it’s not true that all founder-led searches are for desirable jobs—almost all executive roles start as founder-led searches before working their way through the pipeline.</p><p>​​Looking at the pipeline, there are many ways to increase your odds of getting executive opportunities at each step. The basics still matter: Maintain an <a href="https://lethain.com/executive-linkedin-profile/">updated LinkedIn profile</a>, and respond politely to recruiters who do reach out. Both have a surprising way of creating job search serendipity, and ensuring your network is aware that you’re looking. If you don’t personally know many recruiters at investors or executive recruiters, your network can be particularly helpful for making those introductions.</p><p>There are also a small number of companies that do post executive roles publicly, and there’s certainly no harm in looking through those as well. The one challenge is that you’ll have to figure out whether it’s posted publicly because the company is very principled about searching for talent outside their personal networks (often a good sign), or if the role has already passed unsuccessfully through the entire funnel described above (often not a good sign). Most companies with strong principles like to talk about them a lot, and you should be able to find public evidence to support their posting coming from a principled belief. If you can’t, then it’s likely desperation.</p><p>Finally, if you’re laying the groundwork for an executive search a few years down the road, there’s quite a bit you can do to prepare. You can join a large or high-growth company to expand your network (more on this in Chapter 12), work in a role where you get exposure to the company’s investors, create more visibility of your work (more on this in Chapter 12 and Chapter 15) to make it more likely for founders to reach out to you, or get more relevant experience growing and operating an Engineering organization.</p> Interview Process <p>The interview process for executive roles is always a bit chaotic. The most surprising thing for most candidates is that the process often feels less focused or effective than their other recent interviews. This is because your hiring manager as a director of Engineering is usually an experienced engineering leader, but your hiring manager as an Engineering executive is usually someone with no engineering experience at all. In the first case, you’re being interviewed by someone who understands your job quite well, and in the second, the interviewer usually has never worked in the role.</p><p>There are, inevitably, exceptions! Sometimes your interviewer was an Engineering executive at an earlier point in their career, but that usually isn’t the case. A relatively common scenario in startups is when a technical founder interviews you for the role, potentially with them staying as the CTO and you taking on the VPE title. But, even then, it’s worth noting that the title is a bit of a smokescreen, and they likely have limited experience as an Engineering executive.</p><p>Consequently, Engineering executive interviews depend more heavily on perceived fit, prestige, the size of the teams you’ve previously managed, being personable, and navigating the specific, concrete concerns of would-be direct reports and peers. This makes the “little things” particularly important in executive interviews: send quick and polite follow-ups, use something like the <a href="https://lethain.com/star-method/">STAR method</a> to keep your answers concise and organized, prepare questions that show you’re strengthening your mental model of how the company works, and generally show energy and excitement.</p><p>The general interview process that I’ve seen for executive roles is as follows:</p>  <p>Call with a recruiter to validate you meet the minimum requirements, are a decent communicator, and won’t embarrass them if you talk to the CEO. Recruiters are heavily scrutinized on the quality of candidates they bring forward and will go out of their way to help you show up well. This is also a good opportunity for you to understand whether there are obvious issues that might make this a bad role for you, such as wrong job location, wrong travel expectations, and so forth.</p>  <p>Call with the CEO or another executive to assess interest in the role, and very high-level potential fit for the role. You’ll be evaluated primarily on your background, your preparation for the discussion, the quality of your communication, and perceived excitement for the company.</p>  <p>Series of discussions with the CEO or founder, where you dig into the business and their priorities for the role. This will be a mix of you learning from them, them learning about you, and getting a mutual sense of whether you’ll work well together. The exact structure will vary depending on the CEO or founder, and it will give you an understanding of what kind of person they are to work with.</p>  <p>One-on-one discussions with a wide smattering of peer executives and members of the team that you would manage. These vary widely across companies, and it is surprisingly common for the interviews to be poorly coordinated—for example, the same topics may come up multiple times across different interviewers. This is somewhat frustrating. Generally, it means the company is missing someone with the right position, experience, and energy to invest into designing the loop. I’ve had these interviews turn into general chats, <a href="https://increment.com/teams/do-engineering-managers-need-to-be-technical/">programming screens</a>, architecture interviews, and anything else you can imagine. All I can say is: Roll with it to the best of your ability.</p>  <p>Presentation interview to the executive team, your directors, or a mix of both. Usually, you’ll be asked to run a 60-minute presentation describing your background, a point of view on what’s important for the business moving forward, your understanding of what you would focus on in the new role if hired, and your plan for your first 90 days.</p>  <p>Here are a few tips that I’ve found effective for these interviews:</p>  <p>Ask an interviewer for feedback on your presentation before the session.</p>  <p>Ask what other candidates have done that was particularly well received.</p>  <p>Make sure to follow the prompt directly.</p>  <p>Prioritize where you want to spend time in the presentation (on the highest-impact topics).</p>  <p>Make sure to leave time for questions (while also having enough bonus content to fill the time if there aren’t many).</p>  <p>If this sounds surprisingly vague and a bit random, then you’ve read it correctly. Gone are the days of cramming in all the right answers. Now, it’s a matter of reading each individual effectively and pairing the right response to their perspective. If that feels arbitrary, keep in mind that navigating various perspectives will be a significant part of your role as an executive going forward!</p> Negotiating the Contract <p>Once a company decides to make you an offer, you enter into the negotiation phase. While the general rules of negotiation still apply—particularly, don’t start negotiating until the company knows it wants to hire you—this is a moment when it’s important to remember that these are one of one jobs. Compensation consultants and investors will have recommended pay ranges, but each company only hires one Engineering executive at a time, and every company is unique.</p><p>Fair pay will vary greatly depending on the company, the size of its business, your location, and your own background. Your best bet will be reaching out to peers in similar roles to understand their compensation. I’ve found folks to be surprisingly willing to share compensation details. It’s also helpful to read DEF 14A filings for public companies, which explain their top executives’ base, bonus, and equity compensation (for example, here is <a href="https://www.splunk.com/en_us/about-splunk/investors.html">Splunk’s DEF 14A from 2022</a>).</p><p>There are a few aspects of this negotiation that are sufficiently different from earlier compensation negotiations:</p><p>Equity</p><p>Equity is issued in many formats: stock options, Restricted Stock Units, and so on. Equity is also issued with many conditions: vesting periods (often 4 years), vesting cliffs before vesting accrues (often 1 year), and the duration of the period after you depart when you’re able to exercise options before they expire (often 90 days).</p><p>Most of these terms are negotiable in an executive offer, but it all comes down to the particular company you’re speaking with. You may be able to negotiate away your vesting cliff, and immediately start vesting monthly rather than waiting a year; or negotiate an extended post-departure exercise window, even if that isn’t an option more widely; or have the company issue you a loan to cover your exercise costs, which combined with early exercise might allow you to exercise for “free” except for the very real tax consequences.</p><p>To determine your negotiation strategy, I highly recommend consulting with a tax advisor, as the “best” option will depend on your particular circumstances.</p><p>Equity acceleration</p><p>Equity acceleration is another negotiation point around equity. This is worth calling out as it’s common in executive offers, and extremely uncommon in other cases. Acceleration allows you to vest equity immediately if certain conditions are met. Many consider this a standard condition for a startup contract, although there are many executives who don’t have an acceleration clause.</p><p>One topic that gets perhaps undue attention is the distinction between single and double trigger acceleration. “Single trigger” acceleration has only one condition to be met (for example, your company is acquired), whereas “double trigger” acceleration will specify two conditions (for example, your company is acquired and you lose your job). My sense is that people like to talk about single and double triggers because it makes them sound knowledgeable about the topic rather than it being a particularly nuanced aspect of the discussion.</p><p>Severance packages</p><p>Severance packages can be negotiated, guaranteeing compensation after you exit the role. There is little consistency on this topic. Agreements range from executives at very small companies that have pre-negotiated a few months’ salary as a severance package, to executives leaving highly compensated roles that require their new company to make them whole on the compensation they’re leaving behind. There are also many executive contracts that don’t pre-negotiate severance at all, leaving the negotiation until the departure (when you admittedly have limited leverage).</p><p>Bonus</p><p>Bonus size and calculation can be negotiated. On average, bonus tends to be a larger component of roles outside of engineering, such as a sales executive, but like everything, this is company- and size-specific. A CTO at a public company might have their bonus be equal in size to their salary. A CTO at a Series C company might have a 20% bonus. A CTO at a 50-person company might have no bonus at all.</p><p>In addition to the size of your bonus, you may be able to negotiate the conditions for earning it. This won’t matter with companies that rely on a shared bonus goal for all executives (sometimes excluding sales), but may matter a great deal with other companies that employ bespoke, per-executive goals instead.</p><p>Parental leave</p><p>Parental leave can be negotiated. For example, some companies might only offer paid parental leave after a year of service, but you can absolutely negotiate to include that after a shorter amount of service. (It’s worth noting that this is often negotiable in less senior roles, as well.)</p><p>Start date</p><p>Start date is generally quite easy to negotiate in less senior roles but can be unexpectedly messy for executive roles. The reason it gets messy is that the hiring company often has an urgent need for the role to be filled, while also wanting to see a great deal of excitement from the candidate about joining.</p><p>The quiet part is that many recruiters and companies have seen executive candidates accept but later not join due to an opposing offer being sweetened, which makes them uncomfortable delaying, particularly for candidates who have been negotiating with other companies, including their current one.</p><p>Support</p><p>Support to perform your role successfully is another point that can be negotiated. The typical example of vain requests for support are guaranteed business- or first-class seats on business travel, but there are other dimensions of support that will genuinely impact your ability to perform your role. For example, negotiating for an executive assistant can free up hours every week for focus work, and negotiating a sufficient budget to staff your team can easily be the difference between a great and terrible first year.</p><p>The negotiation phase is the right time to ask for whatever you’ll need to succeed in the role. You’ll never have an easier time to ensure you and your organization can succeed.</p><p>Negotiate knowing that anything is possible but remember that you have to work with the people you’re negotiating with after the negotiation ends. If you push too many times, you won’t be the first candidate to have their offer pulled because the offering company has lost confidence that you really want to be there.</p> Deciding to Take the Job <p>Once you get an offer for an executive position, it can be remarkably hard to say no. The recruiters you’re working with will push you to accept. The company you’re speaking with will push you to accept. You’ll have invested a great deal of work into the process, and that will bias you toward wanting to accept as well.</p><p>It’s also challenging to evaluate an executive offer, because ultimately you’re doing two very difficult things. First, you’re trying to predict the company’s future trajectory, which is hard even for venture capitalists who do it a lot (and they’re solving for an easier problem as they get to make many concurrent investments, and you can only have one job at a time). Second, you’re trying to make a decision that balances all of your needs, which a surprising number of folks get wrong (including taking prestigious or high-paying jobs that they know they’re going to hate, but just can’t say no to).</p><p>I can’t really tell you whether to accept your offer, but there are a few steps that I would push you to take before finalizing your decision:</p>  <p>Spend enough time with the CEO to be sure you’ll enjoy working with them, and that you’ll trust them to lead the company. While it changes a bit as companies scale, and particularly as they go public, the CEO is the person who will be deciding company direction, determining the members of the executive team, and taking responsibility to resolve the trickiest decisions.</p>  <p>Speak to at least one member of their board. Admittedly, board members won’t directly tell you anything too spicy, but their willingness to meet with you is an important signal, and it’s the best opportunity to start building your relationship with the board.</p>  <p>Make sure you’ve spoken with every member of the executive team that you’d work with regularly. Sometimes you’ll miss someone in your interview process due to scheduling issues, and it’s important to chat with everyone and make sure they’re folks you can build an effective working relationship with.</p>  <p>Make sure they’ve actually answered your questions. I once interviewed to be a company’s head of Engineering, and they refused to share their current valuation with me! I pushed a few times, but ultimately they told me it was unreasonable to ask, and I decided I couldn’t move forward with a company that wouldn’t even share their valuation with an executive candidate.<br><br>Don’t assume they’ll disclose this information after you join the company if they won’t tell you when trying to convince you to accept their offer. You will never have more leverage to get questions answered than during the hiring process: If it’s important and they won’t answer, be willing to walk away.</p>  <p>If the company has recently had executives depart, see if you can get an understanding for why. This could be learned through mutual friends with the departed executive, or even chatting with them directly. Sometimes you’ll even have executives who interviewed you depart before, or shortly after, you join. You should absolutely reach out to them and understand the reasons for their departure.</p>  <p>As you work through these steps, ask yourself: Are you still excited? Have you explained your thinking about the role to at least two friends (who didn’t raise any concerns)? If the answer to these questions is yes, then take the job!</p> Not Getting the Job <p>You can’t talk about running an Engineering executive search without talking about not getting the job. Who doesn’t have a story of getting contacted by a recruiter who then ghosts them after an initial screen? A public company recently invited a friend of mine to interview in their CTO search. They got my friend very excited, and then notified them the next week that they had already tentatively filled the role. I’ve had first discussions with CEOs where we both immediately knew we wouldn’t be a good fit to work together. I’ve discussed roles where both I and the CEO wanted to move forward, but where I lacked a specific skill they felt was required to succeed (for example, deep experience in machine learning).</p><p>Although rejection isn’t fun, the perspective that I find helpful is: The goal of your search is not to find an executive job, but rather to find an executive job where you will thrive. It’s much better to realize a job isn’t the right fit for you before taking it, and each opportunity that doesn’t move forward is for the best.</p>   <p>Gergely again. I hope you have enjoyed this in-depth look into how to get a coveted engineering executive role, via The Engineering Executive’s Primer. </p><p><a href="https://learning.oreilly.com/library/view/the-engineering-executives/9781098149475/">Check out the full book</a></p><p>Many things Will discusses above are open secrets among engineering leaders; like that each executive role and hiring process is unique, and CTO roles at different companies often have vastly different processes, expectations, and compensation ranges!</p><p>Nearer the heights, compensation also becomes more negotiable; not only equity and salary, but also equity acceleration. That’s why, when negotiating a compensation package in a new country, it’s sensible to invest in tax advice about <a href="https://blog.pragmaticengineer.com/equity-for-software-engineers/">the equity component</a> from a local accountant or equity expert. I recently spoke with an engineering director in San Francisco who was offered a CTO role with generous equity in Germany; but equity taxation is quite different there and they were researching it, first.</p><p>The rest of this book keeps up the quality, with lots of behind-the-scenes insights. If you’ve gotten something from this excerpt, you’ll likely get even more from the rest. So, if you’re an engineering executive, or want to become one, I reckon <a href="https://learning.oreilly.com/library/view/the-engineering-executives/9781098149475/">The Engineering Executive’s Primer</a> is a very useful volume, as the author hoped for.&nbsp;</p><p>This week, there won’t be a new issue of The Pulse on Thursday, as <a href="https://newsletter.pragmaticengineer.com/about#%C2%A7publishing-schedule-and-holidays">I’m on spring break</a>. As of next Tuesday onward, things are back to the usual schedule. Thanks for your support!</p><p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc1d25fe-ab5a-47d3-8e3e-e3cd1f1003c6_1600x1200.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:01 &bull;
   28 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2024/05/07/burning-to-write-and-vice-versa/'>Burning to Write. And Vice Versa.</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Among all artists, writers alone suffer the illusion that the world needs to hear what they have to say. I thought that line, or something like it, came from Rollo May, probably in The Courage to Create. But a search within that book says no. ChatGPT and Gemini both tell me May didn’t say it […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<a href="https://flickr.com/photos/docsearls/822861493/"></a>I shot this photo in July 2007, while sitting with friends in that same booth at the Eagle and Child in Oxford.
<p>Among all artists, writers alone suffer the illusion that the world needs to hear what they have to say.</p>
<p>I thought that line, or something like it, came from Rollo May, probably in The Courage to Create. But <a href="https://www.google.com/books/edition/The_Courage_to_Create/Oj_Z1xImAwoC?hl=en&amp;gbpv=1&amp;bsq=writers">a search within that book</a> says no. <a href="https://chatgpt.com/c/cce36df1-9f54-4c89-8f0c-94cb65db2176">ChatGPT</a> and <a href="https://gemini.google.com/app/555816f12830ff6f">Gemini</a> both tell me May didn’t say it anywhere.</p>
<p>Still, I think it’s true that writers write because they have to. They can’t not write. It’s what they do, how they live.&nbsp;And I’m one of them.</p>
<p>The need to write is for me a vivid fact this morning, a few days past a <a href="https://en.wikipedia.org/wiki/Catheter_ablation">catheter ablation</a> on my heart. There is so much I need to say, because I suffer the illusion that the world needs to hear it. Does it?</p>
<p>I am sure <a href="https://en.wikipedia.org/wiki/C._S._Lewis">C.S. Lewis</a> and <a href="https://en.wikipedia.org/wiki/J._R._R._Tolkien">J.R.R. Tolkein</a> had the same affliction. I am also sure that the world is better for having read both of them, even if the world could have lived without their writing.</p>
<p>As for time, I have had twelve more years to write than Lewis got, so far, and five less than Tolkein.</p>
<p>Time to say what?</p>
<p>I want to say that <a href="https://projectvrm.org/2024/03/20/personal-ai-at-vrm-day-and-iiw/">personal AI</a> will do far more for all of us than what we will ever get from AI as a corporate service. And to say it better than I just did.</p>
<p>I want to say that we will do better with rulers who care about people than with rulers who merely rule. And to say that better than I just did.</p>
<p>I want to complete the work of <a href="https://en.wikipedia.org/wiki/John_McPhee">John McPhee</a> by reporting as best I can what has happened to the great characters that anchored every one of his essays and books. But that project is not on the back burner. It’s in the fridge, where I’ve kept it for decades (while continuing to read the entire McPhee oeuvre, much of it repeatedly).</p>
<p>Speaking of burning, I am impelled by <a href="https://en.wikipedia.org/wiki/Dylan_Thomas">Dylan Thomas</a>, who wrote “Do not go gentle into That Good Night,” before dying at just thirty-nine. The poem was for his father:</p>
<p>Old age should burn and rave at close of day,<br>
Rage, rage against the dying of the light.</p>
<p>And so I do.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://150108457.v2.pressablecdn.com/wp-content/uploads/2024/05/822861493_d11a8968e1_b.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   13:37 &bull;
   28 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@m-ruminer/'>Michael Ruminer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://m-ruminer.medium.com/let-me-save-you-some-time-on-azure-open-ai-and-copilot-109f30f0ac03?source=rss-7e85224c0a32------2'>Let Me Save You Some Time on Azure OpenAI And Copilot</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   I continue to develop my interest and some proficiency in AI. I am specifically focusing on Microsoft’s Copilot and Azure offerings. How did I end on that choice? I used to be a Microsoft MVP and also have a personal Visual Studio license that comes with $150 a month in Azure credits. So I have a long history in the Microsoft stack and tools plus some credits to play with each month. Those credits
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*H5VdOii6Aof68Je803Skkw.jpeg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I continue to develop my interest and some proficiency in AI. I am specifically focusing on Microsoft’s Copilot and Azure offerings. How did I end on that choice? I used to be a Microsoft MVP and also have a personal Visual Studio license that comes with $150 a month in Azure credits. So I have a long history in the Microsoft stack and tools plus some credits to play with each month. Those credits don’t go far when dealing with Azure AI pricing but I appreciate them. This last weekend I was going through an exercise I found on connecting up Azure AI search with Azure OpenAI layered on top, and then letting that be the foundation for a Copilot. You can find the excellent video by Lisa Crosbie <a href="https://www.youtube.com/watch?v=FmRmdCpnq8M&amp;t=591s">here</a>. This is a post on what I encountered that took me a while to overcome so that if you have a similar set of conditions you may spend less time to get&nbsp;running.</p><p>The crux of my arrangement and some heartache is the personal Visual Studio Azure subscription id; an emphasis on “personal”. This is where I have the monthly credits, but you need a business Microsoft account to use the Copilot&nbsp;Studio.</p><p>Here is the short end of it. If you can call this short. It’s complicated, what I did to get there, but doesn’t have to be too complicated for you. Here are the steps I went through so you don’t have&nbsp;to:</p>  I needed to apply for a Microsoft OpenAI Studio preview&nbsp;access.  I needed to use a non-personal email. No outlook.com, gmail&nbsp;etc.  Once approved the following day when trying to kick off a Copilot creation from the deployed OpenAi instance it asked me to log in and would only accept a business or school Microsoft account — my personal account wouldn’t&nbsp;work.  I created a new Microsoft business account by subscribing to Office 365 basic&nbsp;business  I tried to kick off the Copilot creation from the OpenAI deployment on my personal account and when it asked me to log in for the Copilot I put in my new business Microsoft account. It complained that it couldn’t connect with the Azure OpenAi deployment. Which made total sense- it was under a different account than the Copilot I was trying to create, but I had to try&nbsp;it.  So, I subscribed to Azure using that newly minted business account. It came with $200 credit the first&nbsp;month.  I tried to apply for OpenAI using the “business” email address the Azure subscription gave me when subscribing to Azure- a ruminio944.onmicrosoft.com domain.  It immediately denied me saying that the email address was a personal email account. I wasn’t expecting that.  I had a dormant domain sitting in GoDaddy (who doesn’t) and added it to my Azure account as a domain and set up a shared email for that domain. I now had factraft@factraft.com email address. I am sure I could have set up an email box directly on GoDaddy for the domain but this was more fun and a learning experience.  I now had a MS business login with an Azure subscription and $200 credit to play with and what should be considered a business&nbsp;email.  I applied for the OpenAI Studio Preview again this time using my new Azure subscription id and new factraft email. I expected it to be denied as all the other information such as name, address, website etc. was the same as I had already submitted and been approved for under my personal Azure subscription id. Surprisingly, the next day I received a welcome message for OpenAi Studio&nbsp;Preview.  I went through the video exercise in my new Azure account and most things worked as in the&nbsp;video.   The Lesson <p>To make it work you need the OpenAI Studio Preview which requires a business email address, a website, and an approved use case to request admission to the preview, and that is no guarantee of approval. You’ll need a business Microsoft account to log into in order to kick off the Copilot studio. Personal emails and a personal Microsoft account won’t cut it. I created a business Microsoft account by subscribing to Office 365 business basic for about $8. Then added an Azure instance to this with $200 credits for the first month. Then I was off to the races- mostly. I was able to make it all work for a mere mortal for the cost of about $8, the one month Office 365 subscription, and a partial days effort&nbsp;. All in all, not bad. I’ll make another write up on what I discovered in the creation itself. If just the right person(s) finds this trying to play with Azure OpenAI and Copilot studio then it might save them a fair amount of&nbsp;time.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*H5VdOii6Aof68Je803Skkw.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   12:14 &bull;
   28 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 06. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.patrick-breyer.de/computerspiele-sterben-piraten-fordern-kulturschutz-fuer-games/'>Computerspiele-Sterben: PIRATEN fordern Kulturschutz für Games</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   In Zusammenarbeit mit der Spielerinitiative „Stop killing games“, die sich gegen das Lahmlegen von Computerspielen durch ihre Hersteller wendet, haben der Europaabgeordnete der Piratenpartei Dr. Patrick … 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>In Zusammenarbeit mit der <a href="https://www.stopkillinggames.com/countries/germany">Spielerinitiative „Stop killing games“,</a> die sich gegen das Lahmlegen von Computerspielen durch ihre Hersteller wendet, haben der Europaabgeordnete der Piratenpartei <a href="https://www.patrick-breyer.de/wp-content/uploads/2024/05/P-9-2024-001352_DE.pdf">Dr. Patrick Breyer und ein Mitglied des Kulturausschusses eine Dringlichkeitsanfrage an die EU-Kommission gerichtet.</a> Hintergrund des Gamer-Protests ist die Entscheidung des französischen Computerspieleherstellers Ubisoft, das bis Dezember 2023 verkaufte Rennspiel „The Crew 1“ ab April 2024 unbenutzbar zu machen. Breyer erklärt:</p>



<p>„Wir wollen eine Stellungnahme der EU-Kommission dazu, ob Spielehersteller beliebte Spiele gewinnbringend verkaufen, sich dabei aber die willkürliche, jederzeitige Unbrauchbarmachung vorbehalten dürfen. Diese Praxis der Branche dürfte als ‚missbräuchliche Klausel in Verbraucherverträgen‘ illegal sein. Sie wird vor allem der kulturellen Bedeutung von Games nicht gerecht, die gewachsene Gemeinschaften von Millionen von Spielern zusammen bringen können. Wir Piraten fordern, dass die EU Games als Kulturgut schützt, ein Lahmlegen nach Gutdünken verbietet und, wenn der Hersteller Games aufgibt, ein Weiterführen durch die Community ermöglicht.“</p>



<p>Die Digitalexpertin und Spitzenkandidatin der Piratenpartei zur Europawahl Anja Hirschel ergänzt: „Aus Sicht der Spieler ist es geradezu unverschämt, ein Spiel anzubieten und dieses dann plötzlich vollständig ‚abzuwürgen‘. Eine gewisse Nutzungsdauer ab Kaufdatum muss garantiert werden. Daher ist dies auch eine Frage des Verbraucherschutzes.“</p>



<p>Breyer <a href="https://www.europarl.europa.eu/doceo/document/E-9-2024-001023_DE.html">hatte bereits vor einem Monat die EU-Kommission zur Einstellung von Computerspielen befragt, </a>wobei die Antwort aussteht. Die jetzt eingereichte Dringlichkeitsanfrage muss innerhalb von drei Wochen, also bis 16. Mai, beantwortet werden.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   11:31 &bull;
   29 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 03. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wrenchinthegears.com/2024/05/03/could-bobbys-tokenized-environmental-markets-really-be-about-using-web3-to-game-quantum-mechanics/'>Could Bobby’s Tokenized Environmental Markets Really Be About Using Web3 to Game Quantum Mechanics?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This will be my last post for a few weeks. During my trips down to Arkansas, I’ve been reading Gary Zukav’s 1979, “The Dancing Wu Li Masters,” a lay person’s guide to the “new” physics that emerged out of meetings he had at Esalen, home base for the human potential movement. I recognize there continue [...]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This will be my last post for a few weeks. During my trips down to Arkansas, I’ve been reading Gary Zukav’s 1979, “The Dancing Wu Li Masters,” a lay person’s guide to the “new” physics that emerged out of meetings he had at Esalen, home base for the human potential movement. I recognize there continue to be many questions about quantum mechanics. At this point, I am leaning towards Hugh Everett’s “Many World’s Theory” and John Wheeler’s&nbsp; “Participatory Universe.” I don’t claim to have any of this pinned down, and yet I wanted to share some thoughts I’ve been having on relativity, perception, spacetime, lattices, and construction of personalized “realities” using consciousness and information theory.</p>
<p></p>
<p>When Leo first started his work on Web3 tokenization of natural capital, he looked into the origins of the International Geophysical Year, use of radio isotopes to track energy transfer in ecosystems, and UNESCO’s “Man and the Biosphere” (Noosphere?) programme. I am starting to think that the focus on One Health, climate, and the UN Sustainable Development Goals could actually be about relativity – employing digital identity and Web3 to situate individual representations of consciousness within webs of relationship over time. That data could then be used not only to discern patterns but to incorporate game mechanics towards some form of networked collective manifestation.</p>
<p>Individual experiences of the ant computer would be shaped by each participant’s environmental context. Through gamification and spatial computing we, as agents, would emit signals that could be used to coordinate a vast sociotechnical computational system of domesticated consciousness. But first, the system would need to know where each of us is at a given time, what information flows we have consumed, what our perception of reality is, and how best to use the archetypal fields that surround us to nudge us towards pre-programmed goal-oriented behaviors, how to optimize the agents with the least amount of friction. Go with the flow….</p>
<p>Thinking of carbon-credit behaviors in this way – as gamified agent signaling within fractal, nested complex systems – has some legs I think. In any event, it is a much more interesting scenario than Klaus wants to turn you into a puppet, take all your things, and make you eat bugs, right?</p>
<p>So here is the video I made last night. </p>
<p></p>
<p>It will be interesting to watch as the RFK Jr. campaign tries to weave together the split personalities of Market-Based Environmental Solutions Bobby and Health FreeDOM Bobby. Liam Sturgess’s initial attempt was rather weak. </p>
<p><a href="https://thekennedybeacon.substack.com/p/has-the-national-resources-defense"></a></p>
<p></p><a href="https://www.depauw.edu/news-media/latest-news/details/19648/"></a><a href="https://www.depauw.edu/news-media/latest-news/details/19648/">https://www.depauw.edu/news-media/latest-news/details/19648/</a><p></p>
<p>I expect a lot more information flows will need to pour forth in order to create a narrative compelling enough to get people to sign on to big data, AI for good green progress. Stay tuned.</p>
<p>This is the excerpt I read. Below it are screenshots of the featured maps if you’d like to explore further. It’s quite fascinating to see how the various threads spread out over many maps have come together around the Camelot candidate.</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p><a href="https://www.youtube.com/watch?v=9HDRqLpeCaA"></a></p>
<p></p><a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/montecarlo.png?ssl=1"></a>Interactive Map: <a href="https://embed.kumu.io/10176e997f243e67277e7d615ba4bbb1#untitled-map?s=bm9kZS1HYVJhVVpTNA%3D%3D">https://embed.kumu.io/10176e997f243e67277e7d615ba4bbb1#untitled-map?s=bm9kZS1HYVJhVVpTNA%3D%3D</a><p></p>
<p></p><a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/ostrolenk.png?ssl=1"></a>Interactive Map: <a href="https://embed.kumu.io/45f342eb350f7b4c707484423f71d6ff#untitled-map?s=bm9kZS1XWmNzcFV0Mg%3D%3D">https://embed.kumu.io/45f342eb350f7b4c707484423f71d6ff#untitled-map?s=bm9kZS1XWmNzcFV0Mg%3D%3D</a><p></p>
<p></p><a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/noosphere.png?ssl=1"></a>Interactive Map: <a href="https://embed.kumu.io/c9645fc5fbabfed02dd66e0a0aea9905#untitled-map?s=bm9kZS05N3prSzlrVg%3D%3D">https://embed.kumu.io/c9645fc5fbabfed02dd66e0a0aea9905#untitled-map?s=bm9kZS05N3prSzlrVg%3D%3D</a><p></p>
<p></p>
<p></p>
<p></p><a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/wellness.png?ssl=1"></a>Interactive map: <a href="https://embed.kumu.io/ddfc18d01ccaffb4db6b8ac0711ec30a">https://embed.kumu.io/ddfc18d01ccaffb4db6b8ac0711ec30a</a><p></p>
<p></p><a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/NRDC.png?ssl=1"></a>Interactive Map:<a href="https://embed.kumu.io/a8d369aeea2eb819d4961542e0e3f91c"> https://embed.kumu.io/a8d369aeea2eb819d4961542e0e3f91c</a><p></p>
<p></p><a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/natural-capital.png?ssl=1"></a>Interactive Map: <a href="https://embed.kumu.io/2f5210a439fe8343ae208a20c70d83fc">https://embed.kumu.io/2f5210a439fe8343ae208a20c70d83fc</a><p></p>
<p></p><a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/florida.png?ssl=1"></a>Interactive Map: <a href="https://embed.kumu.io/d52b8a5f6b97bfde04624f29a903ef4c">https://embed.kumu.io/d52b8a5f6b97bfde04624f29a903ef4c</a><p></p>
<p></p><a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/ross.png?ssl=1"></a>Interactive Map: <a href="https://web.archive.org/web/20210327185247/https://littlesis.org/oligrapher/6635-metronome">https://web.archive.org/web/20210327185247/https://littlesis.org/oligrapher/6635-metronome</a><p></p>
<p></p><a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/new-york.png?ssl=1"></a>Interactive Map: <a href="https://embed.kumu.io/0712f3e054d2055302b00b258e24e1c0#untitled-map?s=bm9kZS1TRkFROG0xRw%3D%3D">https://embed.kumu.io/0712f3e054d2055302b00b258e24e1c0#untitled-map?s=bm9kZS1TRkFROG0xRw%3D%3D</a><p></p>
<p><a href="https://jppollak.com/work"></a></p>
<p><a href="https://scottsdaleinstitute.org/team-details/jp-pollak/"></a></p>
<p><a href="https://littlesis.org/org/388679-Vera_Whole_Health"></a></p>
<p><a href="https://littlesis.org/person/196899-Deborah_Estrin"></a></p>
<p><a href="https://littlesis.org/org/350869-Embedded,_Everywhere:_A_Research_Agenda_for_Networked_Systems_of_Embedded_Computers_(2001)"></a></p>
<p><a href="https://www.openhealthnews.com/tagged/deborah-estrin"></a></p>
<p><a href="https://web.archive.org/web/20200925091209/https://littlesis.org/oligrapher/4954-blockchain-healthcare-challenge-2016"></a></p>
<p><a href="https://www.youtube.com/watch?v=ibilt7K-ShE"></a></p>
<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/IMG_5339.jpg?resize=768%2C1024&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:10 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2024/05/02/world-wide-whiteboard/'>World Wide Whiteboard</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Before there were search engines, there were directories. The biggest and best-known was Yahoo. On the first graphical browser (Mosaic), it looked like this: The directory idea made sense, because the Web is laid out like the directory in your computer. There is a “domain” with a “location” or a “site,” containing something after the […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

We thought the Web was a library. Turns out it’s not. (Image via ChatGPT 4o.)
<p>Before there were search engines, there were directories. The biggest and best-known was Yahoo. On the first graphical browser (Mosaic), it looked like this:</p>
<p><a href="https://www.thismarketerslife.it/site/wp-content/uploads/2016/11/yahoo-nel-1994.png"></a></p>
<p>The directory idea made sense, because the Web is laid out like the directory in your computer. There is a “domain” with a “location” or a “site,” containing something after the last / in a path of /something/something/something. Geeks call these directories too, and the string of somethings a path. Directories in computing also follow an outline format: heads, subheads, and subheads of subheads.</p>
<p>While this design is boundlessly flexible, it also suggests durability, if not permanence, because it’s good to find stuff where it rightly goes and to find it in the same place over and over again.</p>
<p>That was what Yahoo assumed in the early days of the Web—as did everyone who bought a domain name. I’ve had <a href="http://searls.com">searls.com</a> since 1995. <a href="https://en.wikipedia.org/wiki/Dave_Winer">Dave Winer</a> (father of <a href="https://en.wikipedia.org/wiki/Dave_Winer#Early_work_in_outliners">outlining</a> and progenitor of much else) has had <a href="https://scripting.com">Scripting.com</a> for even longer (and has a lot more in that domain).</p>
<p>But we don’t own domain names. We rent them. And the World Wide Web isn’t a library. It’s a whiteboard with stuff written on it. Some of that stuff is located on directory paths. A lot more is coughed up by database systems on an as-needed basis.</p>
<p>The Yahoo directory failed. In its place search engines appeared. Search engines don’t catalog the Web like a library might. They index it. That means they send crawlers down the Web’s directory paths, recording everything they see into a searchable index. I explain <a href="https://doc.searls.com/2021/08/15/beyond-the-web/">here</a> how that works and where this went:</p>
<blockquote><p></p>
<p>The Web is a haystack.</p>
<p>This isn’t what <a href="https://customercommons.org/a-new-way/">Tim Berners-Lee</a> had in mind when he invented the Web. Nor is it what Jerry Yang and David Filo had in mind when they invented&nbsp;<a href="https://money.cnn.com/gallery/technology/2015/03/03/yahoo-20-years-hits-flops/10.html">Jerry and David’s Guide to the World Wide Web</a>, which later became&nbsp;<a href="https://en.wikipedia.org/wiki/Yahoo!">Yahoo</a>. Jerry and David’s model for the Web was a library, and Yahoo was to be the first catalog for it. This made sense, given the prevailing conceptual frames for the Web at the time:&nbsp;real estate and publishing.</p>
<p>Both of those are still with us today. We frame the Web as real estate when we speak of “sites” with “locations” in “domains” with “addresses” you can “visit” and “browse”—then shift to publishing when we speak of “files” and “pages,” that we “author,” “edit,” “post,” “publish,” “syndicate” and store in “folders” within a “directory.” Both frames suggest durability if not permanence. Again, kind of like a library.</p>
<p>But once we added personal movement (“surf,” “browse”) and a vehicle for it (the browser), the Web became a World Wide Free-for-all. Literally. Anyone could publish, change and remove whatever they pleased, whenever they pleased. The same went for organizations of every kind, all over the world. And everyone with a browser could find their way to and through all of those spaces and places, and enjoy whatever “content” publishers chose to put there. Thus the Web grew into billions of sites, pages, images, databases, videos, and other stuff, with most of it changing constantly.</p>
<p>The result was a heaving heap of fuck-all.*</p></blockquote>
<p>Back in 2005, I <a href="https://www.linuxjournal.com/article/8549">wrote in Linux Journal about a split</a> between the “static” Web that was like a library (with its “locations,” “sites,” and “domains” you could “visit” and “browse”), and the “live” Web of blogs and posts. Then social media came along, and the live branch of the Web outgrew the static Web’s trunk.</p>
<p>Last week <a href="https://www.theverge.com/2024/5/28/24166177/google-search-ranking-algorithm-leak-documents-link-seo">came news</a> that a leak revealed lots of interesting poop about how Google actually ranks search results. Here are two things I don’t need those leaked documents to tell me:</p>
 
 Google favors the present over the past, the current over the archival. 
 Google no longer indexes, or ranks very old Web pages. 
 
<p>I speak from experience here, because I have some old pages that are on the Web but don’t seem to be indexed, meaning searches don’t find them. I also have Easter eggs on a couple of those pages: words that exist in no language but made those pages easy to find when I did keyword searches for them. Now I get “No results found for _____.” (I won’t reveal the word because I want to keep testing Google.)</p>
<p>Countless publications have also come and gone on the Web without leaving a trace. Upside was a gigantic publication from the Nineties through the dotcom boom. Not a trace of it remains. Far as I know, nothing remains of Fast Company‘s early issues.</p>
<p>But hey, God bless the <a href="https://archive.org/">Internet Archive</a>. <a href="https://archive.org/details/PC-Mag-1982-12/page/n53/mode/2up?q=searls">Here’s a piece</a> I wrote for&nbsp;<a href="https://archive.org/details/PC-Mag-1982-12/mode/2up?q=searls">PC Magazine in December 1982</a> about a PC application that taught card counting in blackjack:</p>
<p></p>
<p>As the evanescence of “content” increases, so does the importance of archives.</p>
<p>So maybe stop reading here and start reading <a href="https://doc.searls.com/2024/04/21/archives-as-commons/">here</a>. We have a lot of work to do.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://150108457.v2.pressablecdn.com/wp-content/uploads/2024/04/world-wide-whiteboard1.webp'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   01:15 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 02. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://aaronparecki.com'>Aaron Parecki</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://aaronparecki.com/2024/05/02/5/oauth-browser-based-apps-last-call'>OAuth for Browser-Based Apps Working Group Last Call!</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The draft specification OAuth for Browser-Based Applications has just entered Working Group Last Call!
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The draft specification <a href="https://datatracker.ietf.org/doc/html/draft-ietf-oauth-browser-based-apps">OAuth for Browser-Based Applications</a> has just entered Working Group Last Call!</p>

<p><a href="https://datatracker.ietf.org/doc/html/draft-ietf-oauth-browser-based-apps">https://datatracker.ietf.org/doc/html/draft-ietf-oauth-browser-based-apps</a></p>

<p>This begins a two-week period to collect final comments on the draft. Please review the draft and <a href="https://www.ietf.org/mailman/listinfo/oauth">reply on the OAuth mailing list</a> if you have any comments or concerns. And if you've reviewed the document and are happy with the current state, it is also extremely helpful if you can reply on the list to just say "looks good to me"!</p>

<p>If joining the mailing list is too much work, you're also welcome to comment on the <a href="https://github.com/oauth-wg/oauth-browser-based-apps/issues/51">Last Call issue</a> on GitHub.</p>

<p>In case you were wondering, yes your comments matter! Even just a small indication of support goes a long way in these discussions!</p>

<p>I am extremely happy with how this draft has turned out, and would like to again give a huge thanks to <a href="https://pragmaticwebsecurity.com/about.html">Philippe De Ryck</a> for the massive amount of work he's put in to the latest few versions to help get this over the finish line!</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   22:06 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wrenchinthegears.com/2024/05/02/austins-learn-capital-takes-a-field-trip-to-the-vatican-to-talk-entrepreneurial-social-impact/'>Austin’s Learn Capital Takes A Field Trip to the Vatican To Talk Entrepreneurial Social Impact</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Late last April, Lynn Davenport and I recorded a conversation unpacking the visit Evan Baehr, managing partner and social impact investor with Learn Capital, made to the Vatican in the fall of 2022 for the Humanity 2.0 conference hosted in partnership with Harvard University. I think it is useful to consider Humanity 2.0 as an [...]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Late last April, Lynn Davenport and I recorded a conversation unpacking the visit Evan Baehr, managing partner and social impact investor with Learn Capital, made to the Vatican in the fall of 2022 for the <a href="https://www.humanity2-0.org/">Humanity 2.0 conference</a> hosted in partnership with Harvard University. I think it is useful to consider Humanity 2.0 as an extension of the eugenics (optimization) movement that became aligned with the Human Potential Movement in the 1970s and is now merging with quantified-self “wellness management” and alt-health biohacking. </p>
<p>The social science division of the Catholic Church appears to be totally onboard with the idea of aligning big data impact investing&nbsp;with preventative care (cue Bobby’s “<a href="https://www.youtube.com/watch?v=Ekpzkap31Yk">chronic health</a>” epidemic plans) using their far-reaching health system infrastructure. You can explore overlaps between Social Finance’s Ronald Cohen and Rome <a href="https://wrenchinthegears.com/2018/09/21/finding-the-holy-grail-in-poverty-mining-sir-ronald-cohen-visits-harvard-and-the-vatican/">here</a>, <a href="https://wrenchinthegears.com/2019/06/21/doctrine-of-discovery-redux-the-vaticans-plans-for-impact-investing/">here</a>, <a href="https://wrenchinthegears.com/2019/01/04/charter-public-health-and-catholic-charity-interests-help-launch-disruptive-pay-for-success-program/">here</a>, and <a href="https://wrenchinthegears.com/2021/04/12/revocation-of-consent-project-home-and-pay-for-success-housing-in-philadelphia/">here.</a> In particular, note <a href="https://wrenchinthegears.com/2022/06/21/market-alchemy-and-illuminated-wellbeing-synthetic-pretenders-part-8/">this post on the role of Providence Health</a>, a Catholic Health System, in the early days of Covid.</p>
<p><a href="https://www.fiercehealthcare.com/tech/providence-rolls-bluetree-lumedic-into-new-technology-and-services-company"></a></p>
<p><a href="https://vaticanconference2021.org/"></a></p>
<p>Lumedic = light. Note the lighting in the feature image.</p>
<p></p><a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/lumedic.png?ssl=1"></a>Interactive Map: <a href="https://embed.kumu.io/052a2ea893a8f845488a6f5b14515d4d#untitled-map?s=bm9kZS1iMzZHYXkwaw%3D%3D">https://embed.kumu.io/052a2ea893a8f845488a6f5b14515d4d#untitled-map?s=bm9kZS1iMzZHYXkwaw%3D%3D</a><p></p>
<p>We need to understand that networked biology and social gaming will be at the core of light-based, post-Moore’s law computing. Such a system will combine non-traditional interfaces, cyber-physical systems (including nanotechnology and frequency transmission), and spatial computing. In such a system our participation, through the perpetual signaling of our “choices,” will dynamically reshape the “game,” though we may not ever truly understand what the ultimate goal of the game is due to the deployment of sophisticated “knowledge management” techniques. </p>
<p>In my opinion, digital education linked to biometric monitoring is an essential part of a plan that aims to harness embodied group consciousness for collective problem-solving and/or pattern identification, possibly within the context of an abstracted mathematical / metaphysical spacetime continuum the general public cannot readily comprehend. </p>
<p>Networked consciousness, De Chardin’s noosphere, is the goal. Widespread deployment of a “free” “open” global ed-tech protocol layer will hasten the planned evolutionary trajectory using Google’s Lamarckian “selfish ledgers.” Social Impact will be used to justify ongoing monitoring and management of key performance indicators, ubiquitous transmission parsed by machine learning within an ever evolving computational “game of life” where Earth’s beings are remade as agents in an infinitely complex cellular automata program. That’s something surely Nicole Shanahan, an alumna of Santa Clara University (lead in the social impact deployment space) and Sergey Brin’s ex would know about, right?</p>
<p><a href="https://web.archive.org/web/20201126132214/https://littlesis.org/oligrapher/5061-google-2016-selfish-ledger-thought-experiment"></a></p>
<p>Shortly after we recorded it, life got complicated, and the clips got put on the back burner. This week I pulled everything off the shelf, blew off the dust, did some editing and found the content surprisingly current with respect to the manufactured school choice wars, funded by the likes of free market options traders like Jeff Yass (see maps below) as well as the Catholic undertones of Bobby’s presidential campaign. It’s the fourth installment in a series on the roll out of Open Education Resources (aka digital playlist education with competency badges for global gig labor). </p>
<p>Here are links to the other installments if you want context for this one:&nbsp;</p>
<p>OER in Texas: Building Digital Twins for a Global Gig Economy: <a href="https://www.youtube.com/watch?v=vVR9PNgRGj4">Here</a></p>
<p>VR Apprenticeships, Income Sharing Agreements and the Omega Point: <a href="https://www.youtube.com/watch?v=NTE-3Nr_1lo">Here</a></p>
<p>Learn Capital Comes to Austin: <a href="https://www.youtube.com/watch?v=dM7BfwVoha0">Here</a></p>
<p>&nbsp;</p>
<p></p>
<p>&nbsp;</p>
<p><a href="https://docs.google.com/presentation/d/1734ywaVc260oLT9nrGonbxUR4-HzZqH1brCopeuIbG4/edit?usp=sharing">OER Slide Deck</a> – Presentation Starts At Slide 98.</p>
<p><a href="https://docs.google.com/presentation/d/1734ywaVc260oLT9nrGonbxUR4-HzZqH1brCopeuIbG4/edit?usp=sharing"></a></p>
<p></p><a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/yass-1.png?ssl=1"></a>Interactive Map: <a href="https://embed.kumu.io/91af355393887ce4efd9c139cf0d69d4#untitled-map?s=bm9kZS1LeHFNU2Zkcg%3D%3D">https://embed.kumu.io/91af355393887ce4efd9c139cf0d69d4#untitled-map?s=bm9kZS1LeHFNU2Zkcg%3D%3D</a><p></p>
<p>&nbsp;</p>
<p></p><a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/dallas.png?ssl=1"></a>Interactive Map: <a href="https://embed.kumu.io/cbaaef670490292b6d28fd89a578a55c">https://embed.kumu.io/cbaaef670490292b6d28fd89a578a55c</a><p></p>
<p>Learn Capital, launched by <a href="https://www.utahbusiness.com/powder-mountain-could-become-the-entrepreneurial-mecca-of-utah/">Greg Mauro on Powder Mountain outside Salt Lake City</a>, <a href="https://www.youtube.com/watch?v=NwzIWgkzKrA">opened their Austin campus last year</a> and has been an influential player in promoting adoption of digital education and blockchain / web3 technology in the Lone Star state. Lynn and I have a lot more to talk about with respect to Texas, classical antiquity, and archetypal morphic fields. Check out this years Garden’s of Greece Dallas Arboretum benefit! A tribute to Athena, goddess of wisdom and the smart city?&nbsp;</p>
<p><a href="https://www.papercitymag.com/society/mad-hatters-tea-luncheon-dallas/"></a></p>
<p>See <a href="https://wrenchinthegears.com/2022/07/12/mondragon-focolare-and-inclusive-capitalism-synthetic-pretenders-part-15b/">this post</a> on Mondragon and the Catholic Church’s Focolare movement for a model of the freeDOM cell / intentional community collective that I suspect will be the model for the ant computer system.&nbsp;</p>
<p><a href="https://wrenchinthegears.com/2022/07/12/mondragon-focolare-and-inclusive-capitalism-synthetic-pretenders-part-15b/"></a></p>
<p></p><a href="https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/athena.png?ssl=1"></a>Interactive Map: <a href="https://embed.kumu.io/76cd68ced810f82da69e19f2dca42b47">https://embed.kumu.io/76cd68ced810f82da69e19f2dca42b47</a><p></p>
<p><a href="https://www.humanity2-0.org/"></a></p>
<p></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/05/lum.png?resize=913%2C1024&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:17 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-92'>The Pulse #92: GitHub’s AI-assisted developer workflow vision</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Also: Google laying off engineers as their business is booming; a good period for startup fundraising and IPOs; and how WorkOS acquired devtools startup Warrant
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Programming note: next week, I’ll be <a href="https://newsletter.pragmaticengineer.com/about#%C2%A7publishing-schedule-and-holidays">on spring break</a>, spending time in Florida, visiting my in-laws. This will mean no The Pulse on Thursday: but there will still be a Tuesday article, as usual. Thank you for your understanding and support!</p><p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering insights, patterns, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Today, we cover:</p>  <p>Industry pulse. Cloudflare handles an extra 12M req/sec with no issues, how does being a PM at Google or Meta compare, and are nearly 1 in 10 of all developers worldwide using GitHub Copilot? And more.</p>  <p>GitHub’s AI-assisted developer workflow vision: Copilot Workspace. Several startups are aiming to build an AI coding tool that goes beyond GitHub Copilot. With exceptionally fast execution, GitHub seems to have beaten them to it. It’s called GitHub Copilot Workspace, and doesn’t aim to replace devs. A review of this developer-driven, AI-assisted workflow, including insider details from the GitHub team.</p>  <p>Google lays off devs while business booms. Google made layoffs in its Python and Flutter platform teams, right as the company’s stock price hit an all-time high.&nbsp;</p>  <p>Good time for tech scaleups fundraising and IPOs. It’s been a while since we’ve had so much positive news on tech IPOs, and late and early-stage fundraising. Hopefully, this trend continues!</p>  <p>Platform scaleup buys feature startup: WorkOS acquires Warrant. Warrant is an innovative fine-grained access control seed-stage devtools startup, built on the principles of Google’s authorization system. In what looks like a win-win outcome, WorkOS has acquired it, intending to keep intact the team, product, and all existing customers. Details on this acquisition from the founder and CEO of WorkOS.</p>   1. Industry pulse  Cloudflare doesn’t notice 12M extra req/sec <p>Cloudflare engineering director Benjamin Yule shared an interesting non-event:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9feceb27-e71f-449a-a4bd-f54327a710a2_1080x354.png">    </a> <p>I have so many questions as 12.5 million requests per second is a huge load! A few thousand requests per second is usually considered high, and handling a load in the millions surely requires thousands of servers (if not more!) to be spun up to handle it.</p><p>How much load a single server can handle depends on how long a request takes to handle, how much memory a request uses, and then calculating whether a request is limited in CPU or memory. Whichever resource (CPU capacity or memory) runs out first determines a server’s maximum load. You can push things pretty far with clever workarounds; it’s how the WhatsApp team <a href="https://web.archive.org/web/20120709235656/http://www.erlang-factory.com/upload/presentations/558/efsf2012-whatsapp-scaling.pdf">served 2 million requests per second</a> on a server with 24 cores and 100GB RAM, back in 2012!</p><p>In this case, Cloudflare soaked up the load by using <a href="https://developers.cloudflare.com/workers/">Cloudflare Workers</a>, a serverless application platform, which <a href="https://x.com/bjyule/status/1784172632910283064">spun up</a> 40,000 CPU cores (an average of 320 requests/second per CPU core.) The tweet says the Cloudflare Workers team didn’t even notice the increase, which suggests they could be handling 50-100x more load on the service! If we assume a 100x load, that could mean operating 4 million CPU cores or more simultaneously, just for Workers. By comparison, we previously covered travel booking service Agoda <a href="https://newsletter.pragmaticengineer.com/i/124842294/agoda-in-numbers">operating</a> a total of 300,000 physical cores and 600,000 virtual ones. It’s likely Cloudflare is operating more than 10x this number.</p> Do nearly 1 in 10 developers use GitHub Copilot? 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-92">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9feceb27-e71f-449a-a4bd-f54327a710a2_1080x354.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   13:17 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.patrick-breyer.de/der-pirat-o-mat-onlineanwendung-zum-vergleich-der-politischen-meinung-mit-echten-abstimmungen-im-eu-parlament-startet/'>Der Pirat-o-mat: Onlineanwendung zum Vergleich der politischen Meinung mit echten Abstimmungen im EU-Parlament startet</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Mit dem Pirat-o-mat (piratomat.de) ist ab sofort ein innovatives Tool verfügbar, das Nutzern dabei hilft, ihre Standpunkte zu Themen wie Netzpolitik, digitale Rechte und Demokratie … 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Mit dem <a href="https://www.piratomat.de/">Pirat-o-mat (</a><a href="https://www.piratomat.de/">piratomat</a><a href="https://www.piratomat.de/">.de)</a> ist ab sofort ein innovatives Tool verfügbar, das Nutzern dabei hilft, ihre Standpunkte zu Themen wie Netzpolitik, digitale Rechte und Demokratie zu entwickeln und zu vergleichen. Die Besonderheit: Im Quiz sind 12 Fragen zu beantworten, die im EU-Parlament seit 2019 tatsächlich zur Abstimmung standen. In der Auswertung erfährt der Nutzer seine Übereinstimmung mit dem Abstimmungsverhalten des Europaabgeordneten der Piratenpartei und der anderen deutschen Europaabgeordneten.</p>



<p>Doch der Piratomat bietet mehr als nur einen einfachen Vergleich: Geordnet nach Parteien und Themengebieten ist potenziell problematisches Abstimmungsverhalten zusammengestellt. Der Schwerpunkt der ausgewählten Abstimmungen liegt auf Netzpolitik, aber auch die Themen Gesundheit, Demokratie, Umweltschutz und Transparenz sind vertreten.</p>



<p>Der EU-Abgeordnete der Piratenpartei Dr. Patrick Breyer, der die Website bereit stellt, erläutert:</p>



<p>“Als Normalbürger bekommt man in aller Regel nichts davon mit, wer in Brüssel wie abstimmt und ob das Abstimmungsverhalten den Versprechen einer Partei gerecht wird. Weil das EU-Parlament seine Abstimmungsaufzeichnungen maximal unverständlich macht, sorgen unsere Abstimmungsgrafiken für Transparenz. Der Pirat-o-mat wird viele Nutzer überraschen, manche verärgern und hoffentlich auch zu Nachfragen und Gesprächen führen. Unsere Demokratie lebt von Transparenz, Beteiligung, Glaubwürdigkeit und Rechenschaft – der Pirat-o-mat ist unser Beitrag dazu.”</p>



<p><a href="https://www.piratomat.de/">Zum Pirat-o-maten</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   09:55 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 01. May 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/internet-identity-workshop-xxxviii'>Internet Identity Workshop XXXVIII Report</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  We recently completed the 38th edition of the Internet Identity Workshop. We had 330 people from around the world who called 169 sessions. As usual there was lots of energy and thousands of side conversations. IIW is a place to get things done and it showed in the excitement in the rooms and the comments people made to me about how much they enjoyed it.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cd57843-c1e8-4c08-979d-a3d931b78a3d.heic">    </a> <p>We recently completed the 38th edition of the <a href="https://internetidentityworkshop.com/">Internet Identity Workshop</a>. We had 330 people from around the world who called 169 sessions. As usual there was lots of energy and thousands of side conversations. IIW is a place to get things done and it showed in the excitement in the rooms and the comments people made to me about how much they enjoyed it.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb52da8a8-6f5f-45c6-9a4a-5a2a2326c092.heic">    </a>Opening on Tuesday <p>As you can see by the pins in the map at the top of this post, there were attendees from all over the world. Not surprisingly, most of the attendees were from the US (241), followed by Canada (11). Germany, India, and Switzerland rounded out the top five with 9, 8, and 7 attendees respectively. Attendees from India (5), Thailand (3), and Korea (3) showed IIW's diversity with attendees from APAC. And there were 4 attendees from South America this time. Sadly, there were no attendees from Africa again. Please remember we offer scholarships for people from underrepresented areas, so if you'd like to come to IIW39, please let us know. If you're working on identity, we want you there.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4ab3829-4239-404a-8338-c603a147ddca.heic">    </a>Session on Personal AI <p>For states and provinces, California was first with 122. Washington (16), Utah (10), Texas (10) and New York (10) rounded out the top five. San Francisco (14) Oakland (13), San Jose (12), Seattle (11), and New York (9) were the top cities.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf9d0ac0-dd58-4f87-910c-dbf9ecfa792b.heic">    </a>Demo Hour on Wednesday <p>In addition to sessions, we have a demo hour on Wednesday that is a little like speed dating. There were 20 different projects highlighted. </p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23cd0b52-0761-4e82-a8ec-dfbfdda14165.heic">    </a>Drummond Reed hosts a session <p>There's always more than one session that I want to attend in any given time slot and choosing is hard. That's a common refrain. Luckily we have sessions notes that we publish in a <a href="https://internetidentityworkshop.com/past-workshops/">Book of Proceedings</a>.  You can find additional photos from Doc Searls: <a href="https://www.flickr.com/photos/docsearls/albums/72177720316609417/">Day 1</a> and <a href="https://www.flickr.com/photos/docsearls/albums/72177720316577534/">Day 2</a>. </p><p>IIW is where you will meet people to help you solve problems and move your ideas forward. Please come! IIW 39 will be held October 29-31, 2024 at the Computer History Museum. We'll have tickets available soon.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cd57843-c1e8-4c08-979d-a3d931b78a3d.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:57 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 30. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.patrick-breyer.de/piraten-zur-vorratsdatenspeicherung-wo-ein-trog-ist-sammeln-sich-die-schweine/'>Piraten zur Vorratsdatenspeicherung: Wo ein Trog ist, sammeln sich die Schweine</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Der Europäische Gerichtshof hat heute seine Rechtsprechung zur Vorratsdatenspeicherung aufgeweicht und die Nutzung auf Vorrat gespeicherter Internet-Verbindungsdaten schon zur Verfolgung von Filesharing zugelassen. Der Europaabgeordnete der Piratenpartei und langjährige Gegner … 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Der Europäische Gerichtshof hat heute seine Rechtsprechung zur Vorratsdatenspeicherung aufgeweicht und die Nutzung auf Vorrat gespeicherter Internet-Verbindungsdaten schon zur Verfolgung von Filesharing <a href="https://curia.europa.eu/juris/document/document.jsf?text=&amp;docid=285361&amp;pageIndex=0&amp;doclang=DE&amp;mode=req&amp;dir=&amp;occ=first&amp;part=1">zugelassen</a>. Der Europaabgeordnete der Piratenpartei und langjährige Gegner der Vorratsdatenspeicherung Dr. Patrick Breyer fordert:</p>



<blockquote>
<p>„Zuerst hat der EuGH zur Verfolgung von Kindesmissbrauchsdarstellungen und anderer schwerer Straftaten eine flächendeckende Vorratsdatenspeicherung über die Internetverbindungen der gesamten Bevölkerung erlaubt. Ab heute soll unsere Internetnutzung aber schon zur Verfolgung von Filesharing und anderer Bagatelldelikte offengelegt werden. Wo ein Trog ist, sammeln sich die Schweine. Deswegen müssen wir alles daran setzen, die grenzenlose Datengier der Konservativen und Sozialdemokraten zu stoppen. Obwohl Deutschland Internetdelikte auch ohne IP-Vorratsdatenspeicherung sehr erfolgreich aufklärt, arbeitet die EU im Verborgenen bereits daran uns nach der Europawahl wieder eine Internet-Vorratsdatenspeicherung aufzuzwingen. Wir Piraten werden für unser Recht auf anonyme Internetnutzung kämpfen!</p>



<p>IP-Vorratsdatenspeicherung ist, wie wenn jede:r Bürger:in ein sichtbares Kennzeichen um den Hals gehängt bekäme und dieses auf Schritt und Tritt notiert würde. Niemand würde sich eine solche Totalerfassung des täglichen Lebens gefallen lassen. IP-Vorratsdatenspeicherung würde jeden Internetnutzer unter Generalverdacht stellen und die Internetnutzung der gesamten Bevölkerung, die unsere intimsten Vorlieben und Schwächen abbildet, nachvollziehbar machen. Eine so totale Erfassung würde Kriminalitätsvorbeugung durch anonyme Beratung und Seelsorge, Opferhilfe durch anonyme Selbsthilfeforen und auch die freie Presse gefährden, die auf anonyme Informanten angewiesen ist. 99,99 Prozent dieser Daten wären völlig nutzlos, da sie Bürger:innen betreffen, die nie auch nur in den Verdacht einer Straftat kommen.“</p>
Dr. Patrick Breyer</blockquote>



<p>Hintergrund: Der Vorsitzende von CDU/CSU im EU-Parlament Weber <a href="https://www.phoenix.de/csu-parteitag-a-4421582.html">forderte</a> am Samstag eine EU-weite IP-Vorratsdatenspeicherung. Unterdessen arbeitet die sog. #EUGoingDark-Arbeitsgruppe im Auftrag der EU-Kommission und EU-Regierung entsprechende Pläne aus.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   20:31 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/security-engineering-part-2'>What is Security Engineering? Part 2.</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A broad overview of the ever-developing security engineering field; a domain that can feel intimidating to some software engineers. With Nielet D'Mello, security engineer at Datadog. Part 2.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>👋 Hi, this is Gergely with a subscriber-only issue of the Pragmatic Engineer Newsletter. In every issue, I cover challenges at Big Tech and startups through the lens of engineering managers and senior engineers. To get articles like this in your inbox, every week, subscribe:</p><p><a href="https://newsletter.pragmaticengineer.com/subscribe?">Subscribe now</a></p><blockquote> Q: “As a software engineer, I’d like to learn more about security engineering. What’s a good way to understand this vast field?” </blockquote><p>This is the second and final part of exploring this important – and, yet, often intimidating! – topic of security engineering. Giving us an overview of this field is <a href="https://www.linkedin.com/in/nieletdmello">Nielet D'Mello</a>: a security engineer at Datadog (previously at Intel and McAfee).</p><p><a href="https://newsletter.pragmaticengineer.com/p/security-engineering">In Part 1</a> we already covered:</p>  <p>Myths and misconceptions about security engineering</p>  <p>History of security engineering</p>  <p>The present</p>  <p>A mental model: seven core dimensions to think about application security</p>  <p>Towards a secure software development lifecycle (SDLC).</p>  <p>In today’s issue, Nielet takes us through:</p>  <p>Defining the criticality of a system. Security dimensions to consider as we talk about a service or systems’s criticality.</p>  <p>Scoring a system’s criticality. The “napkin math” approach for scoring a system’s security criticality, and a case study to bring all it to life.</p>  <p>Threat modeling. A criteria for threat modeling, and pre-work for this exercise.</p>  <p>Security paved roads. For platform teams, building pre-approved security solutions and configurations is pragmatic.</p>  <p>“Defense in depth,” “least privilege,” and “zero trust.” A strategy, a principle, and a security model. Use in combination to build more layered, secure systems.</p>  <p>The bottom of this article could be cut off in some email clients. <a href="https://newsletter.pragmaticengineer.com/p/security-engineering-part-2">Read the full article uninterrupted, online.</a></p><p><a href="https://newsletter.pragmaticengineer.com/p/384a2ad1-c704-41d4-81a7-60ac441f7fec">Read the full article online</a></p><p>With that, it’s over to Nielet.</p> Common security engineering terms <p>As a brief refresher, we use three terms frequently in this article, so let’s start by defining them:</p>  <p>Vulnerability: An exploitable flaw or weakness in a system’s design, implementation or deployment</p>  <p>Threat: The potential for a threat actor to exploit the vulnerability</p>  <p>Risk: Loss or damage that could occur when a threat actualizes</p>   1. Defining the criticality of a service or system <p>Do all services and systems need to invest in a security design review? Not necessarily, as the need for a review depends on a service’s or system’s business risk profile. Vulnerabilities will surface as you identify security concerns in a system’s design and architecture. Code reviews and dynamic testing also surface security issues.</p><p>For critical systems, it’s worth investing in processes like security design reviews. However, how do you decide just how critical&nbsp; a service or system is? Use the dimensions below for a better sense of this:</p>  <p>Business purpose</p>  <p>Public access</p>  <p>Custom access controls</p>  <p>Users of the system</p>  <p>Deployment environments</p>  <p>Data classification</p>   Business purpose <p>What are the primary objectives and functions of the service or system within the context of the organization's business operations? Identify how the service contributes to achieving business goals, generating revenue, or providing stakeholder value. To figure out the risks, it’s essential to know:</p>  <p>The nature of business</p>  <p>The industry the business operates in</p>  <p>Regulatory requirements</p>  <p>Sensitivity of data involved. For example, is it restricted, or subject to PII?</p>   Public access <p>Is the service accessible to external users outside of the organization's network, or the general public? Public access systems offer expanded attack surfaces.</p><p>For these, you need to assess the potential exposure to security threats and risks associated with providing services over the internet, or other public networks, as these systems are at a much higher risk of automated bot attacks, for example.</p> Custom access controls <p>All systems need custom access controls for their data, apps and resources to determine who has access to it and in what circumstances. Role-based access control (RBAC,) or attribute-based access control (ABAC,) are two examples of custom access controls. These have specific access permissions defined for users and identities, and restrictions tailored to the service’s requirements and security needs to ensure confidentiality.&nbsp;</p><p>The decision to build custom access controls is usually made with the following factors in mind:</p>  <p>Granularity</p>  <p>Dynamic decisions based on real-time information and conditions</p>  <p>Implementation efforts</p>  <p>Simplicity</p>  <p>Flexibility</p>   Users of the system <p>What different types of users interact with the service? This is key information for defining:</p>  <p>User roles</p>  <p>Authentication mechanisms</p>  <p>Access requirements</p>  <p>User activity auditing</p>  <p>Threat detections associated with anomalous user behavior patterns&nbsp;</p>  <p>Adherence to regulatory compliance</p>  <p>The last one is especially important. Several regulatory frameworks and industry standards mandate the protection of sensitive data through user identification and access controls. Examples of such frameworks include the General Data Protection Regulation (<a href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation">GDPR</a>) and California Consumer Privacy Act (<a href="https://en.wikipedia.org/wiki/California_Consumer_Privacy_Act">CCPA</a>). In these cases, putting these in place is not “just” about making the system secure; it’s about what ensures the system is compliant with privacy and security regulation.</p><p>Users include:</p>  <p>Internal users: employees, administrators</p>  <p>External users: customers, partners, third-party vendors.&nbsp;</p>   Deployment environments <p>Development, testing, staging, and production environments in which the service operates. Each environment may have different security requirements and configurations. These varying requirements depend on:</p>  <p>Level of risk tolerance</p>  <p>Need for data protection</p>  <p>Data availability requirements</p>  <p>Compliance with industry standards, regulations, and legal requirements.&nbsp;</p>  <p>For example, a staging environment may have broader internal employee access, meaning it can be accessed by most (if not all) employees. However, the production environment tends to have much stricter access control: only specific employees or groups can access it, and even fewer will have the rights to deploy to it. And while the staging environment is unlikely to have data that is considered confidential customer data: the production environment will! So the production environment will have much more strict data security and monitoring measures deployed on its&nbsp; infrastructure.</p><p>It’s pretty common for an environment to be a shared infrastructure for various services. When this is the case, robust security controls (like stricter isolation for applications and databases) are even more important! <a href="https://learn.microsoft.com/en-us/azure/architecture/guide/multitenant/overview">Multi-tenant architectures</a> are a good example for such “shared infrastructure”&nbsp; where stricter security controls are necessary.</p> Data classification <p>This refers to labeling data based on sensitivity, confidentiality, and regulatory requirements. Understanding the classification of data helps determine:</p>  <p>Appropriate security controls</p>  <p>Suitable encryption methods</p>  <p>Access restrictions for safeguarding sensitive information and preventing unauthorized disclosure or misuse.</p>   2. Scoring a system’s “criticality” <p>It’s helpful to calculate a criticality score for services. For this, I like to assign weights to the security dimensions. Below is a sample of how these scores could look. It’s just an example; simpler than I usually use, and it doesn’t encompass all factors relevant for every system. Just treat it as inspiration:</p><p>Now we’ve established the basic factors for understanding risk and criticality, we can do some napkin math with criticality scores, based on characteristics:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0335d804-37ee-4ab1-885f-e223ffdfa7d4_1332x1082.png">    </a>Calculating criticality, simplified: define dimensions by important security factors <p>A simple way to think about a total risk “score” is to add together the weights for each dimension. So, in this case: Total Risk Score = BP + PA + CAC + US + DE + DC.</p> Scoring criticality <p>Let’s take the example of building a payment system for an e-commerce site. It needs to process a high volume of transactions via credit cards, debit cards, and other payment methods. It also needs to have payment gateway integration, account for fraud prevention, and is subject to PCI DSS compliance.</p><p>Let’s do the napkin math for this system’s criticality:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F072e761f-d4de-4d1d-b04a-61f8907debd9_1364x1318.png">    </a>Scoring the criticality of this example system <p>We get the total risk score by adding up the dimensions. In this case, it comes to 15 out of a maximum of 18 points (3 + 1 + 2 + 3 + 3 + 3.) This score indicates we are talking about a critical system from a security standpoint.</p><p>All companies have unique risk-scoring and risk-tracking processes. As a software engineer, you need to figure out what a “high” service risk score means, and at what point you should reach out to the security team at your organization, if there is one.</p> 3. Threat modeling 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/security-engineering-part-2">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0335d804-37ee-4ab1-885f-e223ffdfa7d4_1332x1082.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:10 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.eventbrite.com/e/851125899477/'>Mini Summit: Universally Buildable Extensions</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Tomorrow, May 1 2024, Yurii Rashkovskii of Omnigres will give the Postgres
Extension Mini-Summit talk, “Universally buildable extensions: dev to prod.”
at noon Eastern / 16:00 UTC, followed by community discussion.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Well that snuck up on me. Tomorrow, May 1 2024, <a href="https://ca.linkedin.com/in/yrashk">Yurii Rashkovskii</a> of
Omnigres will be giving a presentation at the fifth installment of the
Postgres <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">extension ecosystem mini-summit</a>. The tal, “Universally
buildable extensions: dev to prod”, should provoke some interesting
asymmetrical thinking and discussion. I hope to see you <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">there</a>.</p>
<p>Note! If you reserved a spot at a prior mini-summit, you will need to do
so again for each subsequent event or get no reminders from Eventbrite. If
it’s sold out just email <code>david@</code> this domain, ping me <a href="https://xoxo.zone/@theory" title="@theory@xoxo.zone">on Mastodon</a> or via
the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a> or the <a href="https://discord.com/invite/bW2hsax8We">Postgres Discord</a>
for a link or, if you’d like a calendar invitation.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/trunk/">Trunk</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   13:44 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wrenchinthegears.com/2024/04/30/ai-wellness-tokenized-democracy-game-b-will-the-health-freedom-community-buy-into-bobbys-bs/'>AI Wellness + Tokenized Democracy = Game B: Will The Health FreeDOM Community Buy Into Bobby’s BS?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  In between packing and getting my third and final UHaul round ready this weekend, I managed to put together a selection of clips touching on RFK Jr.’s blockchain / token habit and automated Stanford law specialist Nicole Shanahan’s plans to data mine precision health systems, an effort that will underpin quantified self social impact bonds [...]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>In between packing and getting my third and final UHaul round ready this weekend, I managed to put together a selection of clips touching on RFK Jr.’s blockchain / token habit and automated Stanford law specialist Nicole Shanahan’s plans to data mine precision health systems, an effort that will underpin quantified self social impact bonds and global ant computer operations. Last night after stopping for the night in Staunton “Game B – Eugenics – Woodrow Wilson” Virginia, I did an overview of the themes covered in the clip compilation, to make the concepts clearer. After speaking out against blockchain digital ID for four years, it makes me ill to see the manufactured “hero” that is Camelot Bobby pushing blockchain government, crypto, and AI / machine learning for social good. The health freeDOM – biohacking community IS Game B. So many are already totally invested in playing the game, even as they imagine themselves to be the rebels. Once you see it, you can’t unsee it, but it’s lonely looking at the world from this vantage point.&nbsp;</p>
<p>PS: If you want to better understand my concerns around Bobby’s planned blockchain budget, check out some of my old blog posts. The first goes into Michael Bloomberg’s plans for “what works” data-driven government along with a public presentation I did at Wooden Shoe Books in Philadelphia in the summer of 2019. The second touches on token engineering and participatory “democracy.”</p>
<p></p>
<p>&nbsp;</p>
<p></p>
<p>&nbsp;</p>
<p>We have to start to wrap our minds around the idea that the AI automated smart contract law that Shanhan specializes in is actually about running cyberphysical / sociotechnical systems. Sergey Brin’s father Michael, was a specialist on complex dynamical systems. We as agents will knowingly or unknowingly participate in managing the game board for emergent behavior via governance tokens. THESE are the ideas more people need to start educating themselves about and discussing.&nbsp;</p>
<p>&nbsp;</p>
<p><a href="https://www.youtube.com/watch?v=B7S8wd2bLfM&amp;list=PLnQeVMgmt_JcconLGqs20_I_Z0-nRMOLU&amp;index=3">Cyberphysical Systems and Choice Link</a></p>
<p><a href="https://www.youtube.com/watch?v=B7S8wd2bLfM&amp;list=PLnQeVMgmt_JcconLGqs20_I_Z0-nRMOLU&amp;index=3"></a></p>
<p>&nbsp;</p>
<p><a href="https://wrenchinthegears.com/2018/06/24/a-100-million-secret-bipartisan-political-plan-to-privatize-the-public-sector/">What Works Government Link</a></p>
<p><a href="https://wrenchinthegears.com/2022/07/13/tokenized-co-ops-and-moral-markets-synthetic-pretenders-part-15c/"></a></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><a href="https://wrenchinthegears.com/2022/07/13/tokenized-co-ops-and-moral-markets-synthetic-pretenders-part-15c/"><br>
</a></p>
<p><a href="https://wrenchinthegears.com/2022/07/13/tokenized-co-ops-and-moral-markets-synthetic-pretenders-part-15c/">Tokenized Participatory Governance Link</a></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p></p>
<p><a href="https://docs.google.com/presentation/d/1wLNcZ-I8yPzsNOFuJtyZuEwKB92OdUP4W9F2yx9vg88/edit?usp=sharing">Link to Moneyball Government Slide Deck – May 2019</a></p>
<p><a href="https://docs.google.com/presentation/d/1wLNcZ-I8yPzsNOFuJtyZuEwKB92OdUP4W9F2yx9vg88/edit?usp=sharing"></a></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p></p>
<p>&nbsp;</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/04/cyberphysical.png?resize=1020%2C615&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   12:10 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 29. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://kentbull.com/'>Kent Bull</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://kentbull.com/2024/04/29/iiw-38-keri-acdc-session-list/'>IIW 38 KERI & ACDC Session List</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  It’s been two weeks since the 38th bi-annual Internet Identity Workshop. The KERI community had a big showing with 19 session across three days. This included a variety of sessions on the vLEI ecosystem, the Trust Over IP (ToIP) Foundation, the Trust Spanning Protocol, KERI basics, CESR 2.0, new Witness […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
 


<p>It’s been two weeks since the 38th bi-annual <a href="https://internetidentityworkshop.com/">Internet Identity Workshop</a>. The KERI community had a big showing with 19 session across three days. </p>



<p>This included a variety of sessions on the vLEI ecosystem, the Trust Over IP (ToIP) Foundation, the Trust Spanning Protocol, KERI basics, CESR 2.0, new Witness infrastructure, and business-focused sessions. A developing tradition of the “Bearer Tokens are bad, don’t use them” session was held this year as well by Sam Smith titled “I don’t sign my credentials and neither should you!”</p>



<p>See the list below for a complete reference of KERI sessions and their meeting notes.</p>



 IIW 38 – 19 KERI and ACDC Sessions 



 Day 1 – Apr 16  



 
 <a href="https://docs.google.com/document/d/1o8oIxxLovs4bgX9jrkcXu8fMT5x7FQM10mhMN8u6ikI/edit">The Hitchhiker’s Guide To KERI</a>&nbsp; / Nuttawut Kongsuwan 



 <a href="https://docs.google.com/document/d/12WzSr7XqJEJaEgCjqRB4uhj3t0Wsn6wsbpNviKzLiM0/edit">Trust Over IP (ToIP) Foundation</a>&nbsp;/ Judith Fleenor 



 <a href="https://docs.google.com/document/d/1FNo9Nqp4XxqxjjhaTSCbPt7dtHsGruKkniF0Ke3q_2E/edit">Organizational Identity &amp; Verifiable Authority</a>&nbsp;/ Timothy Ruff 



 <a href="https://docs.google.com/document/d/1bjBdPYBDjQrRoyQ64ONsQuI-CCRCw8uSk8uhdIA_F2k/edit">EBA Pilot Using the vLEI</a>&nbsp;/ Karla McKenna &amp; Lance Byrd 



 <a href="https://docs.google.com/document/d/1D4O0GmX4ficrajyWst0saN1fBbdhxHlw_15uvro3Fmc/edit?usp=sharing">Simple SSI – How to make the SSI Codebase attractive for devs to use?</a>&nbsp;/ Jonathan Rayback 



 <a href="https://docs.google.com/document/d/1GMLtwYfdkuyQtFma5kmKxMDtQxj9DKAfBNO1mEdVx1o/edit">The Business of SSI &amp; Authentic Data</a>&nbsp;/ Timothy Ruff 



 <a href="https://docs.google.com/document/d/1D2yBxhFenidL8QdsDgN8O06CBP9o5Wvn5pcGVCbEXHE/edit">TSP draft Part I</a>&nbsp;/ Wenjing Chu 
 



 Day 2 – Apr 17 



 
 <a href="https://docs.google.com/document/d/1cFdUDPL2n8IMjwg31EjrL-v9ZyG1QNWZ6nfJ066LlBk/edit">KERI for Dummies</a>&nbsp;/ Timothy Ruff. Phil Feairheller 



 <a href="https://docs.google.com/document/d/1YL0fUUG9EtzZ5Ca96p7tzSEKDPSp1YEmmQ_INhwrxUA/edit">The 5 Rights of Secure Health Data = a Proof of Concept – KERI-ACDC</a>&nbsp;/ Jared J and Phil Feairheller 



 <a href="https://docs.google.com/document/d/1VkT6Cc5_dFfxjMTYON-INzLBZud3lV7vM6bio5_JHYw/edit">TSP Part II</a>&nbsp;/ Wenjing Chu 



 (notes missing/blank)&nbsp;<a href="https://docs.google.com/document/d/1hMYlnE0ieZ3TKdvRTdjiduBzlCDhO3_LyhuaHp5BEl8/edit">CESR 2.0 Performance features – Deep dive annotation – Comparison to JSON CBOR and more</a>&nbsp;/ Sam Smith 
 



 Day 3 – Apr 18 



 
 <a href="https://docs.google.com/document/d/1o72Pbh0PPuvQNCmj1y-JMS84OIlpiSjA2DdpfynbcjQ/edit">Deploying Multi Tenant Secure Witnesses for KERI</a>&nbsp;/ Phil Feairheller 



 <a href="https://docs.google.com/document/d/1OC6N_umnoWNldtQpkLx6crTMIpnQXIF_ml4wxPCYoUM/edit">TSP Draft Part III Implementation</a>&nbsp;/ Wenjing Chu 



 <a href="https://docs.google.com/document/d/1PgBN3mfMZjbyxUESYmVe44sQ7yL2iz708tg7oUAgivQ/edit">Running Lean w/SSI The business model to go from Idea to Company</a>&nbsp;/ Jared J &amp; Timothy Ruff 



 <a href="https://docs.google.com/document/d/15AdQQwM6NAV43dYHhkBJY1vikXVknxo3FIxzPNoFWo4/edit">Five Failed Blockchains – Why Trade Needs Protocols, Not Platforms</a>&nbsp;/ Timothy Ruff 



 <a href="https://docs.google.com/document/d/1Wlu1mAIWeBWW90BZimx9Ubnyihp0oqW05BfhG2sg99g/edit">Cardano Transactions and KERI / Can a cardano Tx be affected based on AID’s current keys?</a>&nbsp;/ Ed Eykholt 



 <a href="https://docs.google.com/document/d/1OzfnH2z-r9r3TFWipa8gR6azy3yNlNIfMSMpcl-Yn-Q/edit">vLEI (verified Legal Entity Identifier) Demystified</a>&nbsp;/ Nuttawut Kongsuwan 



 (notes missing/blank)&nbsp;<a href="https://docs.google.com/document/d/11QxELvUTJCIEUNUU1VzLc5whOWAjDgmlNESLsr3PfU8/edit">I don’t sign my credentials and neither should you! Why unbound signatures&nbsp; (tokens) are insecure and how to use KERI instead. KERI for Enterprise</a>&nbsp;/ Sam Smith 
 



<p>For more sessions come to the next Internet Identity Workshop in Fall of 2024 from <a href="https://iiw39.eventbrite.com/">OCTOBER 29, 2024 – OCTOBER 31, 2024</a>.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://kentbull.com/wp-content/uploads/2024/04/iiwlogo.png?w=250'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:40 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.patrick-breyer.de/leak-eu-regierungen-wollen-datenschutzfreundliche-und-verschluesselte-messaging-dienste-mit-chatkontrolle-bestrafen/'>Leak: EU-Regierungen wollen datenschutzfreundliche und verschlüsselte Messaging-Dienste mit Chatkontrolle bestrafen</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Ein neues Dokument zur geplanten Chatkontrolle, das zuerst von netzpolitik.org veröffentlicht und dann auch dem Nachrichtenportal Contexte zugespielt wurde, enthüllt weitere Details über die Pläne der EU-Ratspräsidentschaft: Wenn Dienste … 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Ein neues Dokument zur geplanten Chatkontrolle, das zuerst <a href="https://cdn.netzpolitik.org/wp-upload/2024/04/2024-04-10_LEWP_CSAR_Council_Presidency_Classification_WK-3036-REV-2.pdf">von netzpolitik.org veröffentlicht</a> und dann auch <a href="https://www.patrick-breyer.de/wp-content/uploads/2024/04/2024-04-24-conseil-csam-criteres-classification-services-risque.pdf">dem Nachrichtenportal Contexte zugespielt</a> wurde, enthüllt weitere Details über die Pläne der EU-Ratspräsidentschaft: Wenn Dienste unter Pseudonymen oder ohne Nutzeraccount, über VPNs oder mit Verschlüsselung genutzt werden, schneiden sie auf einer Risikoskala schlechter ab und sollen eher zur Chatkontrolle verpflichtet werden. Die Akzeptanz von Kryptowährungen zählt ebenso als Risikofaktor. Wenn ein Dienst den „direkten Austausch von Inhalten ohne Verwendung zentraler Server“ (P2P) ermöglicht, wird dies als Risiko eingestuft. Wenn eine datenschutzfreundliche Plattform keine Daten zur Überwachung des Nutzerverhaltens sammeln kann oder will, wird sie schlechter bewertet. Dienste, bei denen die Nutzer „überwiegend öffentlich kommunizieren“ (d.h. nicht über private Chats), sollen besser abschneiden und somit weniger wahrscheinlich zur Chatkontrolle verpflichtet werden.</p>



<p>Der Piratenabgeordnete und digitale Freiheitskämpfer Dr. Patrick Breyer, im EU-Parlament Schattenberichterstatter des Gesetzentwurfs zur Chatkontrolle, kommentiert: „Grundlegende Dienste wie Signal, TOR, verschlüsselte E-Mail-Dienste wie ProtonMail oder Torrenting-Plattformen sollen verteufelt und per Verpflichtung zur Chatkontrolle letztlich zerstört werden. Bie bisher datenschutzfreundlichesten Kommunikationsdienste sollen künftig zu den am stärksten überwachten Diensten werden. Das durchgesickerte Papier offenbart Massenüberwachung und Unterwanderung sicherer Verschlüsselung von Diensten als wahres Ziel der meisten EU-Regierungen. Im Gegensatz dazu will das Europäische Parlament nur Chats von Personen mit Verbindung zu sexuellem Kindesmissbrauch abhören lassen, stattdessen aber viel mehr Sicherheitsmaßnahmen zum Schutz vor der Anbahnung sexueller Kontakte zu Kindern vorschreiben, die der EU-Rat in seinem Papier nur erwähnt. Dieses Leak ist ein Augenöffner. Wir Piraten werden nicht aufhören, für unser Grundrecht auf digitales Briefgeheimnis und sichere Verschlüsselung zu kämpfen!“</p>



<p>Im Mai sind zwei Beratungsrunden über den Vorschlag zur Chatkontrolle geplant, bevor die EU-Innenminister im Juni entscheiden sollen.</p>



<p><a href="https://www.chatkontrolle.de">Breyers Informationspostal zur Chatkontrolle</a></p>



<p></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:56 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2525'>Securing Verifiable Credentials using JOSE and COSE is now a W3C Candidate Recommendation</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The Securing Verifiable Credentials using JOSE and COSE specification (a.k.a. VC-JOSE-COSE) has reached W3C Candidate Recommendation status. The Candidate Recommendation milestone is described in the W3C Process document. Please review the Candidate Recommendation of VC-JOSE-COSE. Thanks especially to Gabe Cohen, Orie Steele, and Brent Zundel for doing the hard work of getting us to this […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="http://www.w3.org/"></a>The Securing Verifiable Credentials using JOSE and COSE specification (a.k.a. VC-JOSE-COSE) has reached W3C Candidate Recommendation status. The <a href="https://www.w3.org/2021/Process-20211102/#transition-cr">Candidate Recommendation milestone</a> is described in the W3C Process document.  Please review the <a href="https://www.w3.org/TR/2024/CRD-vc-jose-cose-20240425/">Candidate Recommendation of VC-JOSE-COSE</a>.  Thanks especially to <a href="https://twitter.com/decentralgabe">Gabe Cohen</a>, <a href="https://twitter.com/OR13b">Orie Steele</a>, and <a href="https://twitter.com/brent_zundel">Brent Zundel</a> for doing the hard work of getting us to this point!</p>
<p>Since <a href="https://self-issued.info/?p=2328">I last wrote about this work</a>, the <a href="https://www.w3.org/TR/2024/CRD-vc-data-model-2.0-20240416/">W3C Verifiable Credentials Data Model</a> (VCDM), which is also at Candidate Recommendation stage, has been narrowed to only use JSON-LD to represent credentials.  VC-JOSE-COSE secures VCDM payloads with JOSE, SD-JWT, or COSE signatures.  While I’m admittedly not a fan of JSON-LD, to the extent that Verifiable Credentials using the VCDM are in use, I’m committed to finishing a solid VC-JOSE-COSE specification so there is a simple, secure, standards-based way to sign these credentials.</p>
<p>Of course, there are lots of Verifiable Credential formats to choose from, and more on the way.  Choices already existing include ISO <a href="https://www.iso.org/standard/69084.html">mDoc</a>, IETF <a href="https://www.ietf.org/archive/id/draft-ietf-oauth-selective-disclosure-jwt-08.html">SD-JWT</a>, IETF <a href="https://www.ietf.org/archive/id/draft-ietf-jose-json-web-proof-03.html">JSON Web Proof (JWP)</a>, and W3C <a href="https://www.w3.org/TR/2024/CRD-vc-data-model-2.0-20240416/">VCDM</a>.  The IETF is also planning to create a CBOR-based selective disclosure representation in the newly formed <a href="https://datatracker.ietf.org/group/spice/about/">SPICE working group</a>.  It will be interesting to see how these all shake out in the marketplace!</p>
   
  
    
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/w3c_home.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   03:23 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 27. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://benwerd.medium.com/where-im-coming-from-8048fadd36d9?source=rss-3b16402f5b9d------2'>Where I’m coming from</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

      Why I fight for a more inclusive, international perspective.  Continue reading on Medium »  
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/0*uqgWFe2Qf1-zmUWw.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   <p><a href="https://benwerd.medium.com/where-im-coming-from-8048fadd36d9?source=rss-3b16402f5b9d------2"></a></p><p>Why I fight for a more inclusive, international perspective.</p><p><a href="https://benwerd.medium.com/where-im-coming-from-8048fadd36d9?source=rss-3b16402f5b9d------2">Continue reading on Medium »</a></p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/0*uqgWFe2Qf1-zmUWw.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   01:09 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 25. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/04/mini-summit-four/'>Mini Summit Four</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Links, notes, and commentary from Jonathan Katz’s presentation at the fourth
Postgres Extension Ecosystem Mini-Summit, “Trusted Language Extensions for
PostgreSQL”.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>My thanks to <a href="https://jkatz05.com">Jonathan Katz</a> for his presentation, “Trusted Language
Extensions for PostgreSQL”, at last week’s <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">Postgres Extension Ecosystem
Mini-Summit</a>. As usual I’ve collected the transcript here
interspersed with comments from the chat window. First, links!</p>
 
 <a href="https://youtu.be/fu_yDwofkTg">Video</a> 
 PDF Slides [TBD] 
 
<p>And now, rivers of text!</p>
 Introduction 
 
 I opened the meeting and introduced <a href="https://jkatz05.com">Jonathan Katz</a>. 
 
 Presentation 
 
 
<p>Thank you for having me. I’m very excited to talk about this, and
extensions in general. I see a lot of folks here I know and recognize and
some new folks or folks I’ve not met in person yet.</p>
 
 
<p>Borrowed from the original presentation on TLEs from November of 2022, to
level set on why we built it. I know this is a slightly more advanced
audience, so some stuff that might seem relatively introductory to some of
you, though there is some material on the internals of extensions.</p>
 
 
<p>The premise is why we decided to build TLEs, what were the problems we’re
trying to solve. Part of it is just understanding Postgres extensions. In
general this group is very familiar with Extensions but there are two
points I want to hit on.</p>
 
 
<p>One of the things that excites me most about Postgres is that, when you
look back at Postgres as the Berkeley database project researching how to
create an object relational database, an accidental or intentional
features is not just that Postgres is an object-relational database, but
that Postgres is an extensible database, built from the get-go to be
able to add functionality without necessarily having to fork it.</p>
 
 
<p>Early on you’d have to Fork the database to add additional functionality,
but the beauty of the Postgres design was the ability to keep adding
functionality without forking.</p>
 
 
<p>It did require making changes to Postgres to further enhance that
capability, which we’ll discuss in a bit, but that’s a really powerful
concept.</p>
 
 
<p>The second point is that there is the large landscape of both open source
and Commercial extensions. I think a lot of folks in this group are very
familiar with the available open source extensions, but there are entire
businesses built on, effectively, commercial extensions on top of
Postgres. Again, that’s a really powerful notion!</p>
 
 
<p>It’s kind of like the Postgres economy: we created something that’s so
special that it’s able to spawn all these different businesses whether
it’s building things on top of Postgres or using Postgres as the heart of
their business. Extensions have a very large role to to to play in that.</p>
 
 
<p>Which gets us to the history of extensions. The Syntax for Postgres
extensions has been around for over a decade, since Postgres 9.1. I know
there’s folks here well familiar with building extensions prior to that
syntax! But we’re very grateful for the extension syntax because, as a
recovering application developer, I would say it made things a lot easier.</p>
 
 
<p>Instead of having to futz around the file system to figure out where to
install everything (wink wink nudge nudge on the topic today), you had a
very simple syntax. Once an extension is installed, I can <code>CREATE EXTENSION postgis</code> (or whatever extension) and there you go! I have
PostGIS installed.</p>
 
 
<p>Again, that’s really cool! Because anything we can do to make it simpler
to install and use extensions further drives their adoption, and
ultimately makes it even easier to develop and build applications with
Postgres and  continues to drive that forward.</p>
 
 
<p>So what can you build with Postgres, what extensions are available? It’s a
whole range of things. For starters, there are extensions that I call “the
ones that you take for granted”. If you’re using any monitoring tool
you’re likely running and you may not even realize
it. <code>pg_stat_statements</code> helps to aggregate statistics as queries execute
and bubbles it up to whatever monitoring tool you use. It’s a great tool
for performance tuning.</p>
 
 
<p>The example I like to give for my personal life was that, back when I was
an application developer trying to manage my own databases, I had some
query that was blocking my entire logical decoding system, so we weren’t
keeping up with transactions. Looking in <code>pg_stat_statements</code> I see a
recursive query where I should have had a <code>UNION</code> with <code>SELECT DISTINCT</code>
instead of a gigantic query that was amassing so many rows. Fixed it: I
had a 100x speed up in the query. Thank you <code>pg_stat_statements</code>!</p>
 
 
<p>Side note: I got to say, “hey, I sped up this query by 100x,” even though
it was my fault it was slow to begin with.</p>
 
 
<p>There are utility functions to help with data types. UID OSSP is very
widely used. Newer versions of Postgres have a random UUID function, but
previously, anytime you needed a UUI you would <code>CREATE EXTENSION "uuid-ossp"</code>.</p>
 
 
<p>The other fun thing about this extension is that developers
learned about SQL identifiers that require double quotes to
install the extension.</p>
 
 
<p>I think I saw Keith on here today. pg_partman! What’s really cool about
pg_partman too is that a lot of it is PL/pgSQL. This PL/pgSQL code
provides a way to manage partitions across all your tables in your entire
database. Again, that’s really powerful because Postgres has added
declarative partitioning in version 10, but pg_partman is still incredibly
useful because there are all sorts of aspects to partition management not
supported in Postgres today. This is another example where Postgres
provides the core functionality and you can use the extension to package
additional functionality that makes it easier for use.</p>
 
 
<p>Foreign data wrappers.Postgres has a whole interface to interface with
other databases. It could be other Postgres databases, other relational
databases, non-relational databases, file systems, etc. Postgres has a
C-level interface that allows you to design the calls to optimally use all
these different databases. Again, packaged up as an extension, being able
to add things as we go on.</p>
 
 
<p>I’ll wait till the end to answer questions this will be a relatively short
presentation, so we
should have some time for discussion</p>
 
 
<p>Last but not least, a little bit on PostGIS. I think this is one of the
most powerful aspects of Postgres. PostGIS itself is incredibly powerful
because you have a geospatial database that happens to be Postgres
underneath. A lot of heavy PostGIS users don’t even realize they’re using
Postgres! They think they’re using PostGIS. That is really the power of
Postgres extensibility in a nutshell: It looks like you have like a brand
new, domain-specific database – and yet underneath it it’s just boring
old Postgres doing all the things you expect a database to do. That is
also a very powerful notion/</p>
 
 
<p>Tobias Bussmann in chat: Many PostGIS users don’t know they are
using PostgreSQL 🤣</p>
 
 
<p>nils in chat: 🎉</p>
 
 
 
 
<p>To add a coda to it, you have pgRouting, an extension built on top of
PostGIS, which is built on top of Postgres. So you have a cascading effect
of extensions building on top of extensions building on top of Postgres.</p>
 
 
<p>So we’re supposed to talk about trusted language extensions. To really to
TLEs it’s important to understand the anatomy of an extension. There are
certain things that you need in order to have an extension: You need a
control file, which is kind of like your packaging manifest. it tells you
what’s in the extension. It goes into a directory.</p>
 
 
<p>You have SQL files, which effectively map out the objects that you’re
going to have in your database. If you have functions that need to map to
a C function or if you need to create a table access method in order to
build your new your new storage layer, the SQL files are the building
block.</p>
 
 
<p>If you have C- specific code, it goes in a library file or a shared object
file that gets stored in a library directory.</p>
 
 
<p>It’s a very simple layout. What’s cool is if you go to create an
extension, there’s a particular pattern that it forms: You need to know
that when you have an extension, the information in the control file goes
into the Postgres catalog. Then, if there are any functions or objects or
whatever in that extension, we need to install the object itself, but we
also need to make sure that there’s a dependency on the extension. That
way, if we need to remove the extension or upgrade it, we know all the
objects that we’ve collected.</p>
 
 
<p>So why this “extension building 101”? This gets at the heart of why we
built TLes. Because the first thing to notice is that we install
extensions directly on the file system. There are a lot of domains where
that’s restricted — certainly managed service providers.</p>
 
 
<p>I worked a lot with containers previously, and a container is effectively
an immutable file system: once you have things installed on it it’s
installed. You typically don’t want to give your app developers access to
your production systems, because your app developers are going to install
everything under the sun on them, myself included. You certainly want to
be able to restrict certain domains.</p>
 
 
<p>But we also don’t want to inhibit what developers want to build. We want
to make it as easy as possible for them to manage their code and be able
to install in different environments. That gets to another point beyond
restricting the file system. Some extensions may not be universally
available, depending on where you’re running them. You might be running an
extension on an on-premise environment that might not work in a managed
service provider. Or different managed service providers have different
extensions available.</p>
 
 
<p>The final bit — and something that I’ve definitely personally
experienced — is that, between major versions of Postgres, the API or
the ABI will change. These are the interface points at the C layer. When
they change it can break extensions. Ask any of the package managers how
much they need to nag people to upgrade their extensions: they always want
to make sure that they’re keeping it up-to-date, and ensuring that it’s
compatible.</p>
 
 
<p>But this can also lead to other issues, because as the end user, this
makes it challenging to perform major version upgrades — particularly if
I’m dependent on an extension that hasn’t been updated to work with the
latest version of Postgres. A subtle line of code change in Postgres could
end up breaking an extension.</p>
 
 
<p>Quick story: that actually happened to me while I was managing pg_tle. I
think it was a change in Postgres 15.1 actually broke something in the
pg_tle extension. I had to to fix it. I think that’s part of the point: if
you’re able to use a language that’s on top of C, and you have the C
layer abstracted away, in theory it could make it easier to perform major
version upgrades.</p>
 
 
<p>That leads into TLE.</p>
 
 
<p>I think there were two Notions behind trusted language extensions, or
TLEs, when they were initially designed. The first is giving power to app
developers to be able to build extensions. It’s actually one thing I
noticed as I started making the journey from  app developer to quasi-DBA
to, ultimately, product manager not writing code. Part of that Journey was
the power of putting some of my business logic in the database.</p>
 
 
<p>There’s always this tension between how much business logic to put in
application code versus the database. But there are certain things that
were just clear wins for me when they wer in the database. The first was a
lot of search functions I wrote where filtering data down to a very small
set in the database and returning to the application would save on network
time, processing time on the app side, etc. There were some very clear
wins by encapsulating them in functions.</p>
 
 
<p>But also solving things that were just much easier to solve in the
database. Having specific data types that solve a particular problem —
geospatial extensions keep coming to mind, pgvector dealing with Vector
data, and being able to store it in a database without having delegate it
out into an application certainly is a is a clear win.</p>
 
 
<p>The other thing was installing extensions. I think the notion of
portability is very powerful. If I
have a way to manage my extensions from a SQL interface, it
makes it much easier to move it between different systems.</p>
 
 
<p>Now, we do need to be careful as soon as we start saying “SQL interface”.
I don’t want to suggest that we should have a SQL interface to ship C code
everywhere We know there are some challenges with C code. C is highly
performant, you can effectively build anything under the sun using C, but
it’s not memory-safe, and it’s very easy if you’re not familiar with what
you’re doing — and even if you are familiar with what you’re doing! —
you can easily make mistakes that could either lead to crashes or or
possibly worse.</p>
 
 
<p>As we were thinking about all this with TLE, there’s three things. First,
we need an interface to be able to install and manage extension code
effectively regardless of environment. We need a SQL interface to do that.
We also need to make sure there’s an appropriate trust boundary. Now, Postgres
provides a trust boundary with the notion of a trusted language. But
there are other things we need to
build with trust, as well.</p>
 
 
<p>For example, you might not want everyone in your environment to be
be able to install the a TLE, so we need to
make sure there’s appropriate access controls there.</p>
 
 
<p>Finally, we need a way to package it up — which I
think is what we’re going to talk about at the Extension
Summit.</p>
 
 
<p>If there are any takeaways from why we built TLE (I think this is the the
slide that encapsulates it), it’s that, by using by using Postgres’s
built-in trusted language interface it allows you to write extension code
in languages that we know are going to respect Postgres security
boundaries.</p>
 
 
<p>Postgres has this definition of a trusted language which, if you look at
for it, you have to effectively dance around the documentation to find it.</p>
 
 
<p>But effectively I’d summarize as, if you allow an unprivileged user to
write code in a trusted language, they can’t do anything to escalate their
privileges, access the file system directly, or do anything that would
violate Postgres’s security boundary.</p>
 
 
<p>It’s a pretty good definition. Arguably, the easiest way to violate that
definition is that you as the database administrator mark an untrusted
language as trusted in the catalog. But I strongly advise to not do that!</p>
 
 
<p>What trusted languages are available. There’s a great wiki page called the
“PL Matrix” on the Postgres Wiki that shows the status of all all the
known PLs in Postgres and whether they’re trusted or not. I suggest
looking at that.</p>
 
 
<p>David Wheeler (he/him) in chat: <a href="https://wiki.postgresql.org/wiki/PL_Matrix">https://wiki.postgresql.org/wiki/PL_Matrix</a></p>
 
 
<p>Jeremy S in chat: Linux had kernel modules forever, but many people
were rightfully hesitant because a kernel module could easily crash
your entire system. One of the reasons eBPF is exploding today is
because it’s verified and safe and enables code (like entire
networking apps) to run directly in the linux kernel.</p>
<p>I see TLE similarly</p>
 
 
 
 
<p>A big thing regarding a trusted language is performance. There are a
variety of trusted languages, and they all have different properties you
know around them. The ones I’m showing today are the ones available in RDS
Postgres. But the reason I want to show them is that, part of the idea of
trusted language extensions is allowing app developers who may be less
familiar with C to write extension code and access some of the same
internals as a C extension, but from one of these languages.</p>
 
 
<p>Here are some of the known trusted languages today that work with TLE. If
you’re using the TlE open source project, you can use any available
trusted language — or you can use untrusted languages an just use the
TLE packaging mechanism. In that case you lose the trusted part, but
gain the extension installation aspect of TLE.</p>
 
 
<p>There are a few things included in TLE to make sure that TLE can be
installed safely. It is an opt-in feature. We do have a shared preload
library for pg_tle called “pg underscore TLE”, and you do need to have your
database super user install pg_le initially. This ensures that we’re
respecting your security boundary, If you’re going to use trusted language
extensions, you do have an explicit opt-in to using it.</p>
 
 David Wheeler (he/him) in chat: <a href="https://github.com/aws/pg_tle">https://github.com/aws/pg_tle</a> 
 
 
 
<p>After that, an app developer can create their own trusted language
extension.</p>
 
 
<p>Here’s a simple example from the TlE announcement with two functions and
packaged into an extension you can install. You can give it a name like
any Postgres extension; this one is called “tle_test”. The code looks like
the SQL file in any extension. And it’s effectively packaged up like an
extension using the <code>pgtle.install_extension</code> command. If you go to the
pg_le GitHub project you can see the the different interface points.</p>
 
 
<p>Once it’s installed you can use <code>CREATE EXTENSION</code> like any other
extension: it follows all the usual Postgres semantics: extension
installation, uninstallation, software life cycle management. pg_tle has
its own interface for that functionality, bu once you’ve installed it,
managing the extension is just like managing any other Postgres extension,
and follows those conventions.</p>
 
 
<p>Effectively TLE is offering, loosely, a packaging mechanism (I think
packaging has a lot more connotations): it’s a grouping mechanism for
your code. One of the parts that I always found most useful in
pg_tle was this part, effectively versioning my store procedures.</p>
 
 
<p>When I talked about the example where I was putting business logic into
the database, one part I would usually mess up is: what version of my
stored procedures was running in a given database. Particularly if you
have hundreds or thousands of databases that you’re managing, that can be a
challenge.</p>
 
 
<p>Now I had far fewer databases I was managing, I was more dealing with our
developer environments: staging and production. But I was managing the
store procedures within our migration scripts — which is totally fine,
because if I know what version of the migration that I ran then I would
know what version of the stored procedures are on that database. Kind of.
Sort of. Maybe. You know: unless someone manually modified it — in which
case shame on me for giving access to the database. But there basically
could be some gaps in knowing what version of a stored procedure was on a
particular server.</p>
 
 
<p>With pg_le we can significantly reduce the risk of that problem because we
have a way to version our store procedures, and be able to know exactly
what we’re running at any given time, and create a consistent packaging
mechanism wherever we’re running our code. And it goes beyond stored
procedures because there’s far more that you can can build with your code.</p>
 
 
<p>What else does TLE add? We discussed was the packaging mechanism, but a
lot of the power of Postgres extensions is the ability to use the
underlying Postgres internals. One of these types of internals is called a
“hook”.</p>
<p>Hooks are the Postgres feature that you’ve never heard of, that are not
well documented, and yet are the foundational part of many extensions.
Hooks are almost everywhere in Postgres. You particularly see a lot of
them during the query execution process. For example the process utility
hook which allows you to modify any utility command, anything that’s not a
direct SQL statement. There are all sorts of hooks: there are password
check hooks, client authentication hooks, hooks called around shared
memory allocation, hooks called at each step of the the execution phase.</p>
 
 
<p>Florents Tselai in chat: Hooks are almost undocumented indeed</p>
<p>The best resources I’ve found:</p>
 
 <a href="https://github.com/taminomara/psql-hooks">https://github.com/taminomara/psql-hooks</a> 
 and the standard: <a href="https://wiki.postgresql.org/images/e/e3/Hooks_in_postgresql.pdf">https://wiki.postgresql.org/images/e/e3/Hooks_in_postgresql.pdf</a> 
 
 
 
 
 
<p>Hooks are very powerful; particularly enabling a lot of extensions adding
different semantic behavior to Postgres. We could probably do a whole
series of talks just on all the different ways you can extend Postgres. I
mean, that’s why David has organized the summit! But hooks are very simply
a powerful mechanism to define behavior and Postgres.</p>
 
 
<p>Because they’re so powerful, for the hooks that we expose in tle we
make sure that there is a super user opt-in. Remember, an unprivileged user can define
this behavior but you do need someone with privilege
to be able to enable something like a hook.</p>
 
 
<p>For example, a password check hook probably means that you have the
ability to evaluate a plain text password that’s coming through. Now on
that topic we can have a very long debate, but let’s save that for
Vancouver. But with this hook, you do have the ability to do password
checks, so you want to make sure that, when you enable a function that
calling a password check hook that there’s a certain level of privilege to
that function. Or you you know you want to make sure you do your
appropriate evaluation to make sure that you trust that function.</p>
 
 
<p>In addition to that check, there’s an additional check from the pg_tle
admin role that requires someone with administrative privileges over your
TLE to register that hook. The concept of “TLE features” are  the way to
map hooks into the TLE. We’ve been building it up
since we launched TLE by adding a few hooks. There’s both the check
password hook and the client authentication hook.</p>
 
 
<p>There’s also the ability to register custom data types — which is pretty
cool, because data types are what attracted me to Postgres when I was an
app developer: “Oh! There are all these data types! I can do all these
rich comparisons against an index? Cool! Oh wait, you can even add custom
data types? That’s even cooler!”</p>
 
 
<p>TLE allows you to create the base data type, so you can really expand
the data types that you’re able to add. This is what TLE features does: it
enables that safe mapping  between trusted language code and the Postgres
C internals.</p>
 
 
<p>In order to create a hook, you need to match the hook function definition.
The TLE documentation documents how to create it appropriately, but it
doesn’t need all the parameters that you would find in the
hook function.</p>
 
 
<p>In this check password hook — I call this the “delay check password
test”, meaning you’re probably trying to avoid someone trying to guess
your password repeatedly, and if they keep failing so what, because
they’re not going to brute force it anyway. There are actually more
practical examples of check password hooks. But what’s cool is that you
can define everything around the your hook behavior from within the hook
function and then it acts as if you wrote a C-based hook! You just happen
to write it in a in a trusted language.</p>
 
 
<p>Hooks do execute with elevated privileges, particularly around
authentication you want to be very careful. So there are some safeguards
built into TLE to make sure that you only enable hooks when you want to.</p>
 
 
<p>Last but not least: choosing a trusted language. I know this group is more
focused on extension building, but I do want to talk about what an app
developer goes through when choosing a
trusted language.</p>
 
 
<p>Because everything has its trade-offs to consider. The Golden Rule (I
actually took this from Jim Mlodgensky) is: when in doubt use PL/pgSQL,
because it does have a lot of access to context that’s already available
in Postgres. What’s interesting about this is that what we see today is
based on PL/SQL. PL/pgSQL was developed to try to make it simpler to
migrate from Oracle, but at the same time to provide a lot of rich
functionality around Postgres.</p>
 
 
<p>As someone much more familiar with Ruby and Python, I can tell you that
PL/pgSQL can be a little bit quirky. But it is very well documented, and
it can solve all the problems that you need to in Postgres. And it already
has a lot of very simple ways to directly access your data from Postgres.
Certainly an easy choice to go with.</p>
 
 
<p>But wait, there’s more!</p>
 
 
<p>like PL/v8, writing JavaScript in your database, this is really cool! I
remember when it came out and how mind-blowing it was, in particular for
JSON processing. PL/v8 is awesome. PL/v8 came out right around the same
time as the document database! So you kind of had perfect storm of being
able to process JSON and write it in JavaScript — both within your
Postgres database and it could be quite powerful.</p>
 
 
<p>Another really cool feature of PL/v8 is the ability to directly call
another function or another PL/v8 function from within PL/v8, and not
have to go through Postgres function processing, which adds a lot of
additional overhead.</p>
 
 
<p>And now the one that’s all abuzz right now: PL/Rust. Being able to write
and execute Rust code within Postgres. This is pretty cool, because Rust
is a compiled language! There’s a trusted way to run PL/Rust within
Postgres. There are a few techniques to do it. First, whenever you’re
running Rust on your server, to make sure that you’re guarding against
breakouts.</p>
 
 
<p>There is a library, I believe it called postgres FTD, that effectively
compiles out some of the less dressed parts of Rust, such as unsafe
function calls. But you can still get everything that you want in PL/Rust
today: you get the Rust standard Library, the ability to run crates —
and you do want to evaluate crates to make sure that you’re comfortable
running them in your environment. But then you get this compiled language
that is CPU efficient, memory efficient, and memory safe. (Well, a lot of
Rust is memory safe) It’s pretty cool!</p>
 
 Steven Miller in chat: In PL/Rust, does it run the compilation when
the function is created? Then if there is a compiler issue it just
shows up right there? 
 
 
 
<p>I wrote a blog post last year that compared some different function calls
between PL/pgSQL, PL/v8, and PL/Rust. First I was doing some array
processing, and you could see that the Pl/Rust calls were very comparable
to the C calls. And then there’s some additional Vector processing, given
that I’ve been obsessing on vectors for the past 14 months. Seeing rust
actually win against PL/pgSQL and PL/v8 (I don’t remember the numbers off
the top of my head I can look up that blog as soon as I switch windows).
Pretty cool!</p>
 
 
<p>This brings us in some ways to the best of all worlds, because I can take
an extension that normally I would write in C, particularly because I’m
focused on performance, I can write it in PL/Rust, package it as a trusted
language extension, and run it anywhere that TLE and PL/Rust are
supported. Again, that is very powerful, because suddenly I have what I
hope is the best of all worlds: I have this portability, I don’t have to
worry as much about major version upgrades because pg_le is acting as that
abstraction layer between the Postgres C code and the application code
that I’m writing.</p>
 
 
<p>Jeremy S in chat: Versioning of stored procedures is a very
interesting use case</p>
 
 
<p>Darren Baldwin in chat: Agreed! Moving stuff to the database layer
seems to be something very foreign and “scary” to most app devs I’ve
talked to</p>
 
 
<p>Anup Sharma in chat: Is TLE a requirement for any PostgreSQL
extension, or is it dependent?</p>
 
 
<p>Steven Miller in chat: So during a major version upgrade, the
function declaration stays the same, so that’s why your application
doesn’t need to change with respect to the extensions during a major
version upgrade. And at some point during the migration, you create
the function again, which recompiles. So it all works the same! That’s
great</p>
 
 
 
 
<p>Last slide, then I’m certainly looking forward to discussion. pg_tle is
open source, and it’s open source for a lot of reasons. A lot of it is
because we want to make sure that trusted language extension are as
portable as possible. But in some ways the ideas behind TLE are not
original. If you look at other databases there is this notion of, let’s
call it inline extensions, or inline SQL, ou call them modules, you call
them packages. But the idea is that I can take reusable chunks of code,
package them together, and have them run anywhere. It doesn’t matter where
the database is located or hosted.</p>
 
 
<p>This is something that I personally want to work with folks on figuring
out how we can make this possible in Postgres. Because even in Postgres
this is not an original idea. Dimitri Fontaine was talking about this as
far back as 2012 in terms of his vision of where of the extension
framework was going.</p>
 
 
<p>What I’m looking forward to about this Extension Summit — and hopefully
and hopefully I’m not in conflicting meetings while it’s going on — is
talking about how we can allow app developers to leverage all the great
parts of Postgres around function writing, function building, and
ultimately packaging these functions, and making it simple simpler for
them to be able to move it wherever their applications are running.</p>
 
 
<p>So it is open source, open to feedback, under active development, continue
to add more features to support Postgres. Iltimately we want to hear
what’ll make it easier for extension writers to be able to use TLE, both
as a packaging mechanism and as a as a development mechanism.</p>
 
 
<p>So with that uh I that is the end of my slides and happy to uh get into a
discussion about this.</p>
 
 
 Discussion 
 
 
<p>David Wheeler (he/him): Awesome, thank you Jonathan. there was one
question about PL/Rust in the comments. Stephen asks whether it compiles
when you create the function, so if there are compiler issues they they
show up there.</p>
 
 
<p>Jonathan Katz: Correct It compiles when you create the function and that’s
where you’ll get compile errors. I have definitely received my fair share
of those [chuckles]. There is a Discord. PL/Rust is developed principally
by the folks uh responsible for the pgrx project, the folks at ZomboDB,
and they were super helpful and debugging all of my really poor Rust code.</p>
 
 
<p>David Wheeler (he/him): While while people are thinking about the
questions I’ll just jump in here. You mentioned using crates with PL/Rust.
How does that work with pg_le since they have to be loaded from somewhere?</p>
 
 
<p>Jonathan Katz: That’s a good question. I kind of call it shifting the
problem. TLE solves one problem in that you don’t need to necessarily have
everything installed on your on your local file system outside of pg_tle
itself. If you’re using PL/Rust and you need crates, you do need those
crates available either within your file system or within whatever package
management tools you’re using. So it shifts the problem. I think it’s
going to be a good discussion, about what we can do to help ensure that
there is a trusted way of loading those.</p>
 
 
<p>David Wheeler (he/him): Yeah I wonder if they could be vendored and then
just included in the upload through the function call.</p>
<p>Anup Sharma asked asked if pg_tle s a requirement any extension or
is it dependent.</p>
 
 
<p>Jonathan Katz: It’s not requirement. This is a project that is making it
possible to write Postgres extensions in trusted languages. There ar
plenty of extension authors on this call who have written very, very, very
good extensions in C that do not use TLE.</p>
 
 
<p>David Wheeler (he/him): You can use trusted languages to write extensions
without TLE as well. It’s just a way of getting it into the
database without access to the file system, right?</p>
 
 
<p>Jonathan Katz: Correct. I think I saw Keith here. pg_partman is PL/pgSQL.</p>
 
 
<p>Anup Sharma in chat: Understood. Thanks</p>
 
 
<p>Tobias Bussmann in chat: I think it is important not to confuse
Trusted Language Extensions TLE with “trusted extensions” which is a
feature of Postgres 13</p>
 
 
<p>Keith Fiske in chat: Pretty much all of it is. Just the background
worker isn’t</p>
 
 
<p>Jonathan Katz in chat: hat’s what I thought but didn’t want to
misspeak 🙂</p>
 
 
 
 
<p>David Wheeler (he/him): Right Any other questions or comments or any
implications that you’re thinking about through for extension
distribution, extension packaging, extension development?</p>
 
 Steven Miller in chat: Is background worker the main thing that a
TLE could not do in comparison to traditional extensions? 
 
 
 
<p>Jason Petersen: The crates thing kind of raised my interest. I don’t know
if Python has this ability to bring in libraries, or if JavaScript has
those dependencies as well. But has there been any thought within pg_tle
for first classing the idea of having a local subdirectory or a local file
system layout for the “native” dependencies? I’m using “native” in quotes
here because it could be JavaScript, it could be Python, whatever of those
languages, so they could be installed in a way that’s not operating system
independent.</p>
<p>I know this is kind of a complex setup, but what I’m getting at is that a
lot of times you’ll see someone say “you need to install this package
which is called this and Red Hat or this on Mac or this on Debian — and
then you can install my extension. Has there been any push towards
solving that problem by having your TLE extensions load things from like a
a sort of Walled Garden that you set up or something? So it’s specific to
the database instead of the OS?</p>
 
 
<p>Jonathan Katz: That’s a good question. There has been thought around this.
I think this is going to be probably something that requires a thorough
discussion in Vancouver. Because if you look at the trusted languages that
exist in Postgres today, the definition of trusted language is: thou shall
not access the file system. But if you look at all these different
languages, they all have external dependencies in some in some way shape
or form. Through Perl there’s everything in CPAN; through
JavaScript there’s everything in npm. Let’s say installed the appropriate CPAN libs and npm libs within uh your database for everything I recall from playing with trusted PL/v8 and PL/Perl is
that you still can’t access those libraries. You can’t make the include or
the require call to get them.</p>
<p>Where PL/Rust is unique is that first off we just said,
“yes, you can use your Cargo crates here.” But I think that also requires
some more thinking in terms of like how we make that available,
if it’s OS specific, vendor specific, or if there’s
something universal that we can build that helps to make that
more of a trusted piece. Because I think at the end of the day, we
still want to give the administrative discretion in terms of what they
ultimately install.</p>
<p>With the trusted language extensions themselves, we’re able to say,
“here’s the post security boundary, we’re operating within that security
boundary.” As soon as we start introducing additional dependencies,
effectively that becomes a judgment call: are
those dependencies going to operate within that security boundary or not.
We need to be make sure that administrators still have the ability to
to make that choice.</p>
 
 
<p>I think there are some very good discussion topics around this,  not just
for something like PL/Rust but extension distribution in general I think
that is you know one of the I think that’ll be one of the key discussions
at the Extension Summit.</p>
 
 David Wheeler (he/him) in chat: What if the required
modules/packages/whatever were in a table. e.g. in Perl I do <code>use Foo::Bar</code> and it has a hook to load a record with the ID Foo::Bar from
a table 
 
 
 
<p>David G. Johnson: Has there been any thought to having the default version
of an extension tied to the version of PostgreSQL? Instead of it just
being 1.3 and, whether I’m on version 12 or 15, because 1.3 might not even
work on version 12 but it would work on version 15. The versioning of the
an extension and the versioning of PostgreSQL seem like they’re almost
too independent.</p>
 
 
<p>Jonathan Katz: So David, I think what you need to do is chastise the
extension developers to let them know they should be versioning
appropriately to to the the version of Postgres that they’re using.
[Chuckles]</p>
<p>There is a good point in there, though. There is a lot of freedom in terms
of how folks can build extensions. For example, just top of mind, pgvector
supports all the supported versions of Postgres. Version 0.7.0 is going to
be coming out soon so it’s able to say, “pgvector 0.7.0 works with these
versions.” Dumb. PG plan meanwhile maintains several back releases; I
think 1.6.0 is the latest release and it only supports Postgres 16. I
don’t believe it supports the earlier versions (I have to double check),
but there’s effectively things of that nature.</p>
<p>And then there aer all sorts of different things out there, like PostGIS
has its own life cycles. So there’s something good in that and maybe the
answer is that becomes part of the control file, saying what versions ov
Postgres an extension is compatible with. That way we’re not necessarily
doing something to break some environment. I’m just brainstorming on on
live TV.</p>
 
 
<p>David G. Johnson: The other day I open a but report on this. but
PostgreSQL dump and restore will dump it without the version that’s in the
source database, and when yoq restore it, it’s going to restore to
whatever the current version for the control file is even if you’re
upgrading to a different database. versus restoring it to whatever the
original version was. That dynamic just seemed problematic.</p>
 
 
<p>David Wheeler (he/him): I think it’s less problematic for trusted language
extensions or extensions that have no C code in them, because pg_dump does
dump the extension, so you should be able to load it up. I assume base
backup and the others do the same thing.</p>
 
 
<p>David G. Johnson: I haven’t checked into that. It dumps <code>CREATE EXTENSION</code>
and then it dump any user tables that are marked by the extension. So
these code tables are marked as being user tables for TLE?</p>
 
 
<p>David Wheeler (he/him): What do you mean by code tables?</p>
 
 
<p>Regina Obe: That’s a good point. For example my Tiger geocoder is all
PL/pgSQL, but it’s only the <code>CREATE EXTENSION</code> thing that’s named. So for
your TLE table, it would try to reload it from the original source,
wouldn’t it? In which case it would be the wrong version.</p>
 
 
<p>Jonathan Katz: We had to add some things into TLE to make sure it worked
appropriately with pg_dump. Like I know for a fact that if you dump and
load the extension it works it works fine. Of it doesn’t then there’s a
bug and we need to fix it.</p>
 
 
<p>David G. Johnson: Okay yeah I haven’t played with this. Literally this is
new to me for the most part. I found the whole fact that the control file
is not updated when you do <code>ALTER EXTENSION</code> to be, at least in my mind,
buggy.</p>
 
 
<p>Jonathan Katz: In the case of TLE, because it’s in theory major
version-agnostic. When I say “in theory,” it’s because we need to make
sure the TLE code in library itself is able to work with every major
version. But once that’s abstracted away the TLEs themselves can just be
dumped and reloaded into different versions of Postgres. I think we I we
have a TAP test for that, I have to double check. But major version
upgrades was something we 100% tested for</p>
 
 
<p>David Wheeler (he/him): I assume it’d be easier with pg_tle since there’s
no need to make sure the extension is is installed on the file system of
the new server.</p>
 
 
<p>Jonathan Katz: Yep. if you look at the internals for pg_tle, effectively
the TLEs themselves are in a table. When you do a <code>CREATE EXTENSION</code> it
gets loaded from that particular table.</p>
 
 
<p>David G. Johnson: Right, and when you do a pg_dump you make suer that
table was dumped to the dump file.</p>
 
 
<p>Jonathan Katz: Yes. But this is a key thing that we we had to make sure
would does work: When loading in a pg_dump, a lot of the <code>CREATE EXTENSIONS</code> get called before the table. So we need to make sure that we
created the appropriate dependency so that we load the TLE data before
the <code>CREATE EXTENSION</code>. Or the <code>CREATE EXTENSION</code> for the TLE itself.</p>
 
 
<p>Jeremy S in chat, replying to “Is background worker the main…”:
doing a background worker today, I think requires working in C, and I
don’t think core PG exposes this yet. Maybe it could be possible to
create a way to register with a hook to a rust procedure or something,
but maybe a better way in many cases is using pg_cron</p>
 
 
<p>Jonathan Katz in chat: We can add support for BGWs via the TLE API;
it’s just not present currently.</p>
 
 
<p>nils in chat: Creative thinking, if a background worker doesn’t work
in TLE, how about create your UDF in tle and schedule with pg_cron 🤡</p>
 
 
 
 
<p>David Wheeler (he/him): You mentioned in the comments that you think that
background workers could be added. How would that work?</p>
 
 
<p>Jonathan Katz: It would be similar to the the other things that we’ve
added, the data types and the hooks. It’s effectively creating the
interface between the C API and what we’d expose as part of the TLE API.
It’s similar to things like pgrx, where it’s binding to Postgres C API but
it’s exposing it through a Rust API. We do something similar with the TLE
API.</p>
 
 
<p>Steven Miller in chat: Thank you Jeremy. I like this idea to use
TLE, then depend on cron for a recurring function call</p>
 
 
<p>Steven Miller in chat: Ah yes Nils same idea 😄</p>
 
 
<p>Jason Petersen in chat: Thumbs up to nils about pgcron. If you need
a recurring BGW just write it in plpgsql and schedule it</p>
 
 
<p>nils in chat: Great hackers think alike</p>
 
 
<p>Jason Petersen in chat: (I know I do this)</p>
 
 
 
 
<p>David Wheeler (he/him): That that makes sense. I just thought the
background workers were literally applications that are started when the
postmaster starts up shut down when the postmaster shuts down.</p>
 
 
<p>Jonathan Katz: But there’s dynamic background workers.</p>
 
 
<p>David Wheeler (he/him): Oh, okay.</p>
 
 
<p>Jonathan Katz: That’s how a parallel query works.</p>
 
 Jeremy S in chat: Threading? 😰 
 
 
 
<p>David Wheeler (he/him): Gotcha, okay. Sorry my information’s out of date.
[chuckles]</p>
 
 
<p>Jonathan Katz: Well maybe one day we’ll have you know some some form of
threading, too. I don’t think like we’ll get a wholesale replacement with
threads, but I think there are certain areas where threads would help and
certain areas workers are the way to go/</p>
 
 
<p>David Wheeler (he/him): Yeah, yeah that makes sense.</p>
 
 
<p>Jonathan Katz: Hot take!</p>
 
 
<p>David Wheeler (he/him): What other questions do you have for about TLEs or
extensions more broadly and packaging in relation to TLEs?</p>
 
 
<p>David G. Johnson: Just a random thought: Have you thought about
incorporating foreign servers and pointing the TLE, instead of a local
database, point it to a master, company-wide foreign table?</p>
 
 
<p>David Wheeler (he/him): Like a TLE registry?</p>
 
 
<p>David G. Johnson: Right, yeah something global would be nice. like okay we
hosted on PGXN at there’s a TLE registry. But because for a company who
wants maintain code internally between projects, and they want a shared
library, they can publish it on one server, send up a link to it over
foreign server, and then just point at that.</p>
 
 
<p>Jonathan Katz: Could be!</p>
 
 
<p>David Wheeler (he/him): I mean you could just use foreign foreign tables
for that for the tables that TLE uses for its its registry, right?</p>
 
 
<p>David G. Johnson: That’s I’m thinking.</p>
 
 
<p>David Wheeler (he/him): Yeah that’s a cute idea.</p>
 
 
<p>Jonathan Katz: I think that just to to go back a few more minutes. I think
you I was asked to talk about the vision. One one way to view extensions
is trying things out before they’re in core, or before they’re in
Postgres. The aspect that I would ultimately like to see in core someday
is the ability to do that’s called “inline modules.” There is a SQL
standard syntax, <code>CREATE MODULE</code>, that for this purpose. Some folks were
trying to see see if we could get it into, I believe, Postgres 15. There
was some push back on the design and it died on the vine for the time
being.</p>
<p>But I do think it’s something to consider because when I talk to folks,
whether it’s random Postgres users RDS customers, etc., and I go through
TLE, one of the things that really stands out is one of the things that we
had discussed here and I saw in the chat, which is this aspect: being able
to version your stored procedures. This is in part what modules aims to
solve. One is just having a SQL interface to load all these things and
group it together. But then once you have that grouping you have the
ability to version it. This is the part that’s very powerful. As soon as I
saw this I was like, “man I could have used that that would have saved me
like hours of debugging code in production.” Mot saying that I was ever
sloppy and you know in random store procedures in my production database!</p>
 
 David Wheeler (he/him) in chat: I see CREATE MODULE in the db2 docs. 
 
 
 
<p>Jonathan Katz: But that’s kind of the vision. The fact that Postgres is
extensible has led to this very widely adopted database. But I think there
are things that we can also learn in our extensions and bring back
upstream. There are certainly reasons why they we developing things in
extensions! Like pgvector is an example of that, where we talked about it
at PGCon last year. And part of the thought of not trying to add a vector
data type to Postgres was, first, to make sure we could settle on what the
the binary format would be; and once that’s solidified, then we could add
it.</p>
<p>But I had an aside with Tom [Lane] where we talked about the fact that
this is something we need to move fast on, the vector space is moving very
quickly, extensions are a way to be able to move quickly when something
like Postgres moves more deliberately.</p>
<p>This is in
some ways where TLE is, our way to be able to see what kind of interface
makes sense for being able to do inline extension loading
and ultimately how we want that to look in core.</p>
 
 
<p>David Wheeler (he/him): Can you create data types with a binary
representation in TLE?</p>
 
 
<p>Jonathan Katz: Yes as of (I want to say) the the 1.3 release. I have to
double check the version. The way we’re able to do it safely is that it
actually leverages the BYTEA type. When you create that representation it
stores it as a BYTEA. What you get for free today is that, if you create
your equality/inequality operators, you can use a b-tree look up on these
data types.</p>
<p>So there’s a “dot dot dot” there. If we wanted to be able to use like GIST
in GIN and build data types for our other index interfaces, there’s more
work to be done. That would require a TLE interface. I spent a lot of time
playing with GIST and GIN, and the interface calls themselves involve
pointers. So that will require some thought yeah.</p>
 
 
<p>David Wheeler (he/him): I assume it’s a similar issue for Rust data types
that are basically just serde-serialized.</p>
 
 
<p>Jonathan Katz: Yeah we can at least like store things in BYTEA, and that’s
half the battle. It allows us to do a safe representation on disk as
opposed just “here’s some random binary; good luck and don’t crash the
database!”</p>
 
 
<p>Jason Petersen in chat: I also wondered about the function interface
for things like storage features (table access methods).</p>
<p>I assume they’re similarly hairy</p>
 
 
 
 
<p>David Wheeler (he/him): Any other last minute questions?</p>
 
 
<p>Jonathan Katz: Table access methods. Yes table access methods are very
hairy as are index access methods. I spent a lot of time the past 14
months looking at the index access method interface, which has a lot of
brilliance in it, and certainly some more areas to develop. But it’s
amazing! The fact that we can implement vector
indexes and get all the best parts of Postgres is a phenomenal
advantage.</p>
 
 
<p>Jeremy S: One last question. We’re leading up to Vancouver and we’re going
to be starting to think about  some of the topics that we want to make
sure to talk about at the Summit. I think you mentioned one earlier (I
should have written it down), but any final thoughts about topics that we
should make sure to discuss?</p>
 
 
<p>Jonathan Katz: Just in general or TLE specific?</p>
 
 
<p>Jeremy S: Both. I mean for sure TLE-specific, but also just generally
related to extensions</p>
 
 
<p>Jonathan Katz: My TLE-specific one dovetails into the general one. The
first one is: is there ultimately a path forward to having some kind of
inline extension management mechanism in core Postgres. That’s the top,
part one, I spent the past five minutes talking about that.</p>
<p>But I think the big thing, and why we’re all here today, is how do we make
it easier for developers to install extensions, manage extensions, etc. I
think the notion of package management thanks to the work of Andres
finding the backdoor to xz also shines a new light, because there’s a huge
security component to this. I remember, David, some of our earlier chats
around this. I think you know —- again, being ap-developer sympathetic
— I definitely want to see ways to make it easier to be able to load
extensions.</p>
<p>Having spend spent a lot of time on the other side, the first thing that
comes to mind is security. How do we create a protocol for managing the
extension ecosystem that also allows folks to opt into it and apply their
own security or operational or whatever the requirements are on top of it.
That’s the thing that’s most top of mind. I don’t expect to have like a
full resolution from the Extension Summit on it, but at least the start of
it. What is ultimately that universal packaging distribution protocol for
Postgres extensions that we can all agree on?</p>
 
 
<p>David Wheeler (he/him): Thank you so much! Before we go I just wanted to
tee up that in two weeks Yuri Rashkovskii is going to talk about his idea
for universally buildable extensions: dev to prod. That’ll be on May 1st
at noon Eastern and 4pm UTC. Thank you everybody for coming.</p>
 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/jonathan-katz/">Jonathan Katz</a> 
                 <a href="https://justatheory.com/tags/tles/">TLEs</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   22:40 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 25. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-91'>The Pulse #91: The end of US non-competes within sight?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Also: the Humane AI pin flop and how it relates to the “AI goldrush,” and a look at whether developers will move from Redis to Valkey after a license change, or stay.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering insights, patterns, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Today, we cover:</p>  <p>Industry pulse. IBM buys HashiCorp; Google’s new operating reality; Section 174 still not repealed; Meta’s unexpected AI play, and more.</p>  <p>End of non-competes within sight in the US? The US Federal Trade Commission issued a ruling that would ban almost all non-compete agreements nationwide. If this passes, NDAs could become a lot more important for tech companies. However, the rule passing is far from a done deal: whether or not the rule lives will be decided in the courtroom. A closer look at the proposed changes.</p>  <p>The Humane AI pin flop, and “AI goldrush.” After years of developing hardware for the “new iPhone,” startup Humane suddenly pivoted to an AI use case. This bet was made soon after ChatGPT was released. Did Humane expect ChatGPT to improve much faster than it has?</p>  <p>Redis or Valkey? It’s only been a month since Valkey – a permissively licensed Redis ”continuation” announced its formation. I asked developers if they are planning to switch: and a quarter said they do.</p>   1. Industry pulse  IBM buys HashiCorp 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-91">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:03 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 24. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/04/pgxn-tools-pgrx/'>Test and Release pgrx Extensions with pgxn-tools</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The v1.6.0 release of the pgxn-tools Docker image adds a new command to
efficiently build and test pgrx extensions on a wide variety of Postgres
versions.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Yesterday I released v1.6.0 of the <a href="https://github.com/pgxn/docker-pgxn-tools/">pgxn/pgxn-tools Docker image</a> with a new
command: <a href="https://github.com/pgxn/docker-pgxn-tools?tab=readme-ov-file#pgrx-build-test"><code>pgrx-build-test</code></a> works much like the existing <a href="https://github.com/pgxn/docker-pgxn-tools?tab=readme-ov-file#pg-build-test"><code>pg-build-test</code></a>
utility for <a href="https://www.postgresql.org/docs/current/extend-pgxs.html">PGXS</a> extensions, but for <a href="https://github.com/pgcentralfoundation/pgrx">pgrx</a> extensions. Here’s an example
<a href="https://github.com/tembo-io/pg-jsonschema-boon/blob/ea64888/.github/workflows/lint-and-test.yml">from pg-jsonschema-boon</a>, a pgrx extension I’ve been working on:</p>
  <code>name: 🧪 Test
on:
  push:
jobs:
  test:
    runs-on: ubuntu-latest
    container: pgxn/pgxn-tools
    strategy:
      matrix:
        pg: [11, 12, 13, 14, 15, 16]
    name: 🐘 Postgres ${{ matrix.pg }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Start PostgreSQL ${{ matrix.pg }}
        run: pg-start ${{ matrix.pg }}
      - name: Setup Rust Cache
        uses: Swatinem/rust-cache@v2
      - name: Test on PostgreSQL ${{ matrix.pg }}
        run: pgrx-build-test
</code>  <p>The format is the same as for <code>pg-build-test</code>, starting with installing a
specific version of Postgres from the <a href="https://wiki.postgresql.org/wiki/Apt">Postgres Apt repository</a> (supporting
versions 8.2 – 17). It then adds the <a href="https://github.com/Swatinem/rust-cache">Swatinem/rust-cache</a> action to speed up
Rust builds by caching dependencies, and then simply calls <code>pgrx-build-test</code>
instead of <code>pg-build-test</code>. Here’s what it does:</p>
 
 Extracts the pgrx version from the <code>Cargo.toml</code> file and installs it
(requires v0.11.4 or higher) 
 Initializes pgrx to use the Postgres installed by <code>pg-start</code> 
 Builds the extension with <code>cargo pgrx package</code> 
 Tests the extension with <code>cargo pgrx test</code> 
 Installs the extension with <code>cargo pgrx install</code> 
 Checks for a <code>Makefile</code> with <code>installcheck</code> configured and, if it exists,
runs <code>make installcheck</code> 
 
<p>This last step allows one to include <a href="https://www.postgresql.org/docs/current/extend-pgxs.html">PGXS</a>-style <code>pg_regress</code> tests in
addition to Rust/pgrx tests, as pg-jsonschema-boon does. Here’s a <a href="https://github.com/tembo-io/pg-jsonschema-boon/actions/runs/8809394356">successful
run</a>.</p>
<p>Special thanks to Eric Ridge and @Jubilee for all the help and improvements in
<a href="https://github.com/pgcentralfoundation/pgrx/releases/tag/v0.11.4">pgrx v0.11.4</a> that enable this to work transparently.</p>
 pgrx Release Pattern 
<p>The pattern for releasing a prgx extension on PGXN is the same as before,
although you may want to generate the <code>META.json</code> file from a template. For
example, the <a href="https://github.com/tembo-io/pg-jsonschema-boon/blob/ea64888/Makefile">pg-jsonschema-boon Makefile</a> creates <code>META.json</code> from
<code>META.json.in</code> by reading the version from <code>Cargo.toml</code> and replacing
<code>@CARGO_VERSION@</code>, like so:</p>
  <code>DISTVERSION = $(shell perl -nE '/^version\s*=\s*"([^"]+)/ &amp;&amp; do { say $$1; exit }' Cargo.toml)

META.json: META.json.in Cargo.toml
	@sed "s/@CARGO_VERSION@/$(DISTVERSION)/g" $&lt; &gt; $@
</code>  <p>The release workflow uses it like so:</p>
  <code>name: 🚀 Release on PGXN
on:
  push:
    # Release on semantic version tag.
    tags: ['v[0-9]+.[0-9]+.[0-9]+']
jobs:
  release:
    name: 🚀 Release on PGXN
    runs-on: ubuntu-latest
    container: pgxn/pgxn-tools
    env:
      PGXN_USERNAME: ${{ secrets.PGXN_USERNAME }}
      PGXN_PASSWORD: ${{ secrets.PGXN_PASSWORD }}
    steps:
    - name: Check out the repo
      uses: actions/checkout@v4
    - name: Bundle the Release
      env: { GIT_BUNDLE_OPTS: --add-file META.json }
      run: make META.json &amp;&amp; pgxn-bundle
    - name: Release on PGXN
      run: pgxn-release
</code>  <p>Note the “Bundle the Release” step, which first calls <code>make META.json</code> to
generate the dated file, and tells <code>pgxn-bundle</code> to add the <code>META.json</code>
via the <code>GIT_BUNDLE_OPTS</code> environment variable. The project also excludes
the <code>META.json.in</code> file from the bundle in <a href="https://github.com/tembo-io/pg-jsonschema-boon/blob/ea64888/.gitattributes">its <code>.gitattributes</code></a> file,
and excludes <code>META.json</code> from the project repository in <a href="https://github.com/tembo-io/pg-jsonschema-boon/blob/ea64888/.gitignore">its <code>.gigignore</code></a> file.</p>
<p>Looking forward to seeing all your pgrx projects on <a href="https://pgxn.org/">PGXN</a>!</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/pgrx/">pgrx</a> 
                 <a href="https://justatheory.com/tags/pgxn-tools/">pgxn-tools</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:34 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.jonudell.net'>Jon Udell</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.jonudell.net/2024/04/23/the-future-of-sql-conversational-hands-on-problem-solving/'>The future of SQL: conversational hands-on problem solving</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Here’s the latest installment in the series on working with LLMS: https://thenewstack.io/the-future-of-sql-conversational-hands-on-problem-solving/ I keep returning to the theme of choral explanations (#4 on my list of best practices), and it’s especially relevant in the SQL domain where there are just so many ways to write a query. Exploring the range of possibilities used to be … Continue reading
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Here’s the latest installment in the series on working with LLMS: <a href="https://thenewstack.io/the-future-of-sql-conversational-hands-on-problem-solving/">https://thenewstack.io/the-future-of-sql-conversational-hands-on-problem-solving/</a></p>
<blockquote><p>
I keep returning to the theme of choral explanations (<a href="https://thenewstack.io/7-guiding-principles-for-working-with-llms/">#4 on my list of best practices</a>), and it’s especially relevant in the SQL domain where there are just so many ways to write a query.</p>
<p>
Exploring the range of possibilities used to be arduous, time-consuming and hard to justify. Now it’s becoming hard to justify not doing that; optimizations (sometimes major ones) can and do emerge.
</p></blockquote>
<p>The rest of the series:</p>
<p>
1 <a href="https://blog.jonudell.net/2023/05/24/when-the-rubber-duck-talks-back/">When the rubber duck talks back</a> </p>
<p>
2 <a href="https://blog.jonudell.net/2023/06/14/radical-just-in-time-learning/">Radical just-in-time learning</a> </p>
<p>
3 <a href="https://blog.jonudell.net/2023/06/18/why-llm-assisted-table-transformation-is-a-big-deal/">Why LLM-assisted table transformation is a big deal</a> </p>
<p>
4 <a href="https://thenewstack.io/using-llm-assisted-coding-to-write-a-custom-template-function/">Using LLM-Assisted Coding to Write a Custom Template Function</a></p>
<p>
5 <a href="https://thenewstack.io/elevating-the-conversation-with-llm-assistants/">Elevating the Conversation with LLM Assistants</a></p>
<p>
6 <a href="https://thenewstack.io/how-large-language-models-assisted-a-website-makeover/">How Large Language Models Assisted a Website Makeover</a></p>
<p>
7 <a href="https://thenewstack.io/should-llms-write-marketing-copy/">Should LLMs Write Marketing Copy?</a></p>
<p>
8 <a href="https://thenewstack.io/test-driven-development-with-llms-never-trust-always-verify/">Test-Driven Development with LLMs: Never Trust, Always Verify</a> </p>
<p>
9 <a href="https://thenewstack.io/learning-while-coding-how-llms-teach-you-implicitly/">Learning While Coding: How LLMs Teach You Implicitly</a></p>
<p>
10 <a href="https://thenewstack.io/how-llms-helped-me-build-an-odbc-plugin-for-steampipe/">How LLMs Helped Me Build an ODBC Plugin for Steampipe</a></p>
<p>
11 <a href="https://thenewstack.io/how-to-use-llms-for-dynamic-documentation/">How to Use LLMs for Dynamic Documentation</a></p>
<p>
12 <a href="https://blog.jonudell.net/2023/11/06/lets-talk-conversational-software-development/">Let’s talk: conversational software development</a></p>
<p>
13 <a href="https://thenewstack.io/techniques-for-using-llms-to-improve-sql-queries/">Using LLMs to Improve SQL Queries</a></p>
<p>
14 <a href="https://thenewstack.io/puzzling-over-the-postgres-query-planner-with-llms/">Puzzling over the Postgres Query Planner with LLMs</a></p>
<p>
15 <a href="https://thenewstack.io/7-guiding-principles-for-working-with-llms/">7 Guiding Principles for Working with LLMs</a></p>
<p>
16 <a href="https://thenewstack.io/learn-by-doing-how-llms-should-reshape-education/">Learn by Doing: How LLMs Should Reshape Education</a></p>
<p>
17 <a href="https://thenewstack.io/how-to-learn-unfamiliar-software-tools-with-chatgpt/">How to Learn Unfamiliar Software Tools with ChatGPT</a></p>
<p>
18 <a href="https://thenewstack.io/using-ai-to-improve-bad-business-writing/">Using AI to Improve Bad Business Writing</a></p>
<p>
19 <a href="https://thenewstack.io/code-in-context-how-ai-can-help-improve-our-documentation/">Code in Context: How AI Can Help Improve Our Documentation</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   00:06 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 23. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wiki.postgresql.org/wiki/PGXN_v2/Architecture'>PGXN V2 Architecture</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I’ve written and published a document outlining the vision and strategy for the next iteration of PGXN. It includes diagrams. Everybody loves diagrams.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	<a href="https://wiki.postgresql.org/wiki/PGXN_v2/Architecture">
			
		</a>
		
			 PGXN Future Architecture 
			<p>High-level diagram of the six logical services making up the proposed future
extension distribution architecture. The Root Registry sits at the center,
providing APIs for the other services to consume for their own use cases.
Trusted instances of those services submit additional data about extensions
via the Interactions service to enhance and enrich the service to better
inform and delight users.</p>
		
	

         
<p>Over on the <a href="https://wiki.postgresql.org/">Postgres Wiki</a> I’ve published a new document for the <a href="https://wiki.postgresql.org/wiki/PGXN_v2">PGXN v2</a>
project: <a href="https://wiki.postgresql.org/wiki/PGXN_v2/Architecture">PGXN v2 Architecture</a>. It has diagrams, such as the one above! From
the introduction:</p>
<blockquote>
<p>This document outlines the project to build extension distribution,
discovery, and packaging tools and services to power the growth,
accessability, and utility of the Postgres extension ecosystem. Taking the
overall Postgres community as its audience, it defines the services to be
provided and the architecture to run them, as well as the strategic vision
to guide project planning and decision-making.</p>
<p>With the goal to think strategically and plan pragmatically, this document
describes the former to enable the latter. As such, it is necessarily
high-level; details, scoping, and planning will be surfaced in more
project-focused documents.</p>
<p>Bear in mind that this document outlines an ambitious, long-term strategy.
If you’re thinking that there’s too much here, that we’er over-thinking and
over-designing the system, rest assured that project execution will be
fundamentally incremental and pragmatic. This document is the guiding light
for the project, and subject to change as development proceeds and new
wrinkles arise.</p>
</blockquote>
<p>For those of you interested in the future of Postgres extension distribution,
please give it a read! I expect it to guide the planning and implementation of
the the new services and tools in the coming year. Please do consider it a
living document, however; it’s likely to need updates as new issues and
patterns emerge. Log in and hit the “watch” tab to stay in the loop for those
changes or the “discussion” tab to leave feedback.</p>
<p>I’ve also moved the <a href="https://justatheory.com/2024/02/pgxn-architecture/" title="Presentation: Introduction to the PGXN Architecture">previously-mentioned</a> document <a href="https://wiki.postgresql.org/wiki/Extension_Ecosystem:_Jobs_and_Tools">Extension Ecosystem: Jobs
and Tools</a> to the wiki, and created a top-level <a href="https://wiki.postgresql.org/wiki/PGXN_v2">PGXN v2</a> and <a href="https://wiki.postgresql.org/wiki/Category:PGXN">PGXN category</a>
for all PGXN-related content. It also includes another new document, <a href="https://wiki.postgresql.org/wiki/PGXN_v2/Service_Disposition">Service
Disposition</a>, which describes itself as:</p>
<blockquote>
<p>A summary of the ambitiously-envisioned future PGXN services and
architecture, followed by an examination of existing services and how they
will gradually be refactored or replaced for the updated platform.</p>
</blockquote>
<p>Check it out for how I expect existing services to evolve into or be replaced
by the updated platform.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/architecture/">Architecture</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/2024/04/pgxn-v2-architecture/future-extension-ecosystem.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:08 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/bluesky'>Building Bluesky: a Distributed Social Network (Real-World Engineering Challenges)</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Bluesky is built by around 10 engineers, and has amassed 5 million users since publicly launching in February this year. A deep dive into novel design decisions, moving off AWS, and more.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Before we start: AI tooling for software development feels like it has hit "peak hype" across mainstream media. We would like to do a "reality check" and find out how engineers and teams are using these tools (and which tools/use cases are genuinely efficient). Please <a href="https://docs.google.com/forms/d/e/1FAIpQLSfVMJZNH52X7x2uC1qsMiVC-atjzZOFIc8fySCBwqro--fU9Q/viewform?usp=sf_link">help us by filling out this survey.</a> </p><p><a href="https://docs.google.com/forms/d/e/1FAIpQLSfVMJZNH52X7x2uC1qsMiVC-atjzZOFIc8fySCBwqro--fU9Q/viewform?usp=sf_link">Fill out the survey on AI tools</a></p><p>We will share the full report with all of you who share detailed insights. Thanks for your help!</p>   <p>‘Real-world engineering challenges’ is <a href="https://newsletter.pragmaticengineer.com/t/real-world-engineering-challenges">a series</a> in which we interpret interesting software engineering or engineering management case studies from tech companies.</p><p><a href="https://bsky.app/">Bluesky</a> is known as a Twitter-alternative. It launched two years ago, with an invite-only beta launch last year. It’s already grown to an impressive 5.5 million registered users. Interestingly for software engineers, Bluesky is also a fascinating engineering project unlike any other mainstream social network. Martin Kleppman, author of the Designing Data Intensive Applications book, is involved as a technical advisor, and has <a href="https://arxiv.org/pdf/2402.03239.pdf">published a paper</a> outlining the novel approaches Bluesky has taken.</p><p>The biggest differences between Bluesky and other large social networks:</p>  <p>Decentralized. Bluesky is a “decentralized social network,” meaning anyone can run their own servers. If Bluesky’s core team turned off all services today, the network would keep functioning. As such, Bluesky offers a way for users to truly own their data and services.</p>  <p>Open source. Nearly everything about Bluesky builds is open source, and <a href="https://github.com/bluesky-social">hosted on GitHub.</a></p>  <p>Rapid growth. The product went from zero to 5 million users in around 12 months after announcing an invite-only beta.</p>  <p>Small team. Bluesky was built with a small team of 3 engineers during the first year, and with 12 software engineers at the time of publication.</p>  <p>Other social networks have achieved some of these things; such as Mastodon allowing users to own their data and identity, and Meta achieving eye-catching growth by getting <a href="http://newsletter.pragmaticengineer.com/p/building-the-threads-app">100 million users in just a week</a>. Still, only Bluesky has pulled off them all.</p><p>Today, we dive into how Bluesky is built, sitting down with its two founding engineers: <a href="https://dholms.github.io/">Daniel Holmgren</a> and <a href="https://bsky.app/profile/pfrazee.com">Paul Frazee</a>. They take us through:</p>  <p>Development timeline. How Bluesky went from a vague idea with few specific details, to a decentralized social network with millions of users.</p>  <p>Experimentation phase. A team of 2-3 engineers prototyped for 9 months, established the development principles, and laid the groundwork for the protocol and app.</p>  <p>v1 architecture. An overview of Bluesky’s architecture at the launch of its public beta offering. This was a Postgres database built on top of AWS, and used Pulumi.</p>  <p>v2 architecture. Extending Bluesky to support “federation,” allowing users to run their own Bluesky instances.</p>  <p>Scaling the database layer. PostgreSQL didn’t scale with the site’s growth, so it was time to migrate. The team chose ScyllaDB and SQLite.</p>  <p>Infra stack: from AWS to on-prem. AWS was becoming too costly, so Bluesky moved over to dedicated data centers and bare-metal machines.</p>  <p>Reality of building a social network. Typical firefighting issues, Elon Musk, and outages not being “life-or-death” crises.</p>   1. Development timeline <p>Bluesky has been in development for just over 2 years, and has been publicly available for around 12 months. Here’s the timeline:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F359800b3-7bde-421b-9645-fbed2bf32a14_1496x1032.png">    </a>Bluesky’s development timeline and major milestones <p>Adding in the three phases we’ll discuss below:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75476852-faa7-420b-98be-92f82f5d29b6_1600x1003.png">    </a>Bluesky’s development, split into three phases  Phase 1: Experimentation <p>The first 10 months of the project between January and October 2022 were all about exploration, and the team started to work fully in the open after 4 months. The first project the team open sourced was <a href="https://bsky.social/about/blog/5-4-2022-working-in-public">Authenticated Data Experiment</a> (ADX), an experimental personal data server and a command-line client, accompanied by a network architecture overview.</p><p>In April 2022, heavy Twitter user, Elon Musk, raised the prospect of potentially acquiring the site, which created interest in alternatives to the bird app, as any major change in a market-leading social network does.</p><p>The <a href="https://github.com/bluesky-social/social-app/commits/main/?since=2022-06-08&amp;until=2022-09-01&amp;after=3abf302b0b189c50acf11489bf60bdaeb187b722+34">first commit</a> for the Bluesky mobile app was made in June 2022, and Paul Frazee worked on it. It started as a proof-of-concept to validate that the protocol worked correctly, and to aid protocol development via real-world use. Conventional wisdom says that prototypes are thrown away after serving their purpose.&nbsp;</p><p>However, in this case this mobile app that a single person had built, became the production app, following the unforeseen spike of interest in it caused by takeover news at Twitter. This is a good reminder that real world events can push conventional wisdom out of the window!</p><p>In October 2022, the team announced the Authenticated Transfer Protocol (AT Protocol) and the app’s waitlist, just a few days after news that Elon Musk was to acquire Twitter. This led many tweeters to seek alternative social networks, and drove a major signup spike for Bluesky’s private beta. This development put pressure on the Bluesky team to seize the unexpected opportunity by getting the protocol and app ready for beta users. See <a href="https://atproto.com/">details on the AT Protocol.</a></p> Phase 2: invite-only launch and the first 1M users <p>In October 2022, Bluesky consisted solely of Jay Graber CEO, and two software engineers; Daniel and Paul. Engineer #3, Devin, joined the same month. Announcing the AT Protocol and waitlist generated some media buzz and Bluesky attracted more interest during this period.</p><p>In March 2023, the company was confident that the protocol and mobile app were stable enough to invite more users by sending invites.</p><p>“Blocking” was implemented in a single night. After the app opened up to more users, there was an influx of offensive posts and of users verbally harassing other accounts. This made it clear that implementing blocks to restrict individual accounts from viewing and commenting on a user’s posts, was urgently-needed functionality.</p><p>The three earliest developers – Paul, Devin and Daniel – jumped on a call, then got to work. In the community, developers saw the pull requests (PRs) on this feature appear on GitHub, and started to point out bugs, and cheer on the rapid implementation. They wrapped it up and launched the feature by the end of the same day. To date, this is the most rapidly-built feature, and is still used across the protocol and the app!</p><p>In June 2023, Bluesky <a href="https://bsky.social/about/blog/6-02-2023-beta-update">passed</a> the 100,000-users milestone when the team numbered 6 developers, who’d shipped features like custom feeds, blocking and muting, moderation controls, and custom domains. A web application built on React Native was also in production.</p><p>In September 2023, Bluesky passed 1 million users – a 900,000 increase in just 3 months!</p> Phase 3: Preparing for public launch <p>In the 6 months following the 1 million-user milestone, the focus was on preparing to open up Bluesky to the public with no waitlist or throttling of invites.</p><p>Federation (internal.) To prepare for “proper” federation, the team made architecture changes to enable internal federation of Bluesky servers.&nbsp;</p><p>Federation is a key concept in distributed networks. It means a group of nodes can send messages to one another. For Bluesky, it meant that – eventually – users should be able to run their own PDS instances that host their own user information (and user information of users on that server.) And the Bluesky network operates seamlessly with this distributed backend.</p><p>A new logo and a reference to Twitter. The team prepared a new logo for launch, and announced it in December 2023:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22a04102-e99d-456b-8275-4355bc5701f0_1600x838.png">    </a>The old and new logo <p>The butterfly logo is intended as a symbol of freedom and change. Existing centralized social media platforms – like X (formerly Twitter,) Instagram, TikTok, and Youtube – are platforms that want to lock users into their website and apps. Bluesky, on the other hand, offers its protocol, but doesn’t dictate which apps or websites people use. It doesn’t even want to dictate the hosting of content:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa98a6f2-d213-4c67-a355-51c7c1ce3a29_1600x1045.png">    </a>The final slide of Jay Graber’s presentation for Twitter about the vision for Bluesky. It was Twitter that granted Bluesky its initial $13M funding in 2021 – partially based on this vision. The image visualizes the blue bird freed from a closed platform into Bluesky’s open ecosystem. Source: <a href="https://bsky.social/about/blog/12-21-2023-butterfly">Bluesky</a> <p>Let’s dive into each phase of the building process.</p> 2. Experimentation phase <p>During Bluesky’s first 9 months (January-September 2022) two software engineers built the protocol and apps – Daniel Holmgren and Paul Frazee – and Jay the CEO signed off design decisions. The first couple of months were about experimenting and tech “spiking,” which means timeboxing the time and effort spent building and trying out ideas. Here’s Paul:</p><blockquote><p>“We would greenfield for a period, then attack what we had just created to see if it holds up. We gave the existing technologies a really close look; if we didn’t see meaningful improvements from the existing protocols, then we decided we’d use what was already out there.”</p></blockquote><p>When the direction wasn’t clear, the team kept trying out new approaches, says Daniel:</p><blockquote><p>“We set out to use as many existing specs as we could. We spent a lot of time early on investigating things like Activity Pub and seriously trying to figure out how we could make it work, and realizing that it didn't really work for our use case.”</p></blockquote> Development principles <p>The still-small team set up principles to ensure continuous progress:</p>  <p>No backward steps. Ease of use, scale, and feature developer experience, can not be worse than existing social networks’.</p>  <p>Unify app development with protocol development. Never make tech decisions in isolation from practical use cases.</p>  <p>Don’t be precious! If an idea or design doesn’t work, just throw it out!</p>   Approach to building a new, novel decentralized protocol <p>The team prioritized flexible design choices in order to not lock themselves into a technology, until they knew exactly what they were building. Not coupling the data layer too closely with Postgres is an example of this. See below.</p><p>Building for flexibility, not scalability, was deliberate. The idea was to swap this approach to prioritize scale once everyone knew exactly what to build. The knowledge that decisions are hard to undo made the team’s own decision-making more thorough, Daniel reflects:</p><blockquote><p>“The most difficult part of building Bluesky has been the constant awareness that small decisions you make may be locked in for years and have ripple effects. In a decentralized environment, these can be difficult to unwind. It puts a lot of weight on every decision, and we have to double and triple check choices that we make so that we hopefully don’t regret them.”&nbsp;</p></blockquote><p>Inventing new approaches was never a goal. The original idea was to take a protocol or technology off the shelf, and push it as far as possible to reveal a requirement that didn’t quite fit. For example, <a href="https://atproto.com/guides/lexicon">Lexicon</a> – the schema used to define remote procedure call (RPC) methods and record types – started out as JSON schemas. The team tried hard to keep it lightweight, and stuck to JSON schemas. But they ended up bending over backwards to make it work. In the end, the team decided to fork off from JSON schemas and added features to it, which is how Lexicon was born.</p><p>Bluesky gets criticism for inventing new approaches which are non-standard across decentralized networks. Paul explains it like this:</p><blockquote><p>“We never set out to live the ‘not invented here’ (NIH) syndrome. I don’t think anyone building something new has this goal. In the end, it just naturally evolved in this direction.</p><p>No one had done a high-scale decentralized social network before this! If someone had, we probably wouldn’t have needed to invent as many things.”</p></blockquote><p>Bluesky takes inspiration from existing web technologies. As Daniel puts it:</p><blockquote><p>“The AT Protocol is a pretty typical JSON API collection over HTTP. The architecture of Bluesky looks very similar to a traditional social media data center turned inside out. The firehose API looks a lot like Kafka – and we’re probably going to shard it in a similar way.”</p></blockquote> 3. v1 architecture: not really scalable and not federated – yet  Infrastructure choices <p>PostgreSQL was the team’s database of choice when starting development. Postgres is often called the “Swiss Army knife of databases” because it’s speedy for development, great for prototyping, with a vast number of extensions. One drawback is that Postgres is a single bottleneck in the system, which can cause issues when scaling to handle massive loads that never materialize for most projects.</p><p>For the team, using Postgres worked really well while they were unsure exactly what they were building, or how they would query things. Paul’s summary of the choice to use Postgres:</p><blockquote><p>“You start with a giant Postgres database and see how far that can take you, so that you can move quickly early on.”</p></blockquote><p>AWS infrastructure was what the team started with because it’s quick to set up and easy to use, says Daniel:</p><blockquote><p>“We were running everything out of AWS, and that is great because you can just spin up new VMs very easily, and spin up new stacks and services easily.”</p></blockquote><p>The first infra hire at Bluesky, Jake Gold, iterated on the AWS setup:</p><blockquote><p>“The basic idea we have right now is we’re using AWS, we have auto-scaling groups, and those auto-scaling groups are just <a href="https://aws.amazon.com/ec2/">EC2</a> instances running Docker Community Edition (CE) for the runtime and for containers. And then we have a load balancer in front and a Postgres multi-availability zone instance in the back on <a href="https://aws.amazon.com/rds/">Relational Database Service</a> (RDS). It’s a really simple setup.”</p></blockquote><p>To facilitate deployments on AWS, the team used infrastructure-as-code service, <a href="https://www.pulumi.com/">Pulumi</a>.</p><p>Modularizing the architecture for an open network was an effort the team kicked off early. The goal of modularization was to spin out parts of the network which users could host themselves. Daniel says:</p><blockquote><p>“Our early insight was that we should give developers building on top of Bluesky the ability to focus on the parts of the network that they want to focus on. This is the microservices part.</p><p>An external developer building a feed should not need to index every “like” in the network. Someone self-hosting their own account should not need to consume thousands of posts to create a timeline. You can split the network into specific roles and have them work in concert.”</p></blockquote> Personal Data Server <p>At first, the architecture of Bluesky consisted of one centralized server, the PDS (Personal Data Server.)</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d0890b3-44a5-4faf-8c49-adc1cc2e9a8d_1234x606.png">    </a>The Bluesky backend in March 2023 <p>The strategy was to split this centralized service into smaller parts and allow for federation, eventually.</p><p>Bluesky being a federated network means individual users can run their own “Bluesky instance” and curate their own network.</p> The feed generator  <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ba7f05-e56f-4d34-af21-99741598f25d_1382x1026.png">    </a>The backend in May 2023, after the feed generator was moved into its own component <p>In May 2023, the Bluesky team moved the feed generator to its own role. This service allows any developer to create a custom algorithm, and choose one to use. Developers can <a href="https://github.com/bluesky-social/feed-generator">spin up a new Feed Generator service</a> and make it discoverable to the Bluesky network, to add a new algorithm. Bluesky also allows users to choose from several predefined algorithms.</p><p>The Feed Generator interface was the first case of Bluesky as a decentralized network. From then, the Bluesky network was not solely the services which the Bluesky team operated, it was also third-party services like Feed Generator instances that plugged into the Bluesky network.</p> Dedicated “Appview” service <p>For the next step, the view logic was moved from the PDS, to an “Appview” service. This is a pretty standard approach for backend systems, to move everything view-related to its own service, and not to trouble other systems with presenting data to web and mobile applications.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa021c527-98dc-42cc-a285-368be5464ccc_1262x908.png">    </a>Splitting out application view logic into its own service  Relays to crawl the network <p>In the future, there could be hundreds or thousands of PDSs in the Bluesky network. So, how will all the data be synchronized with them? The answer is that a “crawler” will go through all these PDSs. In preparation for this crawl the team introduced a Relay service:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c32635d-bd78-4a05-938f-c996e1d1bcd3_1248x922.png">    </a>Preparing for federation and multiple PDSs by adding a Relay service to “crawl” later  4. v2 architecture: scaleable and federated <p>The v1 architecture needed to evolve in order to support full federation, and the team always planned to move on from it. But they expected v1 to last longer than only 6 months.</p> Federation <p>Federation sandbox. Before shipping a first version of federation, the team built a <a href="https://docs.bsky.app/blog/federation-sandbox">Federation Sandbox</a> to test the architecture, as a safe space to try new features like modulation and curation tooling.</p><p>Internal federation. To prepare for federation proper, the next refactoring was to add support for multiple Personal Data Servers. As a first step, the Bluesky team did this internally. Users noticed nothing of this transition, which was intentional, and Bluesky was then federated! Proving that federation worked was a large milestone.</p><p>As a reminder, federation was critical to Bluesky because it made the network truly distributed. With federation, any user can run their own Bluesky server.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06d54413-1da4-443e-923a-da91f3f16f11_1358x994.png">    </a>Prior to federation, Bluesky created 10 PDS services, wrapped into an Entryway interface <p>The “internally federated” PDS servers worked exactly like a self-hosted PDS. Bluesky made one addition, to wrap the internal PDS servers into a new service called “Entryway,” which provides the “bsky.social” identity to the PDSes. Entryway will become the “official” Bluesky OAuth authorization server for users who choose bsky.social servers, and one operated as a self-hosted server.</p><p>Later, Bluesky increased the number of internal PDS servers from 10 to 20 for capacity reasons, and to test that adding PDS servers worked as expected.</p><p>External federation. With everything ready to support self-hosted Personal Data Servers, Bluesky flipped to switch, and started to “crawl” those servers in February 2024:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F931dce11-23dc-4836-befc-4376db9aa421_1352x1212.png">    </a>Adding support for “proper” federation. Anyone can self-host a “Bluesky instance” in PDS form <p>To date, Bluesky has more than 300 self-hosted PDSs. This change has made the network properly distributed, anyone wanting to own their data on Bluesky can self-host an instance. Over time, we could also see services launch which self-host instances and allow for full data ownership in exchange for a fee.</p> Appview: further refactoring <p>Recently, Bluesky further refactored its Appview service, and pulled out the moderation functionality into its own service, called <a href="https://github.com/bluesky-social/ozone">Ozone</a>:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a4114e7-8815-46d7-a777-9b2728a46fc1_1306x1248.png">    </a>Moving moderation and post labeling functionality from Appview to Ozone <p>Users can run their own Ozone service – meaning to be a moderator in the Bluesky system. Here are <a href="https://github.com/bluesky-social/ozone/blob/main/HOSTING.md">details on how to self-host this service</a>, and <a href="https://bsky.social/about/blog/03-12-2024-stackable-moderation">more about Ozone</a>.</p> An architectural overview, with Martin Kleppman <p>Martin is the author of the popular software engineering book, Designing Data Intensive Applications, and he also advises the Bluesky team in weekly calls.</p><p>Martin and the Bluesky team published a paper describing the Bluesky system, <a href="https://arxiv.org/pdf/2402.03239.pdf">Bluesky and the AT Protocol: Usable decentralized social media</a>. In it, they offer a detailed overview of the architecture:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87a2eec4-af41-4752-816f-15f0d8293394_1432x748.png">    </a>The architecture of Bluesky. Image source: <a href="https://arxiv.org/pdf/2402.03239.pdf">Bluesky and the AT Protocol</a> <p>The diagram above shows how data flows occur in the application:</p>  <p>Personal data server (PDS): these can be Bluesky-hosted (around 20 today) or self-hosted (around 300)</p>  <p>Relays: these collect events from the PDSs. Bluesky has its “official” relay hosted in its own infrastructure, but other developers can set up alternative relays that listen to all PDSs.</p>  <p>Firehose: the output of the relays.</p>  <p>Labelers and feed generators: these digest firehose events. They can be Bluesky-hosted, or be hosted independently of Bluesky.</p>  <p>App View: The Bluesky-hosted “official” app view, or alternate app views</p>  <p>Data flowing back to PDSs: feed generators hosted by Bluesky or externally, feed events data back to the PDSs.</p>   5. Scaling the database layer  Scaling issues with Postgres <p>Scaling issues emerged 2-3 months after the public beta launch in mid-2023.</p>  <p>Connection pool issues and lock contention. The Postgres connection pool backup and Node’s event loop got into a bad feedback loop. The team observed Postgres lock contention issues. This refers to multiple processes trying to access the same data simultaneously, but the data is locked to all except one process. For example, when multiple processes attempt to update the same row.</p>  <p>Small Postgres outages. Postgres doesn’t give the developer much control over which <a href="https://www.postgresql.org/docs/current/using-explain.html">query plan</a> it will take. Bluesky had a few smaller outages due to a query plan randomly flipping to something that ran about 1,000x times slower.</p>  <p>The need for horizontal scaling. Horizontal scaling is adding more machines to a service, so that the throughput of this system improves linearly with each new machine. But Postgres does not support horizontal scaling because it runs as a single database with transactional guarantees, meaning it becomes a bottleneck – if a necessary one – for the entire network.</p>  <p>As a reminder, the team was still tiny when all these scaling challenges emerged. There were only 6 developers (<a href="https://bsky.app/profile/dholms.xyz">Daniel</a>, <a href="https://bsky.app/profile/divy.zone">Devin</a>, <a href="https://bsky.app/profile/bnewbold.net">Bryan</a> and <a href="https://bsky.app/profile/jacob.gold">Jake</a> on the backend, and <a href="https://bsky.app/profile/pfrazee.com">Paul</a> and <a href="https://bsky.app/profile/anshnanda.com">Ansh</a> on the frontend). Then in summer 2023, Daniel had a dream:</p><blockquote><p>“After one stressful day, I dreamt that me, Jay [Bluesky’s CEO], and Devin were in my backyard. There were snakes everywhere you looked. We were going to wrangle and round up the snakes in a panic. But that that point, Devin stops and says to all of us: ‘wait, wait, guys, I think there’s a Postgres extension for this!’”</p></blockquote> ScyllaDB replacing Postgres <p>The team knew they needed a horizontally scalable data storage solution, with fine-grained control of how data is indexed and queried.</p><p>ScyllaDB was an obvious choice because it supports horizontal scalability due to being a wide-column database (a NoSQL type.) Wide-column databases store data in flexible columns that can be spread across multiple servers or database rows. They can also support two rows having different columns, which gives a lot more flexibility for data storage!</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F878c2812-8d3a-4926-a3da-cd3211a5b7d9_1550x812.png">    </a>Wide-column databases store data in columns so it’s highly scalable and flexible. Two rows in one table can have different types or numbers of columns. Source: <a href="https://docs.aws.amazon.com/whitepapers/latest/choosing-an-aws-nosql-database/types-of-nosql-databases.html">AWS</a> <p>The biggest tradeoffs:</p>  <p>Data must be denormalized, meaning it isn’t stored as efficiently as in a relational database. Basically, you’ll store more data and require more storage space.</p>  <p>Data needs to be indexed on write. Writing to a wide column database is more expensive than to a relational database. For each row and column changed, the relevant indexes need to be updated, which typically makes these databases more write-intensive than relational ones.</p>  <p>The team was satisfied with their early choice of Postgres, says Daniel:</p><blockquote><p>“Postgres was great early on because we didn’t quite know exactly what questions we’d be asking of the data. It let us toss data into the database and figure it out from there. Now we understand the data and the types of queries we need to run, it frees us up to index it in Scylla in exactly the manner we need and provide APIs for the exact queries we’ll be asking.”</p></blockquote> SQLite <p>ScyllaDB is used for the Appview, which is Bluesky’s most read-heavy service. However, the Personal Data Servers use something else entirely: SQLite. This is a database written in the C language which stores the whole database in a single file on the host machine. SQLite is considered “zero configuration,” unlike most other databases that require service management – like startup scripts – or access control management. SQLite requires none of this and can be started up from a single process with no system administrative privileges. It “just works.”</p><p>Daniel explains why SQLite was ideal for the PDSs:</p><blockquote><p>“We took a somewhat novel approach of giving every user their own SQLite database. By removing the Postgres dependency, we made it possible to run a ‘PDS in a box’ without having to worry about managing a database. We didn’t have to worry about things like replicas or failover. For those thinking this is irresponsible: don’t worry, we are backing up all the data on our PDSs!”</p><p>SQLite worked really well because the PDS – in its ideal form – is a single-tenant system. We owned up to that by having these single tenant SQLite databases.</p><p>We also leaned into the fact that we’re building a federated network. We federated our data hosting in the exact same manner that it works for non-Bluesky PDSs.”&nbsp;</p></blockquote><p>Migrating the PDSs from Postgre to SQLite created fantastic improvement in operations, Daniel adds:</p><blockquote><p>“PDSs have been a dream to run since this refactor. They are cheap to operate (no Postgres service!) and require virtually no operational overhead!”</p></blockquote> 6. Infra stack: from AWS to on-prem <p>Bluesky’s infrastructure was initially hosted on Amazon Web Services (AWS) and the team used infrastructure-as-a-code service, <a href="https://www.pulumi.com/">Pulumi</a>. This approach let them move quickly early on, and also to scale their infra as the network grew. Of course, as the network grew so did the infrastructure bill.&nbsp;</p> Move to on-prem <p>Cost and performance were the main drivers in moving on-prem. The team got hardware that was more than 10x as powerful as before, for a fraction of the price. How was this decision made? A key hire played a big role.</p><p>Bluesky’s first hire with large-scale experience was Jake Gold, who joined in January 2023, and began a cost analysis of AWS versus on-prem. He eventually convinced the team to make this big change.</p><p>But how did the team forecast future load, and calculate the hardware footprint they’d need? Daniel recalls:</p><blockquote><p>“We looked at the trends and tried to make a safe bet. We were thinking: ‘okay, today we're over-provisioned. We want to stay over-provisioned, so we have room to grow without upgrading the hardware, but also just so we have stability if something happens in the world, and everyone decides to post about it.’</p><p>We built our architecture to be horizontally scalable so that we can add more capacity just by throwing more machines at it. There is some lead time to buying new machines, but we have space in the rack. We have room in the network connections. The switches are good for it.</p><p>If we need to scale, it’s really just about ‘get some more servers and hook them up!’ We can get to twice the capacity after doubling the machines we’re running in our data center. This is sweet!”</p></blockquote><p>Becoming cloud-agnostic was the first step in moving off AWS. By June 2023, six months after Jake joined, Bluesky’s infrastructure was cloud agnostic.&nbsp;</p><p>Bluesky always has the option of using AWS to scale if needed, and is designed in a way that it would not be overly difficult to stand up additional virtual machines on AWS, if the existing infrastructure has capacity or scaling issues.</p><p>Today, the Personal Data Servers are bare-metal servers hosted by cloud infrastructure vendor, <a href="https://www.vultr.com/">Vultr</a>. Bluesky currently operates 20 and shards them so that each PDS supports about 300,000 users.</p> Bluesky’s load by the numbers <p>Currently, Bluesky’s system sees this sort of load:</p>  <p>60-100 events/second received by the firehose service, which is the “main” service that emits messages sent on the network in real time. During the public launch of Bluesky in February, the peak was 400 events/second.</p>  <p>400 timeline loads/second. A timeline load is when a user (or client) makes a request to fetch their current timeline.</p>  <p>3,500 requests/second across the network.</p>   7. Reality of building a social network <p>To close, we (Gergely and Elin) asked the teams some questions on what it’s like to build a high-growth social network.</p><p>What is a typical firefighting issue you often encounter?</p><blockquote><p>“Every influx of users brought new problems, and we found ourselves doing quite a bit of firefighting. One day, after a particularly notable incident, growth showed no signs of stopping, and we had to temporarily disable signups in order to keep the service running.” – Daniel</p></blockquote><p>What were the events referred to as “Elon Musk?”</p><blockquote><p>“We never quite knew when a user bump was going to come, and invites were out in the wild waiting to be used. Then something would happen, and thousands of users suddenly joined. We started referring to these days as EMEs (Elon Musk Events) because they were normally precipitated by some change on Twitter.” – Daniel</p><p>“It was a bit like throwing a party and everybody showing up 2 hours early, while you’re still setting up the chairs and telling people to get drinks from the fridge. And then about ten times more people show up than expected.” – Paul</p></blockquote><p>How are outages different for a social network?</p><blockquote><p>“Disabling signups or pausing the service is never fun to do, but it actually created a bunch of excitement and a strange sense of pride in the user base.” – Daniel</p><p>“Outages are not fun, but they’re not life and death, generally. And if you look at the traffic, usually what happens is after an outage, traffic tends to go up. And a lot of people who joined, they’re just talking about the fun outage that they missed because they weren’t on the network.” – Jake</p></blockquote><p>The whole developer team is on Bluesky, and actively responding to user feedback. How do you do this, and why?</p><blockquote><p>“People just pinging us in the app and explaining their problem, is so good. We can just respond, "Hey, can you give me a screenshot? What platform are you on?" It's such a fast support turnaround. The big benefit of building a social app is that your customers are right there, and will tell you if something's not working.</p><p>Real time user feedback was how mute words got prioritized, recently. In terms of a signal about how important something is, when you start getting PRs to add the feature, and you get a ton of people plus-oneing the issue – not to mention people asking for it in the app – that tells you a lot.” – Paul</p></blockquote> Takeaways <p>Gergely here. Many thanks to <a href="https://bsky.app/profile/dholms.xyz">Daniel</a> and <a href="https://bsky.app/profile/pfrazee.com">Paul</a> for part one of this deep dive into how Bluesky works! You can <a href="https://bsky.app/">try out Bluesky</a> for yourself, learn more about <a href="https://atproto.com/">Bluesky’s AT Protocol</a>, or <a href="https://arxiv.org/pdf/2402.03239.pdf">about its architecture</a>. And I’m also <a href="https://bsky.app/profile/gergely.pragmaticengineer.com">on Bluesky</a>.</p><p>Decentralized architectures require a different way of thinking. I’ll be honest, I’m so used to building and designing “centralized” architecture, that the thought of servers being operated outside of the company is very alien. My immediate thoughts were:</p>  <p>Is it secure enough? Malicious actors could run anything on those servers and attempt to overload the network or exploit vulnerabilities in the system. The Bluesky team also stressed how the security model is something you thoroughly need to consider as you design APIs for such a system.</p>  <p>What about external nodes that don’t ever update the version of the software? How do they get bug fixes? And what about versioning? How to ensure “outdated clients” are cut off from the network?</p>  <p>Finally, I thought; “wow, this kind of reminds me of the confusion I initially felt about <a href="https://en.wikipedia.org/wiki/Skype_protocol">Skype’s peer-to-peer network</a>”</p>  <p>I’m delighted we did a deep dive about Bluesky because it has forced me to think more broadly. A server drawing on a diagram no longer just means “a group of our servers,” it can also mean “plus, a group of external servers.” Once this is understood, it’s easy. And this skill of designing distributed and federated systems may be useful in the future, as I expect the concept of distributed architecture to become more popular.</p><p>It’s impressive what a tiny team of experienced engineers can build. I had to triple-check that Bluesky’s core team was only two engineers for almost nine months, during which time they built the basics of the protocol, and made progress with the iOS and Android apps. Even now, Bluesky is a very lean team of around 12 engineers for the complexity they build with and the company’s growth.&nbsp;</p><p>In the next part of this deep dive into Bluesky, we cover more on how the team works.&nbsp;</p><p>Owning your own infrastructure instead of using the cloud seems a rational choice. Bluesky found large savings by moving off AWS once they could forecast the type of load they needed. Jake Gold, the engineer driving this transition, has been vocal about how cloud providers have become more expensive than many people realize. Speaking on the podcast, Last Week in AWS, <a href="https://www.lastweekinaws.com/podcast/screaming-in-the-cloud/building-reliable-open-source-social-media-with-jake-gold/">he said</a>:</p><blockquote><p>“With the original vision of AWS I first started using in 2006, or whenever launched, they said they would lower your bill every so often, as Moore’s law makes their bill lower. And that kind of happened a little bit here and there, but it hasn’t happened to the same degree as I think we all hoped it would.”</p></blockquote><p>Don’t forget, it’s not only Bluesky which rejects cloud providers for efficiency. We previously did a deep dive into travel booking platform Agoda, <a href="https://newsletter.pragmaticengineer.com/p/inside-agodas-private-cloud">and why it isn’t on the cloud</a>.</p><p>I’m slowly changing my mind about decentralized and federated social networks. I also tried out Mastodon, which is another federated social network, when it launched. At the time, Mastodon felt a lot more clunky in onboarding than Bluesky. You had to choose a server to use, but different servers have different rules, whereas Bluesky was much smoother. Still, as a user, I was blissfully unaware of how different these social networks are from the dominant platforms.</p><p>It was only by learning about Bluesky’s architecture that I appreciated the design goals of a decentralized social network. Currently, mainstream social networks are operated exclusively by the company that owns them. But a decentralized network allows servers to be operated by other teams/organizations/individuals. This might not seem like a big deal, but it means a social network is no longer dependent on the moderation policies of a parent company.</p><p>Decentralized social networks also allows users to use custom algorithms, websites and mobile apps, which creates opportunities for developers to build innovative experiences. In contrast, you cannot build a custom third-party client for X, Threads, or LinkedIn.</p><p>I’m still unsure how much mainstream appeal decentralized social networks hold for non-technical people, but I’m rooting for Bluesky, Mastodon, and the other decentralized social apps. Perhaps they can challenge Big Tech’s dominance of social media, or at least change people’s understanding of what a social network can be.</p><p>In a follow-up issue, we’ll look deeper into the engineering culture at Bluesky: the company culture, a deeper look at the tech stack, and how they are building seemingly so much with a surprisingly small team and company. I suspect we can all learn a lot in how a dozen engineers help a startup scale to more than 5 million users.</p><p>Enjoyed this issue? Subscribe to get this newsletter every week 👇</p><p><a href="https://newsletter.pragmaticengineer.com/subscribe?">Subscribe now</a></p><p></p><p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F359800b3-7bde-421b-9645-fbed2bf32a14_1496x1032.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:01 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2024/04/5776/'>NIST SP 800-63Bへの補遺が出ました〜パスキーの追加です</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  近年、多要素認証(MFA)の普及が進む中、ユーザーの利便性とセキュリティのバランスを取ることが課題となっています。MFAとは、パスワードに加え、生体認証やワンタイムパスワードなど複数の認証方式を組み合わせることで、アカウントへの不正アクセスを防ぐセキュリティ手法です。しかし、MF…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>近年、多要素認証(MFA)の普及が進む中、ユーザーの利便性とセキュリティのバランスを取ることが課題となっています。MFAとは、パスワードに加え、生体認証やワンタイムパスワードなど複数の認証方式を組み合わせることで、アカウントへの不正アクセスを防ぐセキュリティ手法です。しかし、MFAの導入はユーザーにとって面倒な手順が増えることを意味します。この課題を解決するために登場したのが、Syncable Authenticator(同期可能認証器)、別名Passkey(パスキー)と呼ばれる新しい認証技術です。</p>



<p>同期可能認証器は、認証に用いる秘密鍵を複数のデバイス間で同期できるようにすることで、ユーザーはどのデバイスでも同じ認証情報を使えるようになります。これにより利便性が大幅に向上します。一方で、秘密鍵を複数の端末で共有することはセキュリティ上のリスクを伴います。そこで、米国国立標準技術研究所(NIST)は、シンカブル認証器の安全な利用に関するガイドライン(SP 800-63Bの補遺)を公開しました。</p>



 
 <a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63Bsup1.pdf">NIST: Incorporating Syncable Authenticators Into NIST SP 800-63B</a> 
 



<p>この文書は、NIST Special Publication 800-63Bへの補足であり、認証キーを複製しデバイス間で同期できる同期可能認証器(パスキーとしても知られている)の使用に関するガイダンスを提供するものです。主なポイントは以下の通りです。</p>



 
 適切に構成された同期可能認証器は、中間者攻撃、検証者のなりすまし、リプレイ攻撃などの脅威を軽減し、認証の意図を提供することで、認証保証レベル2(AAL2)を達成できます。AAL2は、フィッシング耐性など高度なセキュリティを求める一方で、ある程度の使いやすさも確保するレベルです。 



 本文書では、キーの生成、保存、アクセス制御に関する一定の要件を満たすことを条件に、同期可能認証器の認証キーの複製を許可するようSP 800-63Bを更新しています。これによって、政府機関でもパスキーを使うことができるようになります。 



 WebAuthn仕様のフラグを使用して、認証器がAAL2の要件を満たしているかどうかを判断するなど、実装上の考慮事項について説明しています。エンタープライズでのユースケースでは、Attestationを利用してオーセンティケータの機能を検証することができます。 



 同期可能認証器の潜在的な脅威と課題を概説し、不正なキーの使用、同期ファブリックの侵害、失効の難しさなどを示し、それらを軽減するための方法を提案しています。 



 本文書は、一部の実装におけるユーザー間のキー共有のリスクを認識し、エンタープライズおよび公開向けのユースケースについてガイダンスを提供しています。 
 



<p>全体として、この補足資料は、適切に導入されれば、便利でフィッシング耐性のある認証を提供できる同期可能認証器の統合について、機関がリスクに基づいた informed な意思決定を行うことを目的としています。</p>



<p>本補遺は、AAL2への同期可能認証器の適合性を検証する上で重要な基準を提供しています。また、鍵の不正使用やクラウドストレージの侵害といった新たな脅威についても言及し、対策を提言しています。同期可能認証器の潜在的なリスクを認識しつつ、適切に導入することで、利便性を損なわずにセキュリティを向上できることを示しています。</p>



<p>本補遺は、セキュリティ担当者だけでなく、同期可能認証器の導入を検討する全ての関係者にとって有益な情報を提供しています。NISTによる公的なガイドラインの存在は、同期可能認証器の安全性に対する信頼を高め、普及を後押しするでしょう。ユーザー数の増加は、さらなる技術革新を促します。本補遺は、セキュリティと利便性を両立する新認証技術の健全な発展に寄与する重要な一里塚になると考えられます。</p>



<p>なお、背景等については @phr_eidentity さんのこの<a href="https://idmlab.eidentity.jp/2024/04/nist-sp800-63b.html?m=1">記事</a><a href="https://www.sakimura.org/2024/04/5776/#easy-footnote-bottom-1-5776" title="富士榮 NIST SP800-63Bへの同期可能な認証器に関する補足文書がリリースされています https://idmlab.eidentity.jp/2024/04/nist-sp800-63b.html?m=1 ">1</a>が詳しいです。</p>



<p></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   12:21 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2024/04/5762/'>アイデンティティをやっているなら読むべき本〜『メタバース進化論――仮想現実の荒野に芽吹く「解放」と「創造」の新世界』</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  まずはじめにアイデンティティ業界の人へ とりあえず、アイデンティティに関心のある人は、本書『メタバース進化論』を読んだほうが良い。メタバース・バ美肉として現実の物理的制限から存在を解き放ち、抽象空間で取り扱うことができるようになることによって、アイデンティティとプライバシー管理の…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 まずはじめにアイデンティティ業界の人へ 



<p>とりあえず、アイデンティティに関心のある人は、本書<a href="https://amzn.to/4d6ddRF">『メタバース進化論』</a>を読んだほうが良い。メタバース・バ美肉として現実の物理的制限から存在を解き放ち、抽象空間で取り扱うことができるようになることによって、アイデンティティとプライバシー管理の課題や要件がこんなにも明確になるのか！曰く</p>



 
 そもそも「アイデンティティ（自己同一性）とは、私たちが私たち自身をどのようなものと捉えるかという「認識」であり、他者や社会からそれが認められているという「感覚」のことです。 



 物理現実では、基本的に生まれたままの名前・姿・声を受け入れるしかありませんでした。つまり、アイデンティティとは「与えられる」ものでした。 



 基本的には与えられた固定のものを「受け入れる」しかなかった物理現実時代のそれとは違い、メタバース時代のアイデンティティは自由に「デザインする」ものになり、「なりたい自分」としての人生を送ることが可能になる 



 人間を分割可能な「分人（Dividual)」として捉える 



 複数のアイデンティティを「切り替える」ことで人生を自由にデザインできる 



 プラトンの「イデア論」〜見ている現実世界は、実体ではなく、イデアの影にすぎないのだ 



 アイデンティティを自在にデザインして「なりたい自分」になれるメタバース 
 



<p>（出所）『メタバース進化論』</p>



<p>わたしの本<a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-1-5762" title="<a href=&quot;https://amzn.to/3xXfqP6&quot;>﨑村夏彦『デジタルアイデンティティ』日経BP (2021)</a>">1</a>やブログ、講演などに接しているかたは、「はいはいアレね」と思うだろう。次の図は、2011年6月に書いた<a href="https://www.sakimura.org/2011/06/1124/">「非技術者のためのデジタル・アイデンティティ入門」</a>の頃から使いまわしている図だが、ほとんど同様のことを言っている。分人 v.s. Identity (Partial Identity) など使っている言葉は違えど殆どマッピングできる。</p>



<p>ただ、多分、本書のほうが100倍わかりやすい。これは、この図では自己像と言って抽象的に扱っているものにアバターという実体を与えて手触り感を持たせているのと同時に、何でもかんでも「属性」という分類でくくってしまわずに、「言霊世界」「視覚世界」「音響世界」のようにカテゴライズして説明しているからだろう。もちろん単純な筆力の違いもあるが。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/04/image-3.png?ssl=1"></a>



<p>【図表1】自己像（アイデンティティ）と幸福追求</p>



<p>とりわけ重要なのが、魂の自由性とか、アイデンティティは他者から与えられるものではなく自分で確立するものだというところだ。このあたり、「アイデンティティの専門家」でも忘れている人達が多い。神だとか政府だとかに与えてもらいそれを受容するものだという中世的な考え方の人たちが多いのだ。</p>



 中世的アイデンティティ v.s. 近代的アイデンティティ 



<p>近代的な個人観と中世的な個人観の主な違いは以下のようにまとめらるだろう。</p>



<p>中世的個人観</p>



 
 個人は共同体の一部として存在し、共同体からの役割や地位によって定義されていた。<a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-2-5762" title="<a href=&quot;https://archives.bukkyo-u.ac.jp/rp-contents/BS/0026/BS00260L276.pdf&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;>新矢 昌昭『近代 における孤独 の誕生』佛大社会学 第26号 (2001)</a>">2</a> 



 キリスト教の影響で、個人は神との関係性において「世俗外個人」として捉えられていた。<a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-3-5762" title="<a href=&quot;https://archives.bukkyo-u.ac.jp/rp-contents/BS/0026/BS00260L276.pdf&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;></a><a href=&quot;https://archives.bukkyo-u.ac.jp/rp-contents/BS/0026/BS00260L276.pdf&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;>新矢 昌昭『近代 における孤独 の誕生』佛大社会学 第26号 (2001)</a>">3</a>  <a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-4-5762" title="<a href=&quot;https://www.jstage.jst.go.jp/article/sstj/1/0/1_114/_pdf&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;>藤本龍児『アメリカにおける個人主義とニュー工イジ運動 一一現代宗教の問題と課題一一』現代社会学理論研究 (2007)</a>">4</a> 



 個人の内面性よりも、共同体への所属と役割が重視されていた。<a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-5-5762" title="<a href=&quot;https://hermes-ir.lib.hit-u.ac.jp/hermes/ir/re/16625/studys0200000010.pdf&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;>阿部謹也「西洋中世における個人（人格） の成立に関する予備的考察」一橋大学社会科学古典資料センター Study Report, 20 (1990)</a>">5</a> 
 



<p>近代的個人観</p>



 
 個人は自律的な存在として捉えられ、個人の内面性や自我意識が重視されるようになった。<a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-6-5762" title="<a href=&quot;https://archives.bukkyo-u.ac.jp/rp-contents/BS/0026/BS00260L276.pdf&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;>新矢 昌昭『近代 における孤独 の誕生』佛大社会学 第26号 (2001)</a>">6</a><a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-7-5762" title="<a href=&quot;https://www.jstage.jst.go.jp/article/sstj/1/0/1_114/_pdf&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;></a><a href=&quot;https://www.jstage.jst.go.jp/article/sstj/1/0/1_114/_pdf&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;>藤本龍児『アメリカにおける個人主義とニュー工イジ運動 一一現代宗教の問題と課題一一』現代社会学理論研究 (2007)</a>">7</a> 



 啓蒙主義の影響で、個人は中世的な共同体からの拘束から解放され、「内面的孤独」を経験するようになった。<a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-8-5762" title="<a href=&quot;https://www.jstage.jst.go.jp/article/sstj/1/0/1_114/_pdf&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;></a><a href=&quot;https://www.jstage.jst.go.jp/article/sstj/1/0/1_114/_pdf&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;>藤本龍児『アメリカにおける個人主義とニュー工イジ運動 一一現代宗教の問題と課題一一』現代社会学理論研究 (2007)</a>">8</a> 



 個人の権利や自由が強調され、個人主義的価値観が台頭した。<a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-9-5762" title="<a href=&quot;https://ja.wikipedia.org/wiki/%E5%80%8B%E4%BA%BA%E4%B8%BB%E7%BE%A9&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;>Wikipedia「個人主義</a>">9</a><a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-10-5762" title="<a href=&quot;https://www.jstage.jst.go.jp/article/sstj/1/0/1_114/_pdf&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;></a><a href=&quot;https://www.jstage.jst.go.jp/article/sstj/1/0/1_114/_pdf&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;>藤本龍児『アメリカにおける個人主義とニュー工イジ運動 一一現代宗教の問題と課題一一』現代社会学理論研究 (2007)</a>">10</a> 



 プライバシーの概念が生まれ、私的領域が神聖視されるようになった。<a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-11-5762" title="<a href=&quot;https://ja.wikipedia.org/wiki/%E5%80%8B%E4%BA%BA%E4%B8%BB%E7%BE%A9&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;>Wikipedia「個人主義</a>">11</a> 
 



<p>つまり、中世では個人は共同体の一部として捉えられていたのに対し、近代では個人の自律性と内面性が強調され、個人主義的価値観が確立されたということができる。魂の自由性と言っても良い。でも残念ながら、この個人主義的価値観・近代的個人観に行き着いていない人が結構多い。だから、アバターは現実世界の自分に似ていなければならないだとか、アイデンティティウォレットには政府発行の個人属性証明(PID)を入れて、それを中心に回していくとかの考えになってくる。</p>



<p>これは別にウォレットに限ったことではなくて、Web 2.0的なSNSにおける本人確認でも似たようなことが起きてくる。先ほど経験したことだと、LinkedInで本人確認済みにするには、氏名はパスポート表記と同一でなければならないという。パスポート表記などというのは、国境を超えるときの識別子として政府に登録されたものであって、自分を表出するものではない。わたしは英語では Nat Sakimura として活動しているが、それが「本人確認」をすると、Natsuhiko Sakimuraであることを強制される。なんだそれは。そんなことしたら過去の業績とのリンクができなくなるではないか。いやそんな些細なことではない。名乗りを奪うということは、人格を否定することに等しい。ちょうど千と千尋の神隠しの湯婆婆が千尋の名前を奪うように。</p>



<p>まさに、本書がいうように、名前は「言霊世界のアイデンティティ」なのだ。この「名乗りの自由性」はとても大切だ。なぜならば、本書が言うように『周りの方々からこの「言霊」で毎日繰り返し呼びかけられ続けることで、わたしは◯◯としての自己認識を獲得することができる』からだ。</p>



<p>また、言うまでもなく「アバター：視覚世界のアイデンティティ」（本書）の自由性も重要だ。『自分自身の「アイデンティティ表現」としてみた時は現実のような「人間」のかたちに縛られたくない』（本書）からだ。それともあれだろうか。四肢が不自由だったら、メタバースでも四肢が不自由でなければいけないとでも言うのか。</p>



<p>その自由性の象徴がバ美肉である。日本では、人形浄瑠璃や歌舞伎の女形などの文化的背景がありだいぶ受容が進んできたが、西洋ではヘタをしたら５６されかねない。バ美肉を受容できるかどうかは、潜在的な差別意識のあるなしのリトマス試験紙になる。</p>



<p>本書ではMetaが規制を入れてくる可能性に触れているが、AppleもVision Pro でやってきても驚かない。場合によっては、VRのアバタを簡単に作らさせるためにUXとしてそういうことにするといってくるかもしれない。だがこうした動きには徹底抗戦しなければなるまい。日本では、自己イメージコントロール権説や、「身だしなみなどに関する自己決定権」から、「なりたい自分になれる権利」は認められ、間接適用説からmetaなど事業者もこの権利の侵害は許されないはずだ<a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-12-5762" title="naka2656 <a href=&quot;https://twitter.com/so6287/status/1781800580421398972&quot;>https://twitter.com/so6287/status/1781800580421398972</a> ">12</a>。</p>



<p>こうしたアイデンティティに関する話は第4章にまとめられているので、時間がなかったらまずそこだけでも読んでほしいところだ。</p>



 プライバシーに関しても参考になる 



<p>また、アイデンティティと違って一つの章になっているわけではないが、プライバシーに関しても参考になる記述がいろいろとある。曰く、</p>



 
 同じく日本の伝統芸能「人形浄瑠璃」でも、優美に踊る美しい女性の人形の背後には黒子のおじさんが見えてしまっています。しかし、それを指摘するのは野暮 
 



<p>これは、「データが見えてしまっても、意図された目的以外では使ってはいけない」というデータ保護の基本につながる。わたしが昔から<a href="https://www.sakimura.org/2015/08/3183/">「大人のプライバシー」</a>と呼んでいるものだ。もちろん「なりたい自分になれる」というのが、「自己イメージコントロール権」<a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-13-5762" title="「人間が自由に形成しうるとことの社会関係の多様性に応じて、多様な 自己イメージを使い分ける自由をプライヴァシーと呼ぶ」(自由な社会関係を前 提として、自己イメージのコントロール権) 出所：棟居快行『人権論の新構成』信山社(1992)187-195頁。">13</a>と表裏一体であるのは言うまでもなく。</p>



<p>ISOでは現在、メタバースのプライバシーについての検討が始まっているが、関係者にぜひ本書を読ませたいところだ。英語版がほしい。</p>



 もちろんメタバースの解説も 



<p>さて、自分がアイデンティティ＆プライバシー業界の人間だから、いきなりそのあたりの各論に入ってしまったが、そもそも本書はメタバースについての本だ。「メタバースの定義：実現に必要な七要件」であるとか、4大ソーシャルVRの紹介であるとか、それらを支える技術であるとか、メタバースに住む住民たちの特性、なぜ人々はバ美肉になるのか、などなど、データの裏付けをとりながらきっちりと解説している。しかも、マーカーで線を引きたくなるようなキャッチーなフレーズが満載で。たとえばこんな感じ：</p>



 
 声：音響世界のアイデンティティ 



 自分の精神に作用するプロテウス効果 



 バ美肉は枯山水。見立ての文化の延長線。 



 （現在は）「永遠に新しいことを学び続けていく」ことが当然（の社会）。すべての人が永遠に「未熟」であることを受け入れざるを得ない社会において、「大人としての自尊心」に変わる新たな価値観こそが、お互いの「未熟さ」を愛おしいと思う価値観「かわいい」 
 



<p>（出所）『メタバース進化論』</p>



<p>わたしは、Kindleで大量に線を引きながら読んでしまった。上記はそのほんの一部だ。ぜひ本書を買って読んいただきたい。</p>



 動画による解説も一緒にみるとなお良し 



<p>なお、本書は文章による紹介としては大変優れているが、やはりメタバースのイメージを得るには映像のほうが適している。そういう意味で、筆者である「バーチャル美少女ねむ」さんが、国連のIGF 2023で行ったプレゼン「バーチャル美少女だけど国連登壇してみた」も並行して見ることをおすすめする。非常にわかりやすいプレゼンだ。英語ですが…。</p>



 

 



<p></p>



<p>これは、国連が主催するインターネット・ガバナンス・フォーラム（IGF<a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-14-5762" title="Internet Governance Forum">14</a> ）2023 京都のDay 0</p>



 
 <a href="https://intgovforum.org/en/content/igf-2023-day-0-event-134-talk-with-metaverse-residents-%E2%80%93-a-new-identity-and-diversity">「Event No.134 Talk with Metaverse residents – a new identity and diversity | Internet Governance Forum」</a>（2023年10月8日 18:00〜19:00)  
 



<p>の模様を録画したものだ。わたしもこの日だけは、自分のセッション<a href="https://www.sakimura.org/2024/04/5762/#easy-footnote-bottom-15-5762" title="<a href=&quot;https://intgovforum.org/en/content/igf-2023-day-0-event-42-%E2%80%9Ctrusted-personal-data-management-service%EF%BC%88tpdms%EF%BC%89%E2%80%9D-program&quot;>IGF 2023 Day 0 Event #42 “Trusted Personal Data Management Service（TPDMS）” Program</a>">15</a>があったのでIGFの会場にいたのに、生参加できなくて残念だった。多分この時間はOECDの人とTrusted Data Intermeidaries (TDI: 信頼できるデータ仲介者) について話していたと思う。</p>



 というわけで 



<p>本書、本当におすすめだ。サンフランシスコからの機内で読んでいて、第４章を読んでいて涙が自然に流れ出てきたくらい。ぜひ買って読みましょう！</p>



<p>【変更履歴】</p>



 
 2024-04-23 初版 



 2024-04-24 中世的アイデンティティ v.s. 近代的アイデンティティを追加、敬体を常態に変更。 
 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/04/image-3.png?resize=790%2C447&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   11:20 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 22. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.youtube.com/watch?v=P2dN_We23tU'>🎙️ Hacking Postgres s02e03</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I quite enjoyed the chat with Ry Walker on the “Hacking Postgres” podcast, posted last week.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	<a href="https://www.youtube.com/watch?v=P2dN_We23tU">
			
		</a>
		
			<p>Hacking Postgres S2E3: David Wheeler, Principal Architect, Tembo</p>
		
	

         
<p>Last week I appeared on <a href="https://www.youtube.com/watch?v=P2dN_We23tU" title="Hacking Postgres S2E3: David Wheeler, Principal Architect, Tembo">s02e03</a> of the <a href="https://www.youtube.com/playlist?list=PL11N188AYb_Z04oQJgllNEY5m7gCcY8tH">Hacking Postgres</a> podcast.</p>
<blockquote>
<p>The experience I had after my independent Consulting gig for 10 years working
in companies was, like, bringing up other people and being supportive of other
people and hearing from a diversity of voices and perspectives makes
everything better.</p>
<p>That’s part of why I want to get so much input on and feedback on the stuff
that I’m hoping do with PGXN v2 — or whatever we ultimately call it. But
people matter, more than the technology, more than any of the rest of it.</p>
</blockquote>
<p>I quite enjoyed this wide-ranging discussion. We covered my history with the
Postgres community, a bunch of the projects I’ve worked on over the years,
plans and hopes for the <a href="https://wiki.postgresql.org/wiki/PGXNv2" title="Postgres Wiki: PGXN v2">PGXN v2</a> project, perspectives on people and
technology, and exciting new and anticipated features of Postgres. Find it
wherever fine podcasts are streamed, including:</p>
 
 <a href="https://www.youtube.com/watch?v=P2dN_We23tU">YouTube</a> 
 <a href="https://podcasts.apple.com/us/podcast/s2e3-david-wheeler-principal-architect-tembo/id1710401793?i=1000652861945">Apple Podcasts</a> 
 <a href="https://overcast.fm/+BEMRUuGv3w">Overcast</a> 
 <a href="https://x.com/tembo_io/status/1780974567860941004">Twitter</a> 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/podcast/">Podcast</a> 
                 <a href="https://justatheory.com/tags/hacking-postgres/">Hacking Postgres</a> 
                 <a href="https://justatheory.com/tags/sqitch/">Sqitch</a> 
                 <a href="https://justatheory.com/tags/pgtap/">pgTAP</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/2024/04/hacking-postgres/hps02e03.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:28 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/using-x509-certs-for-did-provenance'>Using X.509 Certs for DID Provenance</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Summary: The abundance of X.509 certificate authorities who already perform identity proofing for businesses provides a rich resouce that can be leveraged to boot the verifiable data ecosystem. Here’s how it could work. When you used a verifiable credential to prove something about yourself, the verifier can know cryptographically: (1) the identifiers for the issuer, (2) the credential hasn't been 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Summary: The abundance of X.509 certificate authorities who already perform identity proofing for businesses provides a rich resouce that can be leveraged to boot the verifiable data ecosystem. Here’s how it could work. </p><p>When you used a verifiable credential to prove something about yourself, the verifier can know cryptographically: (1) the identifiers for the issuer, (2) the credential hasn't been tampered with, (3) the credential was issued to you, and (4) the credential hasn't been revoked. These four checks are important because their establish the fidelity of the data being transferred. They don't, however, tell them <a href="https://www.windley.com/archives/2020/07/relationships_and_identity#risk">whether they can trust the issuer</a>. For that, they need to take the issuer's decentralized identifier (DID) that they got from credential presentation and determine who it belongs to.</p><p>At <a href="https://internetidentityworkshop.com">Internet Identity Workshop</a> last week, Drummond Reed gave a session on how X.509 certificates could help with this. The first step, like always, is to resolve the DID and retrieve the DIDDoc that associates keys and endpoints with the DID. The endpoint can be an HTTP server and, of course, should have an X.509 certificate providing TLS security. That certificate, at the very least, has a a domain name to bind that to the certificate's public key. It can, if you pay for the feature, also include information about the entity that applied for the certificate. The certificate authority proofs that information and is vouching for it when they sign the certificate.</p><p>The key to making the X.509 certificate useful for checking the provenance of a DID lies in one key change. X.509 certificates can contain and extended field called a <a href="https://en.wikipedia.org/wiki/Subject_Alternative_Name">Subject Alternative Name</a>. This following figure shows how it can help.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8d494fa-9a2c-46fb-ba9b-81180e00beaf.heic">    </a>Using X.509 Certificates to establish the owner of a DID   <p>The issuer (Attestor) creates the DID they will use to issue the certificate along with its associated DIDDoc, including an HTTP endpoint for DID verification.</p>  <p>Attestor applies for a X.509 certificate for that endpoint, including in the application the DID they created in (1).</p>  <p>The certificate authority does it's usual proofing of the application and issues a certificate that includes the DID in the Subject Alternative Name field.</p>  <p>The issuer creates a credential definition in the usual way that includes their DID and writes it to whatever Verifiable Data Registry their DID method dictates.</p>  <p>Attestor issues a credential to a holder (Alice) using that credential definition.</p>  <p>At some later time, Alice presents the credential to the verifier (Certiphi).</p>  <p>Certiphi resolves the DID to get the DIDDoc and retrieves the verfication endpoint from the DIDDoc</p>  <p>Certiphi retrieves the certificate for that endpoint1.</p>  <p>Certiphi verifies the certificate by checking it's signature and ensures that the DID in the DIDDoc for the credential matches the one in certificate.2</p>  <p>The issuer's DID has now been tied in a verifiable way to whatever information is in the certificate. Provided the certificate includes information about the entity beyond the domain name, the verifier can use that information to determine whether or not the credential is authentic (i.e., issued by who the credential definition purports issued it). That might be all the evidence they need to determine whether to trust the entity. Certificate authorities could also issue verifiable credentials to the customer attesting the same verified claims—after all, it's one more product they can offer.</p><p>The benefit of doing issuer validation using X.509 certificates is that there are already many trusted X.509 certificate authorities in business who already do proofing of attributes about businesses. That's a huge chunk of the verifiable data ecosystem that doesn't need to be built because it can be leveraged. To make this work, digital certificate authorities would need to start offering to validate DIDs and include them in a certificate as a Subject Alternative Name. I don't discount that this will take some bureaucratic maneuvering. Certificate authorities will need to see a business opportunity. I'd love to see <a href="https://digitcert.com/">Digitcert</a> or someone do a pilot on this.</p> Notes   <p>Note that this step might be combined with the previous step if the Verifiable Data Registry is the same server as the endpoint, but that's not necessarily going to be the case for a number of reasons.</p>  <p>Note that this does not create a call back wherein Attestor can determine which credential was used, preserving the privacy of the presentation. Attestor does know one of its credentials has been presented to Certiphi. </p>  <p><a href="https://www.technometria.com/subscribe?">Subscribe now</a></p><p><a href="https://www.technometria.com/p/using-x509-certs-for-did-provenance?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share">Share</a></p><p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8d494fa-9a2c-46fb-ba9b-81180e00beaf.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:37 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2024/04/21/archives-as-commons/'>Archives as Commons</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The Santa Barbara News-Press was born in 1868 and died in 2023 at age 155. Its glory years ran from 1932 until 2000, when the New York Times&nbsp;sold it to Wendy McCaw, who rode it to hell. That ride began with the Santa Barbara&nbsp;News Press&nbsp;Controversy in 2006 and ended when Ampersand, the company McCaw created […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

The Santa Barbara library, viewed from the county courthouse. Is this where the dead local paper’s archives will go? How about future archives of all the local news organs?
<p>The <a href="https://en.wikipedia.org/wiki/Santa_Barbara_News-Press">Santa Barbara News-Press</a> was born in 1868 and <a href="https://www.theguardian.com/us-news/2023/jul/24/santa-barbara-news-press-bankruptcy">died</a> in 2023 at age 155. Its glory years ran from 1932 until 2000, when the New York Times&nbsp;sold it to <a href="https://en.wikipedia.org/wiki/Wendy_McCaw">Wendy McCaw</a>, who <a href="https://www.youtube.com/watch?v=hXb53q_kDfc">rode it to hell</a>.</p>
<p>That ride began with the <a href="https://en.wikipedia.org/wiki/Santa_Barbara_News-Press_controversy">Santa Barbara&nbsp;News Press&nbsp;Controversy</a> in 2006 and ended when Ampersand, the company McCaw created to hold the paper’s bag of assets (which did not include its <a href="https://flickr.com/photos/docsearls/252584558/in/album-72157594299505924/">landmark building</a> downtown, which <a href="https://www.independent.com/2024/01/05/bankruptcy-trustee-for-santa-barbara-news-press-attempts-to-claw-back-buildings/">McCaw kept</a>), filed for Chapter 7 bankruptcy in late July of last year. Here are stories about the death of the paper in three local news journals that have done a great job of taking up the slack left when the News-Press began to collapse, plus one in the LA Times:</p>
 
 <a href="https://www.independent.com/2023/07/23/santa-barbara-news-press-files-for-bankruptcy/">‘Santa Barbara News-Press’ Files for Bankruptcy: Publisher Ampersand Claims Few Assets and Many Creditors</a>, by <a href="https://www.independent.com/author/jeanindependent-com/">Jean Yamamura</a> in the&nbsp;<a href="https://www.independent.com/">Santa Barbara Independent</a> (July 23, 2023) 
 <a href="https://www.noozhawk.com/santa-barbara-news-press-staff-told-all-jobs-eliminated-as-newspaper-stopped-publishing/">Santa Barbara News-Press Declares Bankruptcy, Staff Told All Jobs ‘Eliminated’,</a> by <a href="https://www.noozhawk.com/author/joshua-molina/">Joshua Molina</a>, in <a href="https://noozhawk.com">Noozhawk</a> (July 23, 2023) 
 <a href="https://www.edhat.com/news/santa-barbara-news-press-files-for-bankruptcy-staff-fired/">Santa Barbara News-Press Files for Bankruptcy, Staff Fired</a>, by <a href="https://edhat.com">Edhat</a> Staff (July 24, 2023) 
 <a href="https://www.latimes.com/california/story/2023-07-29/santa-barbara-news-press-bankruptcy-closure-newspaper-owner-wendy-mccaw">Santa Barbara News-Press bankruptcy brings uneasy end to an owner’s bitter tenure</a>, by <a href="https://www.latimes.com/people/james-rainey">James Rainey</a>, in the Los Angeles Times. (July 29, 2023) 
 
<p>I followed those with this in <a href="https://doc.searls.com/2023/08/18/we-need-deep-news/">We Need Deep News</a>:</p>
<blockquote><p>From what I’ve read so far (and I’d love to be wrong) none of those news reports touch on the subject of the&nbsp;News-Press‘ archives, which conceivably reach back across the century and a half it was published. There can’t be a better first draft of history for Santa Barbara than that one. If it’s gone, the loss is incalculable. (August 18 2023)</p></blockquote>
<p>Last month brought bad news about that:</p>
 
 <a href="https://www.independent.com/2024/03/08/santa-barbara-news-press-online-assets-nearly-sold/">‘Santa Barbara News-Press’ Online Assets to Be Sold: Bankruptcy Sale for $250,000 Subject to Bids in April</a>, by Jean Yamamura in the Santa Barbara Independent&nbsp;March 8, 2024 
 <a href="https://www.fastcompany.com/91077421/santa-barbara-news-press-bankruptcy-sale-max-noremo">A local paper went bankrupt. Now a faraway buyer wants its assets: The Santa Barbara News-Press’s digital assets are up for sale. Locals worry they could become a farm for AI-generated SEO bait</a>. By Ernie Smith in&nbsp;Fast Company. (March 5, 2024) 
 <a href="https://www.independent.com/2024/03/29/santa-barbaras-collective-memory-sold-for-kindling/">Santa Barbara’s Collective Memory, Sold for Kindling: Will ‘NewsPress.Com’ Become a Zombie Website?</a> by William Belfiore in the&nbsp;Independent. (March 29, 2024) 
 
<p>But then, thanks to William Belfiore’s appeal in that last piece, we learned this:</p>
 
 <a href="https://www.independent.com/2024/04/09/santa-barbara-news-press-website-goes-to-local-kids/">‘Santa Barbara News-Press’ Website Goes to ‘Local Kids’ Group Fronted by Ben Romo Makes Winning Auction Bid of $285,000</a>, by Jean Yamamura in&nbsp;the Independent (Apr 09, 2024) 
 
<p>The only mention of archives was in the closing sentences of that piece:</p>
<blockquote><p>The purchase of the website included the Santa Barbara News-Press trademark, which would be important to the groups looking at the physical archive of back issues, photographs, and clippings by topic. Romo, who was once a paper boy for the daily, acknowledged that his group was supportive of the archive remaining local, too.</p></blockquote>
<p>I don’t know what that means, and I haven’t checked. But I am sure that the archives ought to be managed by the community as a <a href="https://en.wikipedia.org/wiki/Common-pool_resource">common pool resource</a>.</p>
<p>As it happens, my wife and I are visiting scholars at the <a href="https://ostromworkshop.indiana.edu/">Ostrom Workshop</a> at Indiana University, which is concerned with this kind of thing, because its namesake, <a href="https://en.wikipedia.org/wiki/Elinor_Ostrom">Elinor Ostrom</a>, won a <a href="https://en.wikipedia.org/wiki/Nobel_Memorial_Prize_in_Economic_Sciences">Nobel Prize in Economics</a> for her work on how commons are self-governed. In her landmark book, <a href="https://archive.org/details/governingthecommons">Governing the Commons: The Evolution of Institutions for Collective Action</a>, she lists eight principles for managing a commons, which are summarized <a href="https://www.onthecommons.org/magazine/elinor-ostroms-8-principles-managing-commmons/index.html">here</a>:</p>
 
 Define clear group boundaries. 
 Match rules governing use of common goods to local needs and conditions. 
 Ensure that those affected by the rules can participate in modifying the rules. 
 Make sure the rule-making rights of community members are respected by outside authorities. 
 Develop a system, carried out by community members, for monitoring members’ behavior. 
 Use graduated sanctions for rule violators. 
 Provide accessible, low-cost means for dispute resolution. 
 Build responsibility for governing the common resource in nested tiers from the lowest level up to the entire interconnected system. 
 
<p>Journalists, especially those who report news, are not herding animals. They tend to be competitive and territorial by both nature and purpose. So the collection of news entities I wrote about in <a href="https://doc.searls.com/2023/08/31/we-need-wide-news/">We Need Wide News</a> and <a href="https://doc.searls.com/2023/09/15/we-need-whole-news/">We Need Whole News</a> will almost certainly not cohere into a commons such as Lin (her nickname) Ostrom addresses in that list.</p>
<p>But they should cohere around archives—not only because that’s the right thing to do, but because they need those archives. We all do.</p>
<p>So I hope Santa Barbara’s many journals, journalists, friends, supporters, and interested local institutions get together around this challenge. Build a commons around those archives, whatever and wherever they happen to be.</p>
<p>Meanwhile here in Bloomington, my other hometown, we are pushing forward with <a href="https://doc.searls.com/2024/03/19/the-online-local-chronicle/">The Online Local Chronicle</a> that <a href="https://bsquarebulletin.com/">Dave Askins</a> wrote about in the previous installment in <a href="https://doc.searls.com/news-commons/">this series</a>. We might call that a commons interest here.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://150108457.v2.pressablecdn.com/wp-content/uploads/2024/04/222009310_749de189d1_b.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   03:02 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wrenchinthegears.com/2024/04/21/on-the-road-again-vermont-trip-wrap-up/'>On The Road Again – Vermont Trip Wrap Up</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I’m Arkansas bound in the morning, so I don’t have time for a long post. All I have time for is to share a video with musings about the second part of my trip north and the photos that go along with it. Keep me in your prayers these next few weeks. I’m going to [...]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I’m Arkansas bound in the morning, so I don’t have time for a long post. All I have time for is to share a video with musings about the second part of my trip north and the photos that go along with it. Keep me in your prayers these next few weeks. I’m going to be putting a lot of miles behind me.</p>
<p></p>
<p>&nbsp;</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p>&nbsp;</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p>&nbsp;</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p>&nbsp;</p>
<p></p>
<p></p>
<p></p>
<p>&nbsp;</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2024/04/img_4986-2.jpg?resize=1020%2C765&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   02:05 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 21. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2024/04/20/this-thing-is-bigger-than-journalism/'>This Thing is Bigger Than Journalism</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Journalism as we knew it is washing away. But the story is bigger than journalism alone, and bigger than a story alone can tell. (Image&nbsp;borrowed from&nbsp;the brilliant&nbsp;Despair.com.) We who care about journalism are asked to join the&nbsp;Save Journalism Project, and its fight against Big Tech. Their pitch begins, and adds, On the first point, we […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>


  Journalism as we knew it is washing away. But the story is bigger than journalism alone, and bigger than a story alone can tell. (Image&nbsp;<a href="https://despair.com/products/bitterness">borrowed from</a>&nbsp;the brilliant&nbsp;<a href="http://despair.com/">Despair.com</a>.)
<p>We who care about journalism are asked to join the&nbsp;<a href="https://savejournalism.org/">Save Journalism Project</a>, and its fight against Big Tech. Their pitch begins,</p>

  

<p>and adds,</p>

  

<p>On the first point, we should note that journalists have been working for magazines, broadcasters, newsletters and themselves for many dozens of years. So journalism isn’t just about newspapers. Also, because so many journalists have long made livings in those other media, the loss of work is far greater than the 2,400 gone from newspapers. It’s truly massive. I don’t know any field where the loss of paying jobs is larger on a percentage basis. Not taxi driving, not hospitality, not retail, not manufacturing… not anything I can think of. (Well, maybe nuns. I don’t see many of those these days.)</p>
<p>We should also respect the simple fact that now there is more journalism than ever: in blogs, social media, podcasting, and other places. Most of those kinds of journalism don’t pay, but that doesn’t disqualify the work from the label. Hell, I’m committing journalism here and this doesn’t pay.</p>
<p>“The story of big tech’s threat to journalism” (what the Project wants us all to tell) is also something of a red herring because it distracts our attention from causes much bigger than Big Tech.</p>
<p>Every new technology “works us over completely,” Marshall McLuhan says (in&nbsp;<a href="https://en.wikipedia.org/wiki/The_Medium_Is_the_Massage">The Medium is the Massage</a>). And no new medium, no new technologies, have ever worked us more than the digital kind. The change began with digital tech and integrated circuits and then went absolute with the Internet. Together, digital technologies and the Internet have radiacally changed our species, our civilization, and our planet.</p>
<p>Not long ago, in a conversation about this with <a href="https://en.wikipedia.org/wiki/Joi_Ito">Joi Ito</a>, I asked him how big he thought the digital transformation was. Bigger than broadcast? Print? Writing? Speech? Stone tools?</p>
<p>“No,” he replied. “It’s the biggest thing since&nbsp;<a href="https://en.wikipedia.org/wiki/Great_Oxygenation_Event">oxygenation</a>.” In case you don’t remember, that happened between about two and a half billion years ago. (Joi also writes about it <a href="https://www.wired.com/story/ideas-joi-ito-great-digitization-event/">here</a>.)</p>
<p>So, while journalism matters enormously, it’s just one casualty of digitalization. And, let’s face it, a beneficiary as well. Either way, we need to understand the whole picture, which is about a lot more than what journalism sees happening in the mirror.</p>
<p><a href="http://www.digitallife.center/">Here’s one outfit</a> working on that bigger picture. I‘m involved with it.</p>
<p>I also don’t expect most journalists to take much interest in the subject, because it’s too big, and it doesn’t make full sense as a story, which is journalism’s stock in trade. (I explain a bit about journalism’s “story problem” in&nbsp;<a href="https://tedxsantabarbara.com/2018/doc-searls/">this TEDx talk</a>.)</p>
<p>Still, some journalists are on the case, including me. Love to have others join in. But please don’t bother if you think Big Tech is alone to blame. Because the story is bigger than that, and far more than a story.</p>
 
<p>I just copied and pasted this post from <a href="https://dsearls.medium.com/this-thing-is-bigger-than-journalism-3be89a2cef68">here in Medium</a>, where I posted it in July 2019. It expands on a post now archived <a href="https://web.archive.org/web/20231206175314/http://my.1999.io/users/dsearls/2019/06/10/thisThingIsBiggerThanJournalism.html">here</a>. It’s kinda sad that not much has changed over all that time.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://miro.medium.com/v2/resize:fit:994/1*zanuWR6qnlATyJW2LOMa2Q.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   01:36 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 18. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2521'>OpenID Federation Session at April 2024 IIW</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  John Bradley and I convened a session on Trust Establishment with OpenID Federation at the Internet Identity Workshop (IIW) on Thursday, April 18, 2024. The material used to drive the discussion was: Trust Establishment with OpenID Federation (PowerPoint) (PDF) The session was well attended and the discussion lively. Numerous people with trust establishment problems to […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://www.linkedin.com/in/ve7jtb/">John Bradley</a> and I convened a session on Trust Establishment with OpenID Federation at the <a href="https://www.internetidentityworkshop.com/">Internet Identity Workshop (IIW)</a> on Thursday, April 18, 2024.  The material used to drive the discussion was:</p>
 
 Trust Establishment with OpenID Federation <a href="https://self-issued.info/presentations/OpenID_Federation_18-Apr-24.pptx">(PowerPoint)</a> <a href="https://self-issued.info/presentations/OpenID_Federation_18-Apr-24.pdf">(PDF)</a> 
 
<p>The session was well attended and the discussion lively.  Numerous people with trust establishment problems to solve contributed, including experts from the SAML federation world, people involved in digital wallet projects, and several people already using or considering using <a href="https://openid.net/specs/openid-federation-1_0.html">OpenID Federation</a>.  Thanks to all who participated!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/openid-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:30 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-90'>The Pulse #90: Devin reversing ambitious claims</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The “world’s first AI developer” tones down expectations and has been outperformed by an open source tool. Also: hiring upticks at Big Tech; a very realistic AI video generator by Microsoft, and more.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering insights, patterns, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Today, we cover:</p>  <p>Industry pulse. Fintech valuations rising again; pre-earnings layoffs at Tesla and Google; Google fires staff trying to interfere with business; Rippling offering a secondary to its employees, and more.</p>  <p>Devin: Reversing ambitious claims. A month ago, Devin launched with fanfare as “the world’s first AI developer,” claiming that it “even completed real jobs at Upwork.” Upon closer inspection, this claim did not hold up. The company behind Devin had since toned down expectations. Also: open source solution AutoCodeRover is offering even better performance than Devin’s closed-source and not-yet-publicly available tool. This space is commoditizing rapidly.</p>  <p>Microsoft’s disturbingly realistic AI video generator. Microsoft Research showcased a tool that generated very realistic videos from a single image. The #1 use case will surely be fraudulent deepfakes generation. This development could well speed up AI regulation in several countries.</p>  <p>Hiring upticks at Meta, Netflix and Amazon? Data from interview preparation website interviewing.io suggests hiring is back at full speed at Meta, Netflix and – possibly – Amazon.</p>   1. Industry pulse  Fintech valuations rising again 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-90">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:02 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 16. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2518'>OpenID Presentations at April 2024 OpenID Workshop and IIW</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  As has become traditional, I gave the following presentation at the Monday, April 15, 2024 OpenID Workshop at Google: OpenID Connect Working Group Update (PowerPoint) (PDF) I also gave this invited “101” session presentation at the Internet Identity Workshop (IIW) on Tuesday, April 16, 2024: Introduction to OpenID Connect (PowerPoint) (PDF)
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>As has become traditional, I gave the following presentation at the Monday, April 15, 2024 <a href="https://openid.net/registration-oidf-workshop-monday-april-15-2024/">OpenID Workshop at Google</a>:</p>
 
 OpenID Connect Working Group Update <a href="https://self-issued.info/presentations/OpenID_Connect_Working_Group_15-Apr-24.pptx">(PowerPoint)</a> <a href="https://self-issued.info/presentations/OpenID_Connect_Working_Group_15-Apr-24.pdf">(PDF)</a> 
 
<p>I also gave this invited “101” session presentation at the <a href="https://www.internetidentityworkshop.com/">Internet Identity Workshop (IIW)</a> on Tuesday, April 16, 2024:</p>
 
 Introduction to OpenID Connect <a href="https://self-issued.info/presentations/OpenID_Connect_Introduction_16-Apr-24.pptx">(PowerPoint)</a> <a href="https://self-issued.info/presentations/OpenID_Connect_Introduction_16-Apr-24.pdf">(PDF)</a> 
 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/openid-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:44 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/security-engineering'>What is Security Engineering? Part 1.</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A deep dive into the ever-changing field of security engineering; a domain that can feel intimidating to some software engineers. With Nielet D'Mello, security engineer at Datadog.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>👋 Hi, this is Gergely with a subscriber-only issue of the Pragmatic Engineer Newsletter. In every issue, I cover challenges at Big Tech and startups through the lens of engineering managers and senior engineers. To get articles like this in your inbox, every week, subscribe:</p><p><a href="https://newsletter.pragmaticengineer.com/subscribe?">Subscribe now</a></p><blockquote> Q: “As a software engineer, I’d like to learn more about security engineering. What’s a good way to understand this vast field?” </blockquote><p>Security is so important in our industry. There’s frequently news stories about security incidents, like the authentication provider Okta which was breached, then responded poorly and <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-67">got schooled on “Security 101” practices by its own customer, Cloudflare</a>. The criticism that followed for Okta was inevitable and also deserved, as it essentially sells security. But what about engineers who want to build things securely, where do they start?</p><p>I figured there’s no better place to find out than by asking a security engineer, so I reached out to <a href="https://www.linkedin.com/in/nieletdmello">Nielet D'Mello</a>. She’s a security engineer at Datadog, whose job is incorporating security into products from the very start of the development process. Nielet has been working in the security domain for nearly a decade, and before that she was at Intel, where she worked closely with the security team, as well as at McAfee, in consumer and enterprise security products. Nielet’s also speaks at security conferences – here’s her 2023 talk <a href="https://www.youtube.com/watch?v=-6sx8HVzVKA&amp;t=9s">on security design and guidance at scale</a>.</p><p>In today’s issue, Nielet takes us through:</p>  <p>Myths and misconceptions about security engineering. Common misconceptions, like that security is only security engineers' responsibility, or that security through obscurity is sufficient, and other myths.</p>  <p>History of security engineering. Security engineering’s evolution since the 1990s; especially network and perimeter defense up to today.</p>  <p>The present. A transformation to a proactive approach, and a shift to “decentralized security.”</p>  <p>A mental model. Seven core dimensions for thinking about application security, with a close look at each one.</p>  <p>Towards a Secure SDLC. An approach to make all steps of the software development lifecycle (SDLC,) “security-first.”</p>  <p><a href="http://newsletter.pragmaticengineer.com/p/security-engineering-part-2">In Part 2</a>, we cover:</p>  <p>Defining the criticality of a system.</p>  <p>Scoring a system’s criticality.</p>  <p>Threat modeling.</p>  <p>Security paved roads.</p>  <p>“Defense in depth,” “least privilege,” and “zero trust.”</p>  <p>As a note, throughout this article we cover application security engineering (aka, “AppSec.”) This is the most common type of security engineering at tech companies building software products. Other specializations within security engineering include cloud security (focusing on cloud infrastructure security,) infrastructure security (securing hardware, operating systems, middleware,) and even physical security (physical access controls and surveillance.) These topics are out of scope for this series.</p><p>With that, it’s over to Nielet.</p> Common security engineering terms <p>Hi! We use three terms frequently in this article, so let’s start by defining them:</p>  <p>Vulnerability: An exploitable flaw or weakness in a system’s design, implementation or deployment</p>  <p>Threat: The potential for a threat actor to exploit a vulnerability</p>  <p>Risk: Loss or damage that could occur when a threat actualizes</p>   Security engineering vs software engineering <p>How intertwined are security engineering and software engineering?</p><p>When it comes to software engineering, there’s nothing too special about security. Yet, its extensive depth, breadth, and nuance, mean the security domain has long felt intimidating to engineers. But it has existed for as long as software engineering; so, why does security engineering still feel like an “emerging” field?&nbsp;</p><p>It’s due to software engineering’s ever-increasing complexity: distributed systems, microservices, cloud computing, Artificial Intelligence (AI,) and more. Security engineering aims to stay ahead in this dynamic, ever-evolving threat landscape, and businesses are starting to prioritize it more.</p><p>Some statistics reveal why investing in security is increasingly important:</p>  <p>$4.45M: global average <a href="https://www.ibm.com/reports/data-breach">cost of a single data breach</a> in 2023, a 15% rise over 3 years</p>  <p>16% more application security attack surfaces. In 2023 alone, this meant <a href="https://www.cvedetails.com/browse-by-date.php">29,000 new vulnerabilities</a> were identified, which organizations need to defend against.</p>  <p>A security engineering organization is usually tasked with:</p>  <p>Risk prevention and detection: Aim to defend an organization's assets: its data, applications, code, infrastructure, etc.</p>  <p>Response and recovery: react to threats and remediate attacks.</p>   1. Myths and misconceptions about security engineering <p>I’ve observed several common misconceptions, and this article seems like a good place to debunk them.</p> Myth #1: Security is only the responsibility of security engineers <p>This is surprisingly common, but not exactly true. Security engineers are stewards of the organization's overall security posture, but realistically, they can never keep up with all developments in the product and platform space – just within their organizations!&nbsp;</p><p>Security teams also tend to be lean, meaning there aren’t many engineers. If they focus too much on the weeds; like constantly triaging incidents or security findings, this will take away from high-value work that brings company-wide impact. Examples of high-value work include:</p>  <p>Security design reviews done product-wide</p>  <p>Building and running programs and services for a secure software development lifecycle</p>  <p>Relying solely on a security team to make all security design decisions is a common anti-pattern. Amazon Web Services, in its “AWS Well-Architected” guide, recommends against this practice, and instead <a href="https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/sec_appsec_build_program_that_embeds_security_ownership_in_teams.html">suggests</a>:</p><blockquote><p>“Build a program or mechanism that empowers builder teams to make security decisions about the software that they create. Your security team still needs to validate these decisions during a review, but embedding security ownership in builder teams allows for faster, more secure workloads to be built. This mechanism also promotes a culture of ownership that positively impacts the operation of the systems you build. (...)<br><br>Common anti-patterns:</p>  <p>Leaving all security design decisions to the security team</p>  <p>Not addressing security requirements early enough in the development process.</p>  <p>Not obtaining feedback from builders and security people on the operation of the program.”</p>  </blockquote> Myth #2: Security through obscurity is sufficient <p><a href="https://en.wikipedia.org/wiki/Security_through_obscurity">Security through obscurity</a> is the assumption that safeguarding certain details or functions of a system's operations can guarantee security. The principle is, “if only we know how this thing works, then it will be secure enough because others won’t be able to figure it out.”</p><p>This approach leads to a false sense of security! It can also lead to exploits. For example:</p><p>You have a web application with an admin panel, and this panel has features like managing users, managing content, and configuring the system. The admin panel has URL endpoints like /admin/user-management, /admin/content-management, /admin/system-configuration. How do you make these endpoints secure? The obvious way is to add authentication. However, this is a lot of effort. A simpler idea is to use obfuscation, remapping URLs to something hard to guess:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5315e72-1cfe-45f4-910f-7a7bb8c88333_1574x1444.png">    </a>Security through obscurity. The newly mapped URLs at the bottom are obscurer, but no more secure than those at the top <p>In this case, the developer relies on the obscurity of the URLs to prevent unauthorized access. However, all it takes is for the URL endpoints information to leak, or an attacker to brute-force the URLs, and the website can be exploited.</p> Myth #3: More security measures makes software more secure <p>It’s tempting to believe, right? Unfortunately, in my experience, it’s simply untrue.</p><p>Implementing multiple security measures can enhance the overall security posture of software, but it’s essential to strike a balance between security and usability. For each security measure, carefully consider these things:</p>  <p>Effectiveness</p>  <p>Complexity</p>  <p>Performance impact</p>  <p>Management overhead</p>  <p>Your goal should be that collectively, the security measures provide meaningful protection against threats to the product or platform.</p> Myth #4: Once secure, always secure <p>So, your system passed all its security reviews and penetration tests, and you have evidence it is secure. Can you now step away, and assume it will continue to be secure? No!</p><p>The threat landscape is constantly changing. Over the past year, there’s been a surge in attacks aimed at businesses and organizations around the world. These attacks intended to damage brands’ reputations, steal sensitive data, seek financial gain, and more. They are often done by ransomware groups, such as BlackCat’s attack on <a href="https://krebsonsecurity.com/2024/03/blackcat-ransomware-group-implodes-after-apparent-22m-ransom-payment-by-change-healthcare/">Change Healthcare</a> and <a href="https://techcrunch.com/2023/06/19/hackers-threaten-to-leak-80gb-of-confidential-data-stolen-from-reddit/">Reddit</a>, or mass account hacking through <a href="https://www.bleepingcomputer.com/tag/credential-stuffing/">credential-stuffing</a>.</p><p>New vulnerabilities and attack vectors emerge regularly. For example, applications built on top of large language models (LLMs) are now susceptible to prompt injection, which is a class of attack against applications built on top of LLMs. They work by concatenating untrusted user input with a trusted prompt constructed by the application’s developer. So, security mechanisms built against existing injection attacks must factor this in, as security measures that used to be effective become obsolete or insufficient against new and advanced threats, rendering software vulnerable.</p><p>Things that can introduce vulnerabilities and weaken your overall security posture:</p>  <p>Accumulation of technical debt</p>  <p>Using deprecated components and libraries</p>  <p>Outdated dependencies</p>  <p>Security vulnerabilities in a dependency, framework, library, or service</p>  <p><a href="https://en.wikipedia.org/wiki/Zero-day_(computing)">Zero-day exploits</a> are disclosed vulnerabilities for which no patch is available. These are a special kind of vulnerability, unknown to all consumers of the software. Finding such exploits is very challenging, but organizations with large security teams can do it. For example, Google <a href="https://blog.google/technology/safety-security/a-review-of-zero-day-in-the-wild-exploits-in-2023/">discovered</a> 93 zero-days in 2023.</p>  <p>Regulatory requirements and industry standards often mandate regular security assessments, audits and updates, to ensure compliance with data protection laws and regulations. Adhering to these requirements may necessitate ongoing security improvements, regardless of the software's initial security status.</p> Myth #5: Penetration testing by itself ensures security <p>Penetration testing, aka pen testing, involves simulating real-world security attacks against a system, network, application, or organization's infrastructure. The main goal is to identify and exploit vulnerabilities in a system's security defenses, mimicking the tactics, techniques, and procedures of attacks. Pen testing allows organizations to understand their security posture and to prioritize remediation efforts accordingly.</p><p>Downsides of pen testing:</p>  <p>It’s a snapshot of the security posture at a single, specific moment</p>  <p>Costly and labor-intensive</p>  <p>A system deemed secure by a penetration test one day, may become vulnerable the next day to new exploits or changes in the environment. Plus, scoping plays a huge role in the impact of penetration test results. Scoping refers to applications, users, networks, devices, accounts, and other assets that should be tested to achieve the organization's objectives. </p><p>When pen tests are incorrectly scoped, broader security issues or systemic weaknesses may be missed, which attackers can exploit. Scoping pen tests correctly means providing enough information for the pen testing team upfront, so they can be productive. Here’s a summary from Jessica La Bouve, Solutions Architect at penetration testing vendor, BishopFox, on <a href="https://bishopfox.com/blog/bad-scope-bad-pen-test">the importance of scoping</a>:</p><blockquote><p>“If a criminal has decided to target you, they have infinite time to find your weaknesses. (...) The assessment team has a finite amount of time to identify critical weaknesses. What they’re able to accomplish in that time depends on the amount of information you give them during scoping. (...)</p><p>Keeping your pen tester in the dark only makes it harder for them to find weaknesses that you need to fix. Even if an attacker starts from zero, they have plenty of time to conduct reconnaissance and learn a lot about your organization, giving your pen tester a head start means they can get right down to the business of finding the real threats to your systems. Attackers also don’t have any limitations on what they can try. They don’t usually worry about knocking your systems offline, but a pen tester would. To maximize a pen tester’s limited time and balance out the technical limitations placed on them, provide as much information as you can.”</p></blockquote> 2. History of security engineering <p>Security engineering teams tend to be lean by design and also by constraints, like the specialized skill sets needed, and budget limitations. This lean approach applies at whatever the scale of a company.&nbsp;</p><p>Security teams are much smaller than product/platform engineering teams, and tend to be “two-pizza teams” of between 5-10 application security engineers. As the security org is small, it focuses on projects and initiatives offering high return on investment in value, risk reduction, and impact terms.</p><p>If we look at the evolution of security engineering, there’s been significant shifts over the decades due to technological advancement, changes in threat landscapes, and systems’ increasing interconnectedness. Below are some examples.</p> The 1990s&nbsp; <p>The widespread adoption of the internet led to the development of various secure protocols (SSL, HTTPS,) and measures like firewalls and antivirus software to protect networks and data. The primary focus of security activities was network and perimeter defense, largely due to the dominance of client server architectures.&nbsp;</p> The 2000s <p>Web applications gained popularity and security engineering shifted focus towards securing web applications and the network. As web vulnerabilities like <a href="https://en.wikipedia.org/wiki/SQL_injection">SQL injection</a>, <a href="https://en.wikipedia.org/wiki/Cross-site_scripting">cross-site scripting</a> and <a href="https://en.wikipedia.org/wiki/Buffer_overflow">buffer overflows</a> became common, so did awareness of and focus on secure coding practices.&nbsp;</p><p>Around the same time, compliance and regulatory frameworks like <a href="https://en.wikipedia.org/wiki/SOX_404_top%E2%80%93down_risk_assessment">SOX</a>, <a href="https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act#Security_Rule">HIPAA</a>, and <a href="https://en.wikipedia.org/wiki/Payment_Card_Industry_Data_Security_Standard">PCI DSS </a>came into effect, and led organizations to boost efforts to comply with security requirements and guidelines.</p> Early 2010s <p>Cloud computing created new security challenges, like data privacy, data encryption, secure authentication, access control, and secure infrastructure configurations. The vulnerability landscape evolved in tandem with rapid technological shifts, and security shifted to efforts to automate security testing and remediation.</p> Mid-2010s, early 2020s <p>The rise of containerization and microservices architecture, the emerging field of AI and machine learning, and a shift to zero-trust architectures. This means security engineering must deal with increased complexity and more attack vectors.</p> 3. Security engineering today 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/security-engineering">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5315e72-1cfe-45f4-910f-7a7bb8c88333_1574x1444.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:23 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/relationships-are-entangled'>Relationships are Entangled</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  People aren't digitally embodied. As a result their online relationships are anemic. As we think about the kinds of digital identity systems that will help people live authentic online lives, we must build identity system that give people tools to operationalize rich digital relationships.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>People aren't digitally embodied. As a result their online relationships are anemic. As we think about the kinds of digital identity systems that will help people live authentic online lives, we must build identity system that give people tools to operationalize rich digital relationships. </p><p>Identity is the ability to recognize, remember, and react to people, organizations, systems, and things. In the current web, companies employ many ponderous technological systems to perform those functions. In these systems, we are like ghosts in the machines. We have "accounts" in companies' systems, but no good way to recognize, remember, and react to them or anyone else. We are not digital embodied.</p><p>One of great benefits of embodiment is the ability to form and operationalize rich digital relationships. I've written a lot about the nature of digital relationships.</p>  <p><a href="https://www.windley.com/archives/2020/07/relationships_and_identity.shtml">Relationships and Identity</a></p>  <p><a href="https://www.windley.com/archives/2020/08/authentic_digital_relationships.shtml">Authentic Digital Relationships</a></p>  <p><a href="https://www.windley.com/archives/2021/08/ephemeral_relationships.shtml">Ephemeral Relationships</a></p>  <p><a href="https://www.windley.com/archives/2020/11/operationalizing_digital_relationships.shtml">Operationalizing Digital Relationships</a></p>  <p><a href="https://www.windley.com/archives/2020/12/relationships_in_the_self-sovereign_internet_of_things.shtml">Relationships in the Self-Sovereign Internet of Things</a></p>  <p><a href="https://www.windley.com/archives/2020/09/the_architecture_of_identity_systems.shtml">The Architecture of Identity Systems</a></p>  <p><a href="https://www.windley.com/archives/2022/03/are_transactional_relationships_enough.shtml">Are Transactional Relationships Enough?</a></p>  <p><a href="https://www.windley.com/archives/2021/09/fluid_multi-pseudonymity.shtml">Fluid Multi-Pseudonymity</a></p>  <p>One of the discussions at <a href="https://projectvrm.org/">VRM Day</a> caused me to think about a feature of digital relationships I hadn't considered before. Someone said that if you think about a graph with people (or things, organizations, and so on) as the nodes, the relationships are the edges, like so:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a4c39e7-82f9-4e61-9d8a-0f5470747980.heic">   </a>Bi-directional relationship <p>In this figure Alice and Bob have a bi-directional relationship. This is how I've normally thought about it and how I'd have drawn it. But in today's discussion, someone said that the relationship is shared and that Alice and Bob both control it. But I realized that viewpoint is too simple. Specifically, Alice and Bob each have a different perspective of that relationship and will use it separately.</p><p>For example, imagine that Alice is the cashier at a grocery store and Bob is a customer. Alice gives great service, so Bob seeks her out when he shops. Alice on the other hand has no particular recollection of Bob from encounter to encounter. For Alice, the relationship is ephemeral, but for Bob, it's longer term. The nature of each relationship is different. So, we might look at it like this[1]:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49645230-8318-48ca-95db-bcaed2f53dd6.heic">   </a>Two, uni-directional relationships <p>But after discussing it some more, I realized that these relationships aren't independent. They're entangled like this:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61aa3d80-6435-4406-b78f-6eb924e0c59b.heic">   </a>Entangled relationships <p>In the example I gave above, as Bob seeks out Alice more and more, Alice might come to recognize him and call him by name, changing the nature of her relationship with Bob. And that may influence the nature of Bob's relationship with Alice. Over time, these interactions influence both relationships. So, while Alice and Bob both have control over their relationship with the other, actions by one influence the other.</p><p>I frequently say that we don't build identity systems to manage identities, but rather to manage relationships. The problem with contemporary identity systems is that they are all one sided, controlled by one party—almost always a company. As I've said before, people are not <a href="https://www.windley.com/archives/2022/01/web3_and_digital_embodiment.shtml">digital embodied</a> and thus have no good way to manage their online relationships. As we strive to build better digital identity systems, I think it's paramount that we build systems that provide people with tools that embody them and provide them with the ability to operationalize their online relationships. These are more than decentralized; they are self-sovereign.</p> Notes   <p>Peer decentralized identifiers (DIDs) are a great technology for creating bi-directional relationships.</p>  <p><a href="https://www.technometria.com/p/relationships-are-entangled?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share">Share</a></p><p><a href="https://www.technometria.com/subscribe?">Subscribe now</a></p><p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a4c39e7-82f9-4e61-9d8a-0f5470747980.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   00:07 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 15. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.eventbrite.com/e/851125899477/'>Mini Summit: Jonathan Katz on TLEs</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The topic of the Postgres Extension Mini-Summit this week is Trusted
Language Extensions, a.k.a. TLEs. Jonathan Katz of AWS will be giving
a brief presentation on the “TLE Vision and Specifics” at noon Eastern /
16:00 UTC on Wednesday April 17, followed by community discussion.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Coming up this week: the fourth installment of the Postgres <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">extension
ecosystem mini-summit</a>. The topic: <a href="https://github.com/aws/pg_tle" title="pg_tle: Framework for building trusted language extensions for PostgreSQL">Trusted Language
Extensions</a>, a.k.a. <a href="https://github.com/aws/pg_tle" title="pg_tle: Framework for building trusted language extensions for PostgreSQL">TLE</a>s. <a href="https://jkatz05.com">Jonathan Katz</a> of <a href="https://aws.amazon.com">AWS</a> will give a brief
presentation on “TLE Vision and Specifics” followed by community discussion of
the issues TLEs aim to address, what works and what doesn’t, and the role of
TLEs in the future of the extension ecosystem.</p>
<p><a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">Join us</a>! Note! that if you reserved a spot at a prior
mini-summit, sadly you will need to do so again for each subsequent summit or
miss out on reminders from Eventbrite. And if Eventbrite says the event is
sold out, rest assured we have plenty more virtual seats! Just send at
<code>david@</code> this domain, ping me <a href="https://xoxo.zone/@theory" title="@theory@xoxo.zone">on Mastodon</a> or via the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel
on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a> or the <a href="https://discord.com/invite/bW2hsax8We">Postgres Discord</a>.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/tle/">TLE</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   15:22 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2024/04/15/implement-a-secure-blazor-web-application-using-openid-connect-and-security-headers/'>Implement a secure Blazor Web application using OpenID Connect and security headers</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This article shows how to implement a secure .NET 8 Blazor Web application using OpenID Connect and security headers with CSP nonces. The NetEscapades.AspNetCore.SecurityHeaders nuget package is used to implement the security headers and OpenIddict is used to implement the OIDC server. Code: https://github.com/damienbod/BlazorWebOidc OpenIddict is used as the identity provider and an OpenID connect
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This article shows how to implement a secure .NET 8 Blazor Web application using OpenID Connect and security headers with CSP nonces. The NetEscapades.AspNetCore.SecurityHeaders nuget package is used to implement the security headers and OpenIddict is used to implement the OIDC server.</p>



<p>Code: <a href="https://github.com/damienbod/BlazorWebOidc">https://github.com/damienbod/BlazorWebOidc</a></p>



<p>OpenIddict is used as the identity provider and an OpenID connect client is setup to allow an OpenID Connect confidential code flow PKCE client. The Web application is a server rendered application using Blazor server components implemented using Blazor Web, ASP.NET Core and .NET 8.</p>







<p></p>



 Step 1: Init solution from the .NET Blazor samples 



<p>The solution was created using the Blazor samples from Microsoft. The .NET 8 BlazorWebAppOidc project was used to setup the solution.</p>



<p><a href="https://github.com/dotnet/blazor-samples/tree/main/8.0/BlazorWebAppOidc">https://github.com/dotnet/blazor-samples/tree/main/8.0/BlazorWebAppOidc</a></p>



<p>The code sample implements the client profile parts and the CSRF protection. Login and Logout plumbing is also implemented.</p>



 Step 2: Switch the OpenID Connect server 



<p>OpenIddict is used as the identity provider and so the OIDC client set up needs to be changed. The program file was updated and the OpenID Connect Microsoft Entra ID client was replaced with the OpenIddict client. The client on the server is setup directly in the worker class in the Openiddict server. Both of the setups must match. The client uses an OpenID Connect confidential client with code flow and PKCE.</p>



<p></p>


  builder.Services.AddAuthentication(OIDC_SCHEME)
    .AddOpenIdConnect(OIDC_SCHEME, options =&gt;
    {
        // From appsettings.json, keyvault, user-secrets
        // "OpenIDConnectSettings": {
        //  "Authority": "https://localhost:44318",
        //  "ClientId": "oidc-pkce-confidential",
        //  "ClientSecret": "--secret-in-key-vault-user-secrets--"
        // },
        builder.Configuration.GetSection("OpenIDConnectSettings").Bind(options);

        options.SignInScheme = CookieAuthenticationDefaults.AuthenticationScheme;
        options.ResponseType = OpenIdConnectResponseType.Code;

        options.SaveTokens = true;
        options.GetClaimsFromUserInfoEndpoint = true;
        options.MapInboundClaims = false; // Remove Microsoft mappings
        options.TokenValidationParameters = new TokenValidationParameters
        {
            NameClaimType = "name"
        };
    })
    .AddCookie();
  


<p></p>



<p>Note: You could also use the OpenIddict client packages to implement the client. I like to use the defaults.</p>



 Step 3: Disable WASM mode 



<p>Any web application should protect the session, not just implement authentication using an OIDC server. One of the most important browser protection is the CSP header and a good CSP uses a nonce. Blazor Web using WASM does not support this and so this must be disabled. Remove the WASM part from the middleware.</p>



<p>In the program.cs, update Blazor Web to:</p>


  builder.Services.AddRazorComponents()
    .AddInteractiveServerComponents();
  


<p>and</p>


  app.MapRazorComponents&lt;App&gt;()
    .AddInteractiveServerRenderMode()
    .AddAdditionalAssemblies(
            typeof(BlazorWebAppOidc.Client._Imports).Assembly);
  


<p>Remove the WASM usage in the UI components. Switch to InteractiveServer mode.</p>


      &lt;HeadOutlet @rendermode="InteractiveServer" /&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;Routes @rendermode="InteractiveServer" /&gt;
  


 Step 4: Add CSP nonce middleware 



<p>The CSP nonce can be used in Blazor (Server) components with some extra effort because the Blazor components cannot read the HTTP headers from the responses. The CircuitHandler class can be used for this. A BlazorNonceService class can be created to add the nonce. This class inherits the CircuitHandler implementation.</p>


  using Microsoft.AspNetCore.Components;
using Microsoft.AspNetCore.Components.Server.Circuits;

namespace BlazorWebAppOidc.CspServices;

/// &lt;summary&gt;
/// Original src: https://github.com/javiercn/BlazorWebNonceService
/// &lt;/summary&gt;
public class BlazorNonceService : CircuitHandler, IDisposable
{
    private readonly PersistentComponentState _state;
    private readonly PersistingComponentStateSubscription _subscription;

    public BlazorNonceService(PersistentComponentState state)
    {
        if (state.TryTakeFromJson("nonce", out string? nonce))
        {
            if (nonce is not null)
            {
                Nonce = nonce;
            }
            else
            {
                throw new InvalidOperationException(
                         "Nonce can't be null when provided");
            }
        }
        else
        {
            _subscription = state.RegisterOnPersisting(PersistNonce);
        }

        _state = state;
    }

    public string? Nonce { get; set; }

    private Task PersistNonce()
    {
        _state.PersistAsJson("nonce", Nonce);
        return Task.CompletedTask;
    }

    public void SetNonce(string nonce)
    {
        ArgumentException.ThrowIfNullOrWhiteSpace(nonce);

        if (Nonce != null)
        {
            throw new InvalidOperationException("Nonce already defined");
        }

        Nonce = nonce;
    }

    public void Dispose() =&gt; ((IDisposable)_subscription)?.Dispose();
}
  


<p>A NonceMiddleware ASP.NET Core middleware service can now be used to read the nonce from the headers and set this in the BlazorNonceService CircuitHandler implementation. NetEscapades.AspNetCore.SecurityHeaders is used to implement the security headers and if a CSP nonce is created, the NETESCAPADES_NONCE  http header is set.</p>


  namespace BlazorWebAppOidc.CspServices;

public class NonceMiddleware
{
    private readonly RequestDelegate _next;

    public NonceMiddleware(RequestDelegate next)
    {
        _next = next;
    }

    public async Task Invoke(HttpContext context, 
              BlazorNonceService blazorNonceService)
    {
        var success = context.Items
                  .TryGetValue("NETESCAPADES_NONCE", out var nonce);
        if (success &amp;&amp; nonce != null)
        {
            blazorNonceService.SetNonce(nonce.ToString()!);
        }
        await _next.Invoke(context);
    }
}
  


<p>The middleware for the nonce is added to the ASP.NET Core services.</p>


  builder.Services.TryAddEnumerable(
     ServiceDescriptor.Scoped&lt;CircuitHandler, BlazorNonceService&gt;(sp =&gt;
     sp.GetRequiredService&lt;BlazorNonceService&gt;()));

builder.Services.AddScoped&lt;BlazorNonceService&gt;();
  


<p>Use the middleware is in the ASP.NET Core pipelines.</p>


  app.UseMiddleware&lt;NonceMiddleware&gt;();
  


 Step 5: Add HTTP browser security headers 



<p>The NetEscapades.AspNetCore.SecurityHeaders nuget package is used to implement the security headers as best possible for this type of application. The SecurityHeadersDefinitions class implements this. CSP nonces are configuration as well as other security headers.</p>


  namespace BlazorWebAppOidc;

public static class SecurityHeadersDefinitions
{
    public static HeaderPolicyCollection GetHeaderPolicyCollection(
        bool isDev, string? idpHost)
    {
        ArgumentNullException.ThrowIfNull(idpHost);

        var policy = new HeaderPolicyCollection()
            .AddFrameOptionsDeny()
            .AddContentTypeOptionsNoSniff()
            .AddReferrerPolicyStrictOriginWhenCrossOrigin()
            .AddCrossOriginOpenerPolicy(builder =&gt; builder.SameOrigin())
            .AddCrossOriginResourcePolicy(builder =&gt; builder.SameOrigin())
            .AddCrossOriginEmbedderPolicy(builder =&gt; builder.RequireCorp())
            .AddContentSecurityPolicy(builder =&gt;
            {
                builder.AddObjectSrc().None();
                builder.AddBlockAllMixedContent();
                builder.AddImgSrc().Self().From("data:");
                builder.AddFormAction().Self().From(idpHost);
                builder.AddFontSrc().Self();
                builder.AddBaseUri().Self();
                builder.AddFrameAncestors().None();

                builder.AddStyleSrc()
                    .UnsafeInline()
                    .Self();

                // due to Blazor
                builder.AddScriptSrc()
                      .WithNonce()
                      .UnsafeEval() // due to Blazor WASM
                      .StrictDynamic()
                      .OverHttps()
                      .UnsafeInline(); // only a fallback for older browsers
            })
            .RemoveServerHeader()
            .AddPermissionsPolicy(builder =&gt;
            {
                builder.AddAccelerometer().None();
                builder.AddAutoplay().None();
                builder.AddCamera().None();
                builder.AddEncryptedMedia().None();
                builder.AddFullscreen().All();
                builder.AddGeolocation().None();
                builder.AddGyroscope().None();
                builder.AddMagnetometer().None();
                builder.AddMicrophone().None();
                builder.AddMidi().None();
                builder.AddPayment().None();
                builder.AddPictureInPicture().None();
                builder.AddSyncXHR().None();
                builder.AddUsb().None();
            });

        if (!isDev)
        {
            // maxage = one year in seconds
            policy.AddStrictTransportSecurityMaxAgeIncludeSubDomains();
        }

        policy.ApplyDocumentHeadersToAllResponses();

        return policy;
    }
}
  


<p></p>



<p>The security headers are added using middleware as early as possible in the pipeline. I add the headers for all requests.</p>


  app.UseSecurityHeaders(
    SecurityHeadersDefinitions.GetHeaderPolicyCollection(
        app.Environment.IsDevelopment(),
        app.Configuration["OpenIDConnectSettings:Authority"]));
  


<p>The CSP nonce can now be used in the Blazor components and scripts can only be read using the nonce. Unsecure scripts or unsecure inline scripts should never be read anywhere in a browser application.</p>


  &lt;pre class="wp-block-syntaxhighlighter-code"&gt; &lt;a href="http://_framework/blazor.web.js"&gt;http://_framework/blazor.web.js&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;

@code
{
    /// &lt;summary&gt;
    /// Original src: https://github.com/javiercn/BlazorWebNonceService
    /// &lt;/summary&gt;
    [CascadingParameter] HttpContext Context { get; set; } = default!;

    protected override void OnInitialized()
    {
        var nonce = GetNonce();
        if (nonce != null)
        {
            BlazorNonceService.SetNonce(nonce);
        }
    }

    public string? GetNonce()
    {
        if (Context.Items.TryGetValue("nonce", out var item) 
            &amp;&amp; item is string nonce and not null)
        {
            return nonce;
        }

        return null;
    }
}&lt;/pre&gt;
  


<p></p>



<p>The applications can be started and the authentication and the session protection can be validated. Using the WASM mode in Blazor Web requires a weaker security setup and you need to disable the CSP nonces. This is not a good idea.</p>



 Links 



<p><a href="https://github.com/dotnet/blazor-samples/tree/main/8.0/BlazorWebAppOidc">https://github.com/dotnet/blazor-samples/tree/main/8.0/BlazorWebAppOidc</a></p>



 
<blockquote><a href="https://damienbod.com/2024/02/19/using-a-csp-nonce-in-blazor-web/">Using a CSP nonce in Blazor&nbsp;Web</a></blockquote>
 



<p><a href="https://github.com/damienbod/BlazorServerOidc">https://github.com/damienbod/BlazorServerOidc</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2024/04/context-1.png?w=261'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   05:03 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2024/04/5723/'>NHK「最深日本研究～外国人博士の目～」からメタバースとアイデンティティとプライバシーについて考えてみる</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  4/13(土) 午前8:10-午前8:40にNHKで「最深日本研究～外国人博士の目～」という番組が放映されました。4/20(土) 午前8:39 までNHK+で視聴可能ですでは触れられているようです。また、日本における「理想」の実体化としての「カワイイ」の考察などもされています。昨…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>4/13(土) 午前8:10-午前8:40にNHKで「最深日本研究～外国人博士の目～」という番組が放映されました。4/20(土) 午前8:39 まで<a href="https://www.nhk.jp/p/ts/RJ5G2XZ4N3/episode/te/J1K96JLJ9K/">NHK+で視聴可能</a>です<a href="https://www.sakimura.org/2024/04/5723/#easy-footnote-bottom-2-5723" title="その後もNHKオンデマンドで視聴可能です。<a href=&quot;https://www.nhk-ondemand.jp/goods/G2024137649SA000/&quot;>https://www.nhk-ondemand.jp/goods/G2024137649SA000/</a>[/not]。スイスの文化人類学者、リュドミラ・ブレディキナ氏(31)(X: @<a href=&quot;https://twitter.com/BredikhinaL&quot;>BredikhinaL</a>, 以後「ミラ」）の文化人類学的見地からの、日本における「メタバース」という地域・文化、とくに男性の大部分がなぜ「バーチャル美少女キャラ〜バ美肉」化するかということへのアプローチです。</p>



<p>この背景として、日本の伝統文化である歌舞伎では女性を男性が演じること、人形のようなキャラクターを演じるということでは、人形浄瑠璃との関係もミラさんの論文「Babiniku」<span id='easy-footnote-1-5723' class='easy-footnote-margin-adjust'></span><span class='easy-footnote'><a href='https://www.sakimura.org/2024/04/5723/#easy-footnote-bottom-1-5723' title=''><sup>1</sup></a></span>Bredikhina, Liudmila. (2022) Babiniku: what lies behind the virtual performance. Contesting gender norms through technology and Japanese theatre. Electronic Journal of Contemporary Japanese Studies. https://shs.hal.science/halshs-03781957/ ">2</a>では触れられているようです。また、日本における「理想」の実体化としての「カワイイ」の考察などもされています。昨今、アイデンティティとプライバシーの国際標準化の分野でもメタバースが関心の対象になってきているところなので、とても参考になりました。</p>



<p>以下に番組概要のメモと、その後にアイデンティティやプライバシーの観点での考察を記載します。</p>



 番組概要 



<p>番組の概要はこんな感じです。なぐり書きメモなんで整っていませんが、まぁなんとなくは分かるでしょう。NHK+での番組公開終了までは、ぜひ番組をご覧になることをおすすめします。</p>



<a href="https://www.nhk.jp/p/ts/RJ5G2XZ4N3/episode/te/J1K96JLJ9K/"></a>



<p>（出所）NHK+ 「最深日本研究～外国人博士の目～」<a href="https://www.nhk.jp/p/ts/RJ5G2XZ4N3/episode/te/J1K96JLJ9K/">https://www.nhk.jp/p/ts/RJ5G2XZ4N3/episode/te/J1K96JLJ9K/</a></p>



 導入部 



 
 2000年代以降ネット上に広がったもう一つの世界、メタバース。人々はアバターと呼ばれる自らの分身を使い見知らぬ人との出会いを楽しむ。 



 その中で、日本人コミュニティではバ美肉が目立つ。 



 スイスの文化人類学者リュドミラ・ブレディキナ氏(31)<a href="https://www.sakimura.org/2024/04/5723/#easy-footnote-bottom-3-5723" title="Liudmila Bredikhina">3</a>はこのことに興味を持つ。 



 なぜ日本人男性は美少女キャラクターになりたがるのか、その答えを求めて研究 



 アニメ・和食・J-POP日本文化に注がれる世界からの熱い視線。そして、日本を愛しわれわれには無い目線でより深く日本を知ろうとする外国人研究者 



 彼らに日本はどう写っているか。バーチャル人類学者がわれわれの知らない日本の真の姿を解き明かす〜最深日本研究 
 



 第１章：最新メタバース体験施設を調査 



 
 スイスにはこのような施設はない 



 メタバースを体験するためのすべてが揃っている 



 初心者には案内人もついている。やはり美少女キャラクター 



 一般視聴者に向けたメタバースの紹介
 
 アバターの選択〜好きなアバターになる〜アイデンティティの表出 



 バーチャル空間では一瞬で何十万もある世界を移動できる
 
 そこで景色を楽しんだり、人々とお酒を飲んで交流したりなど、現実世界と同じように楽しめる 
 
 



 メタバースの利用者は世界中でおよそ４億人 



 花火をましたから見上げるなどここでしかできない体験 
 
 
 



 第２章：日本のバーチャル世界を研究したい 



 
 ミラさんがバーチャル世界の研究を始めたのは４年前 



 研究調査で驚きを持って知った言葉「バ美肉」（バーチャル美少女受肉）男性が美少女キャラを持つときに使われることば 



 「受肉」には西洋では宗教的な意味があるが、バ美肉たちは宗教に全く関心がない。そこにミラさんは興味を惹かれた。 



 調査をすると驚くべきことがわかった。男性のおよそ８割が美少女キャラクターを使っている
 
 女性　78.3% 



 男性　15.2% 



 その他　6.5% 
 
 



 その見た目には大きな特徴：アニメに出るような可愛い女の子。これに対して西洋では、クールでセクシー。カワイイとは違って強い女性という印象を受ける。 
 



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/04/image-1.png?ssl=1"></a>



<p>（出所）NHK「最深日本研究～外国人博士の目～」<a href="https://www.nhk.jp/p/ts/RJ5G2XZ4N3/episode/te/J1K96JLJ9K/">https://www.nhk.jp/p/ts/RJ5G2XZ4N3/episode/te/J1K96JLJ9K/</a></p>



 
 ミラさん：わたしは西洋で育ち、女性として美しくエレガントでありたいと思っていた。なぜ日本の男性はカワイイと言われたいのか。西洋人としてこの異なる視点をもっと理解したいと思います。 



 文化人類学の紹介
 
 20世紀初頭に一つの学問領域として確立。
 
 クロード・レヴィ・ストロース 



 ルース・ベネディクト 



 梅棹忠夫 
 
 



 机に向かうだけではなく、現地に赴き直接人と触れあるフィールドワークという手法で人間とは何かを探求する学問 



 人類学者は世界各地の民族を訪ね、彼らとともに暮らし、彼らの生活をつぶさに観察、自分の属する社会と比較することで、その共通点や違い、多様性を明らかにしてきた 



 バーチャル世界という新たな地平 



 最初SNSを使いバ美肉たちに話しかけたが、部外者であるわたしは信用されなかった。わたしが西洋人であり女性だったから。そこで、ミラさんは人類学の手法であるフィールドワークを使ってバ美肉たちに接近。自分のアバターを作ることは重要だった。バーチャル世界で彼らの信用を得ることができたから。 



 アバターを纏うことでバ美肉たちと同じ目線に立ち直接ふれあい始めたリュドミラ 



 ときには一緒にライブ配信を行い、オリジナル曲の英訳を買って出た 



 そして１年ほどかけて、徐々にコミュニティの中に入っていった 
 
 
 



 第３章　声までカワイイってどういうこと？ 



 
 両声類<a href="https://www.sakimura.org/2024/04/5723/#easy-footnote-bottom-4-5723" title="男性でありながら女性の声を出せるなど、両性の声を出せる人">4</a>の紹介
 
 声は男性のままか音声変換器を使うのが一般的 



 しかし、<a href="https://www.youtube.com/@johnko_amachi">あまちじょんこ(Youtuber @johnko_amachi</a> X: <a href="https://twitter.com/johnko20100711">@johnko20100711</a>) などは自らの超絶技巧で声を変える。 



 中の人は、じょんアニキ。
 
 めざすもの：天真爛漫で無邪気で、こういう小さい子っているよねという女の子をしたい。 



 高い声にするだけだと、男の人の高い声になってしまう。声は楽器と一緒なので楽器の形を変えてあげる。そうやって音質を変えて、その上で音の高さを変える。両方が合わさってじょんこの声になる 



 ２年かけ、理想の可愛い声を獲得した 



 「別の自分になれるってめちゃくちゃ楽しくて。で、なんかそれで、ただ楽しいということを突き詰めていく。天才じゃないから頑張るしかない。」 



 カワイイ動きのトレーニング。いざやっていむと難しい。ちいさい子の気持ちになって動かないといけない。たとえば、歩くときに右の足を出すと右に重心がよってしまう。これを繰り返すとふらふら見える。このふらふらした動きが、見ている人からしたら守ってあげたいというので可愛く見える。 
 
 



 ミラさん
 
 歩く時にかわいいを意識したことがなかったのでとてもおもしろいと思った 



 前は日本の男性といえばサラリーマンかオタクという表層的なステレオタイプなイメージしかなかった。しかし、研究をしてみると、日本人男性の多様性や日本文化の寛容性を知りとても面白いということがわかってきた 
 
 
 
 



 ミラさんの生い立ちの説明
 
 ロシア生れ。１２歳でスイスに移住。フランス語の勉強に日本の漫画。<a href="https://amzn.to/3xsVDqz">「天使禁猟区」</a><a href="https://www.sakimura.org/2024/04/5723/#easy-footnote-bottom-5-5723" title="<a href=&quot;https://www.amazon.co.jp/%E7%94%B1%E8%B2%B4%E9%A6%99%E7%B9%94%E9%87%8C/e/B0034P2S76/ref=dp_byline_cont_ebooks_1&quot;>由貴香織里</a>「天使禁猟区」（白泉社）<a href=&quot;https://amzn.to/3vRNefW&quot;>https://amzn.to/3vRNefW</a>">5</a>。今までこのような美しい絵を見たことがなかったので新しい世界が開けた。 



 27際のときに日本文化を学ぶためにジュネーブ大学の修士課程に。 



 論文を発表「Babiniku」<a href="https://www.sakimura.org/2024/04/5723/#easy-footnote-bottom-6-5723" title="Bredikhina, Liudmila. (2022) Babiniku: what lies behind the virtual performance. Contesting gender norms through technology and Japanese theatre. Electronic Journal of Contemporary Japanese Studies. <a href=&quot;https://shs.hal.science/halshs-03781957/&quot;>https://shs.hal.science/halshs-03781957/</a> ">6</a>
 
 日本の伝統芸能の一つである歌舞伎の女形と同様に男性が美少女キャラクターを演じる類似点 



 人形浄瑠璃とアバターをあやつるバ美肉の共通性など日本の伝統文化を通してバ美肉を考察した論文は大学から学術賞を受けた 



 日本の文化はとても興味深く、趣味と実益を兼ねた研究をしたいと考えた 
 
 



 ブレンダ・マーフィー教授
 
 「彼女の研究の魅力は西洋で主流ではない”日本文化”に着目している点です」 



 バーチャル人類学はとても新しい分野であり、彼女はグローバルな文化への挑戦者といえる 
 
 



 リュドミラが来日するのはこれで６度目。街に行けば目につくのは美少女キャラクター。看板や飲食店など町のいたるところで見かける。美少女キャラクターは秋葉原だけでなく日本文化で不可欠な存在 



 アニメや漫画の専門店に立ち寄り、次々に新しいものが生まれる日本のサブカルチャーもミラさんにとって研究の一環 
 
 
 



 第４章　美少女キャラに会いたい 



 
 アイドルVTuber <a href="https://www.youtube.com/@NoraCat0415">のらきゃっと</a> (@NoraCat0415 ch. 登録者86000人以上）。ファンネーム「ねずみさん」 



 これを生み出したのが<a href="https://www.youtube.com/@PIMR">「のらねこP」(@PIMR</a>, <a href="https://twitter.com/VR_Produce_Nora">X: @VR_Produce_Nora</a>)インタビュー
 
 のらきゃっとにガチ恋だと言っているファンとの関係についておしえて
 
 のらちゃんがちょっとドジなところもあってでも強くて可愛くてというのはファンがそう思っているから作られたという部分が多くて、ネズミさんたちがのらきゃっとという存在にそうあってほしいという願いのもとに作られたのでファンからの影響はとても強い 
 
 



 のらきゃっとがファンから可愛いと言われるとどう感じるか
 
 娘が公園で仲の良い友達と遊んでいて幸せそうだと親が感じるのに似ている 
 
 



 のらきゃっとが生まれて人生は変わったか？
 
 今までの仕事に行って帰ってきて家で寝るだけという生活でそんなに友達もいないというような人生から、本当に楽しく話せる友達が10倍以上に増えた。誰かと話したいときに話せるので寂しいと思う瞬間がなくなった 
 
 



 かわいい美少女になれるって最高ですね「みんななれる〜」 



 ファンから求められることで美少女キャラはよりかわいくくなり、その美少女を生み出した彼の生活も豊かにしている。今日いちばん重要な発見だった 
 
 
 



 第５章　美少女になる理由を知りたい 



 
 バーチャル美少女とファンとの交流はさらに進化。現実世界のファンが集まる店。友人のバ美肉が出演すると聞き訪ねた。「新宿ねむバー」 



 バーチャル美少女ねむさん(<a href="https://www.youtube.com/@nemchan_nel">Youtube @nemchan_nel</a>, <a href="https://twitter.com/nemchan_nel">X @nemchan_nel</a> )＝<a href="https://amzn.to/3VUkM7X">『メタバース進化論』（技術評論社）</a>の著者で、ミラさんの研究の頼もしい協力者 



 ミラさん：「直接あって美少女やメタバースとか私達が情熱を注いでいることについてもっと話そう」 



 ねむさん：「楽しそう。たくさん話すのを楽しみにしてるよ」 



 ミラさん：「ではリアル世界で」 



 ねむはプライベート時間の殆どをメタバースで過ごし、専門書<a href="https://www.sakimura.org/2024/04/5723/#easy-footnote-bottom-7-5723" title="『メタバース進化論』（技術評論社）<a href=&quot;https://amzn.to/3VUkM7X&quot;>https://amzn.to/3VUkM7X</a> ">7</a>を出版するほどこの世界について考え続けてきた人物。 



 ミラさん：直接美少女キャラクターになる理由を訊いてみたい 



 匿名で活動したいというねむさんの希望により映像と音声を加工しています→モザイクの大きさはかなり調整したらしい 



 SNSを通じ知り合ったのは4年前。ねむさんとはメタバースユーザの生活実態を一緒に調査したことも。 



 ミラさんによるねむさんへのインタビュー 



 初めてねむになったときどう思ったの？
 
 すごく恥ずかしかった。 



 罪悪感〜違う自分になるというのが悪いことをしているような感覚だった 



 でもだんだんいつもの自分では出てこないような言葉が口から出てきた 
 
 



 美少女になると自己表現も変わるということか？
 
 「現実の自分だとカッコつけちゃうのかな」 



 「そこまで心揺れ動かない結構冷静な人間だと自分のことを思っていたけど、ねむちゃんのときはそういうのから開放されてもっと自由に表現できる」 
 
 



 あなたにとっての美少女ってどういう存在なのか？
 
 “カワイイ”という概念を具現化した存在が美少女キャラクターだと思っている 



 肩書とか現実世界のしがらみを剥ぎ取ると人間の本質ってすごくカワイイ。魂って実はカワイイと思っていて、カワイイはその人の本質を褒めている気がする。 



 自信になるというか、ありのままでちゃんと意味があるんだと思える気がする 
 
 



 なぜ日本人男性が美少女になりたいと思うのか。その答えを求めてフィールドワークを続けてきたミラさんの今考える結論
 
 日本人男性は美少女になりカワイイを獲得することでストレスから開放される。 



 なぜならカワイイは失敗しても許されるし責任を負わなくていいからです。 



 私は日本文化から生まれたカワイイに可能性を感じました 



 しかし私は人類学者としてコミュニティを一般化するのではなく、これからも多くの人たちの声に耳を傾けたいと思っています 
 
 
 



<p>エンド：若き人類学者が日本のバーチャル世界に分け入っていく</p>



 考察 



 アイデンティティとバ美肉 



<p>アイデンティティという言葉は番組の中に出て来ませんが、多くの発言は「アイデンティティ」関連だとわたしの中では整理されました。たとえば、じょんこさんの「別の自分になれるってめちゃくちゃ楽しくて。で、なんかそれで、ただ楽しいということを突き詰めていく。」という発言は、現実社会の仕事だとか交友関係という既存のコンテキストを超えた新たなコンテキストでの自我の確立とその表出（自観するアイデンティティの表出）およびそれがコミュニティ内で他の人に観測されるもののフィードバック（他観）という関係性の享受から幸福感を得ているように見えます。（自観と他観のあたりは、<a href="https://www.sakimura.org/2011/06/1124/">「非技術者のためのデジタルアイデンティティ入門」</a>をご参照ください）</p>



<p>類似のことは、ねむさんの「（最初ねむになったとき）すごく恥ずかしかった。罪悪感〜違う自分になるというのが悪いことをしているような感覚だった。でもだんだんいつもの自分では出てこないような言葉が口から出てきた。」「ねむちゃんのときはそういうのから開放されてもっと自由に表現できる。」という言葉にも出てきます。</p>



<p>一方、のらねこPの「のらちゃんがちょっとドジなところもあってでも強くて可愛くてというのはファンがそう思っているから作られたという部分が多くて、ネズミさんたちがのらきゃっとという存在にそうあってほしいという願いのもとに作られたのでファンからの影響はとても強い。」という発言は、自観と他観のズレの最小化を、表出する属性の調整によって他観の方を変えて行うという「通常」のやり方とは逆で、他観の方に自観をあわせてしまうというアプローチに見えます。根源的自我に他観を合わせに行くのではなく、自我の方を調整してしまうわけですね。ただ、これが本当に自分の自我だとかなり心理的負荷が大きくなってくるのではないかと思われます。心理的負荷を減らすには、自分の中からその人格を外だししてしまって客観化してしまうことが必要であるようにも思えます。</p>



<p>このあたりが、のらねこPの「娘が公園で仲の良い友達と遊んでいて幸せそうだと親が感じるのに似ている」という発言からも表出されているように感じられました。</p>



<p>ここの部分で、ミラさんは「ファンから求められることで美少女キャラはよりかわいくくなり、その美少女を生み出した彼の生活も豊かにしている。今日いちばん重要な発見だった」と述べています。</p>



 免罪符としては機能しない美少女キャラクターとプライバシー 



<p>次に検討してみたいのが、ミラさんの「日本人男性は美少女になりカワイイを獲得することでストレスから開放される。なぜならカワイイは失敗しても許されるし責任を負わなくていいから。」という発言です。わたしの解釈違いかもしれませんが、カワイイが免罪符的に機能するので、日本人男性はカワイイを獲得したいと言っているように聞こえます。「日本人男性は」としているので、この「カワイイ」の概念とその通用する範囲が日本にまず限定されているのだろうということは想像できます。そのうえで「カワイイ」が免罪符的に機能するのかと言うと、これはちょっと難しいところだと思っています。</p>



<p>「カワイイ」を体現するものとしては、いわゆるアイドルVTuberなどがいると思います。ところが、その免罪符を持っているはずの彼女たちは、時として誹謗中傷されたりコミュニティから叩かれ、「卒業」を余儀なくされたりしているのは、このあたりをちょっと追っている人ならよくご存知のことと思います。もし「カワイイ」が免罪符として機能するならばこういうことは起きないと思うのです。</p>



<p>おそらく理想としての「カワイイ」には人々の期待がつまっていて、その期待を裏切ると叩かれるという構図があるのではないかとも思えます。つまり「カワイイ」が免罪符として機能するわけではない。ある意味、「”カワイイ”という概念を具現化した存在としての美少女キャラクター」は現代の巫女であり、古代と同じく、期待を裏切ったときにはスケープゴートにされる存在だと言えるかもしれません。で、わたしはスケープゴートって作ってはいけないと思うんですよね。</p>



<p>現代の巫女がスケープゴート化してしまう原因の多くは、コンテキスト外の情報の当該コンテキストへの混入です。たとえば、誰かからのチャットが画面に映り込んでしまったとか、他の人の声が混入してしまったとか、自分の顔が反射で映り込んでしまったとかですね。今後は他のワールドでの行動が当該ワールドにレポーティングされてしまうというような形の情報混入も起きてくることでしょう。場合によっては悪意の人が監視用のキャラクターを各所に配置して意図的にこれをやってくることも考えられます。中の人と、その人が運用する異なるアバターの名寄せとかもこの中に入ってきます。</p>



<p>番組の中でねむさんは「匿名で活動したい」としてモザイクをかけて声も変えて登場していますが、これも望まない名寄せであるとか情報の混入を防ぐ取り組みの一環と整理することもできます。</p>



<p>こうしたことをどうやって防ぐかとか取り締まっていくかなどが、メタバースにおけるプライバシーの尊重という意味では重要性を増してくると思われます。メタバースプライバシーの国際標準作成作業が始まっている背景でもあります。</p>



 しがらみを削ぎ落とした存在としての「ありのままの自分＝美少女キャラクター」 



<p>さて、ではなぜ日本人男性はバ美肉化するのか？上記のことを勘案すると、ミラさんのいう「日本人男性は美少女になりカワイイを獲得することでストレスから開放される。なぜならカワイイは失敗しても許されるし責任を負わなくていいから。」というのはちょっと違いそうです。</p>



<p>番組内でねむさんは「”カワイイ”という概念を具現化した存在が美少女キャラクターだと思っている。肩書とか現実世界のしがらみを剥ぎ取ると人間の本質ってすごくカワイイ。魂って実はカワイイと思っていて、カワイイはその人の本質を褒めている気がする。自信になるというか、ありのままでちゃんと意味があるんだと思える気がする。」とおっしゃっておられます。現実世界のしがらみを剥ぎ取った人の本質は「カワイイ」であり、そのありのままの自分の自我「自観」を表出すると、日本文化においては美少女になるということでしょうか。そうかもしれません。なにせ、８割近くのひとがそうあろうとするのですから。</p>



<p>番組では、各被取材者ごとに数時間のインタビューがあったのがそれぞれ５分くらいにまとめられてしまっているようです。削ぎ落とされたものの中に、このあたりに光を当ててくれる発言も多くあったことでしょう。その意味で、アフタートークも準備されているようなので、それに期待したいと思います。</p>



 
<blockquote><p>【ガチ勢各位】<br>実際にはみんな2 ～3時間喋った内容をそれぞれ一般人向けに5分くらいにまとめられてるので、ガチ勢のみんなとしたはぶっちゃけ物足りない面もあると思う。という訳で、アフタートーク企画中です！！！ <a href="https://twitter.com/hashtag/%E6%9C%80%E6%B7%B1%E6%97%A5%E6%9C%AC%E7%A0%94%E7%A9%B6?src=hash&amp;ref_src=twsrc%5Etfw">#最深日本研究</a> <a href="https://t.co/db9ToxqQpl">https://t.co/db9ToxqQpl</a></p>— バーチャル美少女ねむ/Nem4/13 NHK「最深日本研究」出演！ (@nemchan_nel) <a href="https://twitter.com/nemchan_nel/status/1779473091284107483?ref_src=twsrc%5Etfw">April 14, 2024</a></blockquote>
 



<p>（追記）アフタートークの日時が決まったようです。4/20(土)22:00〜。わたしはサンフランシスコからの飛行機の時間と微妙なのですが…。アーカイブは公開されるのだろうか…。</p>



 
<blockquote><p>【緊急ライブ告知】4/20 (土) 22:00～<br>ゴメン全然喋り足りない！！！ <a href="https://twitter.com/hashtag/%E6%9C%80%E6%B7%B1%E6%97%A5%E6%9C%AC%E7%A0%94%E7%A9%B6?src=hash&amp;ref_src=twsrc%5Etfw">#最深日本研究</a> 出演者4名が、番組には収まりきらなかった”バ美肉”や活動への想いなど、時間無制限で喋り倒します！<br><br>NHK「最深日本研究」非公式アフタートーク【ねむ✕ミラ✕のらねこP✕じょんこ】<a href="https://t.co/2zG4EbuVGO">https://t.co/2zG4EbuVGO</a> <a href="https://t.co/0G2R0zHGEo">pic.twitter.com/0G2R0zHGEo</a></p>— バーチャル美少女ねむ/Nem4/13 NHK「最深日本研究」出演！ (@nemchan_nel) <a href="https://twitter.com/nemchan_nel/status/1779707295653568954?ref_src=twsrc%5Etfw">April 15, 2024</a></blockquote>
 



<p>&lt;変更履歴＞</p>



 
 ミラさんのことを博士と書いていましたが、まだ博士課程在学中のようですので「ミラさん」の表記に変えました。 



 アフタートークの日時を追加しました。 



 サムネがFacebookだとはねられてしまうので差し替えました。 



 「ブレディキナ」を「プレディキナ」と表記してしまっていたのを修正 
 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/04/image.png?resize=546%2C304&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   01:20 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 14. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://kentbull.com/'>Kent Bull</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://kentbull.com/2024/04/14/keri-update-major-release-and-branch-strategy-change/'>KERI Update: Major release and Branch Strategy Change</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The WebOfTrust community recently released a major update to the Key Event Receipt Infrastructure (KERI) and Authentic Chained Data Containers (ACDC) implementation as a coordinated release across the suite of WebOfTrust repositories. Coordinated Release This resulted in the following release versions: Branch Strategy Change Recent Blog Posts See Nuttawut Kongsuwan’s […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The WebOfTrust community recently released a major update to the Key Event Receipt Infrastructure (KERI) and Authentic Chained Data Containers (ACDC) implementation as a coordinated release across the suite of WebOfTrust repositories. </p>



 Coordinated Release 



<p>This resulted in the following release versions:</p>



 
 Repository: KERIpy (used for witnesses, command line-managed decentralized identities, and as the core library for the Python KERY space)<br>Version: 1.1.12 (<a href="https://pypi.org/project/keri/1.1.12/">PyPi</a>) – Apr 9, 2024 



 Repository: KERIA (agent server used for individual decentralized identity agents – your data sits here)<br>Version: 0.2.0.dev0 Pre-Release (<a href="https://pypi.org/project/keria/0.2.0.dev0/">PyPi</a>) – Apr 11, 2024 



 Repository: SignifyPy (edge-signing client implementation – your keys sit here)<br>Version: 0.1.0 (<a href="https://pypi.org/project/signifypy/0.1.0/">PyPi</a>) – Feb 13, 2024 
 



 Branch Strategy Change 



 
 “development” branch merged to “main” as the old GitFlow style branching strategy was dropped in favor of <a href="https://trunkbaseddevelopment.com/">trunk-based development</a> (single main branch strategy). See the <a href="https://github.com/WebOfTrust/keripy/discussions/726">keripy#726 GitHub discussion</a> for the rationale.
 
 This occurred for the following repositories:
 
 Python
 
 <a href="https://github.com/WebOfTrust/keripy">KERIpy</a> 



 <a href="https://github.com/WebOfTrust/keria">KERIA</a> 



 <a href="https://github.com/WebOfTrust/signifypy">SignifyPy</a> 
 
 



 Typescript
 
 <a href="https://github.com/WebOfTrust/signify-ts">SignifyTS</a> 
 
 
 
 
 
 
 



<p>Recent Blog Posts</p>



<p>See Nuttawut Kongsuwan’s explanation of how to use KERI in his “<a href="https://medium.com/finema/the-hitchhikers-guide-to-keri-part-3-how-do-you-use-keri-2d1724afa432">The Hitchhiker’s Guide to KERI. Part 3: How do you use KERI?</a>“</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   18:02 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 13. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@m-ruminer/'>Michael Ruminer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://m-ruminer.medium.com/refocusing-a-career-aa9a9c25f8be?source=rss-7e85224c0a32------2'>Refocusing a Career</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Now for something entirely different. I write only occasionally on a topic of interest to me, verifiable credentials. Today it is something entirely different, my career. Most specifically, on how it may be time for a refocus. A movement from software development life-cycle process back toward a new more technical focus,&nbsp;AI.  I have, in some ways, refocused my career a couple of times. It was
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/256/1*_iRKUjhiS0hOJRdwAsPgcw.jpeg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Now for something entirely different. I write only occasionally on a topic of interest to me, verifiable credentials. Today it is something entirely different, my career. Most specifically, on how it may be time for a refocus. A movement from software development life-cycle process back toward a new more technical focus,&nbsp;AI.</p><p>I have, in some ways, refocused my career a couple of times. It wasn’t a full restart or anything; just moving into new directions within software development. Some were more a natural progressions than a refocus. This time it would certainly be a&nbsp;refocus.</p> What I do and what I have&nbsp;done? <p>For the last several years I have been a software technical release manager focused on not only getting software out the door but on process improvement. That is not where I started my career. It all started as a system admin, then a programmer, then getting involved in migrations and software develpment for mitigating Y2K, on to a software team lead. Then came the first refocus, I got deeply invested in a new Microsoft product Team Foundation Server. This took me down a fun rabbit hole where I spent some years consulting on implementations and use of the product. I spent a few years of my consulting as a Micrsoft MVP on the product, which was very rewarding. Ultimately, with a focus on version control, automated builds and deploys, and release processes with the product and in general, I made the next refocus. I moved into release management. Decidely less technical, but something I have now enjoyed for a number of years. I find the cross-functional nature rewarding and plenty of opportunity to still practice process improvement.</p> Why the consideration of a&nbsp;refocus? <p>I see it as a dying domain. I have found that the role is late to join organizations and more easily released from those same organizations. In my experience, companies find unexpected value in the role and can really enjoy having the “glue” bring higher quality and improved flow to production and beyond, but “enjoy” is too much of an operative word here. When belts are tightened it is one of the first roles to go — seen as a luxury not a requirement. I also see diminishing new opportunities in the marketplace. All this places downward pressure on compensation. Additionally, upward progression from the role is often difficult. So it may be time for a full refocus,&nbsp;again.</p> Too technical for the&nbsp;role? <p>It wasn’t too long ago I was told as a final feedback for a failed pursuit of an open release management position:</p><blockquote>You are too technical.</blockquote><p>Hmmm… I knew what they meant, but I had never had my quals work against me- not that I know of. Most find it to be a boon. It made me think. With all the considerations of why there might need to be a refocus, is it time to shift my momentum?</p> Where to&nbsp;next? <p>I really enjoy what I do and would hate to give it up but the writing might be on the wall. So, as I survey the landscape, I say to myself “what might be next”. Where is the best opportunity to delve into new things and make an impact for organizations? Is it the AI domain? Can I go from zero to hero? What does that look like? How long would it take and what path is there? I see a lot of potential in Microsoft’s Azure AI offerings and the ones that will inevtiably come down the line. Plus, I have a long history in the Microsoft stack. Let’s see where the next few years take me. Regardless of anything, I’ll certainly be digging deeper into the practical applications and implementations of the technology all why continuing my enjoyable current role in software life&nbsp;cycle.</p> Back to a common&nbsp;topic <p>Now, if I can come up with an AI realm play that ties in verifiable, portable, credentials. What will be bigger in the future than identity and AI?&nbsp;Nothing.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/256/1*_iRKUjhiS0hOJRdwAsPgcw.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   18:39 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 12. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2515'>Fully-Specified Algorithms Presentation at 2024 OAuth Security Workshop</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I gave a presentation on Fully-Specified Algorithms for JOSE and COSE at the 2024 OAuth Security Workshop in Rome. The slides used to update participants on the progress of the work are available as PowerPoint and PDF. Thanks to the organizers for another great OAuth Security Workshop! And special thanks to the colleagues from Fondazione […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I gave a presentation on Fully-Specified Algorithms for JOSE and COSE at the <a href="https://oauth.secworkshop.events/osw2024">2024 OAuth Security Workshop</a> in Rome.  The slides used to update participants on the progress of the work are available as <a href="https://self-issued.info/presentations/OSW_2024_Fully-Specified_Algorithms.pptx">PowerPoint</a> and <a href="https://self-issued.info/presentations/OSW_2024_Fully-Specified_Algorithms.pdf">PDF</a>.</p>
<p>Thanks to the organizers for another great OAuth Security Workshop!  And special thanks to the colleagues from <a href="https://www.fbk.eu/en/">Fondazione Bruno Kessler</a> who did a great job with local arrangements in Rome!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/osw.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:53 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2512'>COSE &#8220;typ&#8221; (type) Header Parameter Specification in RFC Editor Queue</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I’m pleased to report that the COSE “typ” (type) Header Parameter Specification has been approved by the IESG and is now in the RFC Editor queue. The version approved by the IESG and sent to the RFC Editor is: https://www.ietf.org/archive/id/draft-ietf-cose-typ-header-parameter-05.html It joins CBOR Web Token (CWT) Claims in COSE Headers in the RFC Editor queue. […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I’m pleased to report that the COSE “typ” (type) Header Parameter Specification has been approved by the <a href="https://www.ietf.org/about/groups/iesg/">IESG</a> and is now in the <a href="https://www.rfc-editor.org/current_queue.php">RFC Editor queue</a>.</p>
<p>The version approved by the IESG and sent to the RFC Editor is:</p>
 
 <a href="https://www.ietf.org/archive/id/draft-ietf-cose-typ-header-parameter-05.html">https://www.ietf.org/archive/id/draft-ietf-cose-typ-header-parameter-05.html</a> 
 
<p>It joins <a href="https://www.ietf.org/archive/id/draft-ietf-cose-cwt-claims-in-headers-10.html">CBOR Web Token (CWT) Claims in COSE Headers</a> in the RFC Editor queue.  Because of the reference to this spec by CWT Claims in Headers, they form <a href="https://www.rfc-editor.org/cluster_info.php?cid=C498">a cluster</a>, and therefore will become RFCs at the same time.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:31 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 11. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-89'>The Pulse #89: The end of Hopin</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  In only 5 years, Hopin went from zero to a $7.7B valuation, and back to zero again. Also: Bending Spoons’ startup acquisition model, hiring upticks at Meta, Netflix and Amazon, and more
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering insights, patterns, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Today, we cover:</p>  <p>Industry pulse. The first mass layoffs at Apple since 1997 (or not?); amateurish URL rewrite at X (formerly Twitter); never-ending job interviews for engineering executives, and more.</p>  <p>The end of Hopin. It took Hopin just two years to become the fastest-ever growing European startup by valuation. Four years later, the company is no more. The final valuable parts of Hopin are being sold, and all staff are expected to be let go. Exclusive details on the StreamYard sale.</p>  <p>Weekend maintenance kicks an Italian bank offline for days. It is now day five that Italian bank Sella has its apps and internetbank down, after a weekend systems update went south. The problem seems to be database-related: “something, something Oracle.”</p>  <p>Adyen, the only major Fintech with zero mass layoffs? All major Fintech startups have let go of some staff over the past two years, except Adyen. Meanwhile, the business has quietly become one of Stripe’s biggest competitors. A close look at this curious phenomenon.</p>   1. Industry pulse  First layoffs at Apple since 1997, or not…? 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-89">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   15:53 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 10. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@dkelts.id'>David Kelts on ID</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://medium.com/@dkelts.id/ripping-ids-pt-2-the-unwinnable-cost-problem-of-identity-verification-5486748c6fb5?source=rss-9bac94a4f241------2'>“Ripping IDs” Pt 2: The Unwinnable Cost Problem of Identity Verification</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   In a story on “Ripping IDs Should Go the Way of CDs”, I wrote about the questionable accuracy, usability, and privacy of “ripping” analog IDs into the digital space — a/k/a Mobile Know Your Customer (mKYC). Now I‘ll analyze the “ $1 cost problem” and you’ll see why important steps in mKYC are being&nbsp;skipped. Analog to Digital. Cost, accuracy, usability, and privacy problems of Mobile ID Verifi
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*tmPF7IrHoUwzYxd_aIqFgQ.jpeg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 In a story on “<a href="https://medium.com/@dkelts.id/ripping-ids-should-go-the-way-of-cds-49fec9206492">Ripping IDs Should Go the Way of CDs</a>”, I wrote about the questionable accuracy, usability, and privacy of “ripping” analog IDs into the digital space — a/k/a Mobile Know Your Customer (mKYC). Now I‘ll analyze the “ $1 cost problem” and you’ll see why important steps in mKYC are being&nbsp;skipped. Analog to Digital. Cost, accuracy, usability, and privacy problems of Mobile ID Verification (image <a href="https://www.cidaas.com/autoident-cidaas-id-validator/">cidaas.com</a>)<p>Services that require identity verification have become increasingly dependent on “scan the front, scan the back, take a selfie, match” for everything from renting a sidewalk scooter to opening bank accounts to provisioning your mobile driver’s license. The cost of inaccuracy in each situation varies greatly. The practical amount of money to spend within these use cases doesn’t — from 0 to 25 cents. Yet most services cost $1. Startups eat the difference hoping the future cost will go down (sorry, it won’t). Many businesses eliminate steps to keep costs down but still roll out a high-demand feature. Avoid offering mobile services that depend on ID and you fall behind. Do it, and accept the risk of fraudulent inputs.</p><p>There are both financial risk mitigation use cases and government identity use cases that are currently utilizing mKYC. I’d like to separate these concepts (read to the end to see&nbsp;why).</p><blockquote>Note: I do not use the overloaded term document verification. I specifically split document verification into two distinct steps: document authentication (is it from a trusted issuer) and document validation (is the data on it accurate and the account behind it still valid). (See also <a href="https://pages.nist.gov/800-63-4/sp800-63.html#sec3">definitions NIST 800–63</a>). Both of these would be resolved with digital government-signed identity documents.</blockquote> Cost Analysis of&nbsp;mKYC <blockquote>Note: I invite those with more insider knowledge of the costs of IDV to connect with me to correct my educated summary of costs&nbsp;below.</blockquote><p>There are <a href="https://medium.com/@dkelts.id/ripping-ids-should-go-the-way-of-cds-49fec9206492">at least 10 mKYC i</a>ssues that negatively impact user experience, accuracy, and privacy. mKYC is a technology where we haven’t yet been able to achieve <a href="https://en.wikipedia.org/wiki/Privacy_by_design">Privacy by Design</a> Principle #4 of full functionality without zero-sum tradeoffs. It’s still cost vs. accuracy vs. user experience vs. privacy (spoiler alert: cost wins). Doing analog-to-digital, this won’t ever&nbsp;change.</p> The Document <a href="https://www.idwise.com/glossary/document-authentication/">Authentication</a> Cost&nbsp;Problem <blockquote>Is the physical document real or fake? How much does it cost to accurately know?</blockquote><p>The cost problem here is simply explained. In physical documents, the goal of winning the card security feature <a href="https://en.wikipedia.org/wiki/Identity_document_forgery">race against fraudsters</a> was to make it expensive, in dollars and knowledge, to produce fake IDs. So cards have proprietary visual, tool-based (8x magnify, multi-spectral lighting), and forensic (destroy the card to detect) <a href="https://www.aamva.org/getmedia/19072a17-e9f2-45e0-975f-5e00588cb3f8/Design-Principles-and-Guidelines-for-Secure-DLID-Cards.pdf">security features</a> built into their structure, printing, and lamination. Digitally created for analog use... At great expense, including <a href="https://patents.google.com/?inventor=Robert+Jones&amp;assignee=Morphotrust&amp;oq=Robert+Jones+Morphotrust">admirable IP</a>, layered over time… but the fraudsters still caught up, able to replicate lower-cost visual features (and therefore make them acceptable by every bar bouncer on&nbsp;earth).</p><p>This gave rise to the supremacy of tool-based authentication for high-risk use cases — use an 8x loupe magnifier; combine white, UV, and IR lights (multi-spectral) to detect secrets built or printed into the card; purposeful design flaws, raised surfaces… amazing technology! Until you try to scan and authenticate it with a white-light phone camera held by a human&nbsp;hand.</p> Now We Expect to Go Back from Analog to&nbsp;Digital? <p>Try to ingest a secure physical card back into the digital realm and you must try to detect all <a href="https://www.aamva.org/getmedia/19072a17-e9f2-45e0-975f-5e00588cb3f8/Design-Principles-and-Guidelines-for-Secure-DLID-Cards.pdf">three levels</a> of physical card security features with a hand-held camera designed for action photos and portraits taken at focal lengths much further away than human arm length. There’s no UV or IR light, no optical magnification, very few steady cams, and no foolproof UX coaching for a person who isn’t aware of what’s being detected.</p><p>Do you see where this is taking us? Massive cost, diminishing return, user friction, bad experience… dropoff. What do mKYC purchasers do? Turn features off to avoid paying for document authentication and user friction.</p> The Document <a href="https://pages.nist.gov/800-63-4/sp800-63a/proofing/">Validation</a> Cost&nbsp;Problem <p>Once you’ve scanned an ID document, extracted the data from the front with OCR, decoded the barcode from the back, and skipped sending it for document authentication (lol), you’re still left with a few questions…</p><blockquote>Is the document still valid? Is the identity data accurate? Are the attributes fresh or have they changed? Can I ask the <a href="https://pages.nist.gov/800-63-4/sp800-63.html#def-and-acr">authoritative source</a> to validate them? How much does it cost me to perform that document data validation?</blockquote><p>If you want to establish the <a href="https://nvlpubs.nist.gov/nistpubs/ir/2018/NIST.IR.8112.pdf">provenance, accuracy, and freshness</a> of the data, you need to connect to the source that mitigates your risk&nbsp;level.</p><p>At the first level of reliability are “credible sources”. For example, data aggregators charge dimes and quarters for the service of validating data you collected against what they algorithmically believe to be accurate. It’s due diligence on your part. Whether you consider that credible depends on your level of risk versus the cost of&nbsp;service.</p><p>At the higher level of reliability are “authoritative sources” — the official issuers of identity documents. Who can connect? Mostly government and law enforcement. You can pay a quarter or two per record to validate data in privacy-preserving manners — yes/no on the accuracy of what you have versus looking up someone’s identity data using an identifier.</p><p>See the problem? It costs more just to validate than businesses should logically spend on the whole identity problem. And most don’t have&nbsp;access.</p><p>Then there’s the “use at your own risk” level of reliability of unofficial sources (not NIST-defined, surprisingly). The problem with using unofficial sources is that all our data was breached. Assume that to be true — zero trust. Check your mailbox for legal breach notices — confirmed. It’s cheaper to buy that data from the dark web than to use either a credible or authoritative source (if available to you). Advantage fraudsters… and the reason <a href="https://www.businesswire.com/news/home/20220420005041/en/Pindrop-Voice-Intelligence-Security-Report-Finds-Fraudsters-Passed-Knowledge-Based-Authentication-Questions-92-Percent-of-the-Time">fraudsters can pass Knowledge-Based Authentication</a> more easily than <a href="https://www.crowdfundinsider.com/2022/04/190239-security-report-reveals-fraudsters-passed-knowledge-based-authentication-questions-92-of-the-time/">our memories</a>. Skip KBA as an alternative (please).</p><p>What’s the point? Eh, given the cost, you’ll skip the document validation step. And you skipped document authentication. So you’re left with having snapped an off-angle photo of that ID document so that you can crop the low-resolution face out of it (resplendent with holograms, security lines, watermarking, and reflective lamination) for biometric matching. Better set that False Accept Rate high enough to get a decent Match&nbsp;Score!</p><blockquote>Or you can just produce digital documents in the first place. (See #3&nbsp;below)</blockquote> Cost Consolidation <p>From what I see, there are some alternatives for us to start to win-win the zero-sum problem we are&nbsp;facing.</p> 1. Do this Once at High Cost/Accuracy and Reuse it at Lower&nbsp;Cost <p>If one analog-to-digital event can be run at a high level of identity assurance(IAL2 minimum) by an authoritative entity, and the event can be re-used, then we start to lower the repeated costs to all future entities. Perhaps then the cost-benefit ratio will make sense. Note that I said an “authoritative” entity and not a credible or trusted entity. I like banks. Banks act in trustworthy ways, perform due diligence when required, and have been willing to <a href="https://www.bankid.com/en/om-oss/historia">step to the forefront in many regions</a>. But if you require a “government-issued ID” for your use case, bank KYC isn’t that. Bank KYC is an <a href="https://pages.nist.gov/800-63-4/sp800-63a/proofing/#identity-verification">Identity Verification</a> not a government signed&nbsp;ID.</p><p>A reusable identity solution should be a government-issued, privacy-protecting credential with user authentication and selective disclosure. You might look at <a href="https://www.mdlconnection.com/whatismdl/">mDL</a> as this, however, there’s no standardized mechanism for user authentication yet in <a href="https://medium.com/@dkelts.id/mobile-driver-licenses-mdl-how-to-use-iso-18013-5-5a1bbc1a37a3">ISO/IEC 18013–5 </a>or -7. Even without built-in user authentication, ingesting a digital mDL will short-circuit dozens of <a href="https://medium.com/@dkelts.id/ripping-ids-should-go-the-way-of-cds-49fec9206492">my list of 10 problems</a> (pun intended), increase accuracy, and lower cost while allowing fewer fraudulent documents through. That’s why NIST wants to solve this first <a href="https://www.nccoe.nist.gov/projects/digital-identities-mdl">through the&nbsp;NCCoE</a>.</p> 2. Separate Financial Risk Mitigation from Official&nbsp;Identity <p>Why this hasn’t happened yet confuses me. Yet we still call it “identity fraud” when criminals are using freely available identity data to perform “financial fraud”.</p><p>But seriously, scooter rentals don’t need my ID document, they need to know I can or will pay. Keep going and we will realize the ACLU’s dystopian <a href="https://www.aclu.org/wp-content/uploads/publications/20210913-digitallicense.pdf">over-identification state</a> (link will download PDF). None of us want&nbsp;that.</p><p>The solution Financial Services needs to look at is reusable <a href="https://fidoalliance.org/implement-passkeys-overview/">user authentication</a> against the KYC they or their partners already performed… without sharing identity attributes that won’t mitigate financial risk.</p> 3. Issue Digitally-Signed Originals <p>Better yet, just skip the analog to digital steps. Issue and accept digital birth certificates. Issue and accept Mobile or digital ID documents. If you’re not yet accepting digital mDLs,&nbsp;<a href="https://www.mdlconnection.com/mdl-uses/">start</a>.</p><p>People in the (self-labeled) Self-Sovereign or Decentralized Identity spaces have created the nearly ideal architecture for issuing authoritatively signed, lightweight, digital birth certificates. So far, to my knowledge, only <a href="https://www.ica.gov.sg/documents/birth/birth_registration">Singapore</a> has jumped on this technology and is realizing the cost savings and accuracy. There’s still one privacy hole to close… whoever controls the distributed ledger can potentially see all authentication and validation requests against the ledger if they control the API layer. This is a solvable technical problem with the architectural concepts of blinding, brokerage, or anonymizing API access (rotating relying party identifiers). This is solvable <a href="https://www.linkedin.com/events/identeratiofficehours-usneedsat7178595199718158336/theater/?lipi=urn%3Ali%3Apage%3Ad_flagship3_event%3BeGWD2ndXQ7OcbijV17A0zQ%3D%3D">with policy</a> (link will open video) if not&nbsp;law.</p> 4. Put the User/Holder in Control of their Credentials and&nbsp;Data <p>When you provide the user (aka citizen, resident, digital subject, holder) with their document in digital form, they gain control over how they share it. <a href="https://eu-digital-identity-wallet.github.io/eudi-doc-architecture-and-reference-framework/1.1.0/arf/">EU Digital Identity Wallets</a> are making progress. More importantly, the ease of sharing means that the recipient (relying party) doesn’t need to store identity attributes to mitigate their risk. Ask, verify, dispose. This is a concept of Interactive Information Privacy(TM) that I’ll write about more in the future and that I <a href="https://www.slideshare.net/DavidKelts/lessons-in-privacy-engineering-from-a-nation-scale-identity-system-connect-id">presented at Cloud Identity Summit in&nbsp;2017</a>.</p><blockquote>Don’t forget to put your reusable identity-proofing credential in that&nbsp;wallet.</blockquote> Call to&nbsp;Action <p>We want to stop “ripping IDs” just like we stopped ripping CDs when digital content services became ubiquitous. Digital Identity Services are the next frontier, but everybody is trying to solve everything everywhere all at once. I laid out 4 pathways above, each with existing possible solutions and a legitimate business model to make them self-sustaining. Let’s create actionable, cost-effective solutions to our “identity fraud” problems. The architects have done decent jobs… now the product, market, and business sides should capitalize in better ways than we are currently employing.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*tmPF7IrHoUwzYxd_aIqFgQ.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:32 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/04/mini-summit-three/'>Mini Summit Three</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A rough transcript of my Devrim Gündüz’s talk, “Overview of
{yum,zypp}.postgresql.org, and the challenges around RPMifying extensions”,
along with interspersed chat comments.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Terrific presentation and discussion at last week’s <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">Postgres Extension
Ecosystem Mini-Summit</a>. In fact, I later learned that some
people couldn’t make it because the Eventbrite event page showed it as
sold out!</p>
<p>This is a limitation of the free level at Eventbrite, which caps tickets at
25. But this is a fully-remote event and we can host more people than that. We
had over 30 last week! So if you’d like to attend one of the remaining three
events and Eventbrite shows it as sold out, have a look at the bookmarks for
the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a>, or email the username
<code>david</code> at this domain and I will send you the link!</p>
<p>Okay, back to business. Last week, <a href="https://github.com/devrimgunduz">Devrim Gündüz</a> kindly gave a super
thorough presentation in his capacity as the maintainer of the Postgres
community <a href="https://yum.postgresql.org" title="PostgreSQL Yum Repository">Yum</a> and <a href="https://zypp.postgresql.org" title="PostgreSQL Zypper Repository">ZYpp</a> repositories. This topic sparked lots of
interesting discussion and commentary, which I detail below. But why read when
you can look?</p>
 
 <a href="https://www.youtube.com/watch?v=6hk4bvXj0QE">Video</a> 
 <a href="https://www.gunduz.org/seminer/pg/DevrimGunduz-Overview_of_yum.postgresql.org_and_extensions-Extension_Ecosystem_Mini_Summit_2024-01.pdf">PDF Slides</a> 
 
<p>Still reading? Okay then, here we go!</p>
 Introduction 
 
 I opened the meeting and introduced Devrim Gündüz. 
 
 Presentation 
 
 
<p>Thank you for the introduction. Going to talk about Yum and ZYpp dot
postgresql.org, these are the challenge for us, and the challenge of
making RPMs from extensions.</p>
 
 
<p>Work for <a href="https://www.enterprisedb.com" title="EDB: Open-Source, Enterprise Postgres Database Management">EDB</a> as Postgres expert, Postgres Major contributor responsible
for the Postgres Yum repo. If you’re using RPMs, I’m building them. I also
contribute to Rocky, SUSE, and Fedora Linux, proud Postgres community
member, live in London, and I’m also doing some DJing, so if I get fired I
have an extra job to do.</p>
 
 
<p>Agenda: Last year at PGConf.eu in Prague, I had 60 slides for 5 minutes,
so expect huge number of slides for today. I want to leave some time for
discussion discussion and questions.</p>
 
 
<p>I’m going to start with how it started back in the 2000s, talk about the
contents of the repos, which distributions we t because that’s another
challenge um how do we do the extension packaging how to RPM how to build
RPMs of an extension and how to update an extension RPM.</p>
 
 
<p>Then the horror story um when what happens when there’s a new Postgres
beta is out,  which is due in the next next month or so for Postgres 17.</p>
 
 
<p>Then we are going to speak about the extension maintainers,  problem for
us, and then the problems about relying on the external repos.</p>
 
 
<p>So if you have any questions please ask as soon as you can. I may not be
able to follow the chat during the presentation, but I’ll try as much as I
can.</p>
 
 
<p>So let’s talk about how it started. When I started using Linux in 1996 um
and Postgres in 1998, we always had Tom Lane and we had we had Lamar for
for who build built RPMs for RedHta Upstream. So they were just building
the distribution packages, nothing community. It was only specific to
RedHat — not RedHat Enterprise Linux but RedHat 3-4-5 — not the
non-enterprise version of RedHat back then, but it was called it Fedora
core back then, the first version was released in November
2003, which was another challenge for packaging</p>
 
 
<p>One of the problems with the distribution packaging was that it was
slightly behind the minor Postgres releases, sometimes major post releases</p>
 
 
<p>So that was one single Postgres version available for a given distro, say
Postgres 6.4 or something, 7.0, and multiple versions were not
available back then, and the minor version was slightly behind.</p>
 
 
<p>I started building packages for my laptop because I wanted to use Postgres
and not all versions wer available back then. So I started building
packages for my laptop and my server. They were based on the packaging of
Tom Lane and Lamar.</p>
 
 
<p>Then I uploaded them to my personal server and emailed the PG mailing
lists lists and said, “I’m running on own packages, use at your own risk.
This is just a rebuild of the upstream packages on the RedHat version that
I’m using or the Fedora version that I’m using. Up to you! This is a personal
project, nothing serious.”</p>
 
 
<p>So then then people started downloading them, and using them. There was no
signature, nothing official back then. Then Lamar said he didn’t have
enough time for the RPMs. He sent an email to the mailing lists and I said
Devrim is stepping up to the plate, and I did it. So that was I think in
2004, about which is about 20 years ago.</p>
 
 
<p>So 19 years ago we had the first domain, <code>postgresql.rpm.org</code>, and then we
had more packages. In 2007 we had the first repository RPM and then we had
yum.postgresql.org. This means that, starting 2007, this began to be the
official RPM repo of the community, which which was a good thing because
we could control everything under the Community.</p>
 
 
<p>I was living in Canada back then. We had the first RPM build farm —
instead of using my laptop and my VMs — we had the actual machine
back then.</p>
 
 
<p>In 2010 we had what was then called multiple postmaster support, which
means the parallel installation of the Postgres RPMs. That was a that was
a revolution because even still, the current Fedora or RedHat RPMs cannot
be installed in parallel. So if you want to install post 13, 14, 15, 16,
and 17 or whatever, it is available in the community repo. This is a great
feature because you may want to test or do something else. This is how
we how we started, 14 years ago we had this feature in the community repo.</p>
 
 
<p>Then next year we moved the repo to the community servers and unified the
spec files. Our builds are extremely simple — like you can start your
own builds in less than 10 minutes: five minutes for pulling the git repo
and then a few minutes for for a package. Extremely simple builds, and now
we are in 2024</p>
 
 
<p>Let’s talk about the contents, because we have four different contents for
the repo. The first one is the “common” repo. We call it “common” in the
repository config file. It has the RPMs that work across all Postgres
major releases. Let’s say pg_badger, or the ODBC driver, JDBC driver, or
GDAL for PostGIS. There’s  lots of stuff: Python packages, which are not
extensions but we like Patroni, which actually works for all Postgres
releases.</p>
 
 
<p>This was an effort to get rid of duplicates in the Postgres repo. I think we
shaved lots of gigabytes back then, and we still save a lot</p>
 
 
<p>Then, to address one of the topics of today’s talks, we have the
“non-common” RPMs. (These are the name of the directories, by the way.)
They are the RPMs which are Postgres version-specific, basically they are
extensions. Say <code>plpgsql_check</code> 15 or whatever. Lots of extensions are
there.</p>
 
 
<p>Then we have extras. They are not actually Postgres packages, they
shouldn’t be included in our repo by default, but many people actually
look for these packages because they they want to use Patroni and they
don’t have the supporting RPMs or supporting RTM RPMs, or they’re not
up-to-date.</p>
 
 
<p>I’m building a console, console-template, ETCD, HAProxy keepalived and
vip-manager. They are all open source, some of them are Go packages —
which, actually, I don’t build them, I just distribute the precompiled
binaries via the repo. So that makes easier for people to deploy the
packages.</p>
 
 
<p>And then we have the “non-free” repo. These are the packages that depend
on closed-source software like Oracle libraries, or that have license
restrictions. For example, ora2pg depends on Perl DBD::Oracle, oracle_fdw
depends on Oracle, pg_storm depends on Cuda Nvidia stuff, timescaledb-tsl
actually is Timescale DB with the TSL license, informix_fdw and db2_ftw.</p>
 
 
<p>So we have some non-free packages which actually depend on non-free stuff
as well. All of them are well-maintained: I’m trying to keep everything
up to date — like real up-to-date! That brings some problems but we
will get there.</p>
 
 
<p>We support RedHat Enterprise Linux and of course Rocky Linux. This year we
started supporting Alma Linux as well. Of course they are more or less
identical, but we test them, install, and support to verify the packages
in these three distributions.</p>
 
 
<p>We have x86_64 aarchm64, ppc64le, and RedHat 9, 8, and 7. We have
also RedHat 6 for Postgres 12, but it’s going to be killed by the end of
this year. We have Fedora, but only two major releases, which matches the
Fedora lifecycle. And SUSE —  my pain in the neck — um I’ll get there.</p>
 
 
<p>Since you all are here for extension packaging, let’s get there: what
happens for extension packaging.</p>
 
 
<p>First of all, we have the first extension, which is the in-core
extensions. They are delivered with the contrib sub-package, which matches
the directory name in The Postgres tarball. There are separate packages
for each major version, so p<code>ostgres15-contrib</code>, <code>postgres13-contrib</code>,
etc. These are the installation directories for each extension. We are
going to use those directories for the other [non-cre] extensions as well.</p>
 
 
<p>When we add a new extension to the repo, it’s going to use these
directories if they have a binary or if they an extension config file, if
the library or the mem files — all are going are all installed under
these directories. This magic is done by PGXS, which has been there
forever. We just provide initial stuff and then the rest is done by the
the PGXS magic. This is the base for a lot of the core extensions.</p>
 
 
<p>So what happens when we do non-common and non-free package? First of all,
they are distributed separately for each Postgres major version. Let’s go
back to the one of the extensions, say <code>plpgsql_check</code>. We have a separate
package for Postgres 14, a separate package for Postgres 15, 16, 13, and
12. If they build against all the supported Postgres versions, we have
separate packages for each.</p>
 
 
<p>Of course from time to time — as far as I remember Timescale DB does
this —- Timescale DB only supports Postgres 15 and 16 nowadays. So we
drop support for the older versions in the Git repo. But they are separate
packages; they are all installed in these directories along with the main
contrib package.</p>
 
 
<p>This is the naming convention that I use: <code>$extensionName_PGMajorVersion</code>.
Four or six years ago, some of the packages didn’t have an underscore
before the PG major version. It was a problem, so someone complained in
the hackers mailing list, and then I made the changes.</p>
 
 
<p>Currently all the previous and all the new packages have this naming
convention except a few packages like PostGIS — because in PostGIS we
have multiple versions of the same extension. let’s say we have PostGIS
3.3, 3.2, 3.4, 3.1, 3.0. We have combinations of each — I think we have
PostGIS 3.0 in some of the distributions but mostly PostGIS 3.1, 3.2, 3.3,
and 3.4, and then all the Postgres versions they support — A lot of
builds! So there are some exceptions where we have the extension name and
extension major version before the Postgres major version.</p>
 
 Jeremy S in chat: question: curious if you have thoughts about
relocatable rpm packages. ever tried? just best to avoid? 
 
 
 
<p>I have a question from Jeremy. This is a very basic question to answer.
This is actually forbidden by the packaging rules. The RPM packaging rules
forbid you to distribute or create relocatable RPM packages. We we stick
to the packaging guidelines, so this this cannot be done.</p>
 
 Jeremy S in chat: Thanks! (Didn’t realize packaging guidelines
forbid this) 
 
 
 
<p>Let’s talk about how we build extensions. Often our develop package is
enough: many of our packages just just rely on on Postgres itself. But of
course packages like PostGIS may depend on some other packages, like GDAL,
which requires lots of lots of extra dependencies as well. The most
problematic one is the GIS Stack: they need EPEL on RHEL and
RHEL and its derivatives.</p>
 
 
<p>There there has been a discussion in the past about whether should require
EPEL by default. The answer is “no,” because not all not all of our users
are installing, for example, the GIS stack or other packages. Most of our
users — not the majority of our users —- rely on the um rely on just
our repo.</p>
 
 
<p>On the other hand, in order to provide completeness for our users, I added
lots of python packages in the past to support Patroni — because the
upstream packages (I’m sorry not maybe upstream packages) were not enough.
The version wasn’t enough or maybe too low. So From some time to time I
add non Postgres-related packages to the repo just to support the Postgres
package. In the past it was PGAdmin, but now it’s not in our repo so it’s
not a problem: their upstream is building their own RPMs, which is a good
thing for us. We are building extra packages mostly for Patroni.</p>
 
 
<p>However, this is a potential problem for some enterprise users because
large companies don’t even want to use the EPEL repo because they feel
like it’s like it’s not a community repo, but a community repo controlled
by Fedora and RedHat. That’s why from time to time I
try to add some of the packages to our repo.</p>
 
 
<p>If it’s a problem for enterprise users, does it mean we should we maintain
tons of extra packages in the EPEL repo for the GIS stack? The answer is
“no, definitely no”. Not just because of the human power that we need to
maintain those those packages —  I mean rebuilding them is easy: I just
get the source RPM, commit the spec files into our repo, and rebuild them.
But maintaining them is something else.</p>
 
 
<p>We will have a similar problem when we release our own ICU package in the
next few years. Because, now that we have the in core collation — but
just for C Locale  —-  and people are trying to get rid of glibc, maybe
we should have an ICU package. But maintaining an ICU Library across a
single Postgres major version is a real big challenge that I don’t know
how to solve for now, at least.</p>
 
 
<p>And then SLES — my pain in the neck — the GIS stack requires lots of
repos on SLES 15. They are they are well documented on on our website.</p>
 
 
<p>Fedora is safe because Fedora is Fedora, everything is there, it’s easy to
get a package there.</p>
 
 Jeremy S in chat: “Building them is easy. Maintaining them is
something else.” 
 
 
 
<p>Yeah that’s that’s the problem, Jeremy.</p>
 
 
<p>So how do you RPMify an extension?</p>
 
 
<p>The first thing is to know that the extension exists. This is one of the
big problems between developers and users. The developer creates a useful
extension and then they don’t create a tarball, they don’t release
anything. They expect people to install Git on their production databases
and <code>git pull</code>, install <code>make</code>, <code>gcc</code>, all the development
libraries and build a binary, blah, blah blah.</p>
 
 
<p>I’m sorry that’ss not going to work. It also doesn’t work for pip — pip
is not a package manager, it just destroys your server. It downloads
things to random places and then everything’s gone. That’s why I added
lots of Python packages to support Patroni, because most of the users use
the packaging package manager to install Postgres and other packages to
their servers. It’s the same for Debian, Ubuntu, for RPMs, for Windows,
for macOS.</p>
 
 
<p>So first of all we know have to know that the extension exists and we have
to have a tallbal. If I see an extension that seems good enough I’ll get
there. PGXN is a good place, because when I go to pgxn.org a few times per
day and see if there is a new version of an extension or if there’s new
extension, it’s a good piece. But there’s a problem: we have hundreds of
extensions — maybe thousands — but not all of them are on PGXN. They
should be!</p>
 
 
<p>David Wheeler (he/him) in chat: You should follow
<a href="https://botsin.space/@pgxn">@pgxn@botsin.space</a> for new releases :-)</p>
 
 
<p>nils in chat: pg_xz incoming</p>
 
 
 
 
<p>I don’t know how to solve this problem, but we should expect every
extension to announce their extensions on PGXN. I’m not just talking about
installing everything through PGXN, but at least have an entry that
there’s a new extension, this is repo, the website, the readme and the is
a tarball. It doesn’t have to be on PGXN, as long as we have something.</p>
 
 
<p>And then I check the version. If there is an extension that will kill your
database and the version is 0.001, that’s not going to be added to the
repo, because we don’t want to distribute an experimental feature.</p>
 
 
<p>David Wheeler (he/him) in chat: LOL, all my extensions start at
0.0.1</p>
 
 
<p>David G. Johnston in chat: If it isn’t on PGXN it doesn’t exist.</p>
 
 
 
 
<p>Another problem is that  lots of people write extensions but some of them
are just garbage. I’m sorry but that’s the truth. I mean they just release
a version and then do nothing.</p>
 
 David Wheeler (he/him) in chat: LOLOLOL 
 
 
 
<p>From the chat, “pgxn_xz is coming”: that’s right! We have <a href="https://bitbucket.org/adunstan/blackhole_fdw/src">blackhole_fdw</a>,
which was written by Andrew Dunstan. When you create blackhole_fdw, it throws all
of your data into black hole, so everything is gone.</p>
 
 
<p>Yeah, “if it’s not on PGXN it doesn’t exist,” that’s what I hope we
achieve achieve in the next year or so.</p>
 
 Jimmy Angelakos in chat, replying to “If it isn’t on PGXN …”: I
would correct that to “If it isn’t on PGXN it isn’t maintained.”
Sometimes even ON PGXN… 
 
 
 
<p>Yeah Jimmy, that’s one of the big problems that we have: maintenance.</p>
 
 
<p>We create the spec file, just copy one of the existing ones and start
editing. It’s easy but sometimes we have to add patches. We used to carry
patches for each Postgres major version to change the <code>Makefile</code>s for the
specific Postgres major version. But I realized that it was [not a great
pattern]. Now we just export the path, which fixes the problem.</p>
 
 David G. Johnston in chat: As a policy though, someone who cares and
wants responsibility needs to apply to be part of the RPM community. 
 
 
 
<p>Then I initiate a scratch build for any missing requirements. If there are
any missing build requirements it fails to build. I only do it on Fedora
latest, not for every package because it doesn’t always help because some
distros may not have the missing dependency</p>
 
 
<p>Alvaro Hernandez in chat: Hi everybody!</p>
 
 
<p>David G. Johnston in chat: Delegated to PGXN for that directory.</p>
 
 
 
 
<p>Let’s say we rely on some really good feature that comes with a latest
version of something, but that latest version may not appear in RedHat 7
or 8. So some dist dros may have it, but the version may be lower than
required. Or some distros may have the dependency under different name.
Now in the spec file we have “if SUSE then this” and “if RedHat then this”
“if Fedora then”, “if RedHat nine then this”, etc. That’s okay, it’s
expected. As long as we have the package, I don’t care.</p>
 
 
<p>Then I push it to the Git repo, which I use not just for the spec files
and patches, but also for carrying the spec files and pitches to the build
instances.</p>
 
 
<p>Jorge in chat: How to handle extension versioning properly? I mean,
in the control file the version could be anything, like for ex. citus
default_version = ‘12.2-1’ where the “published version” is v12.1.2,
then the “default version” could remain there forever.</p>
<p>Also have seen in the wild extensions that the control file have a
version 0.1 (forever) and the “released” version is 1.5</p>
 
 
 
 
<p>If something fails I go back to the drawing board. GCC may fail (gcc 14
has been released on Fedora 40 and is causing lots of issues for for
packaging nowadays), it could be <code>cmake</code> — too recent or too old. It
could be LLVM — LLVM18 is a problem for Postgres nowadays. I either try
to fix it ping upstream. I often ping upstream because the issue must be
fixed anyway</p>
 
 
<p>If everything is okay, just push the packages to the repo.</p>
 
 Ruohang Feng (Vonng) in chat: question: how about adding some good
extensions written in Rust/pgrx to the repo? like pgml,  pg_bm25,
pg_analytics, pg_graphql…. 
 
 
 
<p>One issues is that there is no proper announcement. Maybe I have an
awesome extension available in the Postgres repo that people crave and, we
build the extensions, it took a lot of time (thank you Jimmy, he helped me
a lot) and then I didn’t actually announce it that much. On the other
hand, people just can use PG stat base [?] to install and start using it
in a few seconds. This is something that we should improve.</p>
 
 
<p>Steven Miller in chat: How to handle system dependencies like libc
versions being updated on the target system? Do extensions need to be
complied against exactly a specific libc version?</p>
 
 
<p>From Nevzat in chat: how can we make sure bugfix or extension is
safe before installing it</p>
 
 
<p>vrmiguel in chat: Interesting approach to identify build/runtime
requirements</p>
<p>Over at Tembo we have a project called trunk-packager which attempts
to automatically create .deb packages given the extension’s shared
object.</p>
<p>We try to identify the dynamic libs the extension requires by parsing
its ELF and then trying to map the required .so to the Debian package
that provides it, saving this info in the .deb’s control file</p>
 
 
 
 
<p>From the chat: How to handle extension versions properly? That’s a good
thing but, extension version and the release version don’t have to match.
Thr extension version isn’t the same thing as the release version. It’s
the version of the SQL file or the functions or the tables, the views,
sort procedures, or whatever. If it’s 0.1 it means it’s 0.1 it means
nothing nothing has changed in this specific regarding the control file.
They they may bump up the package version because they may add new
features, but if they don’t add new features to the SQL file, then they
don’t update the extensions. I hope that answers your question George</p>
 
 
<p>I have another question from Ruohang. Yaaaaay! I was afraid that someone
would ask that one. We have no extensions written in Rust in repo so far.
It’s not like Go; there is a ban against Go because we don’t want to
download the world, all the internet just to build an extension. If I
recall correctly they’re rewriting pg_anonymizer in Rust. They will let me
know when they release it or they’re ready to release it, and then I’ll
build it. It’s not something I don’t like, it just hasn’t happened.</p>
 
 
<p>Keith Fiske in chat: I still update the control file with my
extensions even if it’s only a library change. Makes it easier to know
what version is installed from within the database, not just looking
at the package version (which may not be accessible)</p>
 
 
<p>Ruohang Feng (Vonng) inchat: question: How to handle RPM and
extension name conflicts, e.g., Hydra’s <code>columnar</code> and Citus’s
<code>columnar</code>.</p>
 
 
<p>*David Wheeler (he/him) in chat, replying to “I still update the c…”
Yeah I’ve been shifting to this pattern, it’s too confusing otherwise</p>
 
 
 
 
<p>If you think there are good extensions like these, just create a ticket on
<a href="https://redmine.postgresql.org">redmine.postgresql.org</a>. I’m happy to take a look as long as I know them.
That’s one of the problems: I have never heard about pg_analytics or pgml,
because they’re not on PGXN. Or maybe they are. This is something that we
should improve in the next few months.</p>
 
 
<p>Jimmy Angelakos in chat: Go is a pretty terrible ecosystem. It has
its own package manager in the language, so it’s kinda incompatible
with distro packages</p>
 
 
<p>Jason Petersen in chat: (but that means a build is safe within a
single release, it doesn’t mean you can move a built extension from
one Fedora version to another, right?)</p>
 
 
<p>David Wheeler (he/him) in chat, replying to “How to handle
system…”: Libc is stable in each major version of the OS, and there
are separate RPMs for each.</p>
 
 
 
 
<p>Another question from Steven: how to handle system dependencies like libc
version updates. The answer is no. It’s mostly because they don’t update
the libc major version  across the across across the lifetime of the of
the release. So we don’t need to rebuild the extension against libc.</p>
 
 
<p>Steven Miller in chat, replying to “How to handle system…”: Ok I
see, TY</p>
 
 
<p>Jason Petersen in chat, replying to “How to handle system…”: Is
that how we deploy in trunk, though?</p>
 
 
<p>David Wheeler (he/him) in chat, replying to “Go is a pretty
terri…”: Huh? You can build binaries in a sandbox and then you just
need the binary in your package.</p>
 
 
 
 
<p>[Addressing Nevzat’s question]: That’s a great question. It’s up to you!
It’s no different than installing Postges or any other thing. I just build
RPMs. If you’re reading the hackers mailing list nowadays, people rely on
me an Christoph and others, so that we don’t inject any code into the RPMs
or Debian packages. You just need to trust us not to add extra code to the
packages. But if there’s a feature problem or any bug then you should
complain upstream, not to us. so you should just test.</p>
 
 
<p>Jimmy Angelakos in chat, replying to “Go is a pretty terri…”: Find
me one person that does this.</p>
 
 
<p>Steven Miller in chat, replying to “How to handle system…”: We
don’t have OS version as one of the dimensions of version packaging
but should</p>
 
 
 
 
<p>[Addressing vrmiguel’s comment]: Yeah, that could be done but like I don’t
like complex things, that’s why I’m an RPM packager.</p>
 
 
<p>Jason Petersen in chat, replying to “Go is a pretty terri…”:
(doesn’t go statically link binaries, or did they drop that
philosophy?)</p>
 
 
<p>vrmiguel in chat: I think citus has changed it to citus_columnar</p>
 
 
<p>David Wheeler (he/him) in chat, replying to “Go is a pretty
terri…”: Hugo:
<a href="https://github.com/hugomods/docker/tree/main/docker/hugo">https://github.com/hugomods/docker/tree/main/docker/hugo</a></p>
 
 
<p>David Wheeler (he/him) in chat, replying to “Go is a pretty
terri…”: Jason: Static except for libc, yes</p>
 
 
 
 
<p>Another question from Ruohang: uh how to handle RPM and extension name
conflicts. I think Citus came first, so you should complain to Hydra and
ask them to change the name. They shouldn’t be identical. We have
something similar with Pgpool: they they are conflicting with the PCP
Library ,which has been in the Linux for the last 25 years. I think Pgpool
has to change their name.</p>
 
 Jeremy S in chat, replying to “I still update the c…”: So you
think people will run the “alter extension upgrade” eh? 
 
 
 
<p>[Addressing Keith Fiske’s comment]: I’m not saying I don’t agree with you,
but it means every time I have to update my extension version in my
running database — it’s some extra work but that’s okay. It’s the user
problem, not my problem.</p>
 
 
<p>Question from Jason [on moving an extension from one Fedora to another]:
Right, it may not be safe because the GCC version may be different and
other stuff may be different. One distro to another is not safe, Jason;
sorry about that.</p>
 
 
<p>[Back to Steven’s question]: Yes, David’s answer is right.</p>
 
 
<p>[Addressing vrmiguel’s comment about citus_columnar]: You are right.</p>
 
 
<p>Jimmy I’m not going to read your comment about go because I
don’t think think you can swear enough here.</p>
 
 
<p>vrmiguel in chat, replying to “Go is a pretty terri…”: Are there
known Postgres extensions written in Go? Not sure how Go is relevant
here</p>
 
 
<p>Jason Petersen in chat: you said “gcc” and not “libc” there, are you
implying that things like compiler versions and flags need to be
identical between postgres and extensions</p>
 
 
<p>Keith Fiske in chat, replying to “I still update the c…”: I think
they should …</p>
 
 
<p>David Wheeler (he/him) in chat, replying to “Go is a pretty
terri…”: Were some experiments a few years ago.
<a href="https://pkg.go.dev/github.com/microo8/plgo">https://pkg.go.dev/github.com/microo8/plgo</a></p>
 
 
 
 
<p>Let me continue now. First you have to know the extension exists, and then
the you also need to know that the extension has an update. Unfortunately
the same problem: the extension exists or has an update and they just
don’t let us know.</p>
 
 Jimmy Angelakos in chat, replying to “Go is a pretty terri…”:
@vrmiguel&nbsp;now you know why :D 
 
 
 
<p>This is a big challenge Fedora has in house solution.When you add a new
package to Fedora, I think they crawl their repo once a day and if there’s
new release they create a ticket in their bug tracker automatically, so
that the maintainer knows there’s a new version. This can be done, but
would need a volunteer to do it. Orr maybe the easiest thing is just add
everything to the to PGXN,</p>
 
 
<p>When we update an extension we, have to make sure it doesn’t break
anything. It requires some testing. As I said earlier, building is one
thing, maintaining the extension is a bigger thing. If you want to raise a
baby, you are responsible until until the end of your life. Consider this
like your baby: either just let us know if you can’t maintain an extension
anymore or please respond to the tickets that I open.</p>
 
 
<p>Steven Miller in chat: One other detail about compatibility
dimensions. We have noticed some extensions can be complied with
chip-specific instructions like AVX512, for example vector does this
which optimizes performance in some cases</p>
 
 
<p>Alvaro Hernandez in chat, replying to “you said “gcc” and n…”: I’d
reverse the question: do we have strong guarantees that there are no
risks if versions and/or flags may differ?</p>
<p>I believe extensions are already risky in several ways, and we should
diminish any other risks, like packaging ones.</p>
<p>So I’d say absolutely yes, compile extensions and Postgres in exactly
the same way, versions and environments.</p>
 
 
 
 
<p>Sometimes a new minor version of an extension breaks a previous Postgres
release. For example, an extension drops support for Postgres 12 even
though Postgres 12 is still supported. Or they didn’t do the upgrade path
work. I have to make sure everything is safe.</p>
 
 
<p>nils in chat, rReplying to “I think citus has ch…”: It was never
changed, the extension has always either been embedded in Citus or
later moved to a separate extension called citus_columner.</p>
<p>I think the name conflict comes from the access method being called
<code>columnar</code>, which Citus claimed first. (Hydra’s started actually as a
fork from Citus’ codebase).</p>
<p>(disclaimer; I work on Citus and its ecosystem)</p>
 
 
<p>Jason Petersen in chat, replying to “I think citus has ch…”: hi
nils</p>
 
 
 
 
<p>Next month a new beta comes out. Everyone is happy, let’s start testing
new features. For the packagers that means it’s time to start building
extensions against beta-1. So a build might fail, we fix it, and then it
may fail against beta-2. I understand if extension authors may want to
wait until rc-1. That’s acceptable as long as they let us know. Many of
them fail, and then Christoph and I create tickets against them and
display them <a href="https://wiki.postgresql.org/wiki/PostgreSQL_16_Extension_Bugs">on wiki.postgresql.org</a>. It’s a Hall of Shame!</p>
 
 
<p>Eric in chat: When you decide to package a new extension do you
coordinate with upstream to make that decision?</p>
 
 
<p>David Wheeler (he/him) in chat, replying to “When you decide to
p…”: I learned I had extensions in the yum repo only after the fact</p>
 
 
<p>Eric in chat, replying to “When you decide to p…”: I see</p>
 
 
<p>vrmiguel in chat: @Devrim Gündüz&nbsp;I’m curious about how RPM deals with
extensions that depend on other Pg extensions</p>
 
 
<p>David Wheeler (he/him) in chat: You can test Postgres 17 in the
pgxn-tools docker image today. Example:
<a href="https://github.com/theory/pgtap/actions/runs/8502825052">https://github.com/theory/pgtap/actions/runs/8502825052</a></p>
 
 
 
 
<p>This list pisses off the extension authors because they don’t respond to
ticket. So what do we do next? It happens again and again and again,
because they just don’t respond to us. On Monday uh I got a response from
an extension maintainer. He said “you are talking like you are my boss!” I
said, “I’m talking like I’m your user, I’m sorry. I just asked for a very
specific thing.”</p>
 
 
<p>nils in chat: I love Devrim’s issues against our repo’s! They are
like clockwork, every year 😄</p>
 
 
<p>David Wheeler (he/him) in chat, replying to “You can test
Postgre…”: It relies on the community apt repo</p>
 
 
<p>Eric in chat, replying to “When you decide to p…”: Related: ever had
upstream request you stop packaging an extension?</p>
 
 
<p>Steven Miller* in chat, replying to “One other detail abo…”: Even if
compiled inside a container, on a specific chip it can get
chip-specific instructions inside the binary. For example building
vector on linux/amd64 inside docker on a chip with AVX512, the
container image will not work on another linux/amd64 system that does
not have AVX512</p>
 
 
<p>David Wheeler (he/him) in chat: :boss:</p>
 
 
 
 
<p>Unresponsive maintainers are a challenge: they don’t respond to tickets,
or emails, or they don’t update the extensions for recent Postgres
versions.</p>
 
 
<p>Don’t get me wrong even the big companies also do this, or they don’t
update the extensions for the new GCC versions. I don’t expect them to
test everything against all all the GCC versions; that’s that’s my
problem. But just respond please.</p>
 
 
<p>What’s the responsibility of the packager in this case? Should we fork if
they don’t respond at all? No we are not forking it! VBut going to
conferences helps, because  if the extension author is there I can talk to
them in person in a quiet place, in a good way, just “please update the
package tomorrow or you’re going to die”. Of course not this but you see
what I mean.</p>
 
 
<p>[Looking at chat]: I’m going to skip any word about containers; sorry
about that.</p>
 
 
<p>[Addressing Eric’s question]: That’s a good so so the question! No,
actually they support us a lot, because that’s the way that people use
their extensions. And do we coordinate with upstream? No, I coordinate
with myself and try to build it. Of course upstream just can just create a
ticket, send me email, or find me at a conference. They can say, “hey, we
have an extension, could you package an RPM?” Sure, why not." I don’t
coordinate with Upstream as long as uh there is no problem with the builds.</p>
 
 Eric in chat, replying to “When you decide to p…”: So you haven’t
run into a situation where upstream did not want/appreciate you
packaging for them? 
 
 
 
<p>[Respondinding to nils’s comment]: Thank you, thanks for responding!</p>
 
 
<p>[Responding to vrmiguel’s question about depending on other extensions]:
We actually add dependency to that one. That’s bit of uh work, like PG
rotting depends on PostGIS. In order to provide a seamless installation
the PostGIS package, in the PostGIS spec file, I add an extra line that
says it provides PostGiS without the version as part of the name. Then
when we install pg rotting, it looks for any PostGIS package — which is
fine because it can run against any PostGIS version. So I add the
dependency to other extensions if we need them.</p>
 
 David G. Johnston in chat: The tooling ideally would report, say to
PGXN or whatever the directory location for the initial application
is, the issues and remind them that if the build system cannot build
their extension it will not be included in the final RPM. You are an
unpaid service provider for them and if they don’t meet their
obligations their don’t get the benefit of the service. 
 
 
 
<p>[Responding to Eric’s upstream follow-up question]: I haven’t seen
anything in any upstream  where a person didn’t want me to package. But I
haven’t seen many appreciations, either; I mean they don’t appreciate you.
I’m being appreciated by EDB — money, money, money, must be funny —
thanks EDB! But I haven’t had any rejections so far. Good question!</p>
 
 Eric in chat, replying to “When you decide to p…”: Fair. Cool.
Thanks 
 
 
 
<p>Relying on external repos is a big problem for SUSE. Some of the
maintainers just discontinue their repo. One problem with SUSE is they
don’t have an EPEL-like repo. EPEL is a great thing. The barrier to add a
package to EPEL is not low but not high, either. If you if you’re an
advanced packager you can add a package quick enough. Of course it
requires review from others. But this a big problem for SUSE.</p>
 
 
<p>Lack of maintenance is a problem. We have a repo but they don’t update it;
so I have to go find another repo from build.opensuse.org, change it,
update the website, change our build instance, etc. That’s a big problem.</p>
 
 
<p>David Wheeler (he/him) in chat, replying to “The tooling ideally
…”: I want to add build and test success/fail matrices to extension
pages on PGXN</p>
 
 
<p>Florents Tselai in chat: How do you handle Pl/Python-based
extensions + pip dependencies? Especially with virtualenv-based
installations. i.e. Pl/Python usually relies on a /usr/bin/python3,
but people shouldn’t install dependencies there.</p>
 
 
 
 
<p>And then there’s costs! What’s the cost of RPMifying an extension? Hosting
a build server?  We have a very beefy bare metal build server hosted by
Enterprise DB, just because I’m working for them and they have a spare
machine. Hosting a build server is a cost.</p>
 
 
<p>I have to use some external resources for architecture reasons, like some
of our build instances, like PPC 64 ,is hosted somewhere else. There are
some admin tasks to keep everything and running, like EDB’s IT team
actually helped me to fix an issue today in both of our PPC instances.</p>
 
 
<p>Jason Petersen in chat, replying to “How do you handle Pl…”:  I
think early on he said he makes the extensions rely on RPMs that
provide those Python dependencies</p>
 
 
<p>David Wheeler (he/him) in chat, replying to “How do you handle
Pl…”: I have used this pattern for RPMifying Perl tools</p>
 
 
 
 
<p>Then, maintaining build instances requires keeping them up-to-date, and
also that each update doesn’t break anything. It’s not like “dnf update
and build a package”. No. It may be a problem with Fedora because Fedora
may can update anything any time they want. But it’s a less problem for
SUSE and RedHat, but we have to take care that the updates don’t break
anything.</p>
 
 
<p>Redhat, the company, actually follows our release schedule. We release
every three months. Unless something bad happens, we know the next release
is in May, on a Thursday. So every Wednesday, one day before our minor release,
RedHat releases their new maintenance releases. RedHat is going
to release 9.4 on Wednesday before our minor release. What does that mean
for us as an RPM packager for RedHat?</p>
 
 
<p>*RedHat releases a new version with a new LLVM, for example, and then it
means we have to rebuild the packages against the new LLVM so that people
can use it. That means I have to work until Thursday morning to build the
packages. That’s fine but another problem is for Rocky and Alma Linux
users, because they’re are not going to have the updated LLVM package, or
any any updated package, like GCC. It’s not like the old RedHat days; they
change everything uh in minor versions.</p>
 
 
<p>So I have to rebuild GCC and LLVM on our instances, add them to our
special repo “sysupdates”, which is in the config file, and this takes
many hours because building GCC and LLVM is a big thing.</p>
 
 
<p>In the last two years I have not been able to build the from GCC Source
RPM. I had to edit everything and not edit the spec files blah blah to be
able to build it. I have no idea how how they can break in Source RPM.</p>
 
 
<p>So that’s another cost: in May I’m going to spend lots of cycles to  keep
up with the latest RedHat release, and also make the make the Rocky Linux
and Alma Linux users happier. Maintaining build systems is not as easy as
running Yup or Zypper update. It requires employing the packager —
because I have the bills pay I have the beers to drink.</p>
 
 
<p>[Addressing Florents’s PL/Python question]: I don’t know what the
PL/Python based extensions are, but I tried to get rid of everything
related to pip. I’m not a developer, a DBA isn’t a developer, a Sysadmin
isn’t a developer. They’re not suposed to use pip; they are supposed to
use the package manager to keep up with everything. My point is if someone
needs pip then I should fix it. That’s what I did for Patroni. I added
lots of packages to our Git repo just to be able to support Patroni.</p>
 
 
<p>Ian Stanton in chat: Need to drop, thank you Devrim!</p>
 
 
<p>Jeremy S in chat, replying to “How do you handle Pl…”: A lot of
larger companies have inventory management and risk control processes
that heavily leverage package management</p>
 
 
<p>Alvaro Hernandez in chat: Need to go, ttyl!</p>
 
 
<p>vrmiguel in chat, replying to “you said “gcc” and n…”: Do you
think there are no guarantees at all? For instance, Postgres loads up
the extension with <code>dlopen</code>, which could fail with <code>version mismatch</code>.
If that doesn’t occur and the extension loads ‘fine’, how likely do
you think an issue could be?</p>
<p>Also I’m curious how often you’ve seen problems arise from libc itself
(rather than any of the many things that could cause UB in a C
program) and how these problems have manifested</p>
 
 
<p>Ahmet Melih Başbuğ in chat: Thank you</p>
 
 
 
 
 Conclusion 
<p>I thanked Devrim and all the discussion, and pitched the next <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">mini-summit</a>,
where I think Jonathan Katz will talk about the TLE vision and specifics.</p>
<p>Thank you all for coming!</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/devrim-g%C3%BCnd%C3%BCz/">Devrim Gündüz</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
                 <a href="https://justatheory.com/tags/yum/">Yum</a> 
                 <a href="https://justatheory.com/tags/zypp/">ZYpp</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   20:27 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.webistemology.com/eclipse-photo-takets/'>Eclipse Photos</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   xkcd nails it again.    
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://imgs.xkcd.com/comics/types_of_eclipse_photo.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://xkcd.com/2917?ref=webistemology.com">xkcd</a> nails it again. </p><p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://imgs.xkcd.com/comics/types_of_eclipse_photo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   13:54 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 09. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/what-changed-in-50-years-of-computing-8d0'>What Changed in 50 Years of Computing: Part 2</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  How has the classic book on software engineering, ‘The Mythical Man-Month,’ aged with time, and how have architecture approaches and tech org structures changed in half a century?
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>‘The Mythical Man-Month’ by Frederick P. Brooks, Jr. is a classic in the software industry. First published in 1975, it’s almost 50 years old. Brooks was the project manager of one of the most complex software projects in the world at the time: the IBM System/360 operating system. He published this book based on his personal experience of spending several years building it, and leading several hundred programmers.</p><p>I’ve been making my way through this book from the dawn of software, to see which predictions the book gets right or wrong, and what’s different about engineering today - and which things just never change. In <a href="https://newsletter.pragmaticengineer.com/p/what-changed-in-50-years-of-computing">Part 1 of this series</a>, we covered chapters 1-3. In this article, we cover chapters 4-7:</p>  <p>Evolution of architecture approaches: The outdated separation of “architect” and implementer, the dated “technical manual” concept, software design and the “second-system effect,” &amp; telephone logs.&nbsp;</p>  <p>Architecture approaches: what’s the same. A well-architected system balances simplicity and functionality, and it’s still hard to do. Processes are still needed for proposing and discussing architecture changes for mature products and larger teams.</p>  <p>Communication challenges on large projects: then and now. Surprisingly, communication best practices have changed little in 50 years; mixing informal, formal, and written forms for efficient projects.</p>  <p>Ideal structure of tech orgs. The tree structure was the most favored for tech organizations in the 1970s, and it still is the most popular choice today. Are simplicity and straightforwardness the reason why?</p>   1. Evolution of architecture approaches <p>When it comes to how we design software, a lot has changed in 50 years:</p>
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/what-changed-in-50-years-of-computing-8d0">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   15:55 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.jonudell.net'>Jon Udell</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.jonudell.net/2024/04/09/code-in-context-how-ai-can-help-improve-our-documentation/'>Code in Context: How AI Can Help Improve Our Documentation</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Here’s the latest installment in the series on working with LLMS: https://thenewstack.io/code-in-context-how-ai-can-help-improve-our-documentation/. Writing documentation from scratch is as uncommon as writing code from scratch. More typically, you’re updating or expanding or refactoring existing docs. My expectation was that an LLM-powered tool primed with both code and documentation could provide
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Here’s the latest installment in the series on working with LLMS: <a href="https://thenewstack.io/code-in-context-how-ai-can-help-improve-our-documentation/">https://thenewstack.io/code-in-context-how-ai-can-help-improve-our-documentation/</a>. </p>
<blockquote><p>
Writing documentation from scratch is as uncommon as writing code from scratch. More typically, you’re updating or expanding or refactoring existing docs. My expectation was that an LLM-powered tool primed with both code and documentation could provide a powerful assist, and Unblocked did.</p>
<p>
I don’t know how to measure the boost it gave me. But I do know that I’ll never again want to undertake this kind of project without a tool that can help me assemble the necessary context.
</p></blockquote>
 
<p>The rest of the series:</p>
<p>
1 <a href="https://blog.jonudell.net/2023/05/24/when-the-rubber-duck-talks-back/">When the rubber duck talks back</a> </p>
<p>
2 <a href="https://blog.jonudell.net/2023/06/14/radical-just-in-time-learning/">Radical just-in-time learning</a> </p>
<p>
3 <a href="https://blog.jonudell.net/2023/06/18/why-llm-assisted-table-transformation-is-a-big-deal/">Why LLM-assisted table transformation is a big deal</a> </p>
<p>
4 <a href="https://thenewstack.io/using-llm-assisted-coding-to-write-a-custom-template-function/">Using LLM-Assisted Coding to Write a Custom Template Function</a></p>
<p>
5 <a href="https://thenewstack.io/elevating-the-conversation-with-llm-assistants/">Elevating the Conversation with LLM Assistants</a></p>
<p>
6 <a href="https://thenewstack.io/how-large-language-models-assisted-a-website-makeover/">How Large Language Models Assisted a Website Makeover</a></p>
<p>
7 <a href="https://thenewstack.io/should-llms-write-marketing-copy/">Should LLMs Write Marketing Copy?</a></p>
<p>
8 <a href="https://thenewstack.io/test-driven-development-with-llms-never-trust-always-verify/">Test-Driven Development with LLMs: Never Trust, Always Verify</a> </p>
<p>
9 <a href="https://thenewstack.io/learning-while-coding-how-llms-teach-you-implicitly/">Learning While Coding: How LLMs Teach You Implicitly</a></p>
<p>
10 <a href="https://thenewstack.io/how-llms-helped-me-build-an-odbc-plugin-for-steampipe/">How LLMs Helped Me Build an ODBC Plugin for Steampipe</a></p>
<p>
11 <a href="https://thenewstack.io/how-to-use-llms-for-dynamic-documentation/">How to Use LLMs for Dynamic Documentation</a></p>
<p>
12 <a href="https://blog.jonudell.net/2023/11/06/lets-talk-conversational-software-development/">Let’s talk: conversational software development</a></p>
<p>
13 <a href="https://thenewstack.io/techniques-for-using-llms-to-improve-sql-queries/">Using LLMs to Improve SQL Queries</a></p>
<p>
14 <a href="https://thenewstack.io/puzzling-over-the-postgres-query-planner-with-llms/">Puzzling over the Postgres Query Planner with LLMs</a></p>
<p>
15 <a href="https://thenewstack.io/7-guiding-principles-for-working-with-llms/">7 Guiding Principles for Working with LLMs</a></p>
<p>
16 <a href="https://thenewstack.io/learn-by-doing-how-llms-should-reshape-education/">Learn by Doing: How LLMs Should Reshape Education</a></p>
<p>
17 <a href="https://thenewstack.io/how-to-learn-unfamiliar-software-tools-with-chatgpt/">How to Learn Unfamiliar Software Tools with ChatGPT</a></p>
<p>
18 <a href="https://thenewstack.io/using-ai-to-improve-bad-business-writing/">Using AI to Improve Bad Business Writing</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   15:19 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 08. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2024/04/08/aviation-vs-eclipse/'>Aviation vs. Eclipse</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Contrails form behind jet aircraft flying through the stratosphere. Since high-altitude aviation is happening all around the earth more or less constantly, planes are painting the sky everywhere. (Here is one time-lapse. And another. And one of my own.) Many contrails don’t last, of course, but many do, and together they account for much of […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

Contrails in the stratosphere, smearing sideways into broad cloud cover.&nbsp; This view is toward the place in the sky where a full solar eclipse will happen a few hours later.
<p><a href="https://en.wikipedia.org/wiki/Contrail">Contrails</a> form behind jet aircraft flying through the stratosphere. Since high-altitude aviation is happening all around the earth more or less constantly, planes are painting the sky everywhere. (<a href="https://www.youtube.com/watch?v=XDeYRRe3E6Q">Here is one time-lapse</a>. <a href="https://www.youtube.com/watch?v=dvP4-sVSO-M">And another</a>. <a href="https://www.flickr.com/photos/infrastructure/53622220849/in/dateposted-public/">And one of my own</a>.)</p>
<p>Many contrails don’t last, of course, but many do, and together they account for much of the cloud cover we see every day. The altocumulus, altostratus, and cirrus clouds that contrails produce are <a href="https://www.theweathernetwork.com/news/articles/cloud-atlas-leaps-into-21st-century-with-12-new-cloud-types/80685/">now officially recognized</a>&nbsp;as homogenitus and homomutatus, which are <a href="https://en.wikipedia.org/wiki/Anthropogenic_cloud">anthropogenic</a>: owing to human activity.</p>
<p>And today, Eclipse Day, <a href="https://news.delta.com/eclipse-viewing-30000-feet-delta-offer-path-totality-flight">Delta is offering to fly you along the path of totality</a>. Others too? I don’t know. I’m taking a few moments to write this before we walk up to our hilltop cemetery to watch the eclipse for over four minutes, thanks to our lucky location near the very center of Totality.</p>
<p>I’m curious to see and hear contrail reports from others now awaiting their few minutes out of the sun.</p>
<p>1:14pm—The moon’s shadow made landfall in Mexico a short time ago. Here in Bloomington, the sky is well-painted by contrails. Mostly it looks like high-altitude haze, but believe me: if it weren’t for commercial aviation, the sky would be solid blue. Because the contrails today are quickly smeared sideways, losing their form but not their color.</p>
<p>5:00pm—Contrails were aplenty, and a spread-out contrail did slide in front of the sun and the moon…<br>
</p>
<p>but it was still a spectacular sight:</p>
<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://150108457.v2.pressablecdn.com/wp-content/uploads/2024/04/FEABC5A2-3EF5-4D2E-9C02-30CACAFA9B5A-1024x768.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:04 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2024/04/08/bff-secured-asp-net-core-application-using-downstream-api-and-an-oauth-client-credentials-jwt/'>BFF secured ASP.NET Core application using downstream API and an OAuth client credentials JWT</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This article shows how to implement a web application using backend for frontend security architecture for authentication and consumes data from a downstream API protected using a JWT access token which can only be accessed using an app-to-app access token. The access token is acquired using the OAuth2 client credentials flow and the API does […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This article shows how to implement a web application using backend for frontend security architecture for authentication and consumes data from a downstream API protected using a JWT access token which can only be accessed using an app-to-app access token. The access token is acquired using the OAuth2 client credentials flow and the API does not accept user access tokens from the UI application. OpenIddict is used as the OpenID Connect server. The STS provides both the OAuth2 client and the OpenID Connect client as well as the scope definitions.</p>



<p>Code: <a href="https://github.com/damienbod/bff-aspnetcore-angular-downstream-api">https://github.com/damienbod/bff-aspnetcore-angular-downstream-api</a></p>



<p></p>



<p>The BFF web application is implemented using ASP.NET Core and Angular as the UI tech stack. The Angular part of the web application can only use the ASP.NET Core APIs and secure same site cookies are used to protect the access. The whole application is authenticated using an OpenID Connect confidential code flow client (PKCE). If the web application requires data from the downstream API, a second OAuth client credentials flow is used to acquire the access token. The downstream API does not accept the user delegated access tokens from the UI application.</p>







<p></p>



 BFF OIDC code flow client 



<p>Implementing the OpenID Connect confidential client is really simple in ASP.NET Core. The AddAuthentication method is used with cookies and OpenID Connect. The cookies are used to store the session and the OpenID Connect is used for the challenge. All server rendered applications are setup like this with small changes required for the OIDC challenge. Due to these small differences, the different OIDC implementations provide specific implementations of the client. These are normally focused and optimized for the specific OIDC servers and do not work good with other OIDC servers. Once you use more than one OIDC server or require multiple clients from the same OIDC server, the client wrappers cause problems and you should revert back to the standards.</p>


  var stsServer = configuration["OpenIDConnectSettings:Authority"];

services.AddAuthentication(options =&gt;
{
    options.DefaultScheme = CookieAuthenticationDefaults.AuthenticationScheme;
    options.DefaultChallengeScheme = OpenIdConnectDefaults.AuthenticationScheme;
})
.AddCookie()
.AddOpenIdConnect(options =&gt;
{
    configuration.GetSection("OpenIDConnectSettings").Bind(options);

    options.SignInScheme = CookieAuthenticationDefaults.AuthenticationScheme;
    options.ResponseType = OpenIdConnectResponseType.Code;

    options.SaveTokens = true;
    options.GetClaimsFromUserInfoEndpoint = true;
    options.TokenValidationParameters = new TokenValidationParameters
    {
        NameClaimType = "name"
    };
});
  


<p></p>



 Yarp Proxy 



<p>The Angular UI can only request data from the ASP.NET Core backend using secure http only cookies. The Angular UI is deployed as part of the ASP.NET Core application in production builds. When creating applications, software developers need to use their preferred tools and YARP is used to support this in the development setup. As a further downstream API is used, YARP can also be used to support this. The proxy takes the API request, validates the cookie, uses another access token and forwards the request to the downstream API. YARP has an ITransformProvider interface which is used to implement this. This also means we have two different YARP configuration setups for development and deployments. (test, integration, production).</p>


  using System.Net.Http.Headers;
using Yarp.ReverseProxy.Transforms;
using Yarp.ReverseProxy.Transforms.Builder;

namespace BffOpenIddict.Server.ApiClient;

public class JwtTransformProvider : ITransformProvider
{
    private readonly ApiTokenCacheClient _apiTokenClient;

    public JwtTransformProvider(ApiTokenCacheClient apiTokenClient)
    {
        _apiTokenClient = apiTokenClient;
    }

    public void Apply(TransformBuilderContext context)
    {
        if (context.Route.RouteId == "downstreamapiroute")
        {
            context.AddRequestTransform(async transformContext =&gt;
            {
                var access_token = await _apiTokenClient.GetApiToken(
                    "CC",
                    "dataEventRecords",
                    "cc_secret");

                transformContext.ProxyRequest.Headers.Authorization
                    = new AuthenticationHeaderValue("Bearer", access_token);
            });
        }
    }

    public void ValidateCluster(TransformClusterValidationContext context)
    {
    }

    public void ValidateRoute(TransformRouteValidationContext context)
    {
    }
}
  


<p></p>



<p>The AddReverseProxy is used to add the YARP services.</p>


  builder.Services.AddReverseProxy()
   .LoadFromConfig(builder.Configuration.GetSection("ReverseProxy"))
   .AddTransforms&lt;JwtTransformProvider&gt;();
  


<p>And the middleware:</p>


  app.MapReverseProxy();
  


 API client credentials client 



<p>The YARP proxy uses the OAuth client credentials client to get an access token to access the downstream API. The token is stored in a cache and only rotated when it expires or is missing. The app-to-app security has nothing to do with the delegated client from the web application.</p>


  using IdentityModel.Client;
using Microsoft.Extensions.Caching.Distributed;

namespace BffOpenIddict.Server.ApiClient;

public class ApiTokenCacheClient
{
    private readonly ILogger&lt;ApiTokenCacheClient&gt; _logger;
    private readonly HttpClient _httpClient;

    private static readonly object _lock = new();
    private readonly IDistributedCache _cache;
    private readonly IConfiguration _configuration;
    private const int cacheExpirationInDays = 1;

    private class AccessTokenItem
    {
        public string AccessToken { get; set; } = string.Empty;
        public DateTime ExpiresIn { get; set; }
    }

    public ApiTokenCacheClient(
        IHttpClientFactory httpClientFactory,
        ILoggerFactory loggerFactory,
        IConfiguration configuration,
        IDistributedCache cache)
    {
        _httpClient = httpClientFactory.CreateClient();
        _logger = loggerFactory.CreateLogger&lt;ApiTokenCacheClient&gt;();
        _cache = cache;
        _configuration = configuration;
    }

    public async Task&lt;string&gt; GetApiToken(string api_name, string api_scope, string secret)
    {
        var accessToken = GetFromCache(api_name);

        if (accessToken != null)
        {
            if (accessToken.ExpiresIn &gt; DateTime.UtcNow)
            {
                return accessToken.AccessToken;
            }
            else
            {
                // remove  =&gt; NOT Needed for this cache type
            }
        }

        _logger.LogDebug("GetApiToken new from STS for {api_name}", api_name);

        // add
        var newAccessToken = await GetApiTokenInternal(api_name, api_scope, secret);
        AddToCache(api_name, newAccessToken);

        return newAccessToken.AccessToken;
    }

    private async Task&lt;AccessTokenItem&gt; GetApiTokenInternal(string api_name, string api_scope, string secret)
    {
        try
        {
            var disco = await HttpClientDiscoveryExtensions.GetDiscoveryDocumentAsync(
                _httpClient,
                _configuration["OpenIDConnectSettings:Authority"]);

            if (disco.IsError)
            {
                _logger.LogError("disco error Status code: {discoIsError}, Error: {discoError}", disco.IsError, disco.IsError);
                throw new ApplicationException($"Status code: {disco.IsError}, Error: {disco.Error}");
            }

            var tokenResponse = await HttpClientTokenRequestExtensions.RequestClientCredentialsTokenAsync(_httpClient, new ClientCredentialsTokenRequest
            {
                Scope = api_scope,
                ClientSecret = secret,
                Address = disco.TokenEndpoint,
                ClientId = api_name
            });

            if (tokenResponse.IsError || tokenResponse.AccessToken == null)
            {
                _logger.LogError("tokenResponse.IsError Status code: {tokenResponseIsError}, Error: {tokenResponseError}", tokenResponse.IsError, tokenResponse.Error);
                throw new ApplicationException($"Status code: {tokenResponse.IsError}, Error: {tokenResponse.Error}");
            }

            return new AccessTokenItem
            {
                ExpiresIn = DateTime.UtcNow.AddSeconds(tokenResponse.ExpiresIn),
                AccessToken = tokenResponse.AccessToken
            };

        }
        catch (Exception e)
        {
            _logger.LogError("Exception {e}", e);
            throw new ApplicationException($"Exception {e}");
        }
    }

    private void AddToCache(string key, AccessTokenItem accessTokenItem)
    {
        var options = new DistributedCacheEntryOptions()
            .SetSlidingExpiration(TimeSpan.FromDays(cacheExpirationInDays));

        lock (_lock)
        {
            _cache.SetString(key, System.Text.Json.JsonSerializer.Serialize(accessTokenItem), options);
        }
    }

    private AccessTokenItem? GetFromCache(string key)
    {
        var item = _cache.GetString(key);
        if (item != null)
        {
            return System.Text.Json.JsonSerializer.Deserialize&lt;AccessTokenItem&gt;(item);
        }

        return null;
    }
}
  


<p></p>



 Downstream API 



<p>The downstream API is protected using JWT access tokens. This is setup using the AddJwtBearer method. The scope and other claims should also be validated.</p>


   services.AddAuthentication()
           .AddJwtBearer("Bearer", options =&gt;
           {
               options.Audience = "rs_dataEventRecordsApi";
               options.Authority = "https://localhost:44318/";
               options.TokenValidationParameters = new TokenValidationParameters
               {
                   ValidateIssuer = true,
                   ValidateAudience = true,
                   ValidateIssuerSigningKey = true,
                   ValidAudiences = ["rs_dataEventRecordsApi"],
                   ValidIssuers = ["https://localhost:44318/"],
               };
           });
  


 Notes 



<p>This setup can be used for all server rendered applications. You should always use an external identity provider in enterprise setups and never roll out your own identity system as this is expensive to maintain and many enterprise environments no longer accept this due to the extra operation costs.  Replacing Angular with react, Vue.js, Svelte or Blazor WASM does not require changes to the authentication. The different UI have differences on how the scripts are loaded or used and some require weaker session security setups. </p>



<p>You should also avoid downstream APIs if not required. Modular monoliths have performance advantages.</p>



 Links 



<p><a href="https://github.com/damienbod/bff-aspnetcore-angular">https://github.com/damienbod/bff-aspnetcore-angular</a></p>



<p><a href="https://learn.microsoft.com/en-us/aspnet/core/introduction-to-aspnet-core">https://learn.microsoft.com/en-us/aspnet/core/introduction-to-aspnet-core</a></p>



<p><a href="https://nx.dev/getting-started/intro">https://nx.dev/getting-started/intro</a></p>



<p><a href="https://github.com/isolutionsag/aspnet-react-bff-proxy-example">https://github.com/isolutionsag/aspnet-react-bff-proxy-example</a></p>



<p><a href="https://github.com/openiddict">https://github.com/openiddict</a></p>



<p><a href="https://github.com/damienbod/bff-auth0-aspnetcore-angular">https://github.com/damienbod/bff-auth0-aspnetcore-angular</a></p>



<p><a href="https://github.com/damienbod/bff-azureadb2c-aspnetcore-angular">https://github.com/damienbod/bff-azureadb2c-aspnetcore-angular</a></p>



<p><a href="https://github.com/damienbod/bff-aspnetcore-vuejs">https://github.com/damienbod/bff-aspnetcore-vuejs</a></p>



<p><a href="https://github.com/damienbod/bff-MicrosoftEntraExternalID-aspnetcore-angular">https://github.com/damienbod/bff-MicrosoftEntraExternalID-aspnetcore-angular</a></p>



<p><a href="https://microsoft.github.io/reverse-proxy/articles/transforms.html">https://microsoft.github.io/reverse-proxy/articles/transforms.html</a></p>



<p><a href="https://github.com/microsoft/reverse-proxy/tree/main/samples/ReverseProxy.Transforms.Sample">https://github.com/microsoft/reverse-proxy/tree/main/samples/ReverseProxy.Transforms.Sample</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2024/04/context.png?w=481'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   05:43 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 04. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-88'>The Pulse #88: are we at peak AI hype?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Several signs are pointing that we’ve hit the peak of this AI hype cycle: that things could cool down soon enough. Also: the sudden license change at Redis; HashiCorp looking for a buyer, and more.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering insights, patterns, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Today, we cover:</p>  <p>Industry pulse. A global software hack stopped in an improbable way; security company Rubrik prepares to go public; Mark Zuckerberg personally convincing AI engineers to join Meta, and more.</p>  <p>Peak AI hype? There are several signs pointing that we have hit the peak of the AI hype – and perhaps already passed it by a little. Companies spending 10x on GPUs than revenue generated, AI startups unable to raise the next round of funding, and irrationally high valuations for AI startups whose technology is about to be commoditized.</p>  <p>Commercial open source companies in trouble? Redis Labs changed the formerly permissive open source Redis license to a restrictive one, with the goal to have cloud providers pay when they host Redis. As a response, cloud providers started the Valkey project, which could become the “new and still permissive Redis.” HashiCorp is facing similar challenges with Terraform / OpenTofu. We could be seeing the end of billion-dollar companies built on permissive open source licenses.</p>  <p>Section 174: still hurting US software companies. The US remains the most hostile country in the world to start a software business: because developer salaries cannot be deducted as an expense. The US Senate has a bill that would fix this: but this bill shows no signs of being passed. The US is <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-75">likely to see fewer software engineers hired</a> until these Section 174 changes remain in place.</p>   1. Industry pulse 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-88">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:52 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 02. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.moxytongue.com/'>Moxy Tongue</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='http://www.moxytongue.com/2024/04/america-works-for-people-building-for.html'>America Works For People, Building For Happiness..</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   America comes with a proposition ratified (LLPoH); Sovereignty is processed as a runtime event at the edges. Middleization is an attack vector. Inadequate system designs are a feature flaw of inadequate engineering design processes. Governance by administration of law is absolutely evolutionary. Failures happen. Edits happen. Time continues throughout, and personal views are coherent in context. C
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>America comes with a proposition ratified (LLPoH); Sovereignty is processed as a runtime event at the edges. Middleization is an attack vector. Inadequate system designs are a feature flaw of inadequate engineering design processes. Governance by administration of law is absolutely evolutionary. Failures happen. Edits happen. Time continues throughout, and personal views are coherent in context. Context is an edge driven event requiring insight by Governance under scrutiny. Scrutiny is a cost of Sovereignty. Accurate scrutiny processes are a function of engineering design process. Participation by people in American civil society is an edge-driven experience by people with blood&nbsp;in their veins. Administration of law by Sovereign process requires accurate context for accurate processing.&nbsp;</p>  Current inadequate understandings and administration by poorly/ wrongly trained managers of Sovereign law is a time-based experience. Yes, lives are affected. "We hold these truths to be self-evident..."; turns out, with digital context, Sovereignty is not so self-evident to some... tcp/ip engineers under administrative influence released a tool of military-use into civil society without understanding scale of impact "breaking silos" would have, and what Constitutes a silo where data is transported meaningfully.  <br>  Structure yields results.. welcome to 2024.  <br>  Reminder: Communists have no property Rights.&nbsp;  <br>  <a href="https://www.moxytongue.com/2021/07/recursive-signatory.html">Sovereignty; recursive accuracy for people.</a>  
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:42 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/genz'>GenZ software engineers, according to older colleagues</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Responses to a survey about GenZ suggest this new generation possesses standout differences. We explore what makes GenZ distinctive, and check out ideas for ways to work fruitfully together.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>👋 Hi, this is Gergely with a subscriber-only issue of the Pragmatic Engineer Newsletter. In every issue, I cover challenges at Big Tech and startups through the lens of engineering managers and senior engineers. To get articles like this in your inbox, every week, subscribe:</p><p><a href="https://newsletter.pragmaticengineer.com/subscribe?">Subscribe now</a></p><p>Before we start: I’ve published details on <a href="https://blog.pragmaticengineer.com/pragmatic-engineer-guest-article/">how indicate interest in writing a guest article The Pragmatic Engineer</a>. My goal is to work with hands-on engineering professionals to occasionally bring details, learnings and approaches from "working in the weeds.” A good example was last week’s article on <a href="https://newsletter.pragmaticengineer.com/p/thriving-as-a-founding-engineer">Thriving as a founding engineer</a>. If you’re interested in potentially writing a guest post, <a href="https://blog.pragmaticengineer.com/pragmatic-engineer-guest-article/">see the details</a>.</p><p>A few months ago, I met up with my former engineering manager colleagues for dinner. As we caught up on each other’s news, one topic of chat was the new generation of tech workers. A common thing mentioned was that GenZ feels notably different to work with. Positive in many ways – and a bit confusing in others!</p><p>So, what is the new generation like at work, according to their colleagues? In a survey, we asked this of readers who identify as Millennial, GenX, or Boomer, and have dug into the replies.&nbsp;</p><p>Today, we cover:</p>  <p>Who are GenZ? A recap.</p>  <p>Survey respondents. Mostly managers and seniors who are Millennial or GenX, based in the US and Europe.</p>  <p>Values. Higher salary and benefits expectations, challenging to retain, more distrustful of senior leadership.</p>  <p>Knowledge. GenZ are on the “bleeding edge” of frameworks, higher up on the tech stack, and use different learning resources.</p>  <p>Mentoring GenZ: what works? Display purpose, focus on the individual, and connect!</p>  <p>Working with GenZ: successful approaches. Give feedback, consider extra onboarding and training, and accept they’ll probably leave.</p>  <p>Pandemic impact? Enforced remote work began just as many people started their first jobs. It was a hard time for graduates and young professionals, for whom staying indoors – and on endless Zoom calls – was likely the last thing they wanted to do.</p>  <p>More observations and advice for GenZ. A generation of “mages,” and a possible bimodal distribution for ambition. Also, sharing the unfiltered <a href="https://newsletter.pragmaticengineer.com/i/40654455/reports">survey responses</a>.</p>  <p>This is part 1 of a two-part high-level overview on the new generation of talent in tech workplaces. This article is based on a small survey of people’s personal opinions, beginning with the views of more “seasoned” folks. But it won’t be a one-sided series! In part 2 to come soon, we hand the mic to tech professionals from GenZ, to learn what it’s like being young and in tech today – and what older colleagues are like to work with.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd628304c-a6de-4678-9c89-9dcdbe63108a_1654x1172.png">    </a>Some of the findings we’ll explore in this article  1. Who are GenZ? A recap. <p>Generation Z (GenZ) is the group name for people born between 1997 and 2012, and it’s commonly used in media and popular culture. In workplaces, the most senior members of this new generation have joined colleagues from the cohorts “Millennial,” “GenX,” and “Baby Boomer.” Each of these names refers to a span of time, and birth date determines which one you fit into:&nbsp;</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb720c42f-8d37-421f-8ebc-43a70037c5e7_1600x971.png">    </a>An overview of demographic cohorts. Image source: <a href="https://en.wikipedia.org/wiki/Millennials">Wikipedia</a> <p>The oldest members of GenZ are 27 years old, and the youngest are 12, meaning that graduate developers hired over the past few years are almost all GenZ’ers.</p><p>They’re the first generation to grow up “mobile native,” with consumer tech like smartphones, often from a very young age. The oldest members of this generation were aged 10 when the Apple iPhone launched in 2007 and kicked off the smartphone revolution.</p><p>There are <a href="https://www.researchgate.net/search?q=genz">countless studies</a> claiming GenZ is different from earlier ones. A summary <a href="https://en.wikipedia.org/wiki/Generation_Z">from Wikipedia</a>:</p><blockquote><p>“Compared to previous generations, members of Generation Z tend to live more slowly than their predecessors when they were their age; have lower rates of teenage pregnancies; and consume alcohol (but not necessarily other psychoactive drugs) less often. Generation Z teenagers are more concerned than older generations with academic performance and job prospects, and are better at delaying gratification than their counterparts from the 1960s, despite concerns to the contrary. (...) Nostalgia is a major theme of youth culture in the 2010s and 2020s.”</p></blockquote><p>For reference, here is how Millennials are described, also <a href="https://en.wikipedia.org/wiki/Millennials">by Wikipedia</a>:</p><blockquote><p>“As the first generation to grow up with the Internet, Millennials have also been described as the first global generation. The generation is generally marked by elevated usage of and familiarity with the Internet, mobile devices, and social media. The term "digital natives", which is now also applied to successive generations, was originally coined to describe this generation.</p><p>Millennials have also been called the ‘Unluckiest Generation’ because the average Millennial has experienced slower economic growth since entering the workforce than any other generation in U.S. history. The generation has also been weighed down by student debt and child-care costs.</p><p>Millennials across the world have suffered significant economic disruption since starting their working lives; many faced high levels of youth unemployment during their early years in the job market in the wake of the Great Recession, and suffered another recession in 2020 due to the COVID-19 pandemic.”</p></blockquote><p>And let’s not forget GenX:</p><blockquote><p>“As adolescents and young adults in the 1980s and 1990s, Xers were dubbed the ‘MTV Generation’ (a reference to the music video channel), sometimes being characterized as slackers, cynical, and disaffected. Some of the many cultural influences on Gen X youth included a proliferation of musical genres with strong social-tribal identity such as alternative rock, hip hop, punk, post-punk, rave, and heavy metal. (...) Video games, both in amusement parlors and in devices in Western homes, were also a major part of juvenile entertainment for the first time. Politically, in many Eastern Bloc countries, Generation X experienced the last days of communism and the transition to capitalism as part of its youth. In much of the Western world, a similar time period was defined by a dominance of conservatism and free market economics.<br><br>In their midlife during the early 21st century, research describes Gen Xers as active, happy, and achieving a work–life balance. The cohort has also been more broadly credited as entrepreneurial, and productive in the workplace.”</p></blockquote> 2. Survey Respondents <p>Sixty three readers took part in the survey. Here’s a breakdown of their current job titles:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F214b453a-b350-4d12-b5f2-dc992d08096d_1172x1068.png">    </a>Most are in management roles (manager, director-level, VP or C-level) or software engineers (senior-and-above.) “Other” covers product and data roles <p>Most respondents are based in the US and Europe:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93fda12c-1bbe-431f-82bd-a5447a35a039_1148x1068.png">    </a>Respondents by location <p>Most respondents belong to the Millennial or GenX generations.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F457f3a15-d659-490b-93f8-dd17fd35434c_1230x1122.png">    </a>Respondents, by generation. The lack of Boomers is unsurprising, since most are near retirement <p>What I don’t know is the extent to which participants in the survey reflect the newsletter’s readership. But there’s a way to find out. My newsletter platform (Substack) launched surveys that let you vote with a click. So, I have a request; would you please tap below on the generation you belong to, based on your birth date? Voting is anonymous, and your vote isn’t linked to your email.</p> <p>Many thanks to all who share this detail! With that, let’s dive into the survey’s findings.</p> 3. Values <p>A note of caution: of course, it’s impossible to sum up a whole generation based on a mix of opinions from a few workplaces! The details in this article are anecdotal, based on individuals’ experiences.&nbsp;</p><p>We know labels like “GenZ” are loose and fail to account for the inherent diversity of millions of people, which simply cannot be summed up in a collection of observations, no matter how detailed. There are GenZ folks to whom none of the respondents’ views apply, as well as people with traits which no respondent captured.&nbsp;</p><p>Our goal with this survey is just to try and identify any distinctive characteristics which could be worth paying attention to. How can we work together with this younger generation and build great products and a thriving business? It’s undeniable that differences exist between successive generations, and it’s worth attempting to verbalize these. The new generation of software engineers is no different!</p> Higher expectations for salary, promotions and benefits <p>Common themes from the responses:</p>  <p>GenZ expects bigger raises and faster promotions</p>  <p>Benefits like flexible or remote working are expected</p>  <p>Some respondents say younger colleagues don’t appear to offer as much in return, such as shouldering higher expectations, or taking on extra responsibilities. To be fair, this sounds like fairly typical “early-stage career” behavior; I remember Millennials being described like this 10-15 years ago!</p> Hard to retain, high expectations, &amp; little “loyalty” <p>It’s clear that survey respondents think GenZ expects promotions faster, and aren’t shy to jump ship for better pay. Respondents say there is little to no “corporate loyalty” either. This is understandable; can we entirely blame people for putting their own career first, especially after waves of ruthless job cuts in software engineering by companies following their own interests? Also, switching jobs for better pay occurs across the generations.</p><p>A lack of corporate “loyalty” among young colleagues is noted positively by some respondents:</p><blockquote><p>“Retaining them is hard, there is no loyalty to the company. But this is a good thing, as we have seen that companies, even great ones like Google, aren't loyal to their people! So, you need to be creative, move GenZ colleagues around a lot within the team (keep the work fresh), use retention strategies (options, bonuses, etc.,) and find empathetic managers.” <br><br>– Director of Engineering at a startup acquired by Big Tech (GenX, US)</p></blockquote><blockquote><p>“From my old-school perspective, they are somewhat entitled, expecting flexibility, yearly raises and promotions – even when the company has no need for more managers, for example. They also expect significant benefits on top of our higher-than-market pay.&nbsp;</p><p>As many before me have said, they have cast off outdated expectations of ‘loyalty’ to jobs that aren't loyal to them, in return. It's admirable for the most part, though I do not think everyone should get raises/promotions every year in a small company.”</p><p>– Head of Product at a startup (Millennial, US)</p></blockquote><p>Retaining GenZ workers is harder, according to some responses:</p><blockquote><p>“Motivating and retaining seems to be increasingly difficult, as they have all been advised to job hop around the two-year mark in order to have a chance at earning more. Which I don’t blame them for – it’s just that it is tough for managers who have little say/influence on their reports’ pay packages.”<br><br>– Senior Software Engineer at a Fintech (GenX, US)</p></blockquote><blockquote><p>“I see a challenge around motivation and retention. There is somehow a baked-in assumption that just turning up to work means you should get a pay rise. Until very recently (due to the downturn in tech jobs,) it was extremely difficult to retain without using a lever, such as compensation.”&nbsp;</p><p>– VP of SRE at startup (GenX, US)</p></blockquote><blockquote><p>“I find retaining them hard when it comes to money: sometimes they get crazy offers for positions! They will apply to positions that they are only starting to be ready for. Older peers tend to take a different approach: they sometimes need to feel more confident about their capabilities in the target role before changing jobs.”</p><p>– Platform Director at a Fintech (Millennial, France)</p></blockquote><p>Could social media influencers be fuelling career impatience among young professionals? One respondent reckons so:</p><blockquote><p>“Almost impossible to retain; expected salaries and titles more often than not surpass their actual ability about 6-12 months into tenure. From conversations with fellow managers and university lecturers, it seems YouTube and LinkedIn influencers are fuelling unrealistic expectations.&nbsp;</p><p>Specifically: over-stating market-rate salaries; while understating levels of responsibility that accompany a given salary range”</p><p>– Head of Engineering at a Series A startup (Millennial, UK)</p></blockquote><p>It’s tempting to blame social media, but I reckon what we’re seeing is that GenZ have far more (good and bad) career advice from peers than previous generations did. Sure, this advice isn’t always perfect, even when it’s well-meant.</p> Senior leadership distrusted, demand for transparency <p>Survey respondents think GenZ colleagues have less trust in management, and are skeptical of business decisions:</p><blockquote><p>“I find GenZ to be strong believers in radical transparency. They want to know the ‘why’ behind many decisions. The speed with which they want to consume this information is also much higher than other generations.&nbsp;</p><p>I have observed that GenZ have a stronger distrust of senior leadership than other generations. My hypothesis is that with the tech retraction in 2022-2023, this generation was going through its first downturn and thus questioned why there were no visible consequences for company leadership.&nbsp;</p><p>Looking back to the sub-prime crisis of 2008 – my first downturn after graduating college – I was also concerned. However, I saw that this was more than just a specific company issue, and was a much larger economic problem. One could correctly say that the tech retraction of 2022 and the sub-prime crisis of 2008 are not apples-to-apples, but I think there are some similarities.”</p><p>– Engineering Manager at a startup (Millennial, Boston, US)</p></blockquote><blockquote><p>“They are very aware and cynical of the impact of capitalism on business decisions.” <br><br>– Frontend Engineer at Amazon (GenX, Canada)</p></blockquote><p>Workplaces with low transparency may have a hard time retaining GenZ workers, some respondents believe:</p><blockquote><p>“My hunch would be that GenZ won’t stay long in cultures where transparency is not the norm, and where it’s not okay to question upwards, and in a direct way.”<br><br>– Senior DevOps Engineer in the public sector (Millennial, Singapore)</p></blockquote><p>A fitting summary comes from an engineering manager:</p><blockquote><p>“The don’t take management bullsh*t.”<br><br>– Group Engineering Manager at a scaleup (GenX, Australia)</p></blockquote> Outspoken, open communication style <p>Blunt communication is something older colleagues notice:</p><blockquote><p>“They have no trouble interacting with their peers, older folks. There weren’t any occasions where communication was a problem, I’ve not received or heard of any negative feedback.&nbsp;</p><p>This being said, they were more likely to speak out against and voice their concerns directly. Whether it was work related (disagreeing with an analysis, decision, etc,) or non-work related (good movies and tv shows.) This happens at varying levels<br> between individuals, but it is common among the GenZ’ers I’ve interacted with.<br><br>Some of my colleagues were disarmed by their frankness. Some found that the GenZ were too open, insensitive, and should be more sensitive of their environment, and of people hearing what they said. Others embraced the directness and it built strong relationships that lasted beyond the internship. “<br><br>– Senior DevOps Engineer in the public sector (Millennial, Singapore)</p></blockquote><p>A few other respondents mention that the communication style ruffles the feathers of some senior colleagues:</p><blockquote><p>“It can be hard for older peers, as younger new joiners often feel free to share strong opinions about how things are done, that could be harmful for others.”<br><br>– Platform Director at a scaleup (Millennial, Spain)</p></blockquote><p>The reasons for such directness and bluntness are well-intended, says one respondent:</p><blockquote><p>“They are clearly intent on improving the status quo.” <br><br>– VP of SRE at a scaleup (GenX, UK)</p></blockquote><p>An important trait pointed out by a VP:</p><blockquote><p>“They are usually willing to speak truth to power.”<br><br>– VP of Engineering (Boomer, US)</p></blockquote><p>Does the age-old age gap explain older professionals’ reactions to young colleagues, or is there more to it? Certainly, many survey responses mention GenZ’s “in your face” communication style. Cultural factors may be at play, and perhaps GenZ is a generation that speaks its mind, regardless of whether to a peer, or the business’s Boomer CEO!</p><p>Personally, I think this fits into existing workplace trends; better teams already spend time ensuring graduate colleagues feel unafraid to speak up when senior peers are in the room. Indeed, companies like Meta <a href="https://newsletter.pragmaticengineer.com/i/54316453/standout-differences-between-facebook-and-other-big-tech-companies">got rid of titles of seniority</a> in order to encourage precisely this behavior. But it’s always good to bear in mind the thin line between directness and rudeness.&nbsp;</p> Modern values? <p>Survey respondents say GenZ colleagues are more vocal about work-life balance, flexibility of work, mental health, and diversity. They are more protective of free time, but friendlier and more personal in the office than older colleagues. Survey responses from GenZ tech workers confirm these observations.</p><p>Protecting free time:</p><blockquote><p>“They are much less willing to do the work to grow on their own time. They are also much more willing to take a break and protect their own time.”<br><br>– Head of Design at a scaleup (GenX, US)</p></blockquote><blockquote><p>“They have a dim view of working outside the 9-5.“<br><br>– Staff Engineer at a startup (Boomer, US)</p></blockquote><p>Informality and openness:</p><blockquote><p>“GenZ is much more open about sharing personal situations: mental health, life events, etc. As a manager, I like this as it lets me know what is going on and how to help them.<br><br>– Director of Software development, large tech company (GenX, US)</p></blockquote><blockquote><p>“Much more informal in the workplace; they tend to have friendlier, closer relationships with colleagues. They also excel at workplace culture and work-life balance.”<br><br>– Frontend engineer at Amazon (Millennial, US)</p></blockquote><p>Diversity and inclusivity:</p><blockquote><p>“They are very much pushing for a great, healthy culture and a lot of diversity – this is good! Mental health is something front and center for them, which is great to see in the workplace.”<br><br>– Staff Software Engineer at a scaleup (GenX, New York, US)</p></blockquote><blockquote><p>“I’ve found that GenZ engineers are more inclusive of different backgrounds and people at the company/on teams.”<br><br>– Senior Software Engineer, scaleup (Millennial, US)</p></blockquote><blockquote><p>“They are more aware of diversity issues.”<br><br>– CTO at a mid-sized company (Boomer, Europe)</p></blockquote><p>The importance of flexibility:</p><blockquote><p>“Flexible hours are very important for them, as is independence, and working in a ‘flat’ hierarchy.”</p><p>– CTO at a mid-sized company (Boomer, Europe)</p></blockquote><p>Workplace politics:</p><blockquote><p>“The mere existence of people who don't agree with them politically seems deeply upsetting oftentimes, but maybe that's a symptom of a larger issue of polarization in the U.S. When I entered the workforce, I thought it was understood that religion and politics were completely off the table AT WORK because it should go without saying that not everybody is in complete lockstep with the same ideology, and yet we must still work together and see one another every day.“<br><br>– Senior Software Engineer at a startup (Millennial, US)</p></blockquote><p>Caring about things earlier generations do not:</p><blockquote><p>“From my point of view as a member of GenX, GenZ can be critical of things that I don't even notice. For example, they might use Firefox over Chrome because Firefox is part of the truly open internet.”<br><br>– Head of Design at a dev tools company (GenX, US)</p></blockquote> Purpose and meaning <p>GenZ seems more inspired by a strong sense of purpose and meaning, according to their older colleagues. Maybe this is why they’re seen as questioning everything: they want to know the why of tasks and decisions, and to do things for the right reasons!</p><blockquote><p>“I see the GenZ people need a strong purpose. When the company can provide it to them, it's easier to keep them motivated.”<br><br>– Team Lead at a startup (Millennial, Brazil)</p></blockquote><blockquote><p>“GenZ engineers can be extremely motivated to do right by the user, particularly if the problem they are solving has a deep impact on something meaningful.“<br><br>– Head of Design at a dev tools company (GenX, US)</p></blockquote> 4. Knowledge 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/genz">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd628304c-a6de-4678-9c89-9dcdbe63108a_1654x1172.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:24 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2024/04/02/create-conditional-access-base-policies-for-a-microsoft-entra-id-tenant/'>Create conditional access base policies for a Microsoft Entra ID tenant</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This article shows some of the base conditional access policies which can be implemented for all Microsoft Entra ID tenants. Phishing resistant authentication should be required for all administration flows and some other user policies like sign-in risk MFA or terms of conditions. I recommend these base policies when implementing an Microsoft Entra ID tenant […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This article shows some of the base conditional access policies which can be implemented for all Microsoft Entra ID tenants. Phishing resistant authentication should be required for all administration flows and some other user policies like sign-in risk MFA or terms of conditions. I recommend these base policies when implementing an Microsoft Entra ID tenant using a P2 license.</p>



 Disable security defaults 



<p>The security defaults are a good basic setup, but when a P2 license is used, conditional access policies can be applied and the tenant can be setup to force things like phishing resistant authentication.</p>



<p>Disable on the tenant in the “your-tenant” | Overview | Properties</p>







<p></p>



<p>All the security defaults are disabled and good conditional access policies are now required.</p>



 Activate conditional access policies 



<p>There are many conditional access policies. These are applied and different depending on the tenant requirements. The following base policies make sense in all tenants:</p>



 
 Force MFA conditional access policy (All users) 



 Require Terms of Use policy 



 Block legacy authentication  (All users) 



 Enable Sign-in risk policy  (All users) 



 Require phishing resistant authentication for admins 



 Enable User risk policy (All users) 
 



<p>A single break glass account is excluded from these policies and this account should never be used except in an emergency. Alerts are required on this account. </p>



 1. Force MFA  conditional access policy 



<p>Multi-factor authentication can be forced for all users except the break glass account. This uses the “Require authentication strength” policy and the tenant can set the default strength as required.</p>



<p>Add the following policy ( Force MFA All users except break glass account )</p>







<p></p>



 2. Require Terms of Use policy 



<p>Add a Require Terms of Use for app ( App Require Terms of Use ) policy. You can use Microsoft Entra ID to force the users of the tenant and all the client apps to except the terms of conditions required by the tenant and the hosted applications.</p>



<p>The terms of use needs to be added to the Azure tenant:</p>



<p><a href="https://learn.microsoft.com/en-us/entra/identity/conditional-access/terms-of-use">https://learn.microsoft.com/en-us/entra/identity/conditional-access/terms-of-use</a></p>



<p>The policy can be created for the terms of use. See the Microsoft docs for details.</p>



<p><a href="https://learn.microsoft.com/en-us/entra/identity/conditional-access/require-tou">https://learn.microsoft.com/en-us/entra/identity/conditional-access/require-tou</a></p>



<p></p>



 3. Block legacy authentication 



<p>Block the legacy authentication in the tenant. The Client apps should select only the Exchange ActiveSync clients and Other clients and the access must be blocked.</p>



<p><a href="https://learn.microsoft.com/en-us/entra/identity/conditional-access/howto-conditional-access-policy-block-legacy">https://learn.microsoft.com/en-us/entra/identity/conditional-access/howto-conditional-access-policy-block-legacy</a></p>







<p></p>



 4. Enable Sign-in risk policy 



<p>You can activate the sign-in risk and choose how strict. If a risky sign-in is detected, the user is required to do a multi-factor authentication. This requires a P2 license for user accounts. See the Microsoft docs for details:</p>



<p><a href="https://learn.microsoft.com/en-us/entra/id-protection/howto-identity-protection-configure-risk-policies">https://learn.microsoft.com/en-us/entra/id-protection/howto-identity-protection-configure-risk-policies</a></p>







<p></p>



 5. Require phishing resistant authentication for admins 



<p>Phishing resistant MFA should be applied to app administrator workloads. This can be created from the Azure provided template.</p>



<p><a href="https://learn.microsoft.com/en-us/entra/identity/conditional-access/how-to-policy-phish-resistant-admin-mfa">https://learn.microsoft.com/en-us/entra/identity/conditional-access/how-to-policy-phish-resistant-admin-mfa</a></p>



<p>The policy is applied to the Azure roles:</p>



 
 Global Administrator 



 Application Administrator 



 Authentication Administrator 



 Billing Administrator 



 Cloud Application Administrator 



 Conditional&nbsp;Access Administrator 



 Exchange Administrator 



 Helpdesk Administrator 



 Password Administrator 



 Privileged Authentication Administrator 



 Privileged&nbsp;Role&nbsp;Administrator 



 Security Administrator 



 SharePoint Administrator 



 User Administrator 
 



<p>When a user account has one of the Azure admin roles, phishing resistant authentication is required for access to the tenant.</p>







<p></p>



 6. Enable User risk policy (All users) 



<p>If a user account has a high or medium level possibility that it has been compromised, the user is required to do a multi-factor authentication. Why not Self-service password reset (SSPR)? I don’t really see the point of this if you are using passwordless sign-ins. Without a SSPR for a user with a password, the user-risk is not reset and the user will be forced to MFA again. I am not sure how this policy works with passwordless or phishing resistant authentication flows. This policy only makes sense with the high threat category and the block user. This requires a P2 license for users accounts.</p>







<p></p>



 Summary 



<p>These are the base policies and further policies can be added depending on the tenant requirements. Some session based controls would normally make sense as well.</p>







<p></p>



 Notes 



<p>The examples of the continuous access policies are shown and set up using the Azure portal. This would be way better as a terraform script and a fully automated set up using something like Azure DevOps or Github actions.</p>



 Links 



<p><a href="https://learn.microsoft.com/en-us/entra/identity/conditional-access/howto-conditional-access-policy-block-legacy">https://learn.microsoft.com/en-us/entra/identity/conditional-access/howto-conditional-access-policy-block-legacy</a></p>



<p><a href="https://learn.microsoft.com/en-us/entra/identity/conditional-access/require-tou">https://learn.microsoft.com/en-us/entra/identity/conditional-access/require-tou</a></p>



<p><a href="https://learn.microsoft.com/en-us/entra/identity/conditional-access/howto-conditional-access-policy-block-legacy">https://learn.microsoft.com/en-us/entra/identity/conditional-access/howto-conditional-access-policy-block-legacy</a></p>



<p><a href="https://learn.microsoft.com/en-us/entra/id-protection/howto-identity-protection-configure-risk-policies">https://learn.microsoft.com/en-us/entra/id-protection/howto-identity-protection-configure-risk-policies</a></p>



<p><a href="https://learn.microsoft.com/en-us/entra/identity/conditional-access/how-to-policy-phish-resistant-admin-mfa">https://learn.microsoft.com/en-us/entra/identity/conditional-access/how-to-policy-phish-resistant-admin-mfa</a></p>



<p><a href="https://learn.microsoft.com/en-us/entra/identity/conditional-access/concept-continuous-access-evaluation">https://learn.microsoft.com/en-us/entra/identity/conditional-access/concept-continuous-access-evaluation</a></p>



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2024/03/admin-settings-mau_01.png?w=1024'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   06:37 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 01. April 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.eventbrite.com/e/851125899477/'>Talk: Postgres Yum Extensions</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  On Wednesday at noon US Eastern / 16:00 UTC., Devrim Gündüz will be giving
a talk: “Overview of the yum.postgresql.org architecture, how new RPMs are
added, and issues and challenges with distributing RPMed extensions.”
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Quick reminder that <a href="https://github.com/devrimgunduz" title="Devrim Gündüz on LinkedIn">Devrim Gündüz</a> of <a href="https://www.enterprisedb.com" title="EDB: Open-Source, Enterprise Postgres Database Management">EnterpriseDB</a> will be giving a talk at
this week’s <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">extension ecosystem mini-summit</a>, an overview of the
<a href="https://yum.postgresql.org" title="PostgreSQL Yum Repository">PostgreSQL Yum Repository</a> architecture, how new RPMs are added, and
issues and challenges with distributing RPMed extensions.</p>
<p>The community <a href="https://yum.postgresql.org" title="PostgreSQL Yum Repository">Yum</a> and <a href="https://wiki.postgresql.org/wiki/Apt" title="The PostgreSQL Wiki: “Apt”">Apt</a> repositories are the canonical distributors of
PostgreSQL community builds on Linux platforms, with a long history of robust
support for multiple OSes, OS versions, and PostgreSQL versions. Devrim will
help us understand how the Yum repository works, the inclusion of extensions
from the broader community, and the challenges for automatic binary packaging
for all publicly-available extensions.</p>
<p><a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">Join us</a>! Note! that if you reserved a spot at a prior
mini-summit, sadly you will need to do so again for each subsequent summit or
miss out on reminders from Eventbrite.</p>
<p>Need other information or just want an invitation without using Eventbrite,
hit me up at <code>david@</code> this domain, <a href="https://xoxo.zone/@theory" title="@theory@xoxo.zone">on Mastodon</a>, or via the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a>
channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a>.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/yum/">Yum</a> 
                 <a href="https://justatheory.com/tags/rpm/">RPM</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:17 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 31. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@rufftimo'>Timothy Ruff</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://rufftimo.medium.com/five-failed-blockchains-why-trade-needs-protocols-not-platforms-d12a77386690?source=rss-11906019170a------2'>Five Failed Blockchains: Why Trade Needs Protocols, Not Platforms</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The trade industry needs a “universal translator” First we.trade failed in mid-2022. Then TradeLens at the end of 2022, then Marco Polo in early 2023, then Contour in late 2023. Of the five major trade-related blockchains only Komgo has survived, but only after ditching blockchain.  The Trade Blockchain Graveyard  Each failed blockchain had enormous support and resources at the&nbsp;outset:  We.tra
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/900/0*fzCUKAVimkFpr00H'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

The trade industry needs a “universal translator” First we.trade failed in mid-2022. Then TradeLens at the end of 2022, then Marco Polo in early 2023, then Contour in late 2023. Of the five major trade-related blockchains only Komgo has survived, but only after ditching blockchain.  The Trade Blockchain Graveyard <p>Each failed blockchain had enormous support and resources at the&nbsp;outset:</p><p><a href="https://www.gtreview.com/news/fintech/we-trade-calls-it-quits-after-running-out-of-cash/">We.trade</a>, launched in 2017 and built in collaboration with IBM on Hyperledger Fabric, boasted Deutsche Bank, HSBC, Santander, Societe Generale and&nbsp;UBS.</p><p><a href="https://www.forbes.com/sites/loracecere/2022/12/05/tradelens-discontinues-operations-why-you-should-care/?sh=24f2fc054cec">TradeLens</a>, launched in 2018 and also built on Fabric, had industry giants IBM and&nbsp;Maersk.</p><p><a href="https://www.ledgerinsights.com/marco-polo-blockchain-trade-finance-insolvency/">Marco Polo</a>, launched 2019 and built on R3’s Corda, had Commerzbank, BNY Mellon and SMBC with financial backing from ING Ventures and BNP&nbsp;Paribas.</p><p><a href="https://www.tradefinanceglobal.com/posts/contour-collapses-what-does-this-mean-for-digital-trade-finance/">Contour</a>, launched in 2020 and also built on Corda, had ANZ, BNP Paribas, HSBC, and Standard Chartered banks along with a strong contingent of trade integration and documentation partners including Finastra, CargoX, Bolero and Surecomp.</p><p><a href="https://www.komgo.io/">Komgo</a>, launched in 2018 on Quorum blockchain infrastructure and dedicated to trade finance, still boasts Citi, ING, and a dozen others partners and has over 100 customers apparently still using the platform, but has since <a href="https://www.spglobal.com/marketintelligence/en/news-insights/latest-news-headlines/trade-finance-industry-remains-hopeful-on-blockchain-despite-failed-projects-72557910">ditched blockchain</a>.</p><p>Despite all the money and power behind them, all major trade blockchains have failed in a surprisingly short period of time. There are several <a href="https://www.tradefinanceglobal.com/posts/12-companies-using-blockchain-to-rewire-trade-and-trade-finance/">other blockchains</a> dedicated to trade, but these five are the big&nbsp;ones.</p> Why Business Blockchains Fail <p>So why all the embarrassing failures? Ledger Insights <a href="https://www.ledgerinsights.com/marco-polo-blockchain-trade-finance-insolvency/">explains</a> the troubling trend this&nbsp;way:</p><blockquote>“In most cases, the issue was a failure to achieve market fit and scale before the money ran out rather than any particular blockchain technology.”</blockquote><p>I agree, blockchain’s technology didn’t cause these failures. Blockchains fail because they are platforms — an assertion I defend in detail below — and as we’ve seen in industry after industry, it’s nearly impossible to get entire industries to join the same platform. Blockchains succeed when they out-compete other blockchains for adoption, no different than centralized platforms.</p><p>What’s needed to enable secure global digital interoperability for trade are <a href="https://knightcolumbia.org/content/protocols-not-platforms-a-technological-approach-to-free-speech">protocols, not platforms</a>, specifically protocols for the secure peer-to-peer exchange of verifiable trade instruments. (&lt;&lt; Read the linked essay, it’s important.) While such protocols aren’t the only thing needed to achieve this elusive objective —standard schema also comes to mind , and is discussed below— protocols are necessary to escape the fatal limitations inherent to blockchains and other platforms.</p> How Blockchains Are Platforms <p>Blockchains inherently follow the same “EUM” pattern of the centralized, proprietary platforms they claim to replace, screaming “Everybody Use Me”, or more accurately, “Everybody use my service”. And though permissionless blockchains like Bitcoin and Ethereum may have decentralized governance — which is more than cool — business blockchains have their governance centralized around a single decision-making body, typically a consortium, that makes all the&nbsp;rules.</p><p>But regardless of whether a blockchain has decentralized or centralized governance, one common, inescapable fatal flaw remains: a blockchain is a single logical database that all counterparties must agree to use; a singular, rent-seeking source of truth that sits in the middle of all interactions just like proprietary platforms do.</p><p>Blockchains are not truly peer-to-peer, despite their claims. The fact that all participants depend upon the same master set of data — along with its bespoke technology, economics, security, and governance — is why there are so many blockchains competing for adoption, and with near-zero interoperability between&nbsp;them.</p><p>The problem isn’t with blockchain technology, it’s the blockchain model&nbsp;itself.</p> Platforms vs. Protocols <p>All blockchains are platforms, and are not protocols. This is not a subjective distinction. There is a simple way to determine whether something is a platform or a protocol: all platforms have one telltale sign in common: a rent-seeking entity or network that sits in the middle of all interactions. A protocol, such as TCP/IP, HTML, SMTP, 802.11, etc., is a language used entirely peer-to-peer, with no rent-seeking entity or network between peers and no master dataset generated or depended&nbsp;upon.</p><p>Being a platform doesn’t guarantee failure, however, as we see with Komgo. The world is full of successful platforms and Komgo is still around because it out-competed other trade blockchains for adoption, but then ditched their blockchain when they realized that blockchain was slowing them down and a traditional centralized database was better. The fact that they could make such a smooth transition away from blockchain illustrates my point: whether using a blockchain or a database under the hood, the platform-style relationship between Komgo and its users was identical, with Komgo sitting in the middle of every interaction and all participating counterparties paying rent. That’s a fine way to make money but an impossible way to make global trade digitally interoperable, because for twenty years we’ve learned and re-learned the same lesson, in industry after industry: it’s nearly impossible to get all necessary counterparties to use the same platform, no matter how powerful its partners are and no matter how much money it&nbsp;has.</p><p>Like all industries that struggle with digital interoperability, trade needs protocols, not platforms. For two decades now the trade industry has endeavored to create an electronic bill of lading to replace the paper one, without success. In my view this lack of success has been because competitive platforms lack the incentive to directly interoperate, seeing it as a competitive sacrifice, and they have no protocol available as an indirect alternative. Traditional proprietary platforms started this EUM, winner-takes-all pattern, then the blockchain consortia came along and promised something different: an exciting new way to break through the paper-to-digital logjam. But these trade blockchains acted no differently and did no better than the traditional platforms, ultimately leaving the industry still dependent on paper and paper facsimiles (PDFs).</p><p>In contrast, an open, non-proprietary protocol designed for trade could bring a new alternative: a common language that even competitive trade counterparties can speak to each other that brings interoperability without sacrificing competitive standing.</p> A “Universal Translator” For&nbsp;Trade <p>This brings us to the image and its caption at the beginning of this article: The trade industry needs a “universal translator”.</p><p>In Star Trek, when wearing a universal translator you can speak to any creature in the galaxy using your preferred language and they’ll hear you in theirs, and when they speak their preferred language you hear it in yours. Now imagine if all creatures in the universe were required to use a single, proprietary rent-seeking platform, network, or dataset in the middle to accomplish this, instead of adopting a common protocol/language… it would never work. The only way to solve such a galactic problem: a common, open protocol that’s adopted everywhere that enables devices to use a common, agreed-upon language with each other while speaking bespoke languages to each&nbsp;wearer.</p><p>That’s what protocols are: languages. In the case of Star Trek’s universal translator, the protocol is the language spoken between the devices; it must be free, open, and separate from the language spoken between the device and its wearer. Protocols don’t generate shared datasets like blockchains do and they don’t have rent-seeking actors in the middle. Protocols are like SMTP that enabled email senders and recipients to each build or buy their own favored email clients, rather than having to use a common platform like AOL, Prodigy, or CompuServe to exchange messages. Languages can be spoken peer-to-peer between any two parties without a third-party between them, so they are inherently decentralized.</p> Digital Containers &amp;&nbsp;Payloads <p>It’s tempting to think of a data schema as a protocol, and in a sense it can be: for two parties to have semantic interoperability — to understand each other and be able to ingest and utilize received data from each other — there must be some degree of agreement about data schema. But the same is true with paper, which depends on common data elements if not exact schema. For digital environments, standardized schema for trade is being worked out by the <a href="https://www.dsi.iccwbo.org/">ICC DSI</a> and others, but… even ubiquitously agreed-up on data schema does not address the need for securely exchanging the schematized data; that part is still missing, it’s like agreeing on the format of physical mail without considering the need for a mail carrier. In the digital realm that ‘carrier’ must have verifiable security from origination to delivery. Without security, common schema is pointless; without common schema, security is&nbsp;useless.</p><p>To be crystal clear on this critical point: even when a data schema has been widely agreed to, without a protocol — a common language — for exchanging data securely, there’s no way to verify the source of the data and that it hasn’t been tampered with, revoked, or expired. This security element was supposed to be supplied by blockchains — which are quite secure, though they don’t address critical identity or key management issues — but their platform-like attributes prevented broad adoption for the reasons listed above, leaving trade back where it started: paper and paper-like facsimiles (PDFs).</p><p>What’s needed are protocols for secure data exchange that act like digital ships and containers, with schematized data as the payload. Imagine a trading world where all data is received in the form of a standardized schema that arrived in a secure data container that is instantly verifiable in three critical&nbsp;ways:</p>  The identity of the originators of the data, and of all who contributed to or endorsed&nbsp;it;  It hasn’t been tampered with, revoked, or expired since origination;  It doesn’t require subscribing to any particular blockchain, network, or proprietary platform.  <p>Having secure, globally verifiable data containers with schematized data as the payload is when the global trading game really changes. And it requires protocols, not platforms, to eliminate competitive sacrifices, maximize adoption, and have limitless scale.</p> In Conclusion <p>This is something we are working on at Digital Trust Ventures: free, open protocols for trade that work like digital ships and containers and result in a “universal translator”-like effect for counterparties, enabling each to speak and hear their chosen “language”. And like physical containers, these digital containers will have no opinion about the payload inside; the job of these protocols is to ensure that exchanged data is verifiably not tampered with, revoked, or expired, without need of platforms, networks, other entities in the&nbsp;middle.</p><p>More to come, later this year. Stay&nbsp;tuned.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/900/0*fzCUKAVimkFpr00H'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   06:02 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 29. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://aaronparecki.com'>Aaron Parecki</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://aaronparecki.com/2024/03/29/3/oauth-terminology'>OAuth: "grant" vs "flow" vs "grant type"</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Is it called an OAuth "grant" or a "flow"? What about "grant type"?
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

Is it called an OAuth "grant" or a "flow"? What about "grant type"?
<p>These are common questions when writing documentation for OAuth-related things. While these terms are all used in <a href="https://datatracker.ietf.org/doc/html/rfc6749">RFC 6749</a>&nbsp;and <a href="https://oauth.net/2/">many extensions</a>, the differences between the terminology is never actually explained.</p>
<p>I wanted to finally write down a definition of the terms, along with examples of when each is appropriate.</p>
 
   
    flow - use "flow" when referring to the end-to-end process, for example:
     
       "the client initiates the flow by..." 
       "the flow ends with the successful issuance of an access token" 
       This can also be combined with the type of flow, for example: 
       "The Authorization Code flow starts by..." 
     
   
   
    grant - use "grant" when referring to the specific POST request to the token endpoint, for example:
     
       "The authorization code grant includes the PKCE code verifier..." 
       "The refresh token grant can be used with or without client authentication..." 
       "Grant" also refers to the abstract concept of the user having granted authorization, which is expressed as the authorization code, or implicitly with the client credentials grant. This is a bit of an academic definition of the term, and is used much less frequently in normal conversation around OAuth. 
     
   
   
    grant type - use "grant type" when referring to the definition of the flow in the spec itself, for example:
     
       "there are several drawbacks to the Implicit grant type" 
       "the Authorization Code grant type enables the use of..." 
     
   
 
<p>Let me know if you have any suggestions for clarifying any of this, or any other helpful examples to add! I'm planning on adding this summary to OAuth 2.1 so that we have a formal reference for it in the future!</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   15:15 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 28. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/03/pgxn-language-choices/'>PGXN v2: Go or Rust?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  What programming language(s) should we use to build new and revamp existing
PGXN services and tools: Rust or Go? Vote your preference!
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>TL;DR: I’d like Postgres community input on a decision: Should we build PGXN
v2 services and tools in Go or Rust? Context for the question and some
weighing of options constitutes the rest of this post, but to skip to the end,
<a href="https://dev.star.vote/Election/tqkv3v" title="Poll: What language should PGXN v2 tools and services be written in?">🗳️ Vote</a> your choice! Poll closes April 12 at the end of the day (midnight)
New York time.</p>
 
<p>The <a href="https://github.com/orgs/pgxn/projects/1/views/1">PGXN v2 project</a> now under way requires developing or updating several
services and tools, including:</p>
 
 A <a href="https://github.com/pgxn/planning/issues/8" title="PGXN v2 Planning: #8: Implement Root Registry">root registry</a> for source distribution 
 A <a href="https://github.com/pgxn/planning/issues/11" title="PGXN v2 Planning: #11: Implement Packaging Registry">package registry</a> for binary distribution 
 A <a href="https://github.com/pgxn/planning/labels/client" title="PGXN v2 Planning: #cli">command line client</a> for developing, building, and distributing
extension packages 
 An <a href="https://github.com/pgxn/planning/issues/20" title="PGXN v2 Planning: #20: Implement Interactions Service">interactions service</a> for notifications and stats aggregation 
 
<p>And more. Historically, the PGXN tools were written in <a href="https://www.perl.org" title="The Perl Programming Language">Perl</a>, which was an
ideal choice for me back in 2011, and quite familiar to many members of the
core team, but also deeply foreign to most everyone else. Furthermore, its
dynamic nature and need for dozens of dependencies in most apps makes
installation and packaging a challenge, to say the least.<a>1</a></p>
<p>In the last ten years I’ve become quite proficient in <a href="https://go.dev" title="The Go Programming Language">Go</a>. I appreciate its
near system-level performance, memory safety, robust standard library,
concurrency design, and short learning curve — especially for web services.
But perhaps most eye-opening to me, as a long-time user of dynamic languages,
is that, like C, Go compiles an application into a single static binary. Not
only that, but Go provides <a href="https://en.wikipedia.org/wiki/Cross_compiler" title="Wikipedia: “Cross compiler”">cross compilation</a> natively. This makes
distribution incredibly simple.</p>
 Distribution Digression 
<p>Compare, for example, <a href="https://github.com/sqitchers/docker-sqitch/blob/main/Dockerfile">the <code>Dockerfile</code></a> for <a href="https://sqitch.org" title="Sqitch: the sensible database change management system">Sqitch</a>, the database change
management system I wrote and maintain in Perl. It’s…a lot. Sure there are
a ton of system dependencies, but what’s invisible in this file is the weeks
of work that went into <a href="https://github.com/sqitchers/sqitch/blob/develop/inc/Module/Build/Sqitch.pm">Module::Build::Sqitch</a>, which performs a bunch of
tricks to build the Sqitch “app” as a single directory with all of its Perl
dependencies. Don’t get me wrong, the work was worth it for Sqitch, and powers
the <a href="https://github.com/sqitchers/homebrew-sqitch/blob/main/Formula/sqitch.rb">Homebrew formula</a>, as well. But even there, I’ve not been able to get
Sqitch into the <a href="https://github.com/Homebrew/homebrew-core/pull/129128" title="homebrew-core#129128 sqitch 1.3.1 (new formula)">Homebrew core</a> because every single dependency requires a
checksum, and I’ve not had the time (or energy) to figure out how to generate
them.</p>
<p>Contrast with this <code>Dockerfile</code> for a Go service compiled into a binary named
<code>thinko</code>:</p>
  <code>FROM gcr.io/distroless/base-debian12:latest

# TARGETOS and TARGETARCH: https://docs.docker.com/build/guide/multi-platform/
ARG TARGETOS
ARG TARGETARCH

COPY "_build/${TARGETOS}-${TARGETARCH}/thinko" /thinko/bin/
USER nonroot:nonroot
ENTRYPOINT [ "/thinko/bin/thinko" ]
</code>  <p>That’s the whole thing. There are no dependencies at all, aside from
a few included in <a href="https://github.com/GoogleContainerTools/distroless" title="distroless: 🥑 Language focused docker images, minus the operating system">distroless</a> image. And where does that image come from?
This is the relevant from the project <code>Makefile</code>:</p>
  <code>.PHONY: all # Build all binaries
all: local linux darwin windows freebsd

linux: thinko-linux
darwin: thinko-darwin
windows: thinko-windows
freebsd: thinko-freebsd

thinko-linux: _build/linux-amd64/thinko _build/linux-arm64/thinko
thinko-darwin: _build/darwin-amd64/thinko _build/darwin-arm64/thinko
thinko-windows: _build/windows-amd64/thinko _build/windows-arm64/thinko
thinko-freebsd: _build/freebsd-amd64/thinko _build/freebsd-arm64/thinko

# Build Thinko for specific platform
_build/%/thinko: cmd/thinko
	GOOS=$(word 1,$(subst -, ,$*)) GOARCH=$(word 2,$(subst -, ,$*)) $(GO) build -o $@ ./$&lt;
</code>  <p>This configuration allows me to build <code>thinko</code> for every OS and architecture
at once:</p>
  <code>$ make thinko
go build -o _build/local/thinko ./cmd/thinko
GOOS=linux GOARCH=amd64 go build -o _build/linux-amd64/thinko ./cmd/thinko
GOOS=linux GOARCH=arm64 go build -o _build/linux-arm64/thinko ./cmd/thinko
GOOS=darwin GOARCH=amd64 go build -o _build/darwin-amd64/thinko ./cmd/thinko
GOOS=darwin GOARCH=arm64 go build -o _build/darwin-arm64/thinko ./cmd/thinko
GOOS=windows GOARCH=amd64 go build -o _build/windows-amd64/thinko ./cmd/thinko
GOOS=windows GOARCH=arm64 go build -o _build/windows-arm64/thinko ./cmd/thinko
GOOS=freebsd GOARCH=amd64 go build -o _build/freebsd-amd64/thinko ./cmd/thinko
GOOS=freebsd GOARCH=arm64 go build -o _build/freebsd-arm64/thinko ./cmd/thinko
</code>  <p>Those first two commands build <code>thinko</code> for Linux on amd64 and arm64, right
where the <code>Dockerfile</code> expects them. Building then is easy; a separate <code>make</code>
target runs the equivalent of:</p>
  <code>$ docker buildx build --platform linux/arm64 -f dist/Dockerfile .
$ docker buildx build --platform linux/amd64 -f dist/Dockerfile .
</code>  <p>The <code>--platform</code> flag sets the <code>TARGETOS</code> and <code>TARGETARCH</code> arguments in the
<code>Dockerfile</code>, and because the directories into which each binary were compiled
have these same terms, the binary compiled for the right OS and architecture
can be copied right in.</p>
<p>And that’s it, it’s ready to ship! No mucking with dependencies, tweaking
system issues, removing unneeded stuff from the image. It’s just the bare
minimum.</p>
<p>This pattern works not just for Docker images, of course. See, for example,
how [Hugo], the Go blog generator, <a href="https://github.com/gohugoio/hugo/releases" title="Hugo Releases">releases</a> tarballs for a bunch of OSes and
architectures, each containing nothing more than a <code>README.md</code>, <code>LICENSE.md</code>,
and the <code>hugo</code> binary itself. This pattern allows both the <a href="https://github.com/Homebrew/homebrew-core/blob/8fb177a/Formula/h/hugo.rb">Hugo Homebrew
formula</a> and its <a href="https://github.com/hugomods/docker/blob/main/docker/hugo/snippets/common" title="Hugo base Dockerfile"><code>Dockerfile</code></a> to be incredibly simple.</p>
 Back to PGXN 
<p>I very much want these advantages for the next generation of PGXN tools. Not
only the services, but also the command-line client, which would become very
easy to distribute to a wide variety of platforms with minimal effort.</p>
<p>But there are other variables to weigh in the choice of language for the PGXN
servers and tools, including:</p>
 
 
<p>Familiarity to other developers: Ideally someone can quickly
contribute to a project because they’re familiar with the language, or
there’s a short learning curve.</p>
 
 
<p>Safety from common issues and vulnerabilities such as buffer overflows,
and dangling pointers.</p>
 
 
<p>Tooling for robust and integrated development, including dependency
management, testing, distribution, and of course cross-compilation.</p>
 
 
 Decisions, Decisions 
<p>In my experience, there are two language that fulfill these requirements
very well:</p>
 
 🐿️ <a href="https://go.dev" title="The Go Programming Language">Go</a><a>2</a> 
 🦀 <a href="https://www.rust-lang.org" title="Rust Programming Language">Rust</a> 
 
<p>Which should we use? Some relevant notes:</p>
 
 
<p>I expect to do the bulk of the initial development on PGXN v2, as the only
person currently dedicated full time to the project, and I’m most familiar
with Go — indeed I enjoy writing web services and CLIs in Go!. I’d
therefore be able go ship Go tools more quickly.</p>
 
 
<p>But I’ve played around with Rust a number of times over the years, and
very much would like to learn more. Its syntax and long feature list
steepen the learning curve, but given my background in Perl — another
language with unique syntax and context-sensitive features — I’m certain
I could become incredibly proficient in Rust after a few months.</p>
 
 
<p>My employer, <a href="https://tembo.io/" title="Tembo: Goodbye Database Sprawl, Hello Postgres">Tembo</a>, is a Rust shop, and we’ll likely borrow heavily from
the <a href="https://github.com/tembo-io/trunk" title="trunk: Package manager and registry for Postgres extensions">trunk</a> project, especially for the CLI and binary registry. It would
also be easier for my coworkers to contribute.</p>
 
 
<p><a href="https://github.com/pgcentralfoundation/pgrx" title="pgrx: Build Postgres Extensions with Rust!">pgrx</a>, the tooling to build Postgres extensions in Rust, has taken the
community by storm, rapidly building familiarity with the language among
extensions developers. Perhaps some of those developers would also be
willing to turn their expertise to PGXN Rust contributions, as well.
It’s likely some features could be borrowed, as well.</p>
 
 
<p>Sadly, the <a href="https://gitlab.com/microo8/plgo" title="plgo: easily create postgresql extensions in golang">plgo</a> project appears to have stalled, so has not built
up the same community momentum.</p>
 
 
<p>This leaves me torn! But it’s time to start coding, so it’s also time to
make some decisions. Should PGXN v2 services and tool be:</p>
 
 🐿️ Written in Go 
 🦀 Written in Rust 
 🐿️ + 🦀 Some of each (e.g., Go for web services and Rust for CLIs) 
 
<p>What do you think? If you were to contribute to PGXN, what language would
you like to work in? Do you think one language or the other would be more
compatible with community direction or core development?<a>3</a></p>
<p>Got an opinion? <a href="https://dev.star.vote/Election/tqkv3v" title="Poll: What language should PGXN v2 tools and services be written in?">🗳️ Vote</a>! Poll closes April 12 at the end of the day (midnight)
New York time.</p>
<p>And if those choices aren’t enough for you, please come yell at me <a href="https://xoxo.zone/@theory" title="@theory@xoxo.zone">on
Mastodon</a>, or via the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a>. Thanks!</p>
 
 
 
 
<p>Ever wonder why PGXN isn’t hosted by community servers? It’s
because I screwed up the installation trying to balance all the
dependencies without wiping out Perl modules the systems depend on. 🤦🏻‍♂️&nbsp;<a>↩︎</a></p>
 
 
<p>Pity there’s no gopher emoji yet.&nbsp;<a>↩︎</a></p>
 
 
<p>I can imagine a future where an extension CLI was included in core.&nbsp;<a>↩︎</a></p>
 
 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/go/">Go</a> 
                 <a href="https://justatheory.com/tags/rust/">Rust</a> 
                 <a href="https://justatheory.com/tags/perl/">Perl</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   20:14 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://benwerd.medium.com/an-intrusive-thought-about-trump-2024-7ecc2c0aa257?source=rss-3b16402f5b9d------2'>An intrusive thought about Trump 2024</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

      What if the worst happens?  Continue reading on Medium »  
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/0*leKpUaM2g8O8kKMd.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   <p><a href="https://benwerd.medium.com/an-intrusive-thought-about-trump-2024-7ecc2c0aa257?source=rss-3b16402f5b9d------2"></a></p><p>What if the worst happens?</p><p><a href="https://benwerd.medium.com/an-intrusive-thought-about-trump-2024-7ecc2c0aa257?source=rss-3b16402f5b9d------2">Continue reading on Medium »</a></p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/0*leKpUaM2g8O8kKMd.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:11 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-87-stripes-investment-in'>The Pulse #87: Stripe’s investment in reliability, by the numbers</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The Fintech giant spends more on running test suites than Agoda does for all its infra. Plus, why taking out a loan for equity can backfire, and why did Donald Trump’s social media company use a SPAC?
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering insights, patterns, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Today, we cover:</p>  <p>Industry pulse. The EU’s AI regulation; Reddit’s successful IPO; No raises at Amazon, and more.</p>  <p>Stripe’s investment in reliability, by the numbers. Stripe uses more hardware just to run their test suite (500,000 CPU cores) than travel booking platform Agoda’s complete infrastructure footprint. And more interesting details straight from the Fintech giant.</p>  <p>SPACs and Truth Social. 2020-2021 saw a boom in tech companies sidestepping the IPO process to get listed on public markets by merging with a “shell company.” Donald Trump’s company followed this exact same path: and now the company is worth almost as much as Reddit, despite having less than 1% of Reddit’s revenue.</p>  <p>Loans to exercise Bolt options early was a terrible idea. The cofounder and former CEO of the one-click-checkout offered loans to employees for the early exercising of stock options in the company. He touted it as the “most employee-friendly options program possible,” and convinced more than half of staff to sign up. He ignored warnings about the risks of this from peers – who were right.</p>   1. Industry pulse 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-87-stripes-investment-in">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:51 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2024/03/5701/'>vLEI関連企画である KERI、ACDC、CESRの3つ共に4月20日までのパブリックレビューにかかっています</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Trust Over IP Foundation (ToIP) で、KERI, ACDC, CESRの３つの仕様がパブリックレビューにかかっています。GLEIFが主導している vLEI (Verifiable Legal Entity Identifier) 関連の規格です。締め…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Trust Over IP Foundation (ToIP) で、KERI, ACDC, CESRの３つの仕様がパブリックレビューにかかっています。<a href="https://www.gleif.org/ja">GLEIF</a>が主導している <a href="https://www.gleif.org/ja/vlei/introducing-the-verifiable-lei-vlei/">vLEI</a> (Verifiable Legal Entity Identifier) 関連の規格です。締め切りは4月20日です。</p>



<p><a href="https://trustoverip.org/blog/2024/03/21/authentic-chained-data-containers-acdc-task-force-announces-public-review/">AUTHENTIC CHAINED DATA CONTAINERS (ACDC) TASK FORCE ANNOUNCES PUBLIC REVIEW</a></p>



 
 <a href="https://trustoverip.github.io/tswg-keri-specification/">Key Event Receipt Infrastructure (KERI) specification</a> 



 <a href="https://trustoverip.github.io/tswg-acdc-specification/">Authentic Chained Data Containers specification</a> (ACDC) 



 <a href="https://trustoverip.github.io/tswg-cesr-specification/">Composable Event Streaming Representation specification</a> (CESR) 
 



<p>ToIP曰く</p>



<p>この仕様一式は、識別子、「クレデンシャル」（脚注参照）、および認証の真に分散化された、真正かつ検証可能なエコシステムを構築するための青写真を提供します。</p>



<p>この仕様には、一連のユニークで革新的な機能が記述されています：</p>



 
 鍵の事前ローテーションによる、真に無制限な期間識別子の実現； 



 暗号化ルート・オブ・トラスト； 



 連鎖した「クレデンシャル」［脚注参照］と、完全に検証可能な所有者証明および作者証明； 



 妥協のないスケーラビリティのためにルックアヘッド・ストリーミングをサポートする独自の特性を備えた、テキストとバイナリの両方の表現に等しく最適化されたシリアライゼーション・フォーマット。 
 



<p>この仕様スイートには、Out-Of-Band Introduction、自己アドレス識別子、および組織アイデンティティの包括的なソリューションを提供するために必要な署名付きコンテナの画期的な「パス署名」アプローチなどのサブ仕様が追加されています。</p>



<p>（出所）<a href="https://trustoverip.org/blog/2024/03/21/authentic-chained-data-containers-acdc-task-force-announces-public-review/">ToIP</a></p>



<p>GLEIFの主導しているvLEIではこの仕様が使われているということで、影響が大きくなることが予想される仕様です 。一時はIETFで規格化するという話でしたが、諸般の事情があり、ToIPで規格化することになったようです。ToIPにとっては初めての規格化になるようです。</p>



<p>残念ながらわたしは読む時間はなさそうですが、暗号関係者の方々にはぜひ読んで頂いてコメントをしていただければと思います。</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   00:00 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 27. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/web-20-is-collapsing-under-its-own'>Web 2.0 is Collapsing Under its Own Weight</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The overhead of performing even simple tasks online is getting larger and larger. I question the security of almost all these supposedly "secure" messaging systems. And I'm tired of the 'Utopia of Rules' mindset pervasive in every organization. It's exhausting how they expect customers to constantly adapt to their needs.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The overhead of performing even simple tasks online is getting larger and larger. I question the security of almost all these supposedly "secure" messaging systems. And I'm tired of the 'Utopia of Rules' mindset pervasive in every organization. It's exhausting how they expect customers to constantly adapt to their needs.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ef02110-581b-4474-a3d8-cb9dc177d43b.heic">    </a> <p>I don't know if you recall the game Kerplunk. It's a classic children's game that has been around for decades. I remember playing it with my sister. The basic setup involves a transparent plastic tube, a number of sticks, and marbles. The sticks are threaded through the tube to form a web or nest at the bottom on which the marbles rest. We'd take turns removing a stick at a time, trying not to let any marbles fall through the web and out of the tube. At some point, the remaining sticks can't hold the marbles and everything falls down.</p><p><a href="https://www.technometria.com/p/web-20-is-collapsing-under-its-own?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share">Share</a></p><p>The modern web reminds me more and more of a big Kerplunk game and I think the marbles are about to fall. What started out as an easier way to do things like shop, bank, and get health care information has become increasingly complex over time. More and more of the email I receive seems to be simply directing me to log into some bespoke system to retrieve a message or engage in some workflow. And even with a password manager, the act of logging in is often a chore with different user interfaces, custom MFA requirements, and weird rules for passwords. Once you're on the system, session time-outs induce their own form of anxiety since stepping away for a few minutes to attend to something else might require going through the whole Kafkaesque process all over again. The modern web has turned into a dystopian theater of the absurd where even reading a simple appointment reminder from your doctor requires several minutes of stress-inducing interaction with baroque systems and processes.</p><p>And it's not just doctors, of course, banks, government agencies, hospitals, ecommerce sites, and customer service systems all adopt these special purpose messaging systems. If you ask these organizations why they use bespoke messaging systems, they'll list things like "timely and improved communication," "convenience," and "privacy and security." But the real reason is that it's more convenient for them because these systems are integrated with their backends and make their processes more manageable. There's certainly nothing about them that's more convenient, timely, or better than email for their customers1.</p><p>I also question the privacy and security premise. Email can be insecure. And your email provider can see the contents of your emails. But the messaging system run by your doctor or bank is likely less secure than the email systems run by Apple, Google, and the others. And achieving privacy by making everything incompatible so that you have to use a different system for each correspondent is like chopping off your finger to prevent hangnails.</p><p>How did we get here? Bureaucracy. Not just government bureaucracy, but bureaucracy of all kinds. In <a href="http://www.amazon.com/exec/obidos/ASIN/B00MKZ0QZ2/technometria-20">Utopia of Rules</a>2, David Graeber talks about how power imbalances force the less powerful group to perform what he calls interpretive labor, the work of understanding and implementing what's better or more convenient for the more powerful partner. People are not equal participants in online interactions. We don't have the tools to be fully embodied online3. Because of this we are forced to play by the rules organizations online who are digitally embodied with servers, identity systems, customer management systems, and so on. And part of that is being forced to use their inconvenient and anemic messaging systems.</p><p>What's the answer? People need tools. I think digital wallets (a bad name for an important tool), <a href="https://www.windley.com/archives/2020/09/the_architecture_of_identity_systems.shtml">autonomic (peer) identifiers</a> with strong cryptography, and verifiable credentials are a huge step forward. These tools provide the means for people to be peers online rather that mere ghosts in someone else's machine. That's why I insist on using the term self-sovereign rather than decentralized to describe these systems. <a href="https://www.windley.com/archives/2020/08/cogito_ergo_sum.shtml">Cogito Ergo Sum</a>.</p><p><a href="https://www.technometria.com/subscribe?">Subscribe now</a></p> Notes   <p>For a deeper dive into why one-off messaging systems are never as good as email, see <a href="https://www.windley.com/archives/2012/05/rich_sharing_and_personal_channels.shtml">Rich Sharing and Personal Channels</a>. Email and other useful messaging systems exhibit a property called rich sharing that makes them much more robust that the simple idea of "sharing a message" would bring to mind.</p>  <p>If you're interested in power imbalances and how they come about, I can't recommend Graeber's book highly enough. He had such a keen understanding of this problem and wrote about it in a way that's both informative and entertaining.</p>  <p>I talk about this in more detail in Chapter 17 of <a href="http://www.amazon.com/exec/obidos/ASIN/1098117697/technometria-20">Learning Digital Identity</a> when I discuss <a href="https://www.windley.com/archives/2020/08/authentic_digital_relationships.shtml">authentic digital relationships</a>.</p>     <p>Photo Credit: <a>Playing Kerplunk</a> from DALL-E (public domain) Prompt: Draw a picture of a boy and girl playing kerplunk that's 1200x500 pixels</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ef02110-581b-4474-a3d8-cb9dc177d43b.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   13:43 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2024/03/5691/'>Googleマイアドセンターを見て自分がどう認識されているか見てみよう〜ちなみにわたしは18〜34歳独身女性になってたw</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Googleマイアドセンターは、Googleがユーザーのオンライン行動から収集した情報を基に、ユーザーのプロフィールを推定し、それを本人に開示するためのプラットフォームです。主な特徴は以下の通りです。 マイアドセンターは、Googleがユーザーをどのように理解しているかを本人に開…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://myadcenter.google.com/controls?hl=ja">Googleマイアドセンター</a>は、Googleがユーザーのオンライン行動から収集した情報を基に、ユーザーのプロフィールを推定し、それを本人に開示するためのプラットフォームです。主な特徴は以下の通りです。</p>



 
 興味関心カテゴリ: Googleがユーザーのウェブ閲覧履歴などから推定した、ユーザーの興味関心のあるカテゴリを表示します。 



 人口統計情報: 年齢層や性別など、Googleが推定したユーザーの人口統計情報を確認できます。 



 広告設定: ユーザーは自分のプロフィール情報に基づいたパーソナライズド広告の表示を管理・オプトアウトできます。 



 情報の透明性: Googleがユーザーをどのように認識しているかを本人が確認できるため、情報収集の透明性が高まります。 



 プライバシー管理: ユーザーはマイアドセンターから、Googleによる情報収集・利用に関するプライバシー設定を変更できます。 
 



<p>マイアドセンターは、Googleがユーザーをどのように理解しているかを本人に開示することで、情報の透明性を高め、ユーザーのプライバシー管理を促進するためのツールです。ユーザーは自分のプロフィールを確認し、必要に応じて設定を変更することで、パーソナライズド広告などのサービスを自分の好みに合わせて管理できます。</p>



<p>というわけで、早速行って、Googleが膨大な情報をつかってわたしをどのように認識しているかを見てみましょう！<a href="https://myadcenter.google.com/controls?hl=ja">みなさんもご一緒に</a>。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/03/image-40.png?ssl=1"></a>（図表１）Googleはわたしを女性、18〜34歳、独身 etc とみなしているようだ（出所）Google



<p>どうやら、Googleさんは私のことを18〜34歳女性、独身、大企業所属と認識しているようですw。このくらいの女性って、プライバシーを守るために画面の右上に出ているような老人男性のアバターを使う傾向があるのでしょうか…。なお、正解はアラ還暦既婚男性中小企業所属です。<a href="https://www.sakimura.org/2024/03/5691/#easy-footnote-bottom-1-5691" title="Googleの肩をちょっと持つと、娘と一緒に旅行の検索とかしてたからそういうのが影響してるかも。娘のプロファイルに合わせると、大当たりなんで。アンドロイドの方はパーソナライズド広告オフにしてるし、Youtubeは別のChromeプロファイルでやってるし。">1</a></p>



<p>みなさんはどう出ましたか？面白いのが出たら、ぜひコメントに書き込んでください。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/03/image-41.png?ssl=1"></a>（図表２）推測に使われているアクティビティ（出所）Google



 追伸 



<p>ちなみに、Googleがユーザーの興味関心カテゴリを推定する際には、主に以下の情報源が使われているとのことです。</p>



 
 検索履歴: ユーザーがGoogleで検索したキーワードやフレーズから、関心のあるトピックを推定します。 



 ウェブ閲覧履歴: Googleアナリティクスを導入しているサイトを訪問した際の閲覧履歴から、ユーザーの興味関心を推測します。 



 YouTubeの視聴履歴: ユーザーがYouTubeで視聴した動画の内容から、関心のあるカテゴリを割り出します。 



 Googleアカウントの情報: ユーザーがGoogleアカウントに登録した情報（年齢、性別、職業など）も、興味関心の推定に活用されます。 



 Androidデバイスの使用履歴: Androidスマートフォンやタブレットを使用している場合、インストールしているアプリやアプリの利用状況から興味関心を推測します。 



 位置情報: ユーザーの位置情報から、よく訪れる場所やその場所に関連するカテゴリを推定します。 
 



<p>これらの情報を機械学習アルゴリズムで分析することで、Googleはユーザーの興味関心カテゴリを自動的に推定しています。ただし、ユーザーがプライバシー設定でこれらの情報の収集を制限している場合は、推定の精度が下がる可能性があります。</p>



<p>また、興味関心カテゴリは定期的に更新されるため、ユーザーの関心の変化に合わせてカテゴリも変化していきます。</p>



<p>しかし、行動からのプロファイリングってうまくいかないものなんですね。わたしの場合はまぁ良いですが、こうした間違ったプロファイリングはプライバシー上の被害を産むこともありますから、きちんとチェックしたいものです。　</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/03/image-40.png?resize=546%2C485&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   13:42 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 26. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/thriving-as-a-founding-engineer'>Thriving as a Founding Engineer: Lessons from the Trenches</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Being a founding engineer at an early-stage startup is a vastly different, broader role than many people think. Lessons from “serial” early-stage and founding engineer Apurva Chitnis.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>👋 Hi, this is Gergely with a subscriber-only issue of the Pragmatic Engineer Newsletter. In every issue, I cover challenges at Big Tech and startups through the lens of engineering managers and senior engineers. To get articles like this in your inbox, every week, subscribe: </p><p><a href="https://newsletter.pragmaticengineer.com/subscribe?">Subscribe now</a></p><blockquote> Q: “I’m a software engineer joining an early-stage startup as one of the first engineers. What’s my job, how should I build the product, and how can I help my startup succeed?” </blockquote><p>It’s common for early-stage startups to hire “founding engineers,” who are so called because they’re among the very first recruits. A couple of distinguishing features of the role are generous equity packages (between 0.1 and 1% of company stock), and the fact that expectations are frequently broader than typical developer roles. <a href="https://www.linkedin.com/in/apuchitnis/">Apurva Chitnis</a> is the ideal person to tell us what makes being a founding engineer unique, and to answer questions in the kind of detail that only comes from being on the ground, doing it. And Apurva certainly has done it, almost on repeat:</p>  <p>An early engineering hire at <a href="https://www.improbable.io/">Improbable</a>, a metaverse virtual worlds startup. Among the first 50 hires; stayed for 4 years, until the company reached 650 headcount.</p>  <p>Founded Sidetrack, assuming the founding engineer and CTO roles. Sidetrack was an early-stage company focused on reducing cloud costs.</p>  <p>Joined as a founding engineer at <a href="https://koodos.com/">Koodos</a> (and later became their CTO), building an app called <a href="https://www.shelf.im/">Shelf</a>. Shelf connects users with the media which they and their friends love (check out Apurva’s shelf <a href="http://shelf.im/apu">here</a>). His team currently consists of three founding engineers, and he remains hands-on.</p>  <p>In this issue, Apurva covers:</p>  <p>The goal of a founding engineer. Find product-market-fit (PMF) before the money runs out. It’s a race against time.</p>  <p>Approaches for success. Choose engineering approaches based on how much uncertainty exists, balance scope/quality/timeline, and more.</p>  <p>Tech debt. Startups can use tech debt to achieve a PMF quicker, in ways which large companies don’t. Tech debt can be an advantage early on, so use it!</p>  <p>Talking to customers and end-users. Spend some time away from the code, interacting with customers and end-users directly.</p>  <p>Process problems are features at startups, not bugs! Expect plenty of things to break, and fixing them to not be a priority. This may be purposeful, and you may encounter extreme cases, like a successful startup with no version control.</p>  <p>Why join an early-stage startup. Impact, learning, seeing all parts of the business, and more.</p>  <p>Why avoid early-stage startups. Long hours, compensation, and uncertainty are challenges.</p>  <p>Finally, in the takeaways we’ll go over how Uber was almost shut down a few months after hiring its first mobile engineer, <a href="https://www.linkedin.com/in/jordanbonnet/details/experience/">Jordan Bonnet</a>. He and I worked together for 2 years, including during the ride-hailing app’s epic <a href="https://blog.pragmaticengineer.com/uber-app-rewrite-yolo/">“YOLO” rewrite</a>. Jordan happens to be available for hire, so if you're looking to build a world-class mobile experience, then <a href="https://www.linkedin.com/in/jordanbonnet/details/experience/">reach out to him</a>!</p><p>Relatedly, also check out <a href="https://newsletter.pragmaticengineer.com/i/137484781/from-the-trenches-working-at-a-startup-vs-in-big-tech">From the trenches: working at a startup vs in a Big Tech</a>. To keep up with Apurva’s writing, <a href="https://technicallythinking.substack.com/">subscribe to his newsletter</a>.&nbsp;</p><p>With that, it’s over to Apurva.</p> 1. The goal of a founding engineer <p>Before discussing how to work at a startup as a founding engineer, let’s first define what a startup is. So, a short story.</p><p>During my career, I’ve attended hundreds of all-hands meetings, when the whole team comes together for important company events, like introducing new recruits, strategy changes, and to share updates and successes. Among them all, one at Improbable has stuck with me.</p><p>The chief of staff sketched out our financials in the all-hands; plotting costs, revenue, and predicted revenue growth. He combined them to calculate how our bank balance would change over the next year. This led to a painful realization: our startup was going to run out of money and go bankrupt – unless things drastically changed.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1971fe3f-ce57-4017-be1a-84dd9771ebd6_1580x826.png">    </a>A diagram like Improbable’s chief of staff drew up, showing profit was only possible if revenue shot up <p>This was a bold call to action for us all, to focus all our energy on projects that increased product usage and therefore revenue. Everything else had to be ignored!</p><p>Here is a different way to look at the same idea, from the point of view of how much investment and eventually profitable company needs to “burn” at first:</p> The race to product-market-fit <p>All startups begin as “default dead,” meaning their starting trajectory inevitably leads to bankruptcy. According to <a href="https://startupgenome.com/article/the-state-of-the-global-startup-economy">a 2022 research by Startup Genome</a>, 90% of startups fail and shut down. This is a truth you need to acknowledge and lean into: that if things don’t change, the business will fail.</p><p>However, there is a proven path to avoid this outcome:</p>  <p>Build something people want to use; aka finding <a href="https://brianbalfour.com/essays/market-product-fit">product-market-fit</a> (PMF)</p>  <p>Grow user base, also called finding <a href="https://brianbalfour.com/essays/product-channel-fit-for-growth">product-channel fit</a></p>  <p>Monetize usage and create a profitable business by following the steps above</p>  <p>A software engineer’s responsibilities evolve with the company; from founding, to an exit via an IPO or acquisition, hopefully.</p> A founding engineer’s goal: find product-market fit! <p>As an engineer, your goal is to help your team achieve its goals. As a founding engineer, your team is the company, so the first priority is to build something people want. This is the product-market fit. Without PMF, companies struggle to grow, retain users, raise funding, and become profitable. If no PMF is identified, startups end badly.</p><p>Your goal is to find this all-important PMF, and don’t let anyone tell you otherwise. This constraint affects all aspects of a founding engineer’s work:</p>  <p>Technologies used. Be pragmatic. This generally means using standard, ‘boring’ technologies you're familiar and proficient with, instead of shiny, unproven technologies.</p>  <p>Managing technical risks. Do this while balancing short and long-term priorities. Using your knowledge of the product and tech to pragmatically balance risk, short term features, and longer term enablers.</p>  <p>Getting user feedback. Talk to users early and often. Go deep to understand their motivations, behaviors, and what they're trying to achieve.</p>  <p>Attitude towards bugs, technical debt, and product quality. That phrase again: be pragmatic. Use technical debt tactically to accelerate development, and use opportunities to pay it off as you build.</p>  <p>The software development life cycle. Startups have many superpowers to use to their advantage. The ability to iterate fast is one, so use it! The development life cycle is generally much shorter and simpler than at larger companies – see more in <a href="https://newsletter.pragmaticengineer.com/p/qa-across-tech">Quality Assurance Across the Tech Industry</a>.</p>  <p>Collaborating with others. Working with design, user research, sales, and other teams. These are your partners; you're collectively trying to solve the problem of finding the PMF.</p>  <p>Relationship with end users and customers. See this as a collaboration in jointly building a delightful product.</p>  <p>Using data for product decisions. At an early-stage company with few users, qualitative user research (talking to users to gauge their needs and motivations,) is essential because it's the only way to understand them deeply enough to build what they want. Conversely, quantitative research like data and statistics about aggregate or longitudinal user behaviors, is valuable later for measuring retention and growth.</p>  <p>If you’re working in a leadership role, the need to find PMF also affects how you build and organize your team:</p>  <p>How you manage</p>  <p>How you prioritize features and set product strategy</p>  <p>How you set technical strategy</p>  <p>Who you hire</p>  <p>It’s necessary to think beyond writing code, building features, and solving user needs. A founding engineer’s aim is to build out a company-wide system which enables the business to find its PMF.</p><p>Remember, you’re a founding employee. Success means achieving PMF for the company, and then taking it to the next level. Make this your focus when taking decisions!</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb94f4a87-a761-4696-ba82-555e26f850db_1262x1000.png">    </a>The race to product-market fit. Without finding this: the company would eventually run out of money!  2. Approaches for success <p>As a founding engineer, there are countless decisions to make which impact the product and tech stack, such as:</p>  <p>How scrappy or polished is the minimum viable product (MVP)</p>  <p>Whether to outsource functionality to a vendor, or build and maintain it in house</p>  <p>Whether to build fast and dirty, or with scale and future requirements in mind</p>  <p>So, how do you make these choices, and how do decisions change as a product evolves? A founding engineer must make them pragmatically, using knowledge of the business, product, and users, for guidance.</p> Business uncertainty is a big driver in engineering decisions <p>Begin by taking these factors into account during design decisions:</p>  <p>Users’ needs and product’s value to them. How well do you understand your users and their needs, and how confident are you that the product meets them? If your answer is “not very well” – which is likely in the early days – then it’s best to avoid technical decisions which are expensive or limit flexibility.</p>  <p>Product roadmap and vision. Use your knowledge of the roadmap and the uncertainty it contains, to guide how you build enablers which support upcoming features.</p>  <p>Your team’s expertise. The more knowledge and expertise your team has in a technology, the more confidently you can make bold, complex design choices using it. Stick to what you know well!</p>  <p>Tech debt. Is tech debt a continuous source of bugs which limits engineering velocity? If so, building a new feature might be an opportune time to pay down tech debt (we cover this in the next section.)</p>  <p>You’ll notice most of these constraints are external to engineering; they’re about the business itself!</p><p>Prioritize flexibility during uncertainty. The younger a startup is, the more “cloudy” are the user, product, value proposition, channels, and business model. Early decisions are based on high-level vision and intuition about what should work. As you test ideas in the real world, you’ll need to iterate fast. So, as an engineer, prioritize flexibility in your decisions so the product can change rapidly and evolve.</p><p>As you iterate and the product (hopefully) moves towards PMF – not usually a straightforward process! – you develop more understanding of its requirements, meaning you can justify investing time in building an architecture that targets users’ needs and the product’s aims.</p> Evolve your technical decision making as the product evolves <p>Your decisions must evolve with the product. The right decision on day 1 is almost certainly not the right one on day 365. This was our experience of building our app, Shelf, over the past year. As a reminder, Shelf is an app that connects users with the media they and friends love.</p><p>Start simple. We started with a basic <a href="https://en.wikipedia.org/wiki/Data_model">data model</a>. New songs a user listened to, TV shows and movies they watched, and books they read, were stored as new rows in an append-only table called <code>activity</code>:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97df0c86-696b-47d1-9556-0b65b60fec6b_2258x624.png">    </a>The initial data model was an SQL table with three columns and an append-only structure <p>This spartan data model allowed us to build a highly-functional minimum viable product (MVP.) It was even flexible enough for us to implement new features, including ones we didn’t think of building our MVP, such as weekly recap, which allows users to see a summary of the media they had consumed in the past week; and history, which allows users to see other songs, books, tv shows and movies that had previously been on their Shelf.</p><p>Notice the cracks. However, as our product became more functional and complex, cracks started to show:</p>  <p>Implementing new features took ever longer, dragging down engineering velocity.</p>  <p>Fetching data resulted in complex queries to the database, which were increasingly sluggish and challenging to write. Getting the top songs, artists, and so on for weekly recap often required aggregating 1000s of rows of data and performing complex joins and logic. The user experience became noticeably laggy.</p>  <p>Bugs were hard or even impossible to solve, resulting in poor user experience. For example, users wanted to see how the book that they’re currently reading changed over time. Our data model simply didn’t store this data in an easy-to-retrieve way, and so we couldn’t display it to them!</p>  <p>Migrate when it’s really necessary. Too many of these issues piled up for us, so we decided to migrate to a data model better suited to the product’s and the engineering team’s needs. By this point, we had a better understanding of our app, its constraints and opinions, and the abstractions we wanted to work with as engineers. Given this context, we were able to design a more effective data model.</p><p>There were two key insights. The first was to store <code>history</code> as a first-class concept in the data model. In particular, we stored how a user’s top song, top album, recent TV show or book changed over time:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3d3c7d0-0284-439a-912a-175f27cd2e71_2274x730.png">    </a>The data model was then extended with the history table <p>Understanding how a user’s top artist changed became as simple as digging into two rows of data in this table. Previously, we’d had to aggregate thousands of rows in the <code>activity</code> table!&nbsp;</p><p>The second key insight was to store progress, such as play counts, pages read, and current episodes, as a first-class object in our data model:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03ad25ec-0d20-474b-886c-e68c7fee82b4_2300x730.png">    </a>Introducing a table to store progress <p>Understanding a user’s progress through a TV show, movie, book, etc, was much easier, as we simply needed to look up the relevant row!</p><p>Building this new data model required much greater investment in time to design and implement it, compared to data model v1. Our work paid off, though. The new data model allowed us to build new features faster and at higher quality.</p><p>When we started building Shelf, many things were uncertain. We didn’t really know which features would resonate with users, and our vision of the app was blurry. But after a year, we had gained more certainty about the product’s direction and we were able to make more “expensive” decisions with greater confidence, such as investing in a better data model.</p> Your job is more than building features <p>Building features is one way to help a business achieve its goals, but it’s not the only thing in your toolbox. Founding engineers use tools which have nothing to do with code, at all!</p><p>At Sidetrack – a company I cofounded to reduce cloud hosting costs – my most productive period was when I wrote very little code. This was early in the company's lifecycle, when we believed we could reduce cloud costs by shutting down virtual machines that had low utilization, thereby saving their expense. But how could we test this hypothesis?</p><p>Building an MVP of a working product would have taken months. We were in the business-to-business (B2B) market, and our target customers were large enterprises with several dedicated infrastructure teams. These orgs would obviously not let us run a service that wasn’t properly tested and vetted. There was also the problem of sales; even if we did pull off a rock-solid MVP, we didn’t have anyone to sell it to!</p><p>So, step one was to find companies potentially interested in working with us, as we built the real version of the product. Here’s how we went about locating them:</p>  <p>Find customers. Pinpoint businesses spending too much on infrastructure.</p>  <p>Get their attention. Demonstrate how much they could save with our product.</p>  <p>Collaborate with them. Work alongside them as we built an MVP and a production-ready version.</p>  <p>Step 2 was key for progressing to PMF. We had no problem finding customers, but we needed to get their attention. Otherwise, there was no point building anything.</p><p>Instead of starting to build the “real” product, I wrote a scrappy Python script. This script demonstrated to users how much money they were wasting. The script only took a few hours to write and was very simple. It had two inputs: the workloads our users wanted to run, and their CPU and memory demands, and also the infrastructure they were able to run on, and their CPU and memory supply. The script calculated the minimal infrastructure needed to run these workloads, solving the <a href="https://en.wikipedia.org/wiki/Bin_packing_problem">bin packing problem</a> using an off-the-shelf optimisation <a href="https://github.com/google/or-tools">library</a>.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80ecd4ce-321d-4624-98db-926b11871589_1478x1004.png">    </a>A suboptimal solution to the bin packing problem that uses more machines than necessary  <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2add2966-dbe9-4f8c-921a-8b63d57dd985_1454x942.png">    </a>An optimal solution to the bin packing problem, using the fewest-possible number of machines <p>We then ran this script to calculate how much potential customers were wasting on unused compute. The results were surprising: we found teams were paying up to 4-5x too much, even when accounting for surge capacity! Simply having this information got people excited enough about a real version of our product to sign prospective contracts to continue working together.</p><p>These companies were then happy to spend time talking with a young startup because they saw value in our approach. This scrappy script also validated the existence of demand for our solution, not just for securing early customers. This commitment was gold dust when it came to raising investment from VCs, who wanted to see early signs of traction and market validation.</p><p>As a founding engineer, the job is to do what it takes to help the company reach its next milestone. This lesson was clear, even as later, we pivoted from cost optimization to something <a href="https://leantime.io/unraveling-the-unfair-advantage-in-the-lean-canvas-for-startups">better suited</a> to our backgrounds in product management and engineering. In this example, this meant being creative to get people’s attention.</p><p>It also involves other things, such as:</p>  <p>Developing hacky scripts and prototypes, over insisting on shipping production-ready code</p>  <p>Proactively working with sales and marketing, automating tedious and manual workflows, or creating powerful new ones.&nbsp;</p>  <p>Not limiting interactions to fellow engineers.</p>  <p>Talking to users to better understand their needs and to build community; not assuming founders or product folks do this task.</p>  <p>Building dashboards to analyze data and provide visibility into key metrics, instead of accepting limited information.</p>  <p>Improving internal processes, instead of adapting to broken ones.</p>  <p>Writing documentation, FAQs, and guidelines, which help users use your product, instead of assuming they know how to use it correctly.</p>  <p>Think broadly about your role and contribution as a founding engineer. Don’t limit yourself – mentally or otherwise – to working on product features!</p> Scope, quality, timeline: pick two <p>Often, you want to a build a product feature that is:</p>  <p>Complex (broad scope)</p>  <p>Bug-free (is of high quality)</p>  <p>Quickly built</p>  <p>My concrete experience is that you cannot do all three with limited resources, so the pragmatic approach is to choose two. But which ones? It depends on your company and the market. For startups, “quickly” and “high quality” are often the preferred solution because they force you to reduce scope.</p><p>You should choose high quality over broad scope at a startup because an unreleased feature is a gamble. You don’t know for sure it will have the impact you expect until it’s released and users’ reactions are in. Implementing functionality takes time, obviously. Reducing scope allows you to learn from users and to iterate fast, giving you time to quickly test another hypothesis and approach if the first doesn’t work.</p><p>Reducing scope is also a great forcing function to focus on the core of your idea, pushing you to cut everything that isn’t core. Being able to ruthlessly cut unneeded functionality is a hallmark of an effective founding team.</p><p>When deciding the scope, a founding engineer can – and should! – provide context on what gives the best “bang for your buck.” It’s rare for a founding engineer to own the decision of what’s in or out of scope: the founder or product manager owns this decision. However, founding engineers can – and should! – provide context to the product person about which features are the most time-consuming to build, and why. The best founding engineers help their team understand feasibility and complexity, even before the feature is designed, and especially before engineers start building.</p><p>This kind of conversation is a regular one at my current startup. Just last week, we decided to de-scope a new feature called “stories.” We cut the scope to limit the feature to current users. The “stories” feature won’t be shown to new users we onboard to the initial version.</p> A tech stack is merely a means to an end <p>At Improbable, my first project was to check the correctness of the distributed game engine we were building. I did this by performing an integration test. I outlined my thinking in a design document, listing the taks:</p>  <p>Run different instances of the game engine, simulating real-world users playing a game with tanks, where each player controls a tank and aims to destroy other tanks.</p>  <p>To mimic the chaotic environment of a real-world game, I would add non-player character (NPC) tanks.</p>  <p>Use artificial intelligence (AI) for NPC agents to interact with one another like real-world players would.&nbsp;</p>  <p>Human players are smart, so I decided I need “smarter” AI. I used reinforcement learning to guide NPC tanks to choose targets, using the learnings from runs to continuously improve strategy. I’d spent the previous year building theoretical machine learning models like this, so figured it was the perfect experience to utilize.</p>  <p>Satisfied with my thorough approach, I shared my proposal with teammates in the expectation of receiving congratulations. Instead, I got immediate pushback! Someone said building would take too long, someone else said it would be near-impossible to maintain the custom reinforcement model-based AI, and another didn’t even think I could make it work, and that it was probably unneeded, anyway.</p><p>It turns out I didn’t actually need all this complexity; I didn’t need a complicated, reinforcement model-based AI, even though it would be fun to implement. The goal of the project was to answer the question:</p><p>“Is our distributed game engine working or not?”</p><p>A very simple simulation would do, wherein tanks interacted with each other using a very simple, rule-based logic (if-then-else). I built this in a fraction of the time that reinforcement model-based AI takes, and finished the project in a couple of weeks, not several months.</p><p>It’s tempting to reuse old technologies and patterns – but avoid this temptation. Reusing what I already knew was exactly what I was doing: I had just finished my engineering degree, and spent the year prior building theoretical machine learning models – including reinforcement learning. It was instinctive to use that tool for a design challenge, and I’ve seen this so many times at early-stage startups:</p>  <p>Using an approach that worked at a large or mid-sized company</p>  <p>Copying an approach or using a new product from Hacker News</p>  <p>Taking an approach from university or bootcamp that was taught as the ‘right way’</p>  <p>I don’t recommend copying old approaches, as they often come from very different environments:</p>  <p>Typically post-PMF</p>  <p>In a different market or geography</p>  <p>At a company with a different technical history</p>  <p>A different product</p>  <p>A different problem from the one you’re solving</p>  <p>Don’t forget, all startups and companies are different. If you know which technologies are appropriate for your context, then great – use them! If not, carefully reflect on why technologies you used in previous experiences worked, and whether they’re actually appropriate now.</p><p>A final note on the tech stack; as an engineer, you likely take pride in things like writing elegant code using TypeScript, and managing your infrastructure using Kubernetes. However, users and the business don’t care about this! All they care about is impact: the usefulness of the product, and whether it solves their problems.</p> 3. Tech debt 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/thriving-as-a-founding-engineer">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1971fe3f-ce57-4017-be1a-84dd9771ebd6_1580x826.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:18 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2024/03/5683/'>ナショナルオーストラリア銀行の円卓会議で開会の辞を行いました</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  3月26日日本時間正午ナショナルオーストラリア銀行のコーポレートデジタルIDラウンドテーブルでオープニングアドレス（開会の辞）を行いました。National Australia Bank (ナショナルオーストラリア銀行)、通称NABはオーストラリア国内最大の資産を持つ都市銀行です…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>3月26日日本時間正午ナショナルオーストラリア銀行のコーポレートデジタルIDラウンドテーブルでオープニングアドレス（開会の辞）を行いました。National Australia Bank (ナショナルオーストラリア銀行)、通称NABはオーストラリア国内最大の資産を持つ都市銀行です。</p>



 プログラム 



<p>コーポレートデジタルIDラウンドテーブルのプログラムは以下のとおりです。（和訳は以下）</p>



<p>Corporate ID in a nutshell</p>



 
 What is the purpose and benefits of Corporate Digital ID? 



 What are the current pain points for ID&amp;V for corporate entities? 



 What are the similar and unique challenges for Corporate Digital ID vis-a-vis Individual Digital ID? &nbsp; 



 Is a common framework linking Individual Digital ID and Corporate Digital ID necessary/desirable? Why/Why not? 
 



<p>Spotlight on the Global LEI systems (as a potential global and interoperable framework for Corporate Digital ID)</p>



 
 What are the origins of the GLEI, how does it operate and what is its purpose? 



 What are the key adoption challenges and opportunities? 



 A role for global standards? 



 What’s needed for success? 
 



<p>Industry / Government / Cross-border collaboration and partnerships</p>



 
 What is the role of Government/Industry in Corporate Digital ID ecosystems (how does this differ from Individual Digital ID)? 



 What are the opportunities for industry collaboration, public-private partnerships? 



 What are the opportunities for cross border co-operation to promote adoption and interoperability of corporate digital ID systems like the GLEI system? 



 How do we ensure interoperability, accommodate within the broader framework of Digital ID in&nbsp;Australia&nbsp;where this is warranted/desirable 
 



<p>Conclusion and next steps</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/03/image-38.png?ssl=1"></a>



<p>企業IDの概要</p>



 
 企業デジタルIDの目的とメリットとは？ 



 法人向けID&amp;Vの現在のペインポイントとは？ 



 個人デジタルIDに対する法人デジタルIDの類似点、独自課題とは？ 



 個人デジタルIDと企業デジタルIDをつなぐ共通のフレームワークは必要/望ましいか？なぜ／なぜそうしないのか？ 
 



<p>グローバル LEI システムのスポットライト（企業デジタル ID のための潜在的なグローバルで相互 運用可能な枠組みとして）</p>



 
 GLEI の起源、運用方法及び目的は何か？ 



 主要な導入の課題と機会は何か？ 



 グローバルスタンダードの役割とは？ 



 成功のために何が必要か？ 
 



<p>産業界／政府／国境を越えた協力とパートナーシップ</p>



 
 企業デジタル ID エコシステムにおける政府／産業の役割（個人デジタル ID との違い）とは？ 



 産業界の協力、官民パートナーシップの機会とは？ 



 GLEI システムのような企業デジタル ID システムの採用と相互運用性を促進するための 国境を越えた協力の機会とは？ 



 相互運用性を確保し、オーストラリアにおけるデジタル ID の広範な枠組みの中で相互 運用性が保証される／望ましい場合には、どのように対応するか？ 
 



 背景文献 



<p>このラウンドテーブルの背景文献としてはDouglas Arner 他による国際決済銀行(BIS)の論文 「&nbsp;<a href="https://www.bis.org/publ/bppdf/bispap126.pdf">Corporate digital identity: no silver bullet, but a silver lining」</a>が指定されていました。なお、Douglas Arner氏も本ラウンドテーブルに参加していました。</p>



 スピーチ 



<p>これを受けて、オープニングアドレスとしてわたしは以下のようなスピーチをしています（和訳はこの後にあります）。</p>



 Introduction: 



<p>Thank you for the introduction and thank you for inviting me to this roundtable. It is my honour to be with you all. Let me provide a few remarks on Corporate Digital Identity.&nbsp;</p>



<p>Corporate digital identity, or corporate ID, has the potential to dramatically simplify the identification and verification of companies, reducing the risks and costs of doing business. It can act as an admission ticket for companies to access financial services more efficiently. But developing effective corporate ID systems requires addressing several key challenges. I have enumerated seven of them for today.&nbsp;</p>



 Key points: 



 
 Corporate ID is more complex than individual ID. A company’s attributes like directors and ownership structure can change frequently and span multiple jurisdictions. Identifying ultimate beneficial owners is a particular challenge.&nbsp; 



 Company registries play a foundational role as the authoritative source of core company data. But many registries need to enhance data openness, quality, depth and connectivity to better support corporate ID. Often, data do not get updated in a timely manner and we would be looking at stale data.&nbsp; 



 With Bank-related initiatives: Banks have an opportunity to monetise their KYC investments by providing corporate ID services, thus turning a cost centre into a profit centre. The Open Digital Trust Initiative, jointly launched by the Institute of International Finance and the OpenID Foundation is such an initiative, but face obstacles around cost, data sharing, liability and competition. KYC utilities offer potential but have seen mixed results so far. 



 The Legal Entity Identifier (LEI) provides a global, unique, and interoperable identifier as a starting point for corporate ID. Enhancements like the verifiable LEI and LEI embedded in digital certificates could expand benefits and adoption. However, coverage of the registered companies is yet to be improved. Also, vLEI being built on a completely different technical stack than other verifiable credentials systems may pose adoption challenges.&nbsp; 



 OpenID, an open standard and decentralized authentication protocol, and its extension “OpenID for Identity Assurance”, which expresses the provenance and quality of the attributes, have significant potential for enabling secure and trusted identity assurance and data sharing between companies and service providers. The Global Assured Identity Network (GAIN) initiative aims to build on banks’ existing KYC processes and the OpenID standard to create a global, interoperable corporate ID and authentication system. It is like building bridges among islands of ecosystems. Interoperability among different ecosystems has been demonstrated through a technical proof of concept, which was led by Dima Postonikov in the Sydney room, but business and operational reality must catch up before it becomes ready to take off.&nbsp; 



 Decentralized identifiers and verifiable credential models, sometimes built on blockchain, aim to give companies more control over their data while enabling trusted data sharing. However, significant infrastructure investment and maturity are still needed.&nbsp; 



 Some governments are proactively developing corporate ID infrastructure as a public good, such as the account aggregator framework in India which empowers SMEs to digitally share their data for better access to finance. While it has found tractions in these economies, whether the pattern will propagate to other parts of the world is yet to be determined.&nbsp; 
 



 Conclusion: 



<p>In summary, corporate ID is progressing but remains fragmented today. There is no silver bullet – a range of stakeholders including registries, banks, service providers and policymakers have important roles to play. Enhancements to the LEI system, OpenID-based initiatives like GAIN, decentralized identity standards, and public infrastructure could help accelerate the development of a trusted and inclusive corporate ID ecosystem supporting improved financial stability, integrity, and access. Political will and multi-stakeholder coordination is essential to realizing the full potential.</p>



<p>I am hopeful that I will find hints to some of the key challenges that I have cited in today’s roundtable.&nbsp;&nbsp;</p>



<p>以下、自動翻訳です。</p>



 はじめに： 



<p>この円卓会議にお招きいただき、ありがとうございます。皆さんとご一緒できて光栄です。コーポレート・デジタル・アイデンティティについて少し述べさせていただきます。</p>



<p>コーポレート・デジタル・アイデンティティ、すなわち企業IDは、企業の識別と確認を劇的に簡素化し、ビジネスを行う上でのリスクとコストを削減する可能性を秘めています。企業が金融サービスをより効率的に利用するための入場券として機能します。しかし、効果的な企業IDシステムを開発するには、いくつかの重要な課題に取り組む必要があります。今日はそのうちの7つを列挙してみました。</p>



 キーポイント 



 
 法人IDは個人IDよりも複雑です。役員や所有形態など企業の属性は頻繁に変更され、複数の法域にまたがる可能性があります。最終的な受益者の特定は特に難しい課題です。 



 企業レジストリは、企業データの権威ある情報源として中核的な基礎的な役割を果たしています。しかし、多くのレジストリは、企業IDをよりよくサポートするために、データの公開性、質、深さ、および接続性を強化する必要があります。多くの場合、データはタイムリーに更新されず、私たちは古いデータを見ることになります。 



 銀行関連のイニシアティブ 銀行は企業IDサービスを提供することで、KYCへの投資を収益化し、コスト・センターをプロフィット・センターに変えるチャンスがあります。国際金融研究所（Institute of International Finance）とOpenIDファウンデーション（OpenID Foundation）が共同で立ち上げたオープン・デジタル・トラスト・イニシアチブ（Open Digital Trust Initiative）はそのようなイニシアチブですが、コスト、データ共有、責任、競争などの面で障害に直面しています。KYCユーティリティは可能性を秘めていますが、これまでのところ結果はまちまちです。 



 取引主体識別子（LEI）は、企業 ID の出発点として、グローバルで一意かつ相互運用可能な 識別子を提供。検証可能な LEI や電子証明書に組み込まれた LEI のような機能強化は、便益と採用を拡大する可能性がある。しかし、登録企業のカバレッジはまだ改善されていない。また、vLEI は他の検証可能なクレデンシャル・システムとは全く異なる技術スタック上に構築され ていることから、採用の課題が生じる可能性がある。 



 オープンスタンダードで分散型の認証プロトコルである OpenID と、その拡張機能である「OpenID for Identity Assurance」は、属性の実証性と品質を表現するものであり、企業とサービスプロバイダ間で安全で信頼できる ID 保証とデータ共有を可能にする大きな可能性を秘めています。グローバル・アシュアード・アイデンティティ・ネットワーク（GAIN）イニシアチブは、銀行の既存のKYCプロセスとOpenID標準を基盤として、グローバルで相互運用可能な企業IDおよび認証システムを構築することを目指しています。これは、エコシステムの島々の間に橋を架けるようなものです。異なるエコシステム間の相互運用性は、シドニーの部屋で参加しているディマ・ポストニコフが主導した技術的な概念実証を通じて実証されていますが、ビジネスと運用の現実は、それが離陸する準備が整う前に追いつかなければなりません。 



 非中央集権的な識別子と検証可能なクレデンシャル・モデルは、時にはブロックチェーン上に構築され、信頼できるデータ共有を可能にしながら、企業がデータをよりコントロールできるようにすることを目指しています。しかし、インフラへの多大な投資と成熟がまだ必要です。 



 インドのアカウント・アグリゲーター・フレームワークのように、公共財としての企業IDインフラを積極的に開発している政府もあります。このような経済圏では牽引役となっていますが、このパターンが世界の他の地域に伝播するかどうかはまだわかりません。 
 



 結論： 



<p>要約すると、企業IDは進歩していますが、現在も断片的なままです。レジストリ、銀行、サービス・プロバイダー及び政策立案者を含む様々な利害関係者 が果たすべき重要な役割。LEI システムの強化、GAIN のようなオープン ID ベースのイニシ アティブ、分散化された ID 標準、及び公共インフラは、金融の安定性、完全性、及びアクセスの改 善を支援する信頼され包括的な企業 ID エコシステムの開発を加速するのに役立つ可能性があ ります。政治的な意志とマルチステークホルダーの協調が、その可能性を完全に実現するために不可欠です。</p>



<p>本日の円卓会議で私が挙げた重要な課題のいくつかにヒントが見つかることを期待しています。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/03/image-39.png?ssl=1"></a>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/03/image-38.png?resize=546%2C385&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:20 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 25. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/03/mini-summit-two/'>Mini Summit Two</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A rough transcript my Ian Stanton’s talk, “Building Trunk: A Postgres
Extension Registry and CLI”, along with interspersed chat comments.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>We had such thoughtful and engaged discussion at this week’s <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">Postgres Extension
Ecosystem Mini-Summit</a>! I did learn that one has to reserve a spot
for each mini-summit individually, however. Eventbrite sends reminders for each
one you sign up for, not all of them.</p>
<p>To reserve a spot and be reminded for forthcoming meetings, hit the <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">Eventbrite
page</a> and select a date and hit “Reserve a Spot” for each date
you’d like to attend.</p>
<p>Back to this week’s meetup. My colleague <a href="https://www.linkedin.com/in/istanton">Ian Stanton</a> of <a href="https://tembo.io" title="Tembo: Goodbye Database Sprawl, Hello Postgres">Tembo</a> gave a great
talk, “Building Trunk: A Postgres Extension Registry and CLI”, that provided
background on the motivations and problems that inspired the creation of
<a href="https://pgt.dev" title="trunk: A Postgres Extension Registry">trunk</a>, a binary packaging system for Postgres extensions.</p>
<p>The presentation was followed by 35+ minutes of questions, discussion, and
brainstorming, which I’ve summarized below. But first, links!</p>
 
 <a href="https://www.youtube.com/watch?v=k3VC_RFL1bQ">Video</a> 
 <a href="https://justatheory.com/shared/extension-ecosystem-summit/building-trunk.pdf">PDF Slides</a> 
 
<p>Now down to business.</p>
 Introduction 
 
 I opened the meeting and introduced Ian Stanton. 
 
 Presentation 
 
 
<p>Ian introduced himself and <a href="https://pgt.dev" title="trunk: A Postgres Extension Registry">trunk</a>, “an extension registry and CLI
supporting <a href="https://cloud.tembo.io">Tembo Cloud</a>. Wants to tell a story, starting with backstory.</p>
 
 
<p><a href="https://tembo.io" title="Tembo: Goodbye Database Sprawl, Hello Postgres">Tembo</a> founded November 2022, provide managed Postgres solution called
<a href="https://cloud.tembo.io">Tembo Cloud</a>. Idea is Postgres can be used for so many different things
through the power of extensions, so built use-case optimized “stacks”
flavors of Postgres powered by extensions and configurations. Super proud of
them, including <a href="https://tembo.io/docs/product/stacks/transactional/message-queue" title="tembo Docs: Message Queue">Message Queue</a>, for which we build an open-source
extension.</p>
 
 
<p>Envisioned ability to install any extension, including user-provided
extensions. Knew we’d need an extension management solution. So we built it.</p>
 
 
<p>It’s called <a href="https://pgt.dev" title="trunk: A Postgres Extension Registry">trunk</a>, an extension registry and CLI, an open-source app for
the community that hosts binary packages for extensions, and powers Tembo
Cloud’s extension management.</p>
 
 
<p>Q1 2023 had build Tembo CLoud v1 with all extensions bundled in containers.
But wanted way to install them on the fly, ideally with installable packages.
Explored the ecosystem for tool we could use.</p>
 
 
<p>PGXN first we found. Love it, backed by the community, been around since
2011, but hosted source code, not binaries. Also little development since 2012.</p>
 
 
<p><a href="https://wiki.postgresql.org/wiki/Apt" title="PostgreSQL packages for Debian and Ubuntu">Apt</a> and <a href="https://yum.postgresql.org" title="PostgreSQL Yum Repository">Yum</a> repositories are community-backed and are binaries, just what we
wanted, but smaller subset of extensions relative to the 1000s available.
Thought it would be too time-consuming to add them all through the community
process.</p>
 
 *Steven Miller: in chat: “Also with apt packaging, it requires to
install to a specific path, but we needed to customize the install
path based on what pg_config shows for share lib and package lib dir.
That way we could persist extension installations on tembo cloud” 
 
 
 
<p>Weighed pros and cons of building one. Pros:</p>
 
 Full control over integration with Tembo Cloud 
 Binary distribution 
 We could build new features quickly 
 We could publish new extensions quickly 
 
<p>Cons:</p>
 
 How will the community react? 
 Recreating the wheel? 
 
 
 
<p>Expected to publish 2–3 extension a day, only do-able with a solution we
built.</p>
 
 
<p>Want to build something meaningful for Tembo Cloud and the community.</p>
 
 
<p><a href="https://registry.astronomer.io">Astronomer Registry for Airflow</a>: Built by Astronomer to find modules
for Airflow, very well received by the community.</p>
 
 
<p><a href="https://pgxn.org" title="PGXN — PostgreSQL Extension Network">PGXN</a>, <a href="https://wiki.postgresql.org/wiki/Apt" title="PostgreSQL packages for Debian and Ubuntu">Apt</a>, and <a href="https://yum.postgresql.org" title="PostgreSQL Yum Repository">Yum</a> repos: Wanted to take the best of them and build on
it.</p>
 
 
<p><a href="https://crates.io" title="The Rust community’s crate registry">crates.io</a>: Wanted a similar great experience for Postgres extensions.</p>
 
 
<p>Vision boiled down to discoverability, categories, ratings system,
certification, and indexing of cloud provider support.</p>
 
 
<p>Want to package any extension, whether SQL, C/SQL, or <a href="https://github.com/pgcentralfoundation/pgrx" title="Build Postgres Extensions with Rust!">pgrx</a>.</p>
 
 
<p>Simple experience, like <code>cargo publish</code> and <code>cargo install cargo-pgrx</code>.</p>
<p>Eric in chat: “❤️”</p>
 
 
<p>Hopes and Dreams: had idea people would magically show up, contribute to the
code, and publish their extensions. Wanted to support multiple platforms,
architectures, and Postgres versions, and for it to be a one-stop shop for
Postgres extensions.</p>
 
 
<p>How it works.</p>
 
 
<p>CLI and Registry, written in Rust, uses Docker to build extensions. Packages
named <code>&lt;trunk-project-name&gt;-&lt;version&gt;-&lt;pg-version&gt;.tar.gz</code>. Published with
<code>trunk publish</code> and installed with <code>trunk install</code>, putting all the files in
the right places.</p>
 
 Steven Miller in chat: “The approach to use docker for building has
been nice. It allows for cross-compile, for example, building for any
platform docker supports with the —platform flag” 
 
 
 
<p>Registry stores metadata and service web site and API, and uses S3 bucket
for the tar-gzip files.</p>
 
 
<p>Example building <a href="https://pgxn.org/dist/semver" title="semver on PGXN">semver</a> extension:</p>
 <code>Create Trunk bundle:
bitcode/src/semver/src/semver.bc
bitcode/src/semver.index.bc
semver.so
licenses/LICENSE
extension/semver--0.10.0--0.11.0.sql
extension/semver--0.11.0--0.12.0.sql
extension/semver--0.12.0--0.13.0.sql
extension/semver--0.13.0--0.15.0.sql
extension/semver--0.15.0--0.16.0.sql
extension/semver--0.16.0--0.17.0.sql
extension/semver--0.17.0--0.20.0.sql
extension/semver--0.2.1--0.2.4.sql
extension/semver--0.2.4--0.3.0.sql
extension/semver--0.20.0--0.21.0.sql
extension/semver--0.21.0--0.22.0.sql
extension/semver--0.22.0--0.30.0.sql
extension/semver--0.3.0--0.4.0.sql
extension/semver--0.30.0--0.31.0.sql
extension/semver--0.31.0--0.31.1.sql
extension/semver--0.31.1--0.31.2.sql
extension/semver--0.31.2--0.32.0.sql
extension/semver--0.32.1.sql
extension/semver--0.5.0--0.10.0.sql
extension/semver--unpackaged--0.2.1. sql
extension/semver.control
extension/semver.sql
manifest. json
Packaged to •/. trunk/pg_semver-0.32.1-pg15.tar.gz
</code> <p>Package up SQL files, control file, SO files, bitcode files into gzip file.</p>
 
 
<p>Once it’s published, API <a href="https://registry.pgtrunk.io/api/v1/trunk-projects/pg_semver">surfaces all this information</a>:</p>
  <code>[
  {
    "name": "pg_semver",
    "description": "A semantic version data type for PostgreSQL.",
    "documentation_link": "https://github.com/theory/pg-semver",
    "repository_link": "https://github.com/theory/pg-semver",
    "version": "0.32.0",
    "postgres_versions": [
      15
    ],
    "extensions": [
      {
        "extension_name": "semver",
        "version": "0.32.0",
        "trunk_project_name": "pg_semver",
        "dependencies_extension_names": null,
        "loadable_libraries": null,
        "configurations": null,
        "control_file": {
          "absent": false,
          "content": ""
        }
      }
    ],
    "downloads": [
      {
        "link": "https://cdb-plat-use1-prod-pgtrunkio.s3.amazonaws.com/extensions/pg_semver/pg_semver-pg15-0.32.0.tar.gz",
        "pg_version": 15,
        "platform": "linux/amd64",
        "sha256": "016249a3aeec1dc431fe14b2cb3c252b76f07133ea5954e2372f1a9f2178091b"
      }
    ]
  },
  {
    "name": "pg_semver",
    "description": "A semantic version data type for PostgreSQL.",
    "documentation_link": "https://github.com/theory/pg-semver",
    "repository_link": "https://github.com/theory/pg-semver",
    "version": "0.32.1",
    "postgres_versions": [
      15,
      14,
      16
    ],
    "extensions": [
      {
        "extension_name": "semver",
        "version": "0.32.1",
        "trunk_project_name": "pg_semver",
        "dependencies_extension_names": null,
        "loadable_libraries": null,
        "configurations": null,
        "control_file": {
          "absent": false,
          "content": "# semver extension\ncomment = 'Semantic version data type'\ndefault_version = '0.32.1'\nmodule_pathname = '$libdir/semver'\nrelocatable = true\n"
        }
      }
    ],
    "downloads": [
      {
        "link": "https://cdb-plat-use1-prod-pgtrunkio.s3.amazonaws.com/extensions/pg_semver/pg_semver-pg14-0.32.1.tar.gz",
        "pg_version": 14,
        "platform": "linux/amd64",
        "sha256": "f412cfb4722eac32a38dbcc7cd4201d95f07fd88b7abc623cd84c77aecc8d4bb"
      },
      {
        "link": "https://cdb-plat-use1-prod-pgtrunkio.s3.amazonaws.com/extensions/pg_semver/pg_semver-pg15-0.32.1.tar.gz",
        "pg_version": 15,
        "platform": "linux/amd64",
        "sha256": "9213771ffc44fb5a88726770f88fd13e62118b0f861e23271c3eeee427a23be9"
      },
      {
        "link": "https://cdb-plat-use1-prod-pgtrunkio.s3.amazonaws.com/extensions/pg_semver/pg_semver-pg16-0.32.1.tar.gz",
        "pg_version": 16,
        "platform": "linux/amd64",
        "sha256": "8ffe4fa491f13a1764580d274e9f9909af4461aacbeb15857ab2fa235b152117"
      }
    ]
  }
]
</code>  <p>Includes different tar-gzip files for different versions of Postgres, the
contents of the control file, dependencies; loadable libraries and
configurations; and the one extension in this package — some can have many
like <a href="https://postgis.net">PostGIS</a>. Then Postgres version support and some other metadata.</p>
 
 
<p>What it looks like <a href="https://pgt.dev/extensions/pg_semver">on the web site</a>, includes README contents, data from
the last slide, install command, etc.</p>
 
 
<p>This is what installation looks like:</p>
  <code>$ trunk install pg_semver
Using pkglibdir: "/usr/lib/postgresql/16/lib"
Using sharedir: "/usr/share/postgresql/16"
Using Postgres version: 16
info: Downloading from: https://cdb-plat-usel-prod-pgtrunkio.s3.amazonaws.com/extensions/pg_semver/pg_semver-pg16-0.32.1.tar.gz
info: Dependent extensions to be installed: []
info: Installing pg_semver 0.32.1
[+] bitcode/src/semver/src/semver.bc =&gt; /usr/lib/postgresql/16/lib
[+] bitcode/src/semver. index.bc =&gt; /usr/lib/postgresql/16/lib
[+] semver.so =&gt; /usr/lib/postgresql/16/lib
info: Skipping license file licenses/LICENSE
[+] extension/semver--0.10.0--0.11.0.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.11.0--0.12.0.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.12.0--0.13.0.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.13.0--0.15.0.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.15.0--0.16.0.sql = /usr/share/postgresql/16
[+] extension/semver--0.16.0--0.17.0.sql =&gt; /us/share/postgresql/16
[+] extension/semver--0.17.0--0.20.0.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.2.1--0.2.4.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.2.4--0.3.0.sql &gt; /us/share/postgresql/16
[+] extension/semver--0.20.0--0.21.0.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.21.0--0.22.0.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.22.0--0.30.0.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.3.0--0.4.0.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.30.0--0.31.0.sql = /usr/share/postgresql/16
[+] extension/semver--0.31.0--0.31.1.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.31.1--0.31.2.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.31.2--0.32.0.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.32.1.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--0.5.0--0.10.0.sql =&gt; /usr/share/postgresql/16
[+] extension/semver--unpackaged--0.2.1.sql =&gt; /usr/share/postgresql/16
[+] extension/semver.control =&gt; /usr/share/postgresql/16
[+] extension/semver.sql =&gt; /usr/share/postgresql/16

***************************
* POST INSTALLATION STEPS *
***************************

Install the following system-level dependencies:
        On systems using apt:
            libc6

Enable the extension with:
       CREATE EXTENSION IF NOT EXISTS semver CASCADE;
</code>  <p>CLI pulls down the tar-gzip, unpacks it, and puts the files in the right
places and tells the users what other commands are needed to enable the
extension.</p>
 
 
<p>Pause to take a sip of water.</p>
 
 David Wheeler (he/him) in chat: “STAY HYDRATED PEOPLE!” 
 
 
 
<p>State of the project. Trunk powers extension management for Tembo Cloud, 200
extensions on the platform, install and enable on the fly. Tembo Cloud likely
trunk’s #1 user.</p>
 
 
<p>Get lots of site traffic, especially around categorization, addresses the
discoverability problem set set out to solve.</p>
 
 Jeremy S in chat: “Interested in insights from site traffic - you
mentioned that ‘categorization’ was popular - any other things that
traffic patterns seem to suggest you might have done really well, or
clearly is needed?” 
 
 
 
<p>But pretty minimal community involvement, out fault for not involving the
community early on.</p>
 
 
<p>Did we solve the problem?</p>
 
 For Tembo Cloud: yes! Trunk is core component of the Tembo Cloud
platform that lest us offer high number of extensions. 
 For the community: no! But helped bring more awareness to the
opportunities to improve the ecosystem as a community. 
 Saw other solutions arise around the same time, including <a href="https://database.dev" title="The Database Package Manager">dbdev</a> and
<a href="https://pgxman.com/" title="npm for PostgreSQL">pgxman</a>, and Yurii at <a href="https://omnigres.com" title="Omnigres: Postgres as a Platform">Omnigres</a> is working on something as well. Huge
opportunity to solve this together. 
 Steven Miller in chat: “I think it is very nice way to install other
extensions via an extension how dbdev works” 
 David Wheeler (he/him) in chat: “GRANT!” 
 Grant Holly in chat: “Oh hi” 
 
 
 
<p>Lessons Learned</p>
 
 
<p>It’s a really hard problem to solve! As you add more layers of
complexity, like different architectures, versions of Postgres,
it gets harder and harder.</p>
 
 
<p>Steven Miller in chat, Replying to “The approach to use …”: “The
downside for this approach includes missing chip-specific
instruction support, for example AVX512, which optimizes performance
on some extensions. However if you are building with docker on the
same architecture as the host, then it still includes these
instructions.”</p>
 
 
<p>David Wheeler (he/him) in chat, Replying to “The approach to use
…” “Also presumably no support for building for non-Linux
platforms, yes?”</p>
 
 
 
 
<p>The extension ecosystem truly is the wild west, not really best
practices around building, versioning, and releasing, and when you’re
collecting and housing them, it makes things difficult. A huge
opportunity for us to come up with those standards and share them with
the community.</p>
 
 
<p>Community involvement is crucial, wish we’d done it better early on,
that’s why we’re all here today! Solution to build together doesn’t
happen if we don’t tackle it as a community.</p>
 
 
<p>Similarly, wish we’d reached out to folks like David and Devrim early
on, to get more insight from them and bring the community into the
project from the beginning</p>
 
 
 
 
<p>The future of trunk</p>
 
 
<p>Registry and CLI will continue to serve Tembo Cloud</p>
 
 
<p>Has paved the way for binary packaging and distribution in PGXN v2 that
David is spearheading, will at least will inform and perhaps be the
basis for that part of the project.</p>
 
 
 
 
<p>That’s all, thank you, back to you, David!</p>
 
 
 Discussion 
 
 
<p>David Wheeler (he/him): Thanks for history an context, Ian! Questions or
discussion topics? Some comments in the thread from Steven and Tobias.</p>
 
 
<p>Tobias Bussmann: in chat: speaking of paths: it would be super helpful
if postgresql would support loading extensions from additional paths
beside the $SHAREDIR/extension and $PKGLIBDIR directories. At least
following directory symlinks within…</p>
 
 
<p>Steven Miller in chat, Replying to “The approach to use …”: I tried
to make it work for Mac, for example, but the docker support didn’t work
basically. I think it might work for Windows, since they have better
container support. However I didn’t try that yet.</p>
 
 
 
 
<p>David Wheeler (he/him): Reads Tobias’s comment. You can specify a
subdirectory in the <code>sharedir</code> and maybe the <code>moduledir</code>? But it’s a
little hinky right now. Steve, do you want to talk about the us of Docker to
build images?</p>
 
 
<p>Steven Miller: Yeah, I’d love to. To Tobias’s point, agree, on Tembo Cloud,
we have a persistent directory where wer’re sintalling extensions, but
because there is no way for an extra <code>sharedir</code> or package dir, we’re
persisting all of the library files, including Postgres core. Not ideal,
especially for upgrades.</p>
<p>Approach for building ind Docker: been nice, do the build ina Dockerfile,
start the container, then install and compare the difference between layers
and zip up all the new files. Great for cross-compile but, not working for
mac or other systems. Will need a fallback option to do a local build.</p>
 
 Jeremy S: in chat, Replying to “speaking of paths: i…”: Exactly same
point was also mentioned just this morning on slack by Matthias 
 
 
 
<p>David Wheeler (he/him): Makes sense, thanks. What other bits do you feel
like could be useful for packaging binaries at a community level?</p>
 
 
<p>Steven Miller: Sometimes we install binaries with trunk, but then difficult
to know what has been installed. Nothing like apt where there is a history
of what is installed or uninstall. Would be nice to do something like <code>trunk list</code> and see everything that has been installed. Also, future should be not
just install but management, including turning extensions on, and there are
a lot of ways to turn them on.</p>
 
 
<p>Ian Stanton: <code>uninstall</code> would be useful, too.</p>
 
 
<p>David Wheeler (he/him): Other questions about trunk or challenges to binary
distribution it brings?</p>
 
 
<p>*Tobias Bussmann in chat, Replying to “speaking of paths: i…”: this
would allow an immutable PostgreSQL base package and still allow to
install extensions on top. This is esp. important if you need to have
singned packages like on macOS</p>
 
 
<p>*nils in chat, Replying to “speaking of paths: i…”: Guess there is
some prior art in how search_path in pg work, or the PATH in unix’s.</p>
<p>Should be doable to allow to specify some kind of ordered search path,
where Postgres will look for extensions. That way, Postgres can protect
it’s own libs to no be overwritten by external libs, but allow for
loading them from extra paths.</p>
 
 
 
 
<p>Yurii: There is <code>CREATE EXTENSION</code> and other extensions like logical
decoding plugins. Does trunk handle them?</p>
 
 
<p>Steven Miller: We think of it as four types extensions into 2x2 matrix: 1.
Does it require <code>CREATE EXTENSION</code> true or false; and 2. Does it have a
loadable library true or false. The false/false category is output plugins;
The true/true category, e.g. <code>pg_partman</code>, <code>pg_cron</code>; <code>CREATE EXTENSION</code> false
and loadable library true, e.g., <code>autoexplain</code>, just a library, no upgrade
concerns; and then <code>CREATE EXTENSION</code> true and loadable library false is the
default case.</p>
 
 
<p>Ian Stanton: Steven wrote <a href="https://tembo.io/blog/four-types-of-extensions">a blog</a>
on this.</p>
 
 
<p>Eric in chat: Does pgrx make the process of building easier or harder
and is there anything we can do today to make rust extension building
better?</p>
 
 
<p>Jason Petersen in chat: Yeah, it sounds like we need some sort of
system database like apt has; would enable management, uninstall,
version list, whether upgrades are available, etc</p>
 
 
 
 
<p>Yurii: That would be great. What other modules are there without extensions,
like <code>autoexplain</code>?</p>
 
 
<p>Ian Stanton: auth delay is another, base backup to shell, considered parts of
postgres, but we have trouble categorizing them. There are 10-15 I’ve come
across.</p>
 
 
<p>Yurii: ARe these categories on Tembo, can you click a button?</p>
 
 
<p>Ian Stanton: Not a category, but would be a good one to add.</p>
 
 
<p>Steven Miller in chat: This one! <a href="https://tembo.io/blog/four-types-of-extensions">https://tembo.io/blog/four-types-of-extensions</a></p>
<p>It’s in the API metadata</p>
<p>Sorry if I mispronounced your name Tobias</p>
 
 
<p>David Wheeler (he/him) in chat: SAME</p>
 
 
 
 
<p>Yurii: Did you say output plugins are handled with Tembo:</p>
 
 
<p>Steven Miller: YOu can install them with trunk, yes.</p>
 
 
<p>Yurii: And you have the build pipeline that will work without plugins too,
yeah, cool.</p>
 
 
<p>David Wheeler (he/him): Tobias, did you want to say more about the path
issues?</p>
 
 
<p>Tobias Bussmann: Sure! We are building the Postgres.app, distribution for
macOS, working different from Linux systems. We distribute some extensions
directly, but also allow building and installing extensions on it. Works
nicely, even with pgxn client, but it’s built within the application, which
breaks the code signature.</p>
<p>We always have to fight against a breaking system to allow that. Possible,
but would be much cleaner to specify an extra directory where extensions
could be loaded, and we could distribute packages with binary extensions
that the user could download and install separately from the Postgres.app.</p>
 
 
<p>David Wheeler (he/him): You’re not suggesting a different directory for
every extension with a module, but just another path in the search path
that’s not subject to the signature verification.</p>
 
 
<p>Tobias Bussmann: Yes, that would be an option, but with a flexible system
could use one per extension or just specify a second directory. Contrib
extensions sometimes seen as part of Postgres, and they’re all stuffed in
the same directory with third party extensions, which gets confusing and
hard to manage.</p>
 
 Steven Miller in chat: In the previous extensions mini summit, Yuri
mentioned that he was working on a patch to postgres for extra libdir,
extra share dir, but I have not been tracking this one 
 
 
 
<p>nils: That’s what I was saying in chat, there is prior art in Postgres and
Unix systems where you can specify a search path in postgres for a list of
schemas, and in Unix the path is to find binaries. Give me a delimited list
of directories on my system. Could be super user only, where they can
specify where they’re installed, and we can go through the list ot find an
extension.</p>
 
 
<p>David Wheeler (he/him): I might be imagining this, but I seem to recall
there was a proposal to have extensions in their own directories, which
would be nice for packaging, but then every time you add one you have to add
another directory to the list and there is some fear the lookup time could
be too long.</p>
 
 
<p>Jeremy S in chat, replying to “speaking of paths: i…”: (Or like
LD_LIBRARY_PATH )</p>
 
 
<p>David Wheeler (he/him) in chat, replying to “speaking of paths: i…”:
LD_LIBRARY_PATH is all but dead on macOS</p>
 
 
 
 
<p>Jason Petersen: If it happens at startup I can’t imagine that being a
concern. If the list changes you reboot. It’s not gonna be a performance
problem, I hope.</p>
 
 
<p>*David Wheeler (he/him): Or HUP it if you don’t want downtime.</p>
 
 
<p>Jason Petersen: Sure, but it doesn’t need to be on every command.</p>
 
 
<p>*David Wheeler (he/him): Eric, do you want to pose your question about pgrx?</p>
 
 
<p>Eric: Sure. Wanted to know, were there stumbling blocks to get pgrx support
built into trunk, and does it make things easy or difficult? Different from
C path, are there things we could do to make things easier today?</p>
 
 
<p>Ian Stanton: Yeah, I think the issue is mostly on our end. We have a
separate image for each version of pgrx, and keeping up with the releases is
challenging. We need to rethink our image building strategy. Shouldn’t be
one image for each version of pgrx. That’s the biggest thing I’ve noticed,
mostly on our side.</p>
 
 
<p>*David Wheeler (he/him): Because you need the install the version of pgrx
that the extension requires before you do the build, and that’s just too slow?</p>
 
 
<p>Ian Stanton: Could be too slow. We’ve known about this problem for some
time, just hasn’t been addressed yet.</p>
 
 
<p>Eric: Okay, maybe we can talk about it offline one day, be happy to chat. I
think we’re close to being able to have the CLI, cargo-pgrx, be a different
version than whatever version the extension uses.</p>
 
 
<p>Ian Stanton: That would be super useful!</p>
 
 
<p>Eric: Yeah, I think we’re close to being at that point, if not there
already. We can talk about that offline.</p>
 
 
<p>Ian Stanton: Nice! We’ll reach out in Discord.</p>
 
 
<p>*David Wheeler (he/him): Other comments or questions, or people who have
worked on other kinds of binary registry things, would love to hear more
from other perspectives. Devrim is going to talk about the Yum repository
next week [ed. correction: in two weeks].</p>
 
 
<p>Steven Miller in chat: Daniele last time mentioned Pip is good example of
mixing source and binary distributions</p>
 
 
<p>Eric: I have a random question related to this. In the past and recent
history, has hackers talked about some way of storing extension in the
database rather than relying on the file system?</p>
 
 
<p>*David Wheeler (he/him): Yes! In <a href="https://www.postgresql.org/message-id/flat/m2r49a5uh8.fsf_-_%402ndQuadrant.fr#3ceccb32533e81a1be084122ebf8d96f">this long thread from 2011</a> [ed.
Correction: 2013] Dimitri was proposing a “unit”, a placeholder name, where
the object would be stored in the database. Very long thread, I didn’t read
the whole thing, lot of security challenges with it. If it needs a shared
object library loading having to be written to the file system it’s just not
going to happen. I don’t know whether that’d be required or not.</p>
<p>Dimitri also worked on a project called <a href="https://github.com/dimitri/pginstall">pginstall</a> where you could install
extensions from the database like <a href="https://database.dev" title="The Database Package Manager">dbdev</a>, but not just TLEs, but anything.
The idea is a build farm would build binaries and the function in the
database would go to the registry and pull down the binaries and put them
in the right places on the file system.</p>
<p>There were a lot of interesting ideas floating around, but because of the
legacy of the PGXS stuff, it has always been a bit of a struggle to decide
not to use it, to support something not just on the machine, but do
something over libpq or in SQL. Lot of talk, not a lot of action.</p>
 
 
<p>*Tobias Bussmann in chat in response to “In the previous ex…”: still
searching on hacker for it. Meanwhile I found:
<a href="https://commitfest.postgresql.org/5/170/">https://commitfest.postgresql.org/5/170/</a></p>
 
 
<p>Steven Miller in chat: That approach is very awesome (install via
extension)</p>
 
 
 
 
<p>Eric: I can see why it would take some time to sort it all out. One thing to
require super user privileges to create an extension, but also having root
on the box itself? Yeah.</p>
 
 
<p>Yurii: TLE plugs into that a little bit for a non-shared object. Not exactly
storing it in the database, but does provide a SQL based/function method of
installing from inside the database, but only for trusted languages, not
shared objects.</p>
 
 
<p>*David Wheeler (he/him): <code>dbdev install</code> does download it from database.dev
and stores it in the database, and has hooks into the <code>CREATE EXTENSION</code>
command and pulls it out of its own catalog. Was a similar model with
<code>pginstall</code>, but with binary support, too.</p>
 
 
<p>Yurii: Back to trunk. When you start building, and have to deal with
binaries, pgxn you can put the source up there, but I want to get to the
whole matrix of all the different versions. Every extension author does it a
little different. Some extensions have versions for Postgres 15, another for
14, some have the same version across all the majors, sometimes an extension
works for some majors and others. Has trunk expanded to other Postgres
versions to support the whole exploding matrix of stuff that does and
doesn’t work, 5-6 majors, gets to be a large matrix, a lot to keep track of.
How’s that working out for the builds and managing that matrix.</p>
 
 Steven Miller in chat: Dimensions I think are:
 
 pg version 
 architecture 
 chip-specific instructions (edge case for native builds?) 
 
 
 
 
 
<p>Steven Miller in chat: We just announced support for 14 and 16</p>
<p>David Wheeler (he/him) in chat, replying to “Dimensions I think a…”: OS,
OS version</p>
 
 
<p>Steven Miller in chat,: Replying to “Dimensions I think a…”: Ah right</p>
 
 
<p>Ian Stanton: Steven do you want to take that one?</p>
 
 
<p>Steven Miller: Oh yeah. We’ve started toe-dipping on this one. Started with
Tembo Cloud’s platform, but have no released Postgres 14 and 16, and also
trunk has built-in support for other architectures, such as arm, or whatever
the Docker <code>--platform</code> flag supports. We looked at mac builds, not working
yet, might work for Windows, which ahs better container support, but I don’t
know, and also there is an edge case for pg_vector especially, which
compiles to include ship-specific instructions for AVX512, which helps with
vector. So that’s another dimension to consider.</p>
 
 
<p>Yurii: Part of the idea behind this forum is to see if we can chart a path
forward, maybe not solve everything. What can we solve, how can we make
something a little better for Postgres at large?</p>
 
 
<p>Eric in chat: Even as a Mac user I don’t know the answer to this…
what’s the common Postgres package there?  Postgres dot app, homebrew,
something else?</p>
 
 
<p>David Wheeler (he/him) in chat: <a href="https://github.com/theory/pgenv">pgenv</a>! (self-promotion)</p>
 
 
<p>Eric in chat: I assume folks don’t use macOS in prod but developers are important too</p>
 
 
<p>nils in chat, Replying to “Even as a Mac user I…”:</p>
 <code>$ git clone ..
$ ./configure
$ make
$ make install
</code> <p>At least that is what I do 😄</p>
 
 
 
 
<p>Steven Miller: In my opinion, the way to approach it is to know all the
dimensions you need, and in the metadata API say which binaries are
available. Then get through it with testing and badging If we let things get
built, to what extent is it tested and used? That can help. Daniele was in
the previous call, said we could look to Pip and Wheel files for
inspiration, and Adam on our team has said the same. This is something that
has some binary and some source, and falls back on doing the build when it
needs to.</p>
 
 
<p>*David Wheeler (he/him): I’ve been thinking about this quite a bit lately.
Can see needing to take advantage of multiple platforms available through
GitHub workflow nodes or the <a href="https://buildfarm.postgresql.org">community’s build farm</a>, which has a vast
array of different architectures and platforms to build stuff. There are
precedents!</p>
<p>I imagine a system where, when something is published on PGXN, another
system is notified and queues it up to all its build farm members to build
binaries, ideally without full paths like trunk, and making them available
for those platforms. Building out that infrastructure will take a fair bit
of effort, I think. With cross-compiling is available it might be…doable?
But most modules and for SQL and maybe Rust or Go extensions, but a
challenge for C extensions.</p>
<p>This is a problem I’d like us to solve in the next year or two.</p>
 
 
<p>Steven Miller in chat, replying to “I assume folks don’t…”: Yeah
exactly, like trunk install after brew install postgres</p>
 
 
<p>Tobias Bussmann in chat, replying to “Even as a Mac user…”: this
seems to be quite spread. There are also people that prefer docker based
installs</p>
 
 
<p>Eric in chat: pgrx supports cross compilation</p>
<p>With a caveat or two!</p>
 
 
<p>Eric in chat, replying to “Even as a Mac user I…” @nils same. For v9.3
though 16!</p>
 
 
 
 
<p>*David Wheeler (he/him): What else? Reading the comments.</p>
 
 
<p>Yurii: I think maybe that PGXN JSON file, I know you’ve been spending time
on it, David, including the <a href="https://justatheory.com/2024/03/extension-namespace-rfc/" title="Extension Registry Namespacing RFC">proposal on namespacing</a> a few days ago. That
feels like it could be helpful to be part of this. IF it could be something
we could center around… The first time I wanted to put an extension on PGXN,
it took me a long time to figure out that JSON file. I didn’t find the blog
post that goes through it in nice detail till like two weeks after. If I’d
found it sooner I could have skipped so many things I tried to figure out on
my own.</p>
<p>If we can center around that file, it’ll draw more attention to it, more
links back to it, more examples people blog about here and there, it helps
going forward. The trick is getting it right not being this massive thing no
one can figure out, or has too many options, but hits all the points we
need.</p>
 
 
<p>nils in chat, replying to “Even as a Mac user I…”: Well, mostly for
extension, for Postgres I rely on David’s pgenv</p>
 
 
<p>*Eric * in chat, replying to “Even as a Mac user I…”: @Tobias Bussmann
hmm. Makes it difficult to get an extension installed.</p>
 
 
 
 
<p>*David Wheeler (he/him): I’ve been thinking about this a lot, drafted a doc
some of my colleagues at Tembo have read over and I hope to publish soon
[ed. Note: <a href="https://justatheory.com/2024/03/rfc-pgxn-metadata-sketch/" title="RFC: PGXN Metadata Sketch">now published</a>], thinking through what a v2 of the <a href="https://pgxn.org/spec/">PGXN Meta
Spec</a> might include. I think we should extend with list of external libraries
required, or the architectures it supports, or it’s a loadable library or an
app that doesn’t even go into the database.</p>
<p>I would like soon to draft an actual revision of the spec, and document it
well but also turn it into a JSON Schema document so we can automate
publishing it and verification in the same place. I also imagine building an
eventual replacement or evolution of the PGXN client or trunk client or some
client that you can use to manage that thing. I think pgrx does that, adding
metadata via the client rather than parse and understand the whole file.</p>
<p>I’m with you it could get really complicated, but I’m not sure I see an
alternative other than building good tooling to minimize the pain.</p>
 
 
<p>Ian Stanton: I think automatically pulling that information when it’s
readily available would be super helpful. We use it as an app to just take
care of things for people.</p>
 
 
<p>*David Wheeler (he/him): Right, and then if we’re successful in getting it
done it’s getting people to take up the tools and start using them. There’s
only so much we can infer. I can tell how to do a build if there’s a
<code>Makefile</code> or a <code>configure</code> file or a <code>cargo.toml</code>, but that doesn’t reveal
what libraries are required. This is why there’s a lot of hand-tuning of
RPM and Apt spec files.</p>
 
 
<p>Steven Miller in chat: We are calling this “system dependencies”</p>
<p>Ssl and glibc the main ones 🙂</p>
 
 
<p>Jason Petersen in chat: And sometimes the package names aren’t even
1—1 mappings</p>
 
 
<p>Eric in chat: Ha!  Try relying on elasticsearch as a runtime
dependency!  😞</p>
 
 
 
 
<p>Yurii: That’s another thing to touch on. A lot of extensions are just a thin
layer of glue between Postgres and some OSS library that someone else
maintains. But the trick, when you want to build a Yum package, the
dependency has a different name than the rest of the RedHat ecosystem vs.
the Debian ecosystem. So part of what Devrim has to do to maintain the RPM
packages is manually sort all that out, because you can’t automatically…
libc! It’s called <code>glibc</code> in RedHat and just <code>libc</code> in Debian, and every
package has slightly different names. Do how do you manage that in trunk? Do
you pull the source for any dependencies? Does your Docker image…I don’t
know how this is working.</p>
 
 
<p>David Wheeler (he/him) in chat: I want to build a json schema
validation extension in Rust using
<a href="https://github.com/Stranger6667/jsonschema-rs">https://github.com/Stranger6667/jsonschema-rs</a> or something</p>
 
 
<p>Tobias Bussmann in chat, replying to “Ha!  Try relying o…”: or V8 🤯</p>
 
 
 
 
<p>Ian Stanton: Two sides to that one is build time dependencies, and there
there are runtime dependencies. I just dropped an example for some random
extension. Tthe way we’ve been building this is to write out a <code>Dockerfile</code>
that can include build time dependencies. [hunts for link…]</p>
 
 Ian Stanton in chat: <a href="https://github.com/tembo-io/trunk/blob/main/contrib/age/Dockerfile">https://github.com/tembo-io/trunk/blob/main/contrib/age/Dockerfile</a> 
 
 
 
<p>Ian Stanton: We specify them all there. But for runtime, we don’t know
what’s required until we test the thing. We have stuff in our CI pipelines
to install and enable the extension to see if it works. If it doesn’t, it
will report a missing dependency. Then we know we need to add it to our
Postgres images. Not the best flow for finding these dependencies. Steven,
want to add anything more to the build time dependency piece?</p>
 
 David Wheeler (he/him) in chat, replying to “Ha!  Try relying on …”:
Next version of plv8 released on PGXN will have v8 bundled 
 
 
 
<p>Steven Miller: A lot share the same ones, SSL and glibc, so we just build
with the same versions we run on Tembo Cloud. In the metadata we list all
system dependencies, that’s what we build towards, and include them in the
Docker image. If you pick a different stack, like the Machine Learning
stack, it has all the Python stuff in the base image. We don’t really love
this, but this is something where Python wheel might inspire us, becaus it
has packaging and system dependencies.</p>
 
 
<p>Eric in chat, replying to “I want to build a js…”: I feel like I’ve
seen one already?</p>
 
 
<p>David Wheeler (he/him) in chat, replying to “I want to build a js…”:
GIMME</p>
 
 
 
 
<p>Yurii: If you really want to od this right, just like in the RPM
repositories, you have to know what the dependencies are. David, I’m
curious, what your thoughts are, if this is to be done right, there has to
be a way to indicate dependencies in the <code>META.json</code> file, but then I’m
talking about Debian and RedHat, but what about Mac? Windows doesn’t really
have a packaging system. There are BSDs, other places Postgres can run,
probably have to narrow the scope a bit to solve something.</p>
 
 
<p>Tobias Bussmann in chat, responding to “Ha!  Try relying o…” Sounds
promising, but for which architectures? I have good hope for pljs as
replacement for plv8</p>
 
 
<p>Ian Stanton in chat:
<a href="https://github.com/tembo-io/trunk/blob/d199346/contrib/fuzzystrmatch/Trunk.toml#L13">https://github.com/tembo-io/trunk/blob/d199346/contrib/fuzzystrmatch/Trunk.toml#L13</a></p>
 
 
<p>David Wheeler (he/him): Fortunately there are only around 1100 extensions in
the world, a relatively low barrier at this point. Some of these other
things have thousands or millions of extensions.</p>
 
 
<p>Yurii: I guess when you put it that way! But I wasn’t going to go through
all 1000 of them one-at-a-time.</p>
 
 
<p>David Wheeler (he/him): No. I posted about this on Ivory a few weeks ago
[ed. correction: he means <a href="https://xoxo.zone/@theory/111983275190519842">on Mastodon</a>]: how does one do this in a
platform-neutral way. There are some emerging standards where people are
trying to figure this stuff out. One is called <a href="https://github.com/package-url/purl-spec" title="purl-spec: A minimal specification for purl a.k.a. a package “mostly universal” URL">purl</a>, where you specify
dependencies by packing URLs, or “purls”, and then it’s up to the installing
client to resolve them vai whatever the packaging system it depends on.</p>
<p>I would assume on Windows we’d have to say “it works great as long as you
use <a href="https://chocolatey.org" title="The Package Manager for Windows">Chocolatey</a>” or something like that. But it’s certainly a difficult
problem. I’m looking forward to your talk about your unique approach to
solving it, Yuriii [ed. note: that’s the May 1 mini-summit], that’s going to
be super interesting.</p>
 
 
<p>David G. Johnston: Ultimately you just crowd sourcing. If we just say “this
is what we call this thing in PostgreSQL world”, then if people need to
compile it on Chocolatey on Windows, they figure it out and contribute it.
Or on Debian or RedHat. Just facilitate crowd-sourcing, metadata in a
database.</p>
 
 
<p>David Wheeler (he/him): My initial idea was a global registry that people
contribute to just by editing files in a GitHub repository.</p>
 
 
<p>David G. Johnston: HashiCorp has to have something like that already,
there’s stuff out there, no need to reinvent the wheel. This is a global
problem if we open-source it we can solve it.</p>
 
 
<p>David Wheeler (he/him): Right. Really appreciate everyone coming. Great
discussion, I appreciate it. In two weeks, Devrim Gündüz is going to talk
about the Yum Community Repository and the challenges of RPMifying
extensions. I had this idea of automating adding extensions to the Yum and
Apt repositories, an Devrim is a little skeptical. So super look forward to
his perspective on this stuff. Two weeks from today at noon [ed.:
America/New_York]. Thanks for coming!</p>
 
 
<p>Eric in chat: Thanks a ton!  This is exciting stuff.</p>
 
 
<p>Tobias Bussmann in chat: Thanks all!</p>
 
 
<p>Grant Holly in chat: Thanks everyone. Great discussion</p>
 
 
<p>Jeremy S: in chat: Thanks david</p>
 
 
<p>Steven Miller in chat: Thanks all! Cya next time</p>
 
 
<p>Jeremy S in chat: Isn’t bagel supposed to come for the end</p>
 
 
<p>Ian Stanton in chat: Thanks all :)</p>
 
 
 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/ian-stanton/">Ian Stanton</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
                 <a href="https://justatheory.com/tags/trunk/">trunk</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   21:49 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 24. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://benwerd.medium.com/how-to-build-a-culture-that-supports-engineers-2f30fec413dc?source=rss-3b16402f5b9d------2'>How to build a culture that supports engineers</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

      And is productive for workers in every creative discipline  Continue reading on Medium »  
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/0*KnPo2VdZ0TTM1bdl.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   <p><a href="https://benwerd.medium.com/how-to-build-a-culture-that-supports-engineers-2f30fec413dc?source=rss-3b16402f5b9d------2"></a></p><p>And is productive for workers in every creative discipline</p><p><a href="https://benwerd.medium.com/how-to-build-a-culture-that-supports-engineers-2f30fec413dc?source=rss-3b16402f5b9d------2">Continue reading on Medium »</a></p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/0*KnPo2VdZ0TTM1bdl.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   02:44 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 22. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://identitywoman.net'>Identity Woman</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://identitywoman.net/real-names-glassdoor-fishbowl/'>Real Names &#038; Glassdoor &#038; Fishbowl</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   There is a Social Contract with sites and their name policies. Some how companies still don’t really understand this (See what WOW did in 2010) . There is a massive story breaking this week about how GlassDoor a site where people who can create profiles and anonymously rate their employers over their entire careers has […] 
 The post Real Names &amp; Glassdoor &amp; Fishbowl appeared first on Iden
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>There is a Social Contract with sites and their name policies. Some how companies still don’t really understand this (See what WOW did in 2010) . There is a massive story breaking this week about how GlassDoor a site where people who can create profiles and anonymously rate their employers over their entire careers has […]</p>
<p>The post <a href="https://identitywoman.net/real-names-glassdoor-fishbowl/">Real Names &amp; Glassdoor &amp; Fishbowl</a> appeared first on <a href="https://identitywoman.net">Identity Woman</a>.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   02:12 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 21. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/03/rfc-pgxn-metadata-sketch/'>RFC: PGXN Metadata Sketch</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Request for comments on a sketch of a new metadata standard for Postgres
extension packaging, distribution, and delivery, building on the
PGXN Meta Spec to address its shortcomings and
emerging use cases 12 years on.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Update 2024-03-22: To compliment high-level discussions on the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a>
channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a>, I also created a <a href="https://github.com/theory/justatheory/pull/4">pull request</a> to make it
easer directly comment to this post point by point. Keep the feedback coming,
and thank you!</p>
<p>Update 2024-03-25: Clarified the definition of “Release” and made
“Maintainers” plural. Thanks to Matthias van de Meent for the suggestions!</p>
 
<p>This post proposes a new metadata standard for extension packaging,
distribution, and delivery, building on the <a href="https://pgxn.org/spec/" title="PGXN Meta Spec - The PGXN distribution metadata specification">PGXN Meta Spec</a> to address its
shortcomings and emerging use cases 12 years on. The goals include:</p>
 
 Don’t break the existing standard, but add to it 
 Add explicit support for different types of Postgres extensions,
such as <a href="https://www.postgresql.org/docs/current/bgworker.html" title="PostgreSQL Docs: Background Worker Processes">background workers</a> and <a href="https://www.postgresql.org/docs/16/gist-extensibility.html">loadable modules</a> 
 Add additional metadata for curation and automated binary compilation 
 Add hooks for registry and third-party-contributed metadata, including
binary distribution options and stats &amp; reports. 
 
<p>There are two high level categories of metadata, split into two separate
files:</p>
 
 <a>Package Metadata</a> provided by extension package
authors 
 <a>Registry Metadata</a> aggregated by the root registry
from various sources, including data derived from the extension source
code or package metadata, but also trusted third parties such as
packagers, smoke testers, security scanners, and more. 
 
<p>Following community discussion of this proposal, the <a>Package
Metadata</a>  will lead to a draft for PGXN Meta Spec version
2.0.0, while the <a>Registry Metadata</a> will guide the design
and implementation of the Root Registry APIs required to provide it.</p>
 Terms 
<p>Definition of terms as used in this document, because every language and
distribution system uses terms differently. These are ours.</p>
 
 
<p>Extension: a software component that extends the capabilities of a
PostgreSQL database or cluster. Extensions may be <code>CREATE EXTENSION</code>
<a href="https://www.postgresql.org/docs/current/sql-createextension.html" title="PostgreSQL Docs: “CREATE EXTENSION”">extensions</a>, <a href="https://www.postgresql.org/docs/current/bgworker.html" title="PostgreSQL Docs: Background Worker Processes">background workers</a>, command-line apps, <a href="https://www.postgresql.org/docs/16/gist-extensibility.html">loadable modules</a>,
shared libraries, and more.</p>
 
 
<p>Package: A collection of extensions that are released, versioned, and
distributed together. Packages may be downloaded directly from version
control repositories or in <a href="https://en.wikipedia.org/wiki/Archive_file" title="Wikipedia: “Archive file”">archive files</a> generated by a release tag.</p>
 
 
<p>Package Path: Identifies a package, declared in the <a>package
metadata</a> file. A package path should describe both
what the package does and where to find it. Typically, a package path
consists of a repository root path — the directory that contains the
metadata file — and a directory within the repository.</p>
 
 
<p>Repository: The version control repository where the package is
developed, referenced as package path or URL.</p>
 
 
<p>Repository root path: The portion of the package path that corresponds
to the root directory of the version control repository where the package
is developed, and which contains the metadata file. Most package are
defined in their repository’s root directory, so this is usually the
entire path. For example, <code>github.com/example/pgtap</code> is the repository
root path for the package of the same name.</p>
 
 
<p>Source Distribution: The contents of a single package bundled together
with <a>package metadata</a> into distributable archive
file, usually named with the last part of the package path or the main
extension, a dash, and the version, e.g., <code>pgtap-1.14.3.zip</code>.</p>
 
 
<p>Binary Distribution: A binary software distribution of a package,
compiled and packaged for a particular OS version, platform, and hardware
architecture. Examples include <a href="https://en.wikipedia.org/wiki/Deb_(file_format)" title="Wikipedia: “Deb (file format)”">deb</a>, <a href="https://en.wikipedia.org/wiki/RPM_Package_Manager" title="Wikipedia: “RPM Package Manager”">RPM</a>, and <a href="https://packaging.python.org/en/latest/specifications/binary-distribution-format/" title="Python Packaging User Guide: Source distribution format">wheel</a>.</p>
 
 
<p>Release: A single version of the package made available to the public on
PGXN, expressed as the package path, an at sign, and the <a href="https://semver.org" title="Semantic Versioning 2.0.0">semver</a>. Example:
<code>github.com/theory/pgtap@v1.14.3</code>.</p>
 
 
 Package Metadata 
<p>The extension author provides this data as part of a release submitted to the
root registry. The registry itself will not change this data in any way, but
provide it as-is.</p>
 Essentials 
<p>Essential information about the package itself, including its path, name,
version, list of authors, license, etc. Ecosystem applications use this data
for indexing, naming conventions, and display information.</p>
<p>Fields (all required):</p>
 
 Package: A package path uniquely identifying the package (usually the
source repo host name and path; see <a href="https://justatheory.com/2024/03/extension-namespace-rfc/" title="Extension Registry Namespacing RFC">this RFC</a>) 
 Name: The name of the extension package, usually the same as the main
extension, not unique but conflicts are resolved by escalation to
<code>package</code> 
 Version: The version of the package, expressed as a <a href="https://semver.org" title="Semantic Versioning 2.0.0">SemVer</a> 
 Abstract: A short description of the purpose of the package 
 Description: A longer description of the package, answering the
question “what is this thing and what value is it?” 
 Maintainers: List of maintainers, each an object with <code>name</code> and either
<code>email</code> or <code>url</code> (or both) 
 License: An <a href="https://spdx.github.io/spdx-spec/v2.3/SPDX-license-expressions/" title="Annex D SPDX license expressions (Normative)">SPDX 2.3 license expression</a> comprised of one or more
licenses from the <a href="https://spdx.github.io/license-list-data/" title="PREVIEW - SPDX License List">SPDX License List</a> 
 Spec: Information about the metadata spec itself, a version and URL 
 
<p>Example:</p>
  <code>{
  "package": "github.com/example/pg-pair",
  "name": "pair",
  "version": "1.1.0",
  "abstract": "A key/value pair data type",
  "description": "Adds a key/value pair data type to Postgres, useful for specifying arbitrary key/value function parameters.",
  "maintainers": [
    {
        "name": "Naomi Nagata",
        "email": "naomi@example.com",
        "url": "https://example.com/roci/naomi"
    }
  ],
  "license": "PostgreSQL OR MIT",
  "spec": {
    "version": "2.0.0",
    "url": "https://pgxn.org/meta/spec.txt"
  }
}
</code>  <p>Other possibilities:</p>
 
 License File: An alternative to <code>license</code> for nonstandard licenses
(from <a href="https://doc.rust-lang.org/cargo/reference/manifest.html#the-license-and-license-file-fields" title="Cargo Manifest Format: The license and license-file fields">cargo</a>) 
 Contributors: List of people who have contributed to the package 
 Emeriti: List of former maintainers and contributors 
 
<p>PGXN Compatibility:</p>
 
 Overloads the string representation of the spec v1 field, but it should be
able to parse either one, especially with the spec version to
differentiate. 
 Uniqueness of <code>package</code> and relationship to <code>name</code> 
 PGXN has <code>maintainer</code>; is it really worth pluralizing it? Maybe we don’t
need it at all, given repository and issue links. 
 
 Artifacts 
<p>A list of links and checksums for downloading the extension package in one or
more formats, including source code, binaries, system packages, and more. Apps
use this information (along with <a>registry metadata</a>) to
determine the best option for installing an extension on a particular system.</p>
<p>This information is optional, and provided in addition to the source code
available from the root registry itself. The idea is that, on release, the
release manager creates artifacts in one or more locations (such as GitHub
releases), and it’s useful to link to them from the registry. Binary builds
may be unlikely today, but leave the option open for now by allowing arbitrary
text in the <code>type</code> field.</p>
<p>Each item must have:</p>
 
 Type: The type of artifact: Source, binary, RPM, etc. 
 URL: Link the the artifact 
 SHA: A SHA checksum 
 
<p>Each URL must properly resolve and the checksum must match.</p>
  <code>[
  {
    "type": "source",
    "url": "https://github.com/theory/pg-pair/releases/download/v1.1.0/pair-1.1.0.zip",
    "sha256": "2b9d2416096d2930be51e5332b70bcd97846947777a93e4a3d65fe1b5fd7b004"
  },
  {
    "type": "binary",
    "url": "https://github.com/theory/pg-pair/releases/download/v1.1.0/pair-1.1.0-linux-amd64.tar.gz",
    "sha1": "12d9bc5cfb6bc3c453627eac69511f48be63cfc0"
  },
  {
    "type": "binary",
    "url": "https://github.com/theory/pg-pair/releases/download/v1.1.0/pair-1.1.0-linux-arm64.tar.gz",
    "sha1": "787dc39137f7d1510a33ab0a1b8905cd5f3f72d1"
  }
]
</code>  <p>PGXN Compatibility:</p>
<p>New object, no conflicts.</p>
 Resources 
<p>External information about the package, mostly links, including source code
repository, bug reporting, documentation, badges, funding, etc. Apps use this
data for links, of course, but also full text indexing, documentation
rendering, and displaying useful information about the extension.</p>
<p>Fields:</p>
 
 Homepage: URL for the package’s home page 
 Issues: URL to issues 
 Documentation: URL to documentation 
 Support: URL to support 
 Repository: Source code repository URL 
 Badges: Array of badge link objects 
 Funding: Array of funding objects 
 
  <code>{
  "homepage": "https://pair.example.com",
  "issues": "https://github.com/example/pair/issues",
  "documentation": "https://pair.example.com/docs",
  "support": "https://github.com/example/pair/discussions",
  "repository": "https://github.com/example/pair",
  "badges": [
    {
      "alt": "Test Status",
      "src": "https://test.packages.postgresql.org/github.com/example/pair.svg"
    }
  ],
  "funding": [
    {
      "type" : "individual",
      "url" : "http://example.com/donate"
    },
    {
      "type" : "patreon",
      "url" : "https://www.patreon.com/example"
    }
  ]
}
</code>  <p>PGXN Compatibility:</p>
 
 <code>issues</code> replaces <code>bugtracker</code> and is just a URL. Can support either 
 <code>repository</code> is just a URL instead of an object. Can support either
but with the <code>package</code> key referencing the repository path perhaps
we don’t need to change it. 
 
 Contents 
<p>A description of what’s included in the package. Often a package consists of
multiple extensions, such as <a href="https://postgis.net" title="PostGIS">PostGIS</a>, which <a href="https://github.com/search?q=repo%3Apostgis%2Fpostgis+%22default_version+%3D%22&amp;type=code" title="postgis/postgis: control files">includes</a> <code>postgis</code>,
<code>postgis_tiger_geocoder</code>, <code>address_standardizer</code>, and more.</p>
<p>Furthermore, some extensions are not <code>CREATE EXTENSION</code>-type <a href="https://www.postgresql.org/docs/current/sql-createextension.html" title="PostgreSQL Docs: “CREATE EXTENSION”">extensions</a> at
all, such as <a href="https://www.postgresql.org/docs/current/bgworker.html" title="PostgreSQL Docs: Background Worker Processes">background workers</a>, command-line apps, <a href="https://www.postgresql.org/docs/16/gist-extensibility.html">loadable modules</a>,
shared libraries, and more. Each should be listed along with documentation
links where they differ from the package overall (or are simply more
specific).</p>
<p>Fields:</p>
 
 Extensions: Object describing <code>CREATE EXTENSION</code> <a href="https://www.postgresql.org/docs/current/sql-createextension.html" title="PostgreSQL Docs: “CREATE EXTENSION”">extensions</a>. Keys
are extension names, fields are objects with the fields <code>sql</code>, <code>doc</code>,
<code>abstract</code>, <code>control</code> (same as control file, but can use the file itself),
<code>tle</code> (meaning it can be used as a <a href="https://github.com/aws/pg_tle" title="pg_tle: Framework for building trusted language extensions for PostgreSQL">trusted language extension</a>), and
<code>preload</code> (to indicate whether its libraries need to be loaded in advance
via <code>shared_preload_libraries</code>, <code>session_preload_libraries</code>, or
<code>local_preload_libraries</code>). 
 Workers: Object describing background workers. Keys are worker names,
fields are objects with the fields <code>bin</code>, <code>doc</code>, and <code>abstract</code>. 
 Apps: Object describing applications, command-line or otherwise. Keys
are app names, fields are objects with the fields <code>bin</code>, <code>doc</code>, and
<code>abstract</code>. 
 Modules: Object describing <a href="https://www.postgresql.org/docs/16/gist-extensibility.html">loadable modules</a> that can be loaded into
Postgres (not necessary for extensions that include libraries). Keys are
module names, fields are objects with the fields <code>lib</code> (without file
extension), <code>doc</code>, <code>abstract</code>, and <code>preload</code>. 
 Libraries: Other libraries that may ship in the package and need to be
installed but are not <a href="https://www.postgresql.org/docs/16/gist-extensibility.html">loadable modules</a>, such as a dynamic library used
by an app. Keys are library names, fields are objects with the fields
<code>lib</code> (without file extension) or <code>dir</code> (for a directory of files), <code>doc</code>,
and <code>abstract</code>. 
 
  <code>{
  "extensions": {
    "pair": {
      "sql": "sql/pair.sql",
      "doc": "doc/pair.md",
      "abstract": "A key/value pair data type",
      "preload": "session",
      "tle": true,
      "control": {
        "directory": "pair",
        "module_pathname": "$libdir/pair",
        "relocatable": true
      }
    }
  },
  "workers": {
    "pair_pruner": {
      "bin": "bin/pair_pruner",
      "doc": "doc/pair_pruner.md",
      "abstract": "A worker to periodically prune pairs"
    }
  },
  "apps": {
    "pair_rand": {
      "bin": "bin/pair_rand",
      "doc": "doc/pair_rand.md",
      "abstract": "Command to generate random pairs of strings"
    }
  },
  "modules": {
    "lib_pair": {
      "lib": "lib/lib_pair",
      "doc": "doc/lib_pair.md",
      "abstract": "A library hooking function calls to convert pairs to named parameters",
      "load": "shared_preload_libraries"
    }
  },
  "libraries": {
    "ruby_pair": {
      "dir": "lib/gems",
      "abstract": "Ruby libraries required to run the extension"
    }
  }
}
</code>  <p>Other options:</p>
 
 Allow or require an array of docs? 
 Specify support for specific doc formats (markdown, manpage, plain text,
MkDocs, etc.)? 
 Support glob patterns? 
 Add libraries as sub-key for workers and apps, e.g. for a Ruby app that
requires a directory of gems? 
 
<p>PGXN Compatibility:</p>
<p>Significantly different from the existing <code>provides</code> object, so key this one
under <code>contents</code> or <a href="https://en.wikipedia.org/wiki/Manifest_file" title="Wikipedia: “Manifest file”"><code>manifest</code></a> and continue to support both.</p>
 Dependencies 
<p>A list of external dependencies required to configure, build, test, install,
and run the extensions in the package. These include not only other extension
packages, but also external libraries and system dependencies. And let’s not
forget the versions of Postgres required, as well as any OS and version
dependencies (e.g, does it work on Windows? FreeBSD? What versions?) and
architectures (<a href="https://en.wikipedia.org/wiki/AArch64" title="Wikipedia: AArch64">arm64</a>, <a href="https://en.wikipedia.org/wiki/amd64" title="Wikipedia: AMD64">amd64</a>, etc.)</p>
<p>Fields:</p>
 
 
<p>platforms: Platforms supported by the package, defined as OS and
architecture. Assume all platforms if none are listed.</p>
 
 
<p>postgres: Versions of PostgreSQL required by the package, specified as
a version range, as well as compiled features (<code>--with-*</code> configure
options)</p>
 
 
<p>pipeline: The build pipeline required to build the extension. Options:
<code>pgxs</code>, <code>meson</code> (?) <code>pgrx</code>, <code>gnu</code> (<code>./configure</code>), <code>gem</code>, <code>cpan</code>, <code>pip</code>,
<code>go</code>, etc.</p>
 
 
<p>prereqs: Object defining external dependencies required for different
phases:</p>
 
 configure: Dependencies to configure the package (e.g., items
required for <code>./configure</code> or <code>make</code> to work) 
 build: Dependencies to build the package (in addition to run
dependencies) 
 test: Dependencies to test the package (in addition to build and
run dependencies) 
 run: Dependencies to run the package 
 develop: Dependencies to develop the package (in addition to all
other phase dependencies) 
 
<p>Each points to an object with at least one of these keys:</p>
 
 requires: Required to use the package 
 recommends: Not required, but recommended as a best practice 
 suggests: Not required, but nice to have 
 conflicts: Package will not work with these items 
 
<p>These, in turn, contain at least one of these keys:</p>
 
 package: Other packages, expressed by their package paths,
pointing to a version expression 
 external: External dependencies not included with the package 
 contrib: List of Postgres contrib or development packages 
 
 
 
<p>variations: A list of dependency variations, each with two fields:</p>
 
 where: Specifies a configuration that, when matched, includes the
dependencies. For example, to specify alternate dependencies for Linux
arm64, it would be <code>"platforms": { "linux": "arm64" }</code>. For a specific
version of Postgres, it would be something like
<code>"postgres": { "version": "&gt;= 16, &lt; 17" }</code>. [This likely needs some
deep thinking through.] 
 dependencies: An object containing of the other key structures in
this object, defining the additional dependencies for environments
that match the <code>where</code> field. 
 
 
 
<p>The <code>packages</code> and <code>external</code> keys point to objects in which the keys are
dependency names and the values are version range expressions (empty string
means any version). The <code>postgres.version</code> field is also a version range
expression.</p>
<p>The <code>contrib</code> key points to an array of Postgres-supplied apps and extensions.
Versions are not necessary because they effectively have the same version as a
supported Postgres version.</p>
<p>Example:</p>
  <code>{
  "postgres": {
    "version": "&gt;= 12, &lt; 17",
    "with": [ "xml", "uuid", "perl" ]
  },
  "pipeline": "pgrx",
  "platforms": {
    "linux": [ "amd64", "arm64" ],
    "darwin": [ "amd64", "arm64" ],
    "windows": [ "amd64" ],
    "freebsd": [ "amd64" ]
  },
  "dependencies": {
    "configure": {
      "requires": {
        "external": { "cargo-pgrx": "" }
      }
    },
    "build": {
      "requires": {
        "external": {
          "curl": "",
          "awk": "",
          "sed": "",
          "perl": "5.20"
        }
      },
      "recommends": {
        "external": {
          "jq": "",
          "perl": "5.38"
        }
      }
    },
    "test": {
      "requires": {
        "contrib": [ "pg_regress", "plpgsql", "plperl" ],
        "packages": {
          "github.com/theory/pgtap": "1.1.0"
        }
      }
    },
    "run": {
      "requires": {
        "contrib": [ "plperl" ],
        "packages": {
          "github.com/theory/hostname": ""
        }
      }
    }
  }
}
</code>  <p>Notes:</p>
<p>The <code>external</code> field is the tricky one. How does one provide a canonical
dependency for a third-party library or app that may have loads of different
names on different platforms, different packaging systems (apt vs rpm), and
even different versions of them, or split apart for development and runtime
use? Some possibilities:</p>
 
 Use <a href="https://github.com/package-url/purl-spec" title="purl-spec: A minimal specification for purl a.k.a. a package “mostly universal” URL">purl</a> and let downstream services handle the conversion 
 Create a rule set that allows one to predict dependency name changes
across ecosystems, similar to <a href="https://github.com/repology/repology-rules" title="Package normalization ruleset for Repology">repology-rules</a> 
 Just use <a href="https://repology.org" title="Repology, the packaging hub">Repology</a>? 
 
<p>Other notes:</p>
 
 Decide on a version range standards to adopt. Start with the <a href="https://pgxn.org/spec/#Version.Ranges" title="PGXN Meta Spec: Version Ranges">PGXN version
range</a>, <a href="https://peps.python.org/pep-0508/" title="PEP 508 – Dependency specification for Python Software Packages">pep-0508</a>, <a href="https://docs.npmjs.com/cli/v6/using-npm/semver#ranges" title="npm docs: semver advanced range syntax">npm ranges</a>, and <a href="https://ruby-doc.org/stdlib-2.5.0/libdoc/rubygems/rdoc/Gem/Version.html">Gem::Version</a>. 
 Is <code>pipeline</code> really necessary, given configure requirements? I think so,
because it tells the client the preferred build system to use, in case it
can’t detect it for some reason. 
 
<p>PGXN Compatibility:</p>
<p>Significantly differs from the existing <code>prereqs</code> object, so this one is keyed
under <code>dependencies</code> and we can support either.</p>
 Classification 
<p>Classification metadata lets the extension developer associate additional
information to improve discovery, such as key words. It might also allow
selections from a curated list of extension classifications, such as the
<a href="https://crates.io/category_slugs" title="crates.io valid category slugs">category slugs</a> supported for the <a href="https://doc.rust-lang.org/cargo/reference/manifest.html#the-categories-field" title="Cargo Manifest Format: The categories field">cargo categories field</a>.</p>
<p>Ecosystem apps use this data to organize extensions under key words or
categories, making it easier for users to find extensions that are frequently
used together or for specific workloads or tasks.</p>
<p>Example:</p>
  <code>{
  "tags": [
    "testing",
    "pair",
    "parameter"
  ],
  "categories": [
    "Machine Learning"
  ]
}
</code>  <p>PGXN Compatibility:</p>
<p>Conflicts with the existing root-level <code>tags</code> array, but we can support either
one.</p>
 Example: PGML Extension 
<p>A hand-coded example that attempts to capture all the complexity of the <a href="https://postgresml.org/docs/resources/developer-docs/contributing#postgres-extension" title="PostgresML Contributing: Postgres extension">pgml
extension</a>:</p>
  <code>{
  "package": "github.com/postgresml/postgresml/pgml-extension",
  "name": "pgml",
  "version": "2.8.2",
  "abstract": "pgml: Created by the PostgresML team",
  "maintainers": [
    {
      "name": "the PostgresML team",
      "url": "https://github.com/postgresml/postgresml/"
    }
  ],
  "license": "MIT",
  "spec": {
    "version": "2.0.0",
    "url": "https://pgxn.org/meta/spec.txt"
  },
  "artifacts": [
    {
      "type": "source zip",
      "url": "https://github.com/postgresml/postgresml/archive/refs/tags/v2.8.2.zip",
      "sha256": "2b9d2416096d2930be51e5332b70bcd97846947777a93e4a3d65fe1b5fd7b004"
    },
    {
      "type": "source tgz",
      "url": "https://github.com/postgresml/postgresml/archive/refs/tags/v2.8.2.tar.gz",
      "sha256": "845f28339c6159ac32daccea1cd17b386ea083c3e60bb8d58fb737725afe7eb5"
    }
  ],
  "resources": {
    "homepage": "https://postgresml.org/",
    "issues": "https://github.com/postgresml/postgresml/issues",
    "documentation": "https://postgresml.org/docs/",
    "support": "https://discord.com/invite/DmyJP3qJ7U",
    "repository": "https://github.com/postgresml/postgresml",
    "badges": [
      {
        "alt": "Tests Passing",
        "src": "https://github.com/postgresml/postgresml/actions/workflows/ci.yml/badge.svg"
      },
      {
        "alt": "Chat Activity",
        "src": "https://camo.githubusercontent.com/1988e7aaaa6a0c8ddcf880faacf1ec3263e23fac4aeff4710922082fde84442a/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f31303133383638323433303336393330303939",
        "link": "https://discord.gg/DmyJP3qJ7U"
      }
    ]
  },
  "manifest": {
    "extensions": {
      "pgml": {
        "sql": "sql/pgml--2.8.2.sql",
        "preload": "shared",
        "tle": false,
        "control": {
          "comment": "pgml:  Created by the PostgresML team",
          "default_version": "2.8.2",
          "module_pathname": "$libdir/pgml",
          "relocatable": false,
          "superuser": true,
          "schema": "pgml",
          "trusted": true
        }
      }
    }
  },
  "dependencies": {
    "postgres": {
      "version": "&gt;= 15, &lt; 16"
    },
    "pipeline": "pgxs",
    "platforms": {
      "linux": [
        "amd64",
        "arm64"
      ],
      "darwin": [
        "amd64",
        "arm64"
      ]
    },
    "dependencies": {
      "configure": {
        "requires": {
          "external": {
            "cargo-pgrx": "",
            "bison": "",
            "cmake": "",
            "flex": "",
            "libclang-dev": "",
            "libopenblas-dev": "",
            "libpython3-dev": "",
            "libreadline-dev": "",
            "libssl-dev": "",
            "pkg-config": ""
          }
        }
      },
      "run": {
        "requires": {
          "external": {
            "libopenblas": "",
            "libpython3": "",
            "libreadline": "",
            "libssl": "",
            "python3": ""
          }
        },
        "recommends": {
          "external": {
            "python(pyarrow)": "=11.0.0",
            "python(catboost)": "",
            "python(lightgbm)": "",
            "python(torch)": "",
            "python(torchaudio)": "",
            "python(torchvision)": "",
            "python(xgboost)": "",
            "python(accelerate)": "",
            "python(bitsandbytes)": "",
            "python(ctransformers)": "",
            "python(huggingface-hub)": "",
            "python(deepspeed)": "",
            "python(einops)": "",
            "python(optimum)": "",
            "python(peft)": "",
            "python(tokenizers)": "",
            "python(transformers)": "",
            "python(transformers-stream-generator)": "",
            "python(InstructorEmbedding)": "",
            "python(sentence-transformers)": "",
            "python(rouge)": "",
            "python(sacrebleu)": "",
            "python(sacremoses)": "",
            "python(datasets)": "",
            "python(orjson)": "",
            "python(langchain)": ""
          }
        }
      }
    },
    "variations": [
      {
        "where": {
          "platforms": {
            "linux": []
          }
        },
        "dependencies": {
          "prereqs": {
            "run": {
              "recommends": {
                "external": {
                  "python(auto-gptq)": "",
                  "python(xformers)": ""
                }
              }
            }
          }
        }
      }
    ]
  },
  "classification": {
    "tags": [
      "machine learning",
      "ml",
      "transformers"
    ],
    "categories": [
      "Machine Learning"
    ]
  }
}
</code>  <p>Note that PostgresML also provides a dashboard app, but it does not ship with
the extension, so it’s not listed here. It could have its own package metadata
and be separately distributed.</p>
<p>For the Python dependencies, in the absence of a neutral standard for
specifying package, here they’re specified using a syntax borrowed from RPM to
make clear that they’re Python dependencies. A couple of those dependencies
are Linux-only, so listed in <code>variations</code> where the OS is Linux.</p>
 Registry Metadata 
<p>The second metadata format is registry metadata. It starts by with a copy the
<a>package metadata</a>, but builds from there, adding
information (such as the artifact link, SHA, and cryptographic signature from
the registry itself) and sections (defined below).</p>
<p>Providing a separate metadata format enables broader community collaboration
to augment the metadata for extension packages.</p>
<p>Ideally an ecosystem of trusted services will be developed by various parties,
who will report back to the root registry as the source of record for all
metadata about extension packages. For example, a binary distribution service
like <a href="https://wiki.postgresql.org/wiki/Apt" title="PostgreSQL packages for Debian and Ubuntu">apt.postgresql.org</a> or <a href="https://pgt.dev" title="Trunk — A Postgres Extension Registry">trunk</a> will submit links and checksums for
binary packages and perhaps periodic download stats.</p>
<p>Broadly speaking, there are three typologies for registry metadata:
Distributions, Reports, and Classifications.</p>
 Distributions 
<p>A list of links and checksums for downloading binary distributions for the
extension package. The root registry page for the package can therefore list
all the ways to install an extension and expand the list as new binary
registries add it.</p>
<p>Along with the <a>Artifacts</a> package metadata, the distribution
metadata allows installer tools to select the best option to install based on
the OS, Postgres version, and cluster configuration (e.g., install the
community Apt package if the cluster was installed from the community Apt
registry).</p>
<p>Each item must have:</p>
 
 Registry: The name of the distribution registry, unique 
 Type: The type of distribution: Apt, RPM, TLE, etc. 
 URL: Link to the the registry’s page for the distribution 
 Command: In lieu of a URL and SHA it can provide instructions to
install the distribution, e.g., <code>apt-get install postgresql16-pair</code>. 
 
<p>Each distribution registry must be trusted by the root registry to submit this
information to add to the metadata, and have its own page with additional
contextual information: its home page, docs, basic instructions, whatever. A
README of sorts.</p>
<p>Example:</p>
  <code>[
  {
    "registry": "trunk.pgxn.org",
    "type": "trunk",
    "command": "pgxn install github.com/example/pair"
  },
  {
    "registry": "apt.postgresql.org",
    "type": "apt",
    "command": "apt-get install postgresql16-pair"
  },
  {
    "registry": "yum.postgresql.org",
    "type": "rpm",
    "command": "yum install postgresql16-pair"
  },
  {
    "registry": "pgt.dev",
    "type": "apt",
    "url": "https://pgt.dev/extensions/pair",
    "command": "trunk install pair"
  },
  {
    "registry": "pgxman.com",
    "type": "apt",
    "url": "https://pgxman.com/x/pair",
    "command": "pgxman install pair"
  },
  {
    "registry": "database.dev",
    "type": "sql",
    "url": "https://database.dev/example/pair",
    "command": "SELECT dbdev.install('example-pair');"
  },
]
</code>  <p>Other Options:</p>
 
 Information about installing from a registry that doesn’t offer URLs, like
Homebrew, where one just needs to know what the formula is called. Maybe
the same should apply to Apt? 
 
 Metrics and Reports 
<p>This object includes data such as number of downloads, build and test status
on various Postgres/OS/version combinations, binary packaging distributions,
test coverage, security certification, vulnerability detection, quality
metrics and user ratings, and more. Some of these bits of data might include
badges, e.g., for ratings or security assessments.</p>
<p>In the broader ecosystem, it would be the responsibility of the root registry
to ensure that data comes only from trusted sources. However this data will be
publicly readable, allowing any downstream applications to extend and
publish metrics and reports with their own information.</p>
<p>Example:</p>
  <code>{
    "aggregates": {
      "downloads": 32
    },
    "sources": {
      "smoke-tester.example.com": {
        "stats": {
          "passes": 32,
          "fails": 8,
          "unknown": 0
        },
        "link": "https://smoke-tester.example.com/extension/pair",
        "badge": "https://smoke-tester.example.com/extension/pair.svg"
      },
      "pgt.dev": {
        "downloads": 12,
        "tags": ["two", "kv"],
        "categories": ["Analytics"]
      },
      "pgxman.com": {
        "downloads": 20
      },
      "ratings.example.com": {
        "stats": {
          "ratings": 324,
          "average": 3.2,
          "comments": 13
        },
        "link": "https://ratings.example.com/extension/pair",
        "badge": "https://ratings.example.com/extension/pair.svg"
      },
      "stats.example.com": {
        "contributors": 2,
        "stars": 14,
        "last_active": "2024-01-12",
      },
      "security-scan.example.com": {
        "advisories": {
          "high": 2,
          "moderate": 11,
          "low": 1,
          "cves": 0
        },
        "link": "https://security-scan.example.com/extension/pair",
        "badge": "https://security-scan.example.com/extension/pair.svg"
      }
    }
}
</code>  <p>Notes:</p>
 
 
<p>The <code>aggregates</code> section aggregates results from multiple sources, for
example summing all downloads or averaging ratings. The list of items to
aggregate could evolve regularly.</p>
 
 
<p>Each key in <code>sources</code> identifies a trusted downstream source of
information. Each would have its own schema describing its objects and
their meaning, along with URI templates to link to. For example,
<code>stats.example.com</code> might have these templates:</p>
  <code>{
  "contributors": {
    "alt": "List of people who have contributed to the {name} project",
    "badge": "https://stats.example.com/{ package }/contributors.svg",
    "uri": "https://stats.example.com/{ package }/contributors"
  },
  "stars": {
    "alt": "Star ratings for the {name} project",
    "badge": "https://stats.example.com/{ package }/stars.svg",
    "uri": "https://stats.example.com/{ package }/stars"
  }
}
</code>   
 
<p>Might be worth adopting more of an API of multiple files, rather than one big file.
Example: <a href="https://formulae.brew.sh/docs/api/#analytics" title="Homebrew JSON API Documentation: Analytics">Homebrew analytics</a>. Note also its support for date ranges for analytics
(30/90/365 days, stats when directly installed vs. installed as a dependency).</p>
 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/packaging/">Packaging</a> 
                 <a href="https://justatheory.com/tags/metadata/">Metadata</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   18:50 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 21. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-86'>The Pulse #86: Is Shopify’s new “mastery” framework a response to higher interest rates?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The e-commerce giant is taking a new and different approach to career growth and promotions. Also: more evidence the tech job market is tougher, and AI consolidation is already underway
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering insights, patterns, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Today, we cover:</p>  <p>Industry pulse. NVIDIA launches an even more powerful AI chip, The US sues Apple for abusing its smartphone market position, Dell says you can’t work remotely AND get a promotion, and more.</p>  <p>Shopify’s Mastery framework: a response to higher interest rates? The e-commerce giant is taking a new approach to career growth and promotions. I’ve talked with insiders about how it’s been received by them, after publishing <a href="https://newsletter.pragmaticengineer.com/p/inside-shopifys-leveling-split">a look into the levels of the Mastery framework, last year</a>.</p>  <p>More evidence the tech job market is tougher. Carta’s report on startup hiring and compensation shows in black-and-white what we’ve long suspected: 2023 really was bad for job seekers. It was the first time in many years that startups shrunk in size by hiring fewer people than who left. Equity compensation also dropped, although salaries increased.</p>  <p>Are we at an AI consolidation “inflection” point, already? The AI industry seems to be speedrunning the creation of categories: rapid fundraising, fast product launches fast, swift regulation, and then… consolidation? Inflection AI raised $1.3B just 9 months ago, and now Microsoft has basically acquired the company in an unusual way which doesn’t trigger antitrust rules.</p>   1. Industry pulse  NVIDIA launches an even more powerful AI chip <p>Producing the world’s most sought-after AI supercomputer – the H100 – isn’t not enough, evidently: NVIDIA has <a href="https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html">unveiled</a> an even more powerful one, called Blackwell (a chip called GB200). Blackwell offers better performance: around 5 times the computational power of the H100 (20 petaflops – <a href="https://en.wikipedia.org/wiki/FLOPS">floating point operations per second</a> – versus 4 petaflops)</p><p>The interesting part of the announcement is that NVIDIA also announced an “inference microservice.” This will allow companies to run inference on their own models. This is much cheaper than training a model, and would allow companies buying NVIDIA GPUs to have a cheaper option than paying services like OpenAI, Atrophic, or others, to use their models.</p><p>NVIDIA is already the de facto hardware provider for AI applications, and is now moving up the stack to offer more software services, and take an even bigger cut of revenue. At this rate, I wouldn’t be surprised if NVIDIA becomes a competitor to AI model companies like OpenAI!</p> Apple refuses to let Spotify’s app update go live <p>Two weeks ago, the European Commission <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-84">fined Apple $2B</a> for anti-competitive practices because of restrictions it placed on music streaming companies like Spotify. The EU also mandated that Apple needs to allow Spotify to display pricing information for EU users.</p><p>Apple still has not approved an app update which Spotify submitted on 5 March, whose approval is mandated by the EU. Apple doesn’t offer exact timelines on how long it takes to approve an app, and approvals typically happen within a day. Apple has used the tactic of delaying app updates or rejecting them as a way to send a message to companies whose actions it doesn’t like. Spotify knows this all too well: in 2016-17 Apple made “more frequent, unexpected and unjustified rejections” to their app updates.</p><p>Apple is doing it again by delaying the latest update for obvious reasons. It’s yet another reminder to Spotify and other app developers that in the App Store, Apple is king. The EU might fine them $2B, it might mandate that Spotify needs to be allowed to show pricing information; but the EU cannot mandate how fast Apple grants approvals, right?</p><p>This is how Apple might slowly but surely <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-84">become the most-hated company by its own developers</a>; by having an overlord mentality and bullying its own customers. In this way, it feels that Apple is turning into the Microsoft of the 1990s, in terms of rock-bottom developer sentiment.</p><p>Apple is making clear why it needs to be regulated – and strictly. A good regulator wants to regulate as lightly as possible. However, what happens when one of the biggest players acts in bad faith, in not following the spirit of regulation?</p><p>Then, there’s no choice but to create more granular regulation, even to the point of “micromanaging.” This regulation will be more invasive and have more side effects. Apple, at this point, is dragging regulators towards making stricter rules. I think this will happen, and the tech giant will have only itself to blame.</p> The US sues Apple for abusing smartphone market position <p>Apple might have chosen a bad time to anger regulators, as it’s no longer just the EU that is stepping up against the Coupertino giant. The US Department of Justice has filed a lawsuit arguing that “Apple exercises its monopoly power to extract more money from consumers, developers, content creators, artists, publishers, small businesses, and merchants, among others.”</p><p>The goal of <a href="https://www.justice.gov/opa/media/1344546/dl?inline">the lawsuit</a> is stated as follows:</p><blockquote><p>“This case is about freeing smartphone markets from Apple’s anticompetitive and exclusionary conduct and restoring competition to lower smartphone prices for consumers, reducing fees for developers, and preserving innovation for the future.”</p></blockquote><p>Basically, the US has a similar issue with Apple as the EU does; Apple became the dominant smartphone platform in the US through innovation, but has since resorted to throttling competition in a way that the regulator says is unlawful.</p><p>This US lawsuit will teach us a lot more about Apple’s alleged anti-competitive practices, and how this secretive company really operates on the inside. If Apple follows a similar strategy as in the EU of ignoring the regulator and provoking a response, then it will backfire.</p> Dell: “Want a promotion? Come to the office” 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-86">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:51 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2503'>Eight Specifications Published in Preparation for IETF 119</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  My co-authors and I published updated versions of eight specifications in preparation for IETF 119 in Brisbane. The specifications span three working groups: JOSE, COSE, and OAuth. The updated specifications and outcomes when discussed at IETF 119 are as follows. 1, 2, &amp; 3: JSON Web Proof, JSON Proof Algorithms, and JSON Proof Token. Updates […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>My co-authors and I published updated versions of eight specifications in preparation for <a href="https://www.ietf.org/how/meetings/119/">IETF 119 in Brisbane</a>.  The specifications span three working groups:  <a href="https://datatracker.ietf.org/wg/jose/about/">JOSE</a>, <a href="https://datatracker.ietf.org/wg/cose/about/">COSE</a>, and <a href="https://datatracker.ietf.org/wg/oauth/about/">OAuth</a>.  The updated specifications and outcomes when discussed at IETF 119 are as follows.</p>
<p>1, 2, &amp; 3: <a href="https://www.ietf.org/archive/id/draft-ietf-jose-json-web-proof-03.html">JSON Web Proof</a>, <a href="https://www.ietf.org/archive/id/draft-ietf-jose-json-proof-algorithms-03.html">JSON Proof Algorithms</a>, and <a href="https://www.ietf.org/archive/id/draft-ietf-jose-json-proof-token-03.html">JSON Proof Token</a>.  Updates were:</p>
 
 Normatively defined header parameters used 
 Populated IANA Considerations sections 
 Allowed proof representations to contain multiple base64url-encoded parts 
 Specified representation of zero-length disclosed payloads 
 Added Terminology sections 
 Updated to use <a href="https://www.ietf.org/archive/id/draft-irtf-cfrg-bbs-signatures-05.html">draft-irtf-cfrg-bbs-signatures-05</a> 
 Updated to use <a href="https://www.ietf.org/archive/id/draft-ietf-cose-bls-key-representations-04.html">draft-ietf-cose-bls-key-representations-04</a> 
 More and better examples 
 Improvements resulting from a full proofreading 
 
<p>Continued reviews and feedback from implementations are requested.</p>
<p>4: <a href="https://www.ietf.org/archive/id/draft-ietf-jose-fully-specified-algorithms-02.html">Fully-Specified Algorithms for JOSE and COSE</a>.  Updates were:</p>
 
 Published initial working group document following adoption 
 Added text on fully-specified computations using multiple algorithms 
 Added text on KEMs and encapsulated keys 
 Updated instructions to the designated experts 
 
<p>It was agreed during the JOSE meeting to describe what fully-specified algorithms for ECDH would look like, for consideration by the working group.</p>
<p>5: <a href="https://www.ietf.org/archive/id/draft-ietf-oauth-resource-metadata-03.html">OAuth 2.0 Protected Resource Metadata</a>.  Updates were:</p>
 
 Switched from concatenating <code>.well-known</code> to the end of the resource identifier to inserting it between the host and path components of it 
 Have <code>WWW-Authenticate</code> return <code>resource_metadata</code> URL rather than <code>resource</code> identifier 
 
<p>It was decided to start working group last call during the OAuth meeting.</p>
<p>6: <a href="https://www.ietf.org/archive/id/draft-ietf-cose-typ-header-parameter-04.html">COSE “typ” (type) Header Parameter</a>.  Updates were:</p>
 
 Added language about media type parameters 
 Addressed working group last call comments 
 Changed requested assignment from 14 to 16 due to conflict with a new assignment 
 Addressed GENART, OPSDIR, and SECDIR review comments 
 
<p>This document is scheduled for the April 4, 2024 IESG telechat.</p>
<p>7: <a href="https://www.ietf.org/archive/id/draft-ietf-cose-bls-key-representations-05.html">Barreto-Lynn-Scott Elliptic Curve Key Representations for JOSE and COSE</a>.  Updates were:</p>
 
 Changed to use key type <code>EC</code> for JOSE and equivalent <code>EC2</code> for COSE for uncompressed key representations 
 Changed identifier spellings from “Bls” to “BLS”, since these letters are people’s initials 
 
<p>We received feedback to not add compressed key representations to the draft.</p>
<p>8: <a href="https://www.ietf.org/archive/id/draft-rha-jose-hpke-encrypt-06.html">Use of Hybrid Public-Key Encryption (HPKE) with JavaScript Object Signing and Encryption (JOSE)</a>.  Updates were:</p>
 
 Use existing <code>"alg": "dir"</code> value for HPKE Direct Encryption mode 
 Aligned choices more closely with those of <a href="https://www.ietf.org/archive/id/draft-ietf-cose-hpke-07.html">Use of Hybrid Public-Key Encryption (HPKE) with CBOR Object Signing and Encryption (COSE)</a> 
 Defined both Integrated Encryption mode and Key Encryption mode 
 Added IANA Considerations section 
 Removed Post-Quantum Considerations 
 
<p>It was decided to start a working group call for adoption during the JOSE meeting.</p>
<p>Thanks to all who contributed to the progress made on these specifications, both before and during IETF 119!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   02:05 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 19. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/03/extension-namespace-rfc/'>Extension Registry Namespacing RFC</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A proposal for an additional level of name uniqueness for Postgres
extension packaging and distribution, based on URIs.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>A few weeks ago I <a href="https://justatheory.com/2024/02/decentralized-extension-publishing/" title="Contemplating Decentralized Extension Publishing">brainstormed</a> about decentralized Postgres extension
publishing, inspired in part by an examination of <a href="https://go.dev/doc/modules/developing#decentralized" title="go.dev: Developing and publishing modules">Go decentralized publishing</a>.
It was…a lot. I’ve been deeply <a href="https://www.pgevents.ca/events/pgconfdev2024/schedule/session/91/" title="pgconf.dev: “The Future of the Extension Ecosystem”">pondering the future</a> of PGXN and the
broader extension ecosystem, and want to start to nail down some decisions. To
that end, I’d like to propose an update to extension namespacing.</p>
 Status Quo 
<p>There are currently three ways in which an extension is considered unique:</p>
 
 Only one extension can have a given name within a single Postgres cluster.
Names are defined by the name of the <a href="https://www.postgresql.org/docs/current/extend-extensions.html#EXTEND-EXTENSIONS-FILES" title="PostgreSQL Docs: Extension Files">control file</a>. It is therefore not
possible to have two extensions with the same name in the same Postgres
cluster. 
 <a href="https://pgxn.org" title="PGXN — PostgreSQL Extension Network">PGXN</a> follows this pattern: Only one extension can have a given name in the
PGXN registry. The first person to release an extension then “owns” its
name, and no one else can release an extension with the same name.<a>1</a>
I think <a href="https://database.dev" title="The Database Package Manager">dbdev</a> follows the same pattern. 
 Other registries like <a href="https://pgt.dev" title="Trunk — A Postgres Extension Registry">trunk</a> and <a href="https://pgxman.com/" title="npm for PostgreSQL">pgxman</a> define an extension by the
distribution name, at least for the purposes of selecting a binary to
install. Thus when you <code>trunk install postgis</code>, you get all of the
extensions included, as you’d expect, while
<code>trunk install address_standardizer</code> wouldn’t work at all. In the few places
that <code>trunk</code> supports installation by extension name, it prompts the user to
use the appropriate package name if there’s a conflict. 
 
 A Modest Proposal 
<p>I’d like to propose the following changes to the <a href="https://pgxn.org/spec">PGXN Meta Spec</a> to start to
move away from extension uniqueness in the broader extension ecosystem and more
toward package name.</p>
 
 Add a new field, call it <code>module_path</code>, <code>project_path</code>, <code>project_uri</code>, that,
if present, uniquely identifies an extension project and all of its parts.
It should be to a <a href="https://go.dev/ref/mod#module-path">Go-style module path</a> (or URI) that identifies the
project repository path where a <code>META.json</code> file lives. 
 Retain the <a href="https://pgxn.org/spec#provides" title="PGXN Meta Spec: provides"><code>provides</code></a> object where keys identify extensions, but those
keys will no longer be globally unique to the registry. In other words, the
combination of <code>module_path</code> and extension name uniquely identifies an
extension, including an empty <code>module_path</code>. 
 
 How it Works 
<p>Some examples. Let’s say there is an existing extension named <code>pair</code>, included
in the distribution named <code>pg_pair</code>:</p>
  <code>{
  "name": "pg_pair",
  "version": "1.2.3",
  "provides": {
    "pair": {
      "file": "pair.sql",
      "version": "1.2.0"
    }
  }
}
</code>  <p>The extension name <code>pair</code> is unique, and <code>pgxn install pair</code> will download
the pg_pair v1.2.3 bundle and compile and install pair v1.2.0.</p>
<p>Now someone else comes along and wants to make their own pair with this
metadata:</p>
  <code>{
  "name": "my_pair",
  "version": "0.2.3",
  "provides": {
    "pair": {
      "file": "pair.sql",
      "version": "0.2.3"
    }
  }
}
</code>  <p>Just like today, this upload would be rejected, because there is already a
registered <code>pair</code> extension. Under my proposal, they can disambiguate by
providing a <code>module_path</code>:</p>
  <code>{
  "name": "my_pair",
  "module_path": "github/example/pair",
  "version": "0.2.3",
  "provides": {
    "pair": {
      "file": "pair.sql",
      "version": "0.2.3"
    }
  }
}
</code>  <p>This upload would be allowed. With these two releases, someone attempting to
install <code>pair</code> would see something like this:</p>
  <code>$ pgxn install pair
ERROR: Duplicate extension name “pair”. Install one of these instead:
       * pgxn.org/dist/pair
       * github/example/pair
</code>  <p>Note the the module path <code>pgxn.org/dist/pair</code> in the the first option. This is
the default module path for distributions without a module path.<a>2</a> But
now the user can select the proper one to install:</p>
  <code>$ pgxn install pgxn.org/dist/pair
INFO: latest version: pgxn.org/dist/pair@1.2.3
INFO: building extension
INFO: installing extension
INFO: done!
</code>  <p>Furthermore, the PGXN client will prevent the user from later installing a
conflicting extension. The failure would look something like:</p>
  <code>$ pgxn install github/example/pair
INFO: latest version: pgxn.org/dist/pair@0.2.3
ERROR: Cannot install extension “pair” from pgxn.org/dist/pair:
ERROR: A conflicting extension named “pair” is already installed
ERROR: from pgxn.org/dist/pair
</code>   Features with Benefits 
<p>I see a number of benefits to this change:</p>
 
 Compatibility with the v1 metadata spec, so that no data migration or
distribution indexing is required. 
 It loosens up extension namespacing (or name registration, if you prefer)
while adding additional metadata to help users evaluate the quality of an
extension. For example, does it come from a well-known developer? You can
see it right in the module path. 
 It creates a pattern to eventually allow auto-indexing of extensions. For
example, if you run <code>pgxn install github.com/example/pew</code>, and PGXN doesn’t
have it, it can look for a <code>META.json</code> file in that repository and, if it
exists, and there’s a semver release tag, it could try to index it and let
the user install it. There are ownership issues to be worked out, but it
has possibilities. 
 It preserves the Postgres core concept of extension identity while putting
in place a well-established (by <a href="https://go.dev/ref/mod" title="Go Modules Reference">Go modules</a> and widespread use of URIs in
general) that the Postgres core could eventually adopt to allow more
flexible extension namespacing. 
 
 Request for Comments 
<p>What do you think? Good idea? Terrible idea? Please hit me with your thoughts
<a href="https://xoxo.zone/@theory" title="@theory@xoxo.zone">on Mastodon</a>, or via the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a>. I’d
like to get this decision (and a few others, stay tuned!) nailed down soon and
start development, so don’t hesitate? I need your help to prevent me from making
a huge mistake.</p>
 
 
 
 
<p>Unless the owner would like to share ownership with someone else, in
which case they can email me to request that another user be granted
“co-ownership”. They can also request to transfer ownership to another user,
after which the original owner will no longer be able to release the
extension.&nbsp;<a>↩︎</a></p>
 
 
<p>Or, if the <code>META.json</code> file has a <a href="https://pgxn.org/spec#resources" title="PGXN Meta Spec: resources">repository resource</a> with a
URL, PGXN could index it as the implied module path. Or, failing that, maybe
it should fall back on the distribution name instead of a <code>pgxn.org</code> path,
and prompt with <code>pg_pair/pair</code>.&nbsp;<a>↩︎</a></p>
 
 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/pgxn-v2/">PGXN v2</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/go/">Go</a> 
                 <a href="https://justatheory.com/tags/namespacing/">Namespacing</a> 
                 <a href="https://justatheory.com/tags/registry/">Registry</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   21:55 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2024/03/5673/'>XNS 技術仕様書1.0版(2002-07-09)が出てきたので貼っておく</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  ファイルサーバをほっていたら、XNS Tecnical Specs version 1.0(2002年7月9日版) が出てきたのでここに貼っておきます。 XDI.orgのサイトが生きていればそこからダウンロードできればよかったのですが、サイトが放棄されてしまっているので、わたしが…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>ファイルサーバをほっていたら、XNS Tecnical Specs version 1.0(2002年7月9日版) が出てきたのでここに貼っておきます。</p>



<p>XDI.orgのサイトが生きていればそこからダウンロードできればよかったのですが、サイトが放棄されてしまっているので、わたしが代わりに置き場所を提供します。</p>



<p>XNSは後にXRI/XDIになり、OpenID 2.0のコア仕様となった技術です。現在のDIDもその延長線上にあります。わりと参考になることが色々あると思います。昨今分散IDとかいう人はこれは読んでおいても良い仕様かとおもいます。</p>



 <a href="https://www.sakimura.org/wp-content/uploads/2024/03/XNS_Technical_Specs.pdf">XNS_Technical_Specs</a><a href="https://www.sakimura.org/wp-content/uploads/2024/03/XNS_Technical_Specs.pdf">ダウンロード</a> 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:20 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/tech-compensation'>Senior-and-Above Compensation in Tech</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  How well does tech pay, really? A deep look into almost 1,000 data points sent in by engineering managers, senior+ engineers, VP, and C-level folks in tech, mostly working in software engineering
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>In April 2023, I invited senior-and-above readers to share their compensation numbers, to research an accurate account of pay at startups, scaleups, and publicly traded companies. The survey was a collaboration with VC firm Creandum, which created an interesting report about early-stage compensation in Europe. Creandum shared a report, based on this data last year: <a href="https://blog.creandum.com/compensation-decoded-tech-leader-salaries-at-early-stage-startups-7faf5465d5b4">Compensation Decoded: Tech leader salaries at early-stage startups.</a></p><p>I’m delighted to share what – I hope – is the most detailed-possible tech compensation report based on latest-available figures. I’ve taken time to slice and dice the data; so strap in because today we’re talking numbers – lots of them! We cover:</p>  <p>Overview of sources. 951 data points, mainly from engineering managers and senior+ engineers in the US, EU and UK.</p>  <p>Base salary, cash compensation, total compensation. A summary of how these compare at the 75th and 90th median percentiles.</p>  <p>Regional pay comparison. Unsurprisingly, the US pays best by quite a distance. However, other regions pay $200K+ in total compensation at the 75th percentile.</p>  <p>Compensation by seniority. How seniors with 5-10 years of experience, those with 10+ years, and executives, are paid across the globe: US, UK, EU, India, Asia, Australia and New Zealand, Latin America, Africa.</p>  <p>Compensation by funding stage. Pay at bootstrapped, early-stage, mid-stage, late-stage and public companies in the US, EU and UK.</p>  <p>A closer look at the 90th percentile, by funding stage. Something odd is happening to executive packages at late-stage companies: it’s extremely high!</p>  <p>Outliers. The top 3 data points from each category of founders, non-founder executives, and publicly traded companies execs. It’s interesting a principal engineer takes home more in cash compensation than most executives filling out the survey, in illiquid stock.</p>  <p>Interesting findings. Bootstrapped companies pay surprisingly well in cash terms, and neither sector nor engineering team size make a difference to how much you earn.</p>  <p>See also these articles related to this topic:</p>  <p><a href="https://newsletter.pragmaticengineer.com/p/compensation-at-tech-companies">Compensation at publicly traded tech companies</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/remote-compensation-stratgies">A CTO’s guide to remote compensation strategies</a></p>   1. Overview of sources <p>After cleaning the data, the report has 951 data sources. Around two-thirds of respondents work in engineering management or engineering leadership, and nearly a third in software engineering:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe417c2a3-a69e-49a7-af73-62c948ed1765_1600x829.png">    </a>Responses by field. Engineering leads product, design, data engineering, and others  <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F689b2ee8-779a-4c5a-8073-c755c64962ec_1600x849.png">    </a>Of 951 data points, 550 are engineering managers, senior+ engineers &amp; directors-and-above <p>Region-wise, most of the data is from US, EU, and UK readers, but responses came in from every continent on earth – except Antarctica!</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9655a17f-3626-4570-aec7-4ad1d7781efd_1390x850.png">    </a>Data from around the world <p>Top 20 countries by response:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3507203d-20bf-417b-9161-c00a068e9f94_1356x888.png">    </a>Data submitted per country  2. Base salary, cash compensation, total compensation <p>Let’s get into the numbers, first by comparing base salary data across all regions. Base salary is the part of your compensation that’s guaranteed, no matter performance or other factors. In the US, Canada, Australia and most of Latin America this is usually paid fortnightly, while in most of the EU and the UK it’s often once per month.</p><p>Here’s a cross-region comparison, with the median, 75th and 90th percentiles in US dollars:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4e0ffa7-ab78-4bcd-982e-245394d1e5fb_1134x1012.png">    </a>Base salaries across regions in US dollars. Browse it <a href="https://newsletter.pragmaticengineer.com/i/40654455/reports">in a Google Doc</a> <p>Total cash compensation is base salary plus cash bonus (usually at the end of the year) and any sign-on bonus. You can’t guarantee to definitely get your total cash compensation, but knowing some numbers might help in negotiations:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee4b88e5-1492-46af-9bf8-fea037c020be_1180x1070.png">    </a>Total cash compensation across regions. All numbers are in US dollars <p>Total compensation is cash total compensation, plus equity grant. This equity can be illiquid at privately traded startups and scaleups, or liquid at public companies. In context, ‘liquidity’ refers to an asset’s ability to become cash. For liquid equity, there’s the option of selling equity on the stock market for cash after it vests – assuming there’s no “blackout” period. Illiquid equity can be sellable at a liquidation event like an acquisition, going public, or a secondary equity sale.</p><p>Breakdown by region:</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55b7f093-4630-4190-bf7f-8c09cb55e625_1262x1138.png">    </a>Total cash compensation across regions in US dollars. Note, the larger differences in the 90th percentile: that’s stock-heavy compensation  3. Regional pay comparison <p>Let’s see all these numbers in a single chart for easy comparison:</p>
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/tech-compensation">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe417c2a3-a69e-49a7-af73-62c948ed1765_1600x829.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:10 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 18. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2024/03/5642/'>NHKクロニクルが便利〜過去の番組の検索</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  NHKの放送した番組というのは国民の財産なわけですが、過去に放映した番組がわからないと、その財産のありかもわからないことになります。そのためには、番組についてのメタデータが検索できるようになっているのが望ましいですが、その第一歩として、過去の番組表が検索できるだけでもだいぶ違いま…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>NHKの放送した番組というのは国民の財産なわけですが、過去に放映した番組がわからないと、その財産のありかもわからないことになります。そのためには、番組についてのメタデータが検索できるようになっているのが望ましいですが、その第一歩として、過去の番組表が検索できるだけでもだいぶ違います。</p>



<p>現在のNHKのメインサイト「<a href="https://www.nhk.jp/">nhk.jp</a>」ではサイトトップから<a href="https://www.nhk.jp/timetable/">番組表検索へのリンク</a>がありますが、これでは直近分しか検索することができません。これに対して<a href="https://www.nhk.or.jp/archives/chronicle/">NHKクロニクル</a>では過去分を検索することができます。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/03/image-23.png?ssl=1"></a>【図表】NHKクロニクルでは過去の番組表を検索できる（出所）<a href="https://www.nhk.or.jp/archives/chronicle/">https://www.nhk.or.jp/archives/chronicle/</a>



<p>ためしに、サイトウ・キネンと入れて検索してみましょう。すると、どさっと大量の検索結果が逆時系列で出てきます。わたしが知りたかったのは2007年に放映されたことを覚えている番組なので、そのあたりまで時系列を遡っていきます。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/03/image-24.png?ssl=1"></a>【図表】14ページ目で2007年に到達



<p>今回は右上のページ表示をクリックして徐々に遡っていきましたが、URLに「&amp;page=14」のように記入することによってページを飛ぶこともできるようです。こうやっていって、わたしが知りたかった番組は2007年12月29日に放映されていたことがわかりました。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/03/image-25.png?ssl=1"></a>【図表】知りたかった番組は2007年12月29日のっぽい



<p>といっても、上記の画像からわかるように、このページだけではどんな番組だったかわかりません。なので、「番組詳細」を順番にクリックしていってこの番組だとわかったのでした。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/03/image-26.png?ssl=1"></a>【図表】詳細をクリックしていって番組を発見 （出所）<a href="https://www.nhk.or.jp/archives/chronicle/detail/?crnid=A200712291800001301000">https://www.nhk.or.jp/archives/chronicle/detail/?crnid=A200712291800001301000</a>



<p>これ、実はサイトウ・キネン・フェスティバルでデュティユー自身も来日して「Le Temps l’Horloge」が初演された録画でとても貴重なものです。作曲者自身が拍手しているところが写っています。また、「瞬間の神秘」も録画されているのはなかなか珍しいのではないかと思います。</p>



<p>この詳細ページのURLは「<a href="https://www.nhk.or.jp/archives/chronicle/detail/?crnid=A200712291800001301000">https://www.nhk.or.jp/archives/chronicle/detail/?crnid=A200712291800001301000</a>」となっています。このcrnid というのがこのコンテンツの識別子のようです。（←ちゃんとIDネタを振った！）</p>



 放送時期がよくわからなかったらどうするか:＝＞＆検索もできる！ 



<p>上記では、放送時期をわたしが覚えていたので遡っていって検索するという技がつかえました。これだと放送時期がわからないと検索がすごく大変になります。ここで「デュティユー サイトウ・キネン」のように&amp;検索ができればぐっと対象を絞れます。なので、やってみましたらできました。半角スペースでキーワードを区切って検索すればよいようです。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/03/image-27.png?ssl=1"></a>【図表】＆検索も可能 （出所）<a href="https://www.nhk.or.jp/archives/chronicle/search/?keyword=デュティユー%E3%80%80サイトウ・キネン">https://www.nhk.or.jp/archives/chronicle/search/?keyword=デュティユー%E3%80%80サイトウ・キネン</a>



<p>この年2回放映されていることがわかります。わたしが実際に見たのは9月8日の生中継の方ですね。実際に演奏された直後なので。番組詳細を見てみます。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/03/image-28.png?ssl=1"></a>【図表】こちらのほうが番組詳細が詳しい （出所）<a href="https://www.nhk.or.jp/archives/chronicle/detail/?crnid=A200709081900001301000">https://www.nhk.or.jp/archives/chronicle/detail/?crnid=A200709081900001301000</a>



<p>なんと、こちらのほうが番組詳細が詳しい！「瞬間の神秘」がそれだけでなく、正式タイトル「瞬間の神秘（２４の弦楽器、ツィンバロムと打楽器のための）<a href="https://www.sakimura.org/2024/03/5642/#easy-footnote-bottom-1-5642" title="妻の﨑村潤子がツィンバロムを演奏しています">1</a>」としてのっています。12月のは再放送なので内容は同じはずですが、番組情報には差があるのですね。一つ一つ見に行ったほうが良さそうです。</p>



 Google検索ではひっかからない（5/20更新：ひっかかるようになりました） 



<p>このように便利に検索できるNHKクロニクルですが、残念なことがひとつあります。それはGoogle検索でひっかからないということです。試しに「site:www.nhk.or.jp 瞬間の神秘（２４の弦楽器、ツィンバロムと打楽器のための）」と検索してみて下さい。2024年3月18日現在、何も返ってきません。これは、robots.txt でクローリングが禁止されているからですね。</p>



<blockquote>
<p>User-agent: * <br>Disallow: /*.cgi$ <br>Disallow: /*.cgi? <br>Disallow: /*/api/ <br>（中略）<br> Disallow: /chronicle/</p>
【図表】chronicleがrobots.txtで禁止されている （出所）<a href="https://www.nhk.or.jp/robots.txt">https://www.nhk.or.jp/robots.txt</a></blockquote>



<p>国民の利便性という観点ではちょっと残念な気がします。ここはallowしてほしいところです。<a href="https://www.sakimura.org/2024/03/5642/#easy-footnote-bottom-2-5642" title="できない理由はあるのでしょうかね。今度訊いてみようかと思います。">2</a></p>



<p>というわけで、NHKクロニクルが便利。でもGoogleからは検索できないので直接NHKクロニクルに行って検索しないとだめ、というお話でした。あと、こうして検索したものにどうやったらアクセスできるようになるかですね。デジタル化の進展だけでなく権利処理の話も関わってきますが、このあたり、アクセスしやすくなるとよいですね。</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/www.sakimura.org/wp-content/uploads/2024/03/image-23.png?resize=546%2C390&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:00 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/03/building-trunk/'>Talk: Building Trunk</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Ian Stanton will be giving a talk, “Building Trunk: A Postgres Extension
Registry and CLI” on Wednesday at noon US Eastern / 16:00 UTC.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	<a href="https://pgt.dev">
			
		</a>
	

         
<p>Update: 2024-03-22: Slides and video linked below.</p>
<p>This week, my fellow Tembonaut <a href="https://www.linkedin.com/in/istanton">Ian Stanton</a> will present the <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">extension
ecosystem mini-summit</a> talk, “Building Trunk: A Postgres Extension
Registry and CLI”. We felt it important to get some insight from a couple of the
recently-developed Postgres extension registries: what problems they set out to
solve, how the were built and operate, successes addressing their issues, and
what issues remain, both for the projects and the ecosystem overall. Ian plans
to give us the low-down on <a href="https://pgt.dev" title="trunk: A Postgres Extension Registry">trunk</a>.</p>
<p><a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">Join us</a>! Need other information or just want an invitation
without using Eventbrite, hit me up at <code>david@</code> this domain, <a href="https://xoxo.zone/@theory" title="@theory@xoxo.zone">on Mastodon</a>, or
via the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a>.</p>
<p>Update: 2024-03-22: Such a great discussion, was great! Missed it? I got
you. Links:</p>
 
 <a href="https://www.youtube.com/watch?v=k3VC_RFL1bQ">Video</a> 
 <a href="https://justatheory.com/shared/extension-ecosystem-summit/building-trunk.pdf">PDF Slides</a> 
 <a href="https://justatheory.com/2024/03/mini-summit-two/">Notes</a> 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/trunk/">Trunk</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/2024/03/building-trunk/trunk.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   18:06 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 15. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/03/mini-summit-one/'>Mini Summit One</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A rough transcript of my talk “State of the Extension Ecosystem”, along with
interspersed chat comments and appearances by Bagel.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Great turnout and discussion for the first in a series of <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">community talks and
discussions</a> on the postgres extension ecosystem leading up to the
<a href="https://www.pgevents.ca/events/pgconfdev2024/schedule/session/191">Extension Ecosystem Summit</a> at pgconf.dev on May 28. Thank you!</p>
<p>The talk, “State of the Extension Ecosystem”, was followed by 15 minutes or so
of super interesting discussion. Here are the relevant links:</p>
 
 <a href="https://www.youtube.com/watch?v=6o1N1-Eq-Do">Video</a> 
 <a href="https://justatheory.com/shared/extension-ecosystem-summit/state-of-the-ecosystem.key">Keynote</a> 
 <a href="https://justatheory.com/shared/extension-ecosystem-summit/state-of-the-extension-ecosystem.pdf">PDF Slides</a> 
 
<p>For posterity, I listened through my droning and tried to capture the general
outline, posted here along with interspersed chat history and some relevant
links. Apologies in advance for any inaccuracies or missed nuance; i’m happy to
update these notes with your corrections.</p>
<p>And now, to the notes!</p>
 Introduction 
 
 
<p>Introduced myself, first Mini Summit, six leading up to the <a href="https://www.pgevents.ca/events/pgconfdev2024/schedule/session/191">in-person
summit</a> on May 28 at PGConf.dev in Vancouver, Canada.</p>
 
 
<p>Thought I would get it things started, provide a bit of history of
extensions and context for what’s next.</p>
 
 
 Presentation 
 
 
<p>Postgres has a long history of extensibility, originally using pure SQL or
<a href="https://www.postgresql.org/docs/current/sql-load.html" title="PostgreSQL Docs: “LOAD”">shared preload libraries</a>. Used by a few early adopters, perhaps a couple
dozen, including …</p>
 
 
<p>Explicit extension support added in Postgres 9.1 by Dimitri Fontaine, with
<a href="https://www.postgresql.org/docs/current/extend-pgxs.html" title="PostgreSQL Extension Building Infrastructure">PGXS</a>, <a href="https://www.postgresql.org/docs/current/sql-createextension.html" title="PostgreSQL Docs: “CREATE EXTENSION”"><code>CREATE EXTENSION</code></a>, and <a href="https://www.postgresql.org/docs/16/app-pgdump.html" title="PostgreSQL Docs: “pg_dump”"><code>pg_dump</code></a> &amp; <a href="https://www.postgresql.org/docs/16/app-pgrestore.html" title="PostgreSQL Docs: “pg_restore”"><code>pg_restore</code></a> support.</p>
 
 
<p>Example <code>pair--1.0.0.sql</code>:</p>
  <code>-- complain if script is sourced in psql and not CREATE EXTENSION
\echo Use "CREATE EXTENSION pair" to load this file. \quit

CREATE TYPE pair AS ( k text, v text );

CREATE FUNCTION pair(text, text)
RETURNS pair LANGUAGE SQL AS 'SELECT ROW($1, $2)::pair;';

CREATE OPERATOR ~&gt; (LEFTARG = text, RIGHTARG = text, FUNCTION = pair);
</code>   
 
<p><a href="https://www.instagram.com/iambagelkitty/">Bagel</a> makes an appearance.</p>
 
 
<p>Example <code>pair.control</code>:</p>
  <code># pair extension
comment = 'A key/value pair data type'
default_version = '1.0'
module_pathname = '$libdir/pair'
relocatable = true
</code>   
 
<p>Example <code>Makefile</code>:</p>
  <code>EXTENSION    = pair
MODULEDIR    = $(EXTENSION)
DOCS         = README.md
DATA         = sql/pair--1.0.sql
TESTS        = test/sql/base.sql
REGRESS      = base
REGRESS_OPTS = --inputdir=test
MODULES      = src/pair
PG_CONFIG   ?= pg_config

PGXS := $(shell $(PG_CONFIG) --pgxs)
include $(PGXS)
</code>   
 
<p>Build and Install:</p>
  <code>$ make
make: Nothing to be done for `all'.

$ make install
mkdir -p '/pgsql/share/extension'
mkdir -p '/pgsql/share/pair'
mkdir -p '/pgsql/share/doc/pair'
install -c -m 644 pair.control '/pgsql/share/extension/'
install -c -m 644 sql/pair--1.0.sql  '/pgsql/share/pair/'
install -c -m 644 README.md '/pgsql/share/doc/pair/'

$ make installcheck
# +++ regress install-check in  +++
# using postmaster on Unix socket, default port
ok 1         - base                                       15 ms
1..1
# All 1 tests passed.
</code>   
 
<p><code>CREATE EXTENSION</code>:</p>
  <code>$ psql -d try -c 'CREATE EXTENSION pair'
CREATE EXTENSION

$ pg_dump -d try
--
-- Name: pair; Type: EXTENSION; Schema: -; Owner: -
--
CREATE EXTENSION IF NOT EXISTS pair WITH SCHEMA public;

--
-- Name: EXTENSION pair; Type: COMMENT; Schema: -; Owner:
--
COMMENT ON EXTENSION pair IS 'A key/value pair data type';
</code>   
 
<p>Many of us saw opportunity in this new feature.</p>
<blockquote>
<p>PostgreSQL today is not merely a database, it’s an application development
platform.</p>
<p>—&nbsp;Me, 2010</p>
</blockquote>
 
 
<p>Proposed to build <a href="https://pgxn.org/" title="The PostgreSQL Extension Network">PGXN</a>. Raised funds to build it in late 2010. Launched
site April 2011; <a href="https://www.varrazzo.com">Daniele Varrazzo</a> released CLI, and <a href="https://github.com/guedes">Dickson Guedes</a>
released the dev CLI.</p>
 
 
<p>Problems PGXN set out to solve:</p>
 
 Source code distribution with user registration and namespacing 
 Discovery: Search, read docs, brows tags 
 Installation: CLI to compile and install using PGXS or Configure 
 
 
 
<p>PGXN Components:</p>
 
 <a href="https://pgxn.org/spec/">Meta Spec</a> 
 <a href="https://manager.pgxn.org">Manager</a> 
 <a href="https://pgxn.org/mirroring/">Root Mirror</a> 
 <a href="https://github.com/pgxn/pgxn-api/wiki">API Server</a> 
 <a href="https://pgxn.org/" title="The PostgreSQL Extension Network">Site</a> 
 <a href="https://github.com/pgxn/pgxnclient">Client</a> 
 <a href="https://github.com/pgxn/pgxn-utils">Utils CLI</a> 
 <a href="https://github.com/pgxn/docker-pgxn-tools">CI/CD Image</a> 
 
 
 
<p>Problems out of scope for PGXN:</p>
 
 Binary packaging and distribution
 
 Defer to apt/yum 
 
 
 Developer tooling (though dev utils helped) 
 Build tooling
 
 Defer to core (<a href="https://www.postgresql.org/docs/current/extend-pgxs.html" title="PostgreSQL Extension Building Infrastructure">PGXS</a>) 
 
 
 
 
 
<p>PGXN Shortcomings:</p>
 
 Little development since 2012 
 Search limitations
 
 Docs preferred 
 Most distributions have few docs 
 Some issues <a href="https://blog.pgxn.org/post/743059495415119873/recent-work" title="PGXN Blog: “Recent PGXN Improvements”">addressed in last few weeks</a> 
 
 
 Source of Record
 
 Minority of available extensions on PGXN 
 Releases uneven or neglected 
 
 
 
<blockquote>
<p>In classic SDLC fashion, PGXN POC shipped as an MVP and was neglected.</p>
<p>— Me, Just Now</p>
</blockquote>
 
 
<p>Been peripheral to Postgres extensions for the last 10-12 years, but some
things have happened.</p>
 
 
<p>Non-Core extension counts:</p>
 
 
 
 
 <a href="https://pgxn.org/about/">PGXN</a>: 382 
 <a href="https://gist.github.com/joelonsql/e5aa27f8cc9bd22b8999b7de8aee9d47">joelonsql/PostgreSQL-EXTENSIONs.md</a>: 1,186 
 
 
 
<p>Daniele asks about that last source, which is just a list in a <a href="https://gist.github.com/joelonsql/e5aa27f8cc9bd22b8999b7de8aee9d47">gist</a>.</p>
 
 
<p>Joe Nelson links to the <a href="https://gist.github.com/joelonsql/e5aa27f8cc9bd22b8999b7de8aee9d47">gist</a> in Zoom chat. It is not his list, contrary to
my off-the-cuff guess</p>
 
 
<p>Why haven’t extensions taken off?</p>
 
 
<p>Lost Opportunities</p>
 
 No one canonical source to discover and install extensions 
 Difficult to find and discover extensions without canonical registry 
 Most extensions are under-documented and difficult to understand 
 They are also hard to configure and install; most people don’t want or
need a compiler 
 The maturity of extensions can be difficult to gauge, not systematized,
must each be independently researched
 
 David Christensen in Chat “attention economy/awareness, NIH, etc” 
 Jeremy S in chat: “Maybe some people don’t know they are using
extensions (I think that’s possible to some degree)” 
 
 
 There is no comprehensive binary packaging 
 Centralized source distribution is insufficient (even if it were complete)
 
 jubilee in chat: Trust aspect? 
 David Johnson in chat: To seem legit you need to touch the repo at
least annually to ensure it works on the newest major release.  Even
if you just do your compile and update the readme. 
 I mention using <a href="https://github.com/pgxn/docker-pgxn-tools">pgxn-utils</a> and GitHub workflows to ensure my
extensions continue working 
 
 
 There is insufficient developer tooling; pgxn-utils not well-maintained,
don’t build on recent Rubies, but <a href="https://github.com/pgcentralfoundation/pgrx">pgrx</a> has a lot of Rust-oriented
tooling
 
 Eric in chat: ❤️ 
 jubilee in chat: 🦀 mentioned! 
 
 
 
 
 
<p>Filling the Gaps</p>
 
 
<p><a href="https://database.dev" title="The Database Package Manager">dbdev</a>: “The Database Package Manager for Trusted Language Extensions”:
Includes only TLEs, no binary extensions</p>
 
 
<p><a href="https://pgt.dev" title="Trunk — A Postgres Extension Registry">trunk</a>: “A Postgres Extension Registry”: Binary distribution of curated
extensions, desires to be comprehensive and cross-platform</p>
 
 
<p><a href="https://pgxman.com/" title="npm for PostgreSQL">pgxman</a>: “npm for PostgreSQL”: Binary Apt package distribution of curated
extensions packaged with, desires to be comprehensive and cross-platform</p>
 
 
<p>Emphases: Ease of Use. Screenshot from <a href="https://pgxman.com/" title="npm for PostgreSQL">pgxman</a>:</p>
<blockquote>
  <code>$ curl -sfL https://install.pgx.sh | sh -
👏🎉 pgxman successfully installed
$ pgxman install pgvector
The following Debian packages will be installed:
postgresql-14-pgxman-pgvector=0.5.1
Do you want to continue? [Y/n] y
pgvector has been successfully installed.
</code>  </blockquote>
 
 Daniele in chat: “Missing a “curl | sudo sh” there…. 👀” 
 Greg Mullane (CrunchyData) [he/him] in chat: “Really not a fan of that
“pipe curl stuff from internet into sh” system.” 
 Jeremy S in chat: “Someone recently reprimanded me for putting curl |
psql in an extension README.  From a security perspective it probably
sets a better example to do curl &gt;file.sql … psql file.sql (encourage
users not to run from Internet but read/review first)” *   jubilee in
chat: “apt/yum install is just a better UI over curl | sh :^)” 
 Jeremy S in chat: “Yes and once you’re to that point there’s already
more supply chain verification happening” 
 Jeremy S in chat: “It’s usually just the initial bootstrap into any
system, if the setup wasn’t already in your distro” 
 
 
 
<p>Emphases: Platform neutrality. Screenshot from <a href="https://pgt.dev" title="Trunk — A Postgres Extension Registry">trunk</a>:</p>
<blockquote>









Architecture
x86-64


Operating system
Debian/Ubuntu



</blockquote>
 
 
<p>Emphases: Stats. Screenshot from <a href="https://database.dev" title="The Database Package Manager">dbdev</a>:</p>
<blockquote>
 Downloads 
 
<p>20 all time downloads
0 downloads in last 30 days
1 download in last 90 days
0 downloads in last 180 days</p>
</blockquote>
 
 
<p>Emphases: Curation. Screenshot from <a href="https://pgt.dev" title="Trunk — A Postgres Extension Registry">trunk</a>:</p>
<blockquote>









Featured
7


Analytics
13


Auditing / Logging
7


Data Change Capture
6


Connectors
27


Data / Transformations
49



</blockquote>
 
 Damien Clochard in chat: gtg, see you later guys ! 
 
 
 
<p>MVPs</p>
 
 
<p><a href="https://pgt.dev" title="Trunk — A Postgres Extension Registry">trunk</a>: Manual integration,  Currently Debian-only</p>
 
 
<p><a href="https://pgxman.com/" title="npm for PostgreSQL">pgxman</a>: Form-based submission, Currently Apt-only</p>
 
 
<p><a href="https://database.dev" title="The Database Package Manager">dbdev</a>: TLEs only, CLI publishing</p>
 
 
<p>David Christensen in chat: “go has a pretty good extensions infra,
imho, wrt discovery/docs, etc. also has the benefit of the package names
being the URL to access it, which is a nice convention.”</p>
 
 
 
 
<p>New Opportunities Today</p>
<p>What are the community opportunities for the extension ecosystem?</p>
<p>Some ideas:</p>
 
 Improved dev tools: More than pgxn-utils and <a href="https://github.com/pgcentralfoundation/pgrx">pgrx</a> 
 Canonical registry: All publicly-available extensions in one pac3 
 Easy publishing: auto-discovery or CI/CD pipeline publishing 
 Continuous Delivery: CI/CD pipeline publishing 
 File-free installation: TLEs 
 Documentation: Something like Go docs or Rust docs 
 File management: Put all the files for an extension in one directory 
 Improved metadata
 
 Library Dependencies: utilities used by extensions 
 Platform Dependencies: system packages 
 Build pipelines: <a href="https://www.postgresql.org/docs/current/extend-pgxs.html" title="PostgreSQL Extension Building Infrastructure">PGXS</a>, <a href="https://github.com/pgcentralfoundation/pgrx">pgrx</a>, <a href="https://www.gnu.org/software/make/">make</a>, <a href="https://www.cpan.org">cpan</a>, <a href="https://pypi.org">pypi</a>, etc. 
 Artifacts: Binaries build on release 
 Classification: Curated in addition to tags 
 Extension Types: Extensions, apps, background workers, loadable libraries 
 
 
 Derived Services
 
 Binary Packaging: Distributed binaries for many platforms 
 Ratings &amp; Reviews: Stars, thumbs, comments 
 Aggregated Stats: Repository stats, etc. 
 Smoke Testing: Matrix of multi-platform test results 
 Security Scanning: Reporting vulnerabilities 
 Badging &amp; Curation: Third-party classification, badging various statuses 
 
 
 
 
 
<p><a href="https://www.pgevents.ca/events/pgconfdev2024/schedule/session/191">Extension Ecosystem Summit</a></p>
<blockquote>
<p>Collaborate to examine the ongoing work on PostgreSQL extension
distribution, examine its challenges, identify questions, propose
solutions, and agree on directions for execution.</p>
</blockquote>
 
 
<p>🏔️ Your Summit Organizers</p>
 
 <a title="Just a Theory">David Wheeler</a>, Tembo, <a href="https://pgxn.org/" title="The PostgreSQL Extension Network">PGXN</a> 
 <a href="https://about.me/jeremy_schneider">Jeremy Schneider</a>, AWS, <a href="https://pgxn.org/dist/dsef/">dsef</a> 
 <a href="https://www.crunchydata.com/blog/author/david-christensen">David Christensen</a>, Crunchy Data, <a href="https://pgxn.org/dist/pg_kaboom/">pg_kaboom</a> 
 <a href="https://pgxn.org/user/keithf4">Keith Fiske</a>, Crunchy Data, <a href="https://pgxn.org/dist/pg_partman/">pg_partman</a> 
 <a href="https://github.com/devrimgunduz">Devrim Gündüz</a>, EnterpriseDB, <a href="https://yum.postgresql.org">yum.postgresql.org</a> 
 
 
 
<p>Devrim Gunduz in chat: Thanks David!</p>
 
 
<p>Schedule:</p>
 
 March 6: <a title="Just a Theory">David Wheeler</a>, PGXN: “State of the Extension Ecosystem” 
 March 20: <a href="https://www.linkedin.com/in/istanton">Ian Stanton</a>, Tembo: “Building Trunk: A Postgres Extension
Registry and CLI” 
 April 3: <a href="https://github.com/devrimgunduz">Devrim Gündüz</a>: “yum.postgresql.org and the challenges
RPMifying extensions” 
 April 17: <a href="https://jkatz05.com">Jonathan Katz</a>: “TLE Vision and Specifics” 
 May 1: <a href="https://ca.linkedin.com/in/yrashk">Yurii Rashkovskii</a>, Omnigres: “Universally buildable extensions:
dev to prod” 
 May 15: <a title="Just a Theory">David Wheeler</a>, PGXN: “Community Organizing  Summit Topics” 
 
 
 
<p>Ultimately want to talk about what’s important to you, the members of the
community to make extensions successful.</p>
 
 
 Discussion 
 
 
<p>Eric: I’m Eric Ridge, one of the developers behind pgrx, as you’re going
through this process of building a modern extension ecosystem, let us know
what we can do on the Rust side to help make your lives easier, we’re happy
to help any way we can.</p>
 
 
<p>Steven Miller in chat:</p>
<blockquote>
<p>These are some areas of interest we noticed building Tembo</p>
<p>Binary packaging / distribution:</p>
 
 Variable installation location 
 System dependencies / uncommon system dependencies or versions 
 Chip specific instructions (e.g. vector compiled with avx512) 
 Extension-specific file types / extra data files (e.g. anonymizer .csv
data) 
 
<p>Turning on extensions automatically</p>
 
 Different ways to enable extensions 
 does it need load (shared_preload_libraries, session_… etc)? 
 Does it use create extension framework? 
 Does it require a specific schema? 
 What about turning on in multiple DBs at the same time in the same
cluster, with background worker? 
 Disabling, what data will be lost? 
 Validating safety / user feedback on upgrade? 
 
<p>In cloud / SaaS:</p>
 
 Installing + enabling extensions quickly, without restart 
 Persisting extension files 
 Extension-specific files (e.g. libraries) versus postgres’ libraries 
 Updating 
 Troubleshooting crashes / core dumps 
 
<p>Anyone else have similar problems / tips?</p>
</blockquote>
 
 
<p>Steven Miller: These were just things I noted during the presentation.
Curious if these are interesting to others on the call.</p>
 
 
<p>Daniele in chat: “Regards binary distributions, <a href="https://pythonwheels.com">python wheels</a> might be a
useful reference.”</p>
 
 
<p>Steven Miller: That’s good point! What do people think of idea to just
install extensions onto servers, not packages, persisted on the disk, next
to PGDATA so they go into a single persistent volume, and the rest is
managed by an immutable container.</p>
 
 
<p>Daniele: Had experience on Crunchy where we had to replace an image to get
an extension. Looked for feature to have a sidecar or a volume with the
extension.</p>
 
 
<p>Steven Miller: Didn’t have a separate directory just for extensions, it’s
just <code>pg_config --libdir</code> fore everything. Had to persist entire directory,
including those files form the base build, their internal files. Would have
been nice to have a separate directory, extra-libdr or extra-sharedir,
something like that.</p>
 
 
<p>Yurii Rashkovskii: I was working on a patch to do exactly that, but
haven’t completed it. Was going to introduce additional directories to
search for this stuff.</p>
 
 
<p>Steven Miller: That would be really awesome.</p>
 
 
<p>Jeremy S in chat: “Advantage of that is that a single image can be shared
among systems with different needs”</p>
 
 
<p>Eric in chat: “Thoughts around “enterprise repositories” that could be
self-hosted and disconnected from the internet?”</p>
 
 Ian Stanton in chat: “I’ll touch on this in the next talk, it’s
crossed our minds when building the Trunk registry” 
 
 
 
<p>Steven Miller: I think that’s a great idea.</p>
 
 
<p><a href="https://www.instagram.com/iambagelkitty/">Bagel</a> reappears.</p>
 
 
<p>David Wheeler: <a href="https://pgxn.org/" title="The PostgreSQL Extension Network">PGXN</a> originally designed so anyone could run Manager and
their own root mirror, and maybe rsync from the community one. Don’t know
that anyone ever did, it’s a little complicated and most people don’t want
to work with Perl. [Chuckles]. Definitely think there’s space for that. If
you work with Java or Go or maybe Rust, lots of orgs like Artifactory that
provide internal registries. Could be cool use case for Postgres extensions.</p>
 
 
<p>David Christensen in chat: “something that could support extension
batches; like groups of related extensions that could be installed in bulk
or loaded in bulk (so could accommodate the shared systems with different
individual extension needs, but could be shared_preload_library configured)”</p>
 
 
<p>“Steven Miller” in chat: “Sounds familiar”</p>
 
 
<p>Greg Mullane (CrunchyData) [he/him] in chat: “All these items remind me of
CPAN. We should see what things it (and other similar systems) get right and
wrong. I’ve learned from CPAN that ratings, reviews, and badging are going
to be very difficult.”</p>
 
 David Christensen in chat: “I’d assumed at the time that it was
largely lifted (at least in philosophy/design) from CPAN. 🙂” 
 David Wheeler (he/him) in chat: “yes” 
 
 
 
<p>Jeremy S: I think this is mostly focused on developers, but I had recent
experience where multiple people in the past few months, new to Postgres,
are trying to understand extensions. They install a version and then see
there are like 15 versions installed, so confused. Goes back to the install
file. Bit of UX angle where there are sharp edges where people trying to
make sense of extensions, the flexibility makes it hard to understand. Some
might be some nice guides, some architectural things explaining PGXS, or
improvements to make to the design. Related, not main topic, but good to
keep end user UX and devs building on Postgres but not Postgres developers,
who run their businesses.</p>
 
 
<p>David Wheeler: Yeah all the files can be confusing, which is why I think
<a href="https://pgt.dev" title="Trunk — A Postgres Extension Registry">trunk</a> and <a href="https://pgxman.com/" title="npm for PostgreSQL">pgxman</a> trying to simplify: Just run this command and then you
have it.</p>
 
 
<p>Steven Miller in chat: “I really agree with what Jeremy is saying. Right
now PGXN and Trunk are taking and approach like “whatever an extension could
do / how it may work, it should work on this registry”. But I think more
standards / “what is a normal extension” would make the UX much easier.”</p>
 
 
<p>Jeremy S: Even with that the available extensions view is still there. Some
of that is just schema management, and that’s how core is doing schema
management.</p>
 
 
<p>Steven Miller in chat: I exactly agree about the concern about multiple
extensions within a package. Also version and name mismatches</p>
 
 
<p>David Wheeler: And not everything is an extension, you just want to work, or
your extension is just utility like pg_top you just want to use. Extensions
I think were a tremendous contribution to Postgres itself, but a lot of it
was wrangling the existing system for building Postgres itself to make it
work for that. Could be very interesting, though quite long term — and I
know Dimitri has tried this multiple times — to build a proper package
management system within Postgres itself, to eas a lot of that pain and
burden.</p>
 
 
<p>Tobias Bussmann in chat: “Thank you for the great overview and for taking
this topic further! Unfortunately, I’ll not be able to join at pgConf.dev
but will follow whatever results this will lead to. As a package maintainer,
I am constantly looking in a easy way to allow users to use extensions
without having to package everything ;)”</p>
 
 
<p>Steven Miller in chat: “Like auto explain for example right. i.e. a LOAD
only “extension””</p>
 
 
<p>*Yurii Rashkovskii: An interesting topic, what extensions are capable of
doing and how they can be more self-contained. Like Steven was saying in
chat: how easy is it to load and unload extensions. Example: want an
extension to hook into a part of Postgres: executor, planner, etc. How do
you go about enabling them? How you unload them, introspect, list of hooks.</p>
<p>Omni extension provides a list of all hooks, and when you remove an
extension it removes the hooks that provide the extension, but still not
part of the core. Hooks one of the greatest ways to expand the functionality
of Postgres, allows us to experiment with Postgres before committing to the
full cycle of getting a patch into Postgres. Lets us get it to users today
to try. if it makes a lot of sense and people want it, time to commit to the
process of getting a patch committed. But if we don’t hve this venue, how to
get extensions in, our ability to try things is limited.</p>
 
 
<p>jubilee in chat: Hmm. It seems my audio is not working.</p>
 
 
<p>David Wheeler: The next session is two weeks from today: Ian Stanton is
going to talk about “Building Trunk: A Postgres Extension Registry and CLI”.
Will be interesting because a number of people have decided to build a
binary packaging system for extensions, just to air out what the challenges
were, what problems they wanted to solve, what problems remain, and where
they want to take it in the future.</p>
 
 
<p>Jeremy S* in chat: “Bagel clearly has something to say”</p>
 
 
<p>*David Wheeler: jubileee I see your audio issues, do you just want to type
your question into chat? We can also discuss things in the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a>
channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a></p>
 
 
<p>David Wheeler: Thank you all for coming!</p>
 
 
<p>jubilee in chat: “I had a question which is about: Does Postgres actually
support docs for extensions? Like, a lot of people don’t really WANT to
read a README. Can you get docstrings for a function in psql?”</p>
 
 
<p>Ian Stanton in chat: “Thank you David!”</p>
 
 
<p>jubilee in chat: And if not, why not?</p>
 
 
 Post Presentation Discussion 
<p><a href="https://postgresteam.slack.com/archives/C056ZA93H1A/p1709762608926059">From Slack</a>:</p>
 
 David Wheeler: I see now that “jubilee” left their question in the Zoom chat. 
 David Wheeler: The closest attempt at this I’ve seen is <a href="https://pgxn.org/dist/pg_readme/0.6.6/README.html">pg_readme</a>, which
will generate Markdown for an extension from comments in the catalog and
write it out to a file. 
 David G. Johnson: The <a href="https://www.postgresql.org/docs/current/sql-comment.html">comment on</a> command adds in database comments that psql
describe commands should display. 
 
<p>Also <a href="https://postgresteam.slack.com/archives/C056ZA93H1A/p1709748156307179">on Slack</a>,
Greg Sabino Mullane started a longish thread on the things we want to do and build.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
                 <a href="https://justatheory.com/tags/bagel/">Bagel</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   20:05 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 12. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@rufftimo'>Timothy Ruff</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://rufftimo.medium.com/the-american-general-data-protection-regulations-agdpr-948fc53b884d?source=rss-11906019170a------2'>The American General Data Protection Regulations (AGDPR)</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   AGDPR doesn’t exist, but it&nbsp;should.  In 2012 the Europeans released the first proposal for their General Data Protection Regulations (GDPR), which ran a four-year political gauntlet and evolved into its final, official version in 2016. Because of the ghastly surveillance situation we now find ourselves in America and elsewhere, perpetrated by both private industry and government, I think Amer
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/376/1*XaUZ7a8L34C4phISqknW-w@2x.jpeg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 AGDPR doesn’t exist, but it&nbsp;should. <p>In 2012 the Europeans released the first proposal for their General Data Protection Regulations (GDPR), which ran a four-year political gauntlet and evolved into its final, official version in 2016. Because of the ghastly surveillance situation we now find ourselves in America and elsewhere, perpetrated by both private industry and government, I think America also desperately needs comprehensive privacy rules, rules informed by now eight years of hindsight and infused with an American value not found in Europe’s GDPR: privacy from government.</p><p>If I could wave a magic wand, an American GDPR would improve on Europe’s GDPR in two fundamental ways:</p> 1. Eliminate the idea that data about people can be “anonymized” or “de-identified”. <p>Data about people can no longer be effectively de-identified by removing personally identifying information (PII) from the dataset. Maybe this was generally still possible in 2016, but not any&nbsp;more.</p><p>GDPR set off a wave of innovation among data scientists who, like the old game “<a href="https://www.youtube.com/watch?v=qBco7LLAcpc">Name That Tune</a>”, took the new regulation as a challenge and developed numerous ways to re-identify people using only a few data points that aren’t considered PII, techniques they’ve since made widely available. And that was before the recent explosion of AI tools which make re-identification easy without the direct use of such tools. Unfortunately, fully GDPR-compliant “de-identified” data sets are now trivially easy to re-identify for anyone with access to a computer.</p><p>(For more details on this topic, see <a href="https://privacytools.seas.harvard.edu/publications/towards-formalizing-gdpr%E2%80%99s-notion-singling-out">this Harvard paper</a> about “simple” attacks that defeat GDPR’s PSO Secure / “singling out” privacy protection provisions.)</p><p>If almost any data about a person is usable to re-identify them, then the whole concept PII is no longer relevant; any data about a person is PII. That is precisely how an AGDPR definition of “personal data” can improve upon the European one: any data about an person. It’s both simpler and more accurate.</p> 2. Include protections for privacy *from government*, in addition to privacy from the private&nbsp;sector. <p>Europe’s GDPR specifically exempts government, AGDPR should do the opposite: it should bind government’s activities in relation to obtaining, using and retaining personal&nbsp;data.</p><p>There are obviously necessary and important reasons for government to obtain, use, and retain personal data, and those reasons should be explicitly enumerated and protected. But the U.S. federal government has gone absolutely bananas when it comes to how much of our personal data they are obtaining about us and our activities, through legal but largely unchallenged means. The degree to which it’s happening is way beyond what’s reasonably needed in a free country that espouses limited government, and that has a Fourth Amendment that&nbsp;secures:</p><blockquote>“the right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures”.</blockquote><p>How is data about us not our “effects”? How is government gathering our effects en masse without our explicit consent not “unreasonable”?</p><p>Journalist <a href="https://www.linkedin.com/feed/#">Byron Tau</a>’s new book about U.S. government surveillance of law-abiding citizens, <a href="https://www.amazon.com/Means-Control-Alliance-Government-Surveillance/dp/B0C6YL6C81/">Means of Control</a>, is outrageous. (<a href="https://www.politico.com/news/magazine/2024/02/28/government-buying-your-data-00143742">Summary article here.</a>) You cannot read it and not become deeply alarmed at the enormous, secret, ubiquitous surveillance state that already exists in the U.S. No doubt European governments are doing the same with their citizens’ data, but — and I’m generalizing here — they don’t have the same foundational distrust of government that the U.S. was born with and so may not be bothered enough to do anything about it, as evidenced by government’s total exemption from their GDPR. I hope, and believe, that once more Americans become aware of the extent of what’s happening, an American GDPR that clearly defines government’s role in privacy can become a real, politically viable possibility.</p><p>There is a model to follow. <a href="https://www.linkedin.com/in/christopher-bramwell-26815515/">Christopher Bramwell</a>, the Chief Privacy Officer of Utah, was the driving force behind a groundbreaking new piece of <a href="https://le.utah.gov/~2024/bills/static/HB0491.html">privacy legislation</a> just enacted in Utah that constrains how personal data must be treated by government, without saying anything about private industry. As far as I know it’s the first of its kind, anywhere, and hopefully the first of many to follow in other states and countries. This bill gained a fortunate political boon as it advanced: it became a ‘vote against privacy’ to not support it; it passed unanimously.</p><p>Of course, privacy from big tech, surveillance capitalism, and private industry in general is a big deal, but private industry doesn’t have a monopoly on violence and taxes like government does; they mostly just want to sell me stuff. Still, the European GDPR shows an example of how to constrain private industry, and an American GDPR should learn from and incorporate those&nbsp;lessons.</p><p>The ideal outcome of an AGDPR would provide privacy both from private industry and from government, and finally put a halt to all federal activities of indiscriminate obtaining, using, and retaining of our personal digital “effects”, whether or not they’re considered PII.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/376/1*XaUZ7a8L34C4phISqknW-w@2x.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   04:06 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 11. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/decentralizing-energy'>Decentralizing Energy</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The end of the Age of Oil
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0264c287-64b9-4f53-9c79-7d996d764a02.heic">    </a> <p>The Accidental Superpower discusses a post-Bretton Woods world where easy, safe shipping cannot be assumed. I think in this "scared new world," renewables change the dynamics of energy flows by moving production closer to energy use.</p><p>My wife, Lynne, recently gave me a copy of Peter Zeihan's book, <a href="https://www.amazon.com/exec/obidos/ASIN/B00JJ322NC/windleyofente-20">The Accidental Superpower: Ten Years On</a>. The book was originally published in 2014, but Zeihan has updated it by inserting chapters talking about what he got right in 2014, what he got wrong, and why. The focus of the book is geopolitics—how geography and demographics shapes the world order—and how <a href="https://en.wikipedia.org/wiki/Bretton_Woods_system">Bretton Woods</a> changed that in significant ways. The book makes the case that so much of what made Bretton Woods useful to the US and why the US engaged with the rest of the world for the 70 years following World War II is changing. As it changes the free trade system enabled by Bretton Woods is also changing. This will have significant impact on every country in the world.</p> <a href="https://www.amazon.com/exec/obidos/ASIN/B00JJ322NC/windleyofente-20">   </a> <p>Much of what changes has to do with energy. One of the things1 Zeihan got right was his assertion that unlike much of the rest of the developed world, the US doesn't need to import energy—specifically oil—we are a <a href="https://www.energy.gov/eere/vehicles/articles/fotw-1310-october-2-2023-united-states-has-been-positive-net-exporter#:~:text=Vehicle%20Technologies%20Office-,FOOTY%20%231310%2C%20October%202%2C%202023%3A%20The%20United%20States,of%20Primary%20Energy%20Since%202019&amp;text=Exports%20of%20primary%20energy%20have,to%2027.5%20quads%20in%202022.">net energy importer</a>. This changes the dynamic wherein the US is willing to be the protector of shipping lanes for the entire world. As a result, the future could see a US that has the luxury of ignoring events in the Middle East, Ukraine, and elsewhere, whereas Europe (to take just one example) cannot. The book is full of other interesting predictions and conclusions just like this one. I encourage you to read it if you find this as fascinating as I do.</p><p>Zeihan makes a big deal of shale oil production, which <a href="https://www.eia.gov/tools/faqs/faq.php?id=847&amp;t=6">accounted for 66% of US production in 2022</a>. But as I read this, I was thinking about renewables. As I wrote in 2020, <a href="https://www.windley.com/archives/2020/03/decarbonizing_locally.shtml">I've gone in big on solar power at my house, love my EV, and have replaced most things in the house (like the furnaces) with versions that run on electricity.</a>&nbsp;&nbsp;I did this because it made my life easier and saves me money. The fact that it's good for the environment is a bonus.</p><p>But, solar and wind are not just renewable, they also allow energy production to be decentralized in ways oil and natural gas can't. Oil and natural gas deposits are where they are. Some countries are blessed with them and the others have to buy from those countries. And they're often far away, requiring shipping through <a href="https://www.theguardian.com/world/2024/jan/03/what-is-the-red-sea-crisis-and-what-does-it-mean-for-global-trade">potentially hostile waters</a>. But that's not true of renewables. They can usually be built and located where ever the need is2. This changes geopolitical equation in significant ways, but areas of the world that are not energy independent, like Europe, are <a href="https://www.eea.europa.eu/en/analysis/indicators/share-of-energy-consumption-from#:~:text=According%20to%20EEA%20early%20estimates,renewables%20has%20slowed%20since%202020.">moving toward renewables too slowly</a> to prevent future energy shocks. The problem with renewables is that they're long-lead items—they take years to plan and bring online.</p><p>Petroleum and Bretton Woods enabled the modern world, providing portable, storable sources of energy that could easily and safely move to where ever it was needed.3 If we are indeed at the end of the Bretton Woods era, the world is in for significant changes as it adjusts to a life where free trade, and easy access to petroleum-based energy, cannot be assumed. Moving energy production closer to the places it's used is one strategy for dealing with this world-altering disruption. Buckle up.</p>    Notes   <p>There are other things that are important to the books overall conclusion besides energy. I'm just cherry picking that because I was thinking about it. For example, the US is largely self-sufficient from an overall import/export standpoint. We don't import nearly as much as many other countries and could replace what we do import relatively easily.</p>  <p>It's not just renewables. Nuclear power can also be located closer to demand than an oil deposit. I started my career as a nuclear metallurgist, so I'm a fan. I think many countries are going to be sorry they've closed nuclear plants and made them too hard to construct profitably.</p>  <p>The feats of engineering that have enabled these energy flows is truly astounding.</p>  <p>Photo Credit: <a href="https://pixabay.com/photos/boat-ship-sunset-bridge-oil-tanker-8163905/">Oil Tanker at Sunset</a> from Terski (<a href="https://pixabay.com/service/license-summary/">Pixabay</a>)</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0264c287-64b9-4f53-9c79-7d996d764a02.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   13:52 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 10. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://benwerd.medium.com/the-internet-addiction-and-me-10d292761d2e?source=rss-3b16402f5b9d------2'>The internet, addiction, and me</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    Sometimes it’s not okay to look down from the world.  Continue reading on Medium »  
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   <p>Sometimes it’s not okay to look down from the world.</p><p><a href="https://benwerd.medium.com/the-internet-addiction-and-me-10d292761d2e?source=rss-3b16402f5b9d------2">Continue reading on Medium »</a></p> 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   23:02 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 08. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://benwerd.medium.com/startup-pitch-social-vip-28277a06ac46?source=rss-3b16402f5b9d------2'>Startup pitch: Social VIP</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

      A way for organizations to more easily join the fediverse  Continue reading on Medium »  
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/0*4Y4XbVuPOARlGMPE.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   <p><a href="https://benwerd.medium.com/startup-pitch-social-vip-28277a06ac46?source=rss-3b16402f5b9d------2"></a></p><p>A way for organizations to more easily join the fediverse</p><p><a href="https://benwerd.medium.com/startup-pitch-social-vip-28277a06ac46?source=rss-3b16402f5b9d------2">Continue reading on Medium »</a></p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/0*4Y4XbVuPOARlGMPE.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:37 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 04. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/03/state-of-the-extension-ecosystem/'>Talk: State of the Extension Ecosystem</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A quick reminder that I’ll be giving a brief talk on the “State of the
Extension Ecosystem” on Wednesday at noon US Eastern / 17:00 UTC.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	<a href="https://www.eventbrite.com/e/851125899477/">
			
		</a>
	

         
<p>Update: 2024-03-06: Slides and video linked below.</p>
<p>A quick reminder that I’ll be giving a brief talk on the “State of the Extension
Ecosystem” on Wednesday at noon US Eastern / 17:00 UTC. This talk is the first
in a series of <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">community talks and discussions</a> on the postgres
extension ecosystem leading up to the <a href="https://www.pgevents.ca/events/pgconfdev2024/schedule/session/191">Extension Ecosystem Summit</a>
at pgconf.dev on May 28.</p>
<p>I plan to give a brief history of Postgres extension tools and distribution,
the challenges encountered, recent developments, and opportunities for the
future. It should take about 30 minutes, followed by discussion. Following
this pattern for all the talks in <a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">the series</a>,  I hope to set up
some engaging discussions and to surface significant topics ahead of <a href="https://www.pgevents.ca/events/pgconfdev2024/schedule/session/191">the
summit</a>.</p>
<p><a href="https://www.eventbrite.com/e/851125899477/" title="Postgres Extension Ecosystem Mini-Summit">Join us</a>! Need other information or just want an invitation
without using Eventbrite, hit me up at <code>david@</code> this domain, <a href="https://xoxo.zone/@theory" title="@theory@xoxo.zone">on Mastodon</a>, or
via the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a>.</p>
<p>Update: 2024-03-06: Great turnout and discussion, thank you! Links:</p>
 
 <a href="https://www.youtube.com/watch?v=6o1N1-Eq-Do">Video</a> 
 <a href="https://justatheory.com/shared/extension-ecosystem-summit/state-of-the-ecosystem.key">Keynote</a> 
 <a href="https://justatheory.com/shared/extension-ecosystem-summit/state-of-the-extension-ecosystem.pdf">PDF Slides</a> 
 <a href="https://justatheory.com/2024/03/mini-summit-one/">Notes</a> 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/shared/extension-ecosystem-summit/summit.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   18:50 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 04. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2024/03/04/multi-client-blob-storage-access-using-asp-net-core-with-entra-id-authentication-and-rbac/'>Multi client blob storage access using ASP.NET Core with Entra ID authentication and RBAC</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This article shows how to onboard different clients or organizations in an ASP.NET Core application to use separated Azure blob containers with controlled access using security groups and RBAC applied roles. Each user in a client group can only access a single blob storage and has no access to blob containers belonging to different clients. […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This article shows how to onboard different clients or organizations in an ASP.NET Core application to use separated Azure blob containers with controlled access using security groups and RBAC applied roles. Each user in a client group can only access a single blob storage and has no access to blob containers belonging to different clients. Microsoft Entra ID is used to implement the blob storage access.</p>



<p>Code: <a href="https://github.com/damienbod/AspNetCoreEntraIdBlobStorage">https://github.com/damienbod/AspNetCoreEntraIdBlobStorage</a></p>



 Blogs in this series 



 
 <a href="https://damienbod.com/2024/02/12/using-blob-storage-from-asp-net-core-with-entra-id-authentication/">Using Blob storage from ASP.NET Core with Entra ID authentication</a> 



 <a href="https://damienbod.com/2024/02/26/delegated-read-and-application-write-access-to-blob-storage-using-asp-net-core-with-entra-id-authentication/">Delegated read and application write access to blob storage using ASP.NET Core with Entra ID authentication</a> 



 Multi client blob storage access using ASP.NET Core with Entra ID authentication and RBAC 
 



<p></p>



 Security context diagram 



<p>The web application uses different Enterprise applications to access the different Azure APIs. One Enterprise application is used to implement the blob application contributor access which can only be used from the application. This is used when a user in the application needs to write a file to the blob through the application. A second Enterprise application is used to create the RBAC access for the blob container reader access to the files. This is used when creating a new client or new organization. A third Enterprise application is used to implement the web application OpenID Connect web client. This is created using an Azure app registration and only allows delegated permissions. The user App roles are defined in this application. The Microsoft Graph APIs can be implemented using delegated permissions or application permissions. If using Graph application permissions to create or remove the groups, a separate Enterprise application is used to create the groups. You can also used the app service managed identity and use the service principal instead of the 3 enterprise applications to assign the required permissions.</p>







<p></p>



<p></p>



 Multi client blob storage setup 



<p>The Blob account uses Microsoft Entra ID to access the blob containers. The application can write to all containers and a security group is given RBAC blob container reader access, one per security group. Users are added to the security groups per client or per organization. The setup for the client blob container is implemented in three steps:</p>



 
 Create a Microsoft Entra ID security group 



 Create an Azure storage blob container 



 Create an RBAC to give the security group Blob storage reader permissions 
 



<p></p>



 Create a Microsoft Entra ID security group 



<p>The CreateSecurityGroupAsync method creates a new security group in Microsoft Entra ID. This is created using Microsoft Graph and returns the group with the group ID. The service uses application permissions and is implemented in a separate Enterprise application.</p>


  using System.Text;
using Microsoft.Graph.Models;

namespace MultiClientBlobStorage.Providers.GroupUserServices;

public class ApplicationMsGraphService
{
    private readonly GraphApplicationClientService _graphApplicationClientService;

    public ApplicationMsGraphService(
       GraphApplicationClientService graphApplicationClientService)
    {
        _graphApplicationClientService = graphApplicationClientService;
    }

    public async Task&lt;Group?&gt; CreateSecurityGroupAsync(string group)
    {
        var graphServiceClient = _graphApplicationClientService
            .GetGraphClientWithClientSecretCredential();

        var formatted = RemoveSpecialCharacters(group);
        var groupName = $"blob-{formatted.Trim()}-{Guid.NewGuid()}".ToLower();

        var requestBody = new Group
        {
            DisplayName = groupName,
            Description = $"Security group for all users from {groupName}",
            MailEnabled = false,
            MailNickname = formatted,
            SecurityEnabled = true
        };

        var result = await graphServiceClient.Groups.PostAsync(requestBody);
        return result;
    }

    private string RemoveSpecialCharacters(string str)
    {
        var sb = new StringBuilder();
        foreach (var c in str)
        {
            if (c is &gt;= '0' and &lt;= '9' || c is &gt;= 'A' and &lt;= 'Z' 
                  || c is &gt;= 'a' and &lt;= 'z' || c == '.' || c == '_')
            {
                sb.Append(c);
            }
        }

        return sb.ToString();
    }
}
  


<p></p>



 Create an Azure storage blob container 



<p>The CreateContainer method creates a new Azure blob container using the BlobServiceClient from the Azure.Storage.Blobs nuget package.</p>


  private async Task&lt;BlobContainerClient&gt; CreateContainer(string name)
{
    try
    {
        var formatted = RemoveSpecialCharacters(name);
        string containerName = $"blob-{formatted.Trim()}-{Guid.NewGuid()}"
           .ToLower();

        var storage = _configuration.GetValue&lt;string&gt;("AzureStorage:Storage");

        var credential = _clientSecretCredentialProvider
           .GetClientSecretCredential();

        if (storage != null &amp;&amp; credential != null)
        {
            var blobServiceClient = 
            new BlobServiceClient(new Uri(storage), credential);

            var metadata = new Dictionary&lt;string, string?&gt;
            {
                { "name", name },
            };

            // Create the root container
            var blobContainerClient = await blobServiceClient
            .CreateBlobContainerAsync(
                containerName,
                PublicAccessType.None,
                metadata);

            if (blobContainerClient.Value.Exists())
            {
                Console.WriteLine(
                   $"Created container: {name} {blobContainerClient.Value.Name}");
            }

            return blobContainerClient.Value;
        }

        throw new Exception($"Could not create container: {name}");
    }
    catch (RequestFailedException e)
    {
        Console.WriteLine("HTTP error code {0}: {1}", e.Status, e.ErrorCode);
        Console.WriteLine(e.Message);
        throw;
    }
}
  


<p></p>



 Create an RBAC to give the security group Blob storage reader permissions 



<p>The ApplyReaderGroupToBlobContainer method creates an RBAC for the security group on the blob container itself. The group and the container were created in the previous steps and this takes an unknown length of time. Polly is used to repeat until the group and the container are ready and it creates the assignment.</p>


  public async Task ApplyReaderGroupToBlobContainer(
     BlobContainerClient blobContainer, string groupId)
{
    var maxRetryAttempts = 20;
    var pauseBetweenFailures = TimeSpan.FromSeconds(3);

    var retryPolicy = Policy
        .Handle&lt;Exception&gt;()
        .WaitAndRetryAsync(maxRetryAttempts, i =&gt; pauseBetweenFailures);

    await retryPolicy.ExecuteAsync(async () =&gt;
    {
        // RBAC security group Blob data read
        await _azureMgmtClientService
            .StorageBlobDataReaderRoleAssignment(groupId,
                blobContainer.AccountName,
                blobContainer.Name);

        // NOTE service principal blob write is configured on root 
    });
}
  


<p></p>



<p>Azure management REST API is used to create the RBAC. This rest API is implemented using a HttpClient and uses an Enterprise application to define the required permissions. This requires an administration Azure role and with this, you have full control of the Azure tenant.</p>


  using System.Net.Http.Headers;
using System.Text.Json.Serialization;

namespace MultiClientBlobStorage.Providers.Rbac;

public class AzureMgmtClientService
{
    private readonly AzureMgmtClientCredentialService _azureMgmtClientCredentialService;
    private readonly IHttpClientFactory _clientFactory;
    private readonly IConfiguration _configuration;
    private readonly ILogger&lt;AzureMgmtClientService&gt; _logger;

    public AzureMgmtClientService(AzureMgmtClientCredentialService azureMgmtClientCredentialService,
        IHttpClientFactory clientFactory,
        IConfiguration configuration,
        ILogger&lt;AzureMgmtClientService&gt; logger)
    {
        _azureMgmtClientCredentialService = azureMgmtClientCredentialService;
        _clientFactory = clientFactory;
        _configuration = configuration;
        _logger = logger;
    }

    /// &lt;summary&gt;
    /// Storage Blob Data Reader: ID: 2a2b9908-6ea1-4ae2-8e65-a410df84e7d1
    /// Role assignment required for application in Azure on resource group
    /// https://learn.microsoft.com/en-us/rest/api/authorization/role-assignments/create-by-id?view=rest-authorization-2022-04-01&amp;tabs=HTTP
    /// https://learn.microsoft.com/en-us/azure/role-based-access-control/role-assignments-rest
    /// &lt;/summary&gt;
    public async Task StorageBlobDataReaderRoleAssignment(string groupId, string storageAccountName, string blobContainerName)
    {
        // The role ID: Storage Blob Data Reader
        var roleId = "2a2b9908-6ea1-4ae2-8e65-a410df84e7d1";
        var roleNameUnique = $"{Guid.NewGuid()}"; // Must be a guid
        var subscriptionId = _configuration["AzureMgmt:SubscriptionId"];
        // the service principal ID
        var servicePrincipalId = groupId;
        // the resource group name
        var resourceGroupName = _configuration["AzureMgmt:ResourceGroupName"];

        var objectId = $"/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Storage/storageAccounts/{storageAccountName}/blobServices/default/containers/{blobContainerName}";
        var url = $"https://management.azure.com{objectId}/providers/Microsoft.Authorization/roleAssignments/{roleNameUnique}?api-version=2022-04-01";

        var client = _clientFactory.CreateClient();
        var accessToken = await _azureMgmtClientCredentialService.GetAccessToken();

        client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue("Bearer", accessToken);
        client.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue("application/json"));

        var roleDefinitionId = $"{objectId}/providers/Microsoft.Authorization/roleDefinitions/{roleId}";

        var PayloadRoleAssignment = new PayloadRoleAssignment
        {
            Properties = new Properties
            {
                RoleDefinitionId = roleDefinitionId,
                PrincipalId = servicePrincipalId,
                PrincipalType = "Group"
            }
        };

        // view containers
        //var getRe = $"https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Storage/storageAccounts/{storageAccountName}/blobServices/default/containers?api-version=2023-01-01";
        //var response = await client.GetAsync(getRe);
        //var test = await response.Content.ReadAsStringAsync();

        var response = await client.PutAsJsonAsync(url, PayloadRoleAssignment);
        if (response.IsSuccessStatusCode)
        {
            var responseContent = await response.Content.ReadAsStringAsync();
            _logger.LogInformation("Created RBAC for read group {blobContainerName} {responseContent}", blobContainerName, responseContent);
            return;
        }

        var responseError = await response.Content.ReadAsStringAsync();
        _logger.LogCritical("Created RBAC for read group {blobContainerName} {responseError}", blobContainerName, responseError);
        throw new ApplicationException($"Status code: {response.StatusCode}, Error: {response.ReasonPhrase}, {responseError}");
    }

    private class PayloadRoleAssignment
    {
        [JsonPropertyName("properties")]
        public Properties Properties { get; set; } = new();
    }

    /// &lt;summary&gt;
    ///     "properties": {
    ///     "roleDefinitionId":
    ///     "subscriptions/SUBSCRIPTION_ID/resourcegroups/RESOURCE_GROUP_NAME/providers/Microsoft.Storage/storageAccounts/STORAGE_ACCOUNT_NAME/providers/Microsoft.Authorization/roleDefinitions/ROLE_ID",
    ///     "principalId": "SP_ID"
    ///     }
    /// &lt;/summary&gt;
    private class Properties
    {
        [JsonPropertyName("roleDefinitionId")]
        public string RoleDefinitionId { get; set; } = string.Empty;
        [JsonPropertyName("principalId")]
        public string PrincipalId { get; set; } = string.Empty;
        [JsonPropertyName("principalType")]
        public string PrincipalType { get; set; } = "Group";
    }
}
  


<p></p>



 Putting it together 



<p>A Razor page can be used to create the new clients. This method takes an unknown length of time to run and the RBAC also take an unknown length of time to get applied.</p>


  [Authorize(Policy = "blob-admin-policy")]
public class CreateClientModel : PageModel
{
    private readonly ClientBlobContainerProvider _clientBlobContainerProvider;
    private readonly ApplicationMsGraphService _applicationMsGraphService;

    [BindProperty]
    public string ClientName { get; set; } = string.Empty;

    public CreateClientModel(
        ClientBlobContainerProvider clientBlobContainerProvider,
        ApplicationMsGraphService applicationMsGraphService)
    {
        _clientBlobContainerProvider = clientBlobContainerProvider;
        _applicationMsGraphService = applicationMsGraphService;
    }

    public void OnGet()
    {
    }

    public async Task&lt;IActionResult&gt; OnPostAsync()
    {
        if (ModelState.IsValid)
        {
            var group = await _applicationMsGraphService
              .CreateSecurityGroupAsync(ClientName);

            var blobContainer = await _clientBlobContainerProvider
                .CreateBlobContainerClient(ClientName);

            if(blobContainer != null &amp;&amp; group != null &amp;&amp; group.Id != null)
            {
                await _clientBlobContainerProvider
                    .ApplyReaderGroupToBlobContainer(blobContainer, group.Id);
            }
        }

        return Page();
    }
}
  


<p></p>



 Notes 



<p>This works well but requires that the application has high privileged access permissions. Most IT departments will not allow this and the creation of blob containers would have to use the IT preferred tools. This type of automation requires 2 different Azure APIs and is not well documented.</p>



 Links 



<p><a href="https://damienbod.com/2024/02/12/using-blob-storage-from-asp-net-core-with-entra-id-authentication/">Using Blob storage from ASP.NET Core with Entra ID authentication</a></p>



 
<blockquote><a href="https://damienbod.com/2023/01/16/implementing-secure-microsoft-graph-application-clients-in-asp-net-core/">Implementing secure Microsoft Graph application clients in ASP.NET&nbsp;Core</a></blockquote>
 



<p><a href="https://learn.microsoft.com/en-us/azure/storage/blobs/authorize-access-azure-active-directory">https://learn.microsoft.com/en-us/azure/storage/blobs/authorize-access-azure-active-directory</a></p>



<p><a href="https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction">https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction</a></p>



<p><a href="https://github.com/AzureAD/microsoft-identity-web">https://github.com/AzureAD/microsoft-identity-web</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2024/02/diagrams-app-write-multi-tenant-1.png?w=1024'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   07:17 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 03. March 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://fossandcrafts.org/'>Foss & Crafts</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://fossandcrafts.org/episodes/062-blender.html'>62: Blender</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Blender, the absolute powerhouse of FOSS 3d (and increasingly 2d) graphics!
We give an overview of the software's history, some personal history of
our relationships to the software, what it can do, and where we're excited
to see it go!  Links:    Blender    Blender history    Grease pencil    Some historical Blender videos from the NeoGeo and Not a Number days: Did It, Done It, Not a Number comme
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>Blender, the absolute powerhouse of FOSS 3d (and increasingly 2d) graphics!
We give an overview of the software's history, some personal history of
our relationships to the software, what it can do, and where we're excited
to see it go!</p><p>Links:</p>  <p><a href="https://blender.org/">Blender</a></p>  <p><a href="https://www.blender.org/about/history/">Blender history</a></p>  <p><a href="https://docs.blender.org/manual/en/latest/grease_pencil/index.html">Grease pencil</a></p>  <p>Some historical Blender videos from the NeoGeo and Not a Number days: <a href="https://www.youtube.com/watch?v=WKAQNBaZ_I8">Did It, Done It</a>, <a href="https://www.youtube.com/watch?v=2AMubc7C1vw">Not a Number commercial</a>, <a href="https://www.youtube.com/watch?v=2qyVRFI0B6g">Come and See</a></p>  <p><a href="https://orange.blender.org/">Elephants Dream</a>, aka Project Orange</p>  <p><a href="https://peach.blender.org/">Big Buck Bunny</a></p>  <p>Previous episodes on blender:</p>  <a href="https://fossandcrafts.org/episodes/16-bassam-kurdali-blender-open-movies-education.html">Blender for open movie productions and education</a>  <a href="https://fossandcrafts.org/episodes/48-sophie-jantak-pet-portraits-grease-pencil.html">Sophie Jantak on pet portraits and Blender's Grease Pencil</a>    <p>Blender Conference videos mentioned:</p>  <a href="https://conference.blender.org/2023/presentations/1928/">Inklines Across The Spiderverse</a>  <a href="https://conference.blender.org/2023/presentations/1823/">My Journey Across the Spider-Verse: from Hobbyist to Hollywood</a>  <a href="https://www.youtube.com/watch?v=RWPdSJ1-Dvs">Forensic Architecture - spatial analysis for human rights cases</a>    <p><a href="https://www.youtube.com/watch?v=xsi_VyzbDrE">The MediaGoblin campaign video</a> (well, the second one)</p>  <p><a href="https://www.youtube.com/watch?v=MUIMYnSbfPQ">14th anniversary animation gift to Morgan</a></p>  <p><a href="https://www.youtube.com/watch?v=88JUfWLJJ5g">In Unexpected Places</a></p>  <p><a href="http://thomaskole.nl/s2s/">Seams to Sewing Pattern</a> (a Blender plugin for making clothes and stuffed animals!) (could we make <a href="https://fossandcrafts.org/episodes/055-free-soft-wear.html">Free Soft Wear</a> patterns with it?)</p>  <p><a href="https://studio.blender.org/films/wing-it/">Wing It!</a></p>  <p><a href="https://www.youtube.com/watch?v=p6ESaby2KDA&amp;list=PLav47HAVZMjkgw-ueySUvvq1aynJ4s7ry">Wing It! Production Logs</a> and <a href="https://www.youtube.com/watch?v=Kc5XEcHGIHU&amp;list=PLa1F2ddGya_8I8QCMCKlQUOQpgOga2nZ-">Blenderheads</a></p>  <p>Episodes about lisp, because obviously Blender needs more lisp (who's going to do it):</p>  <p><a href="https://fossandcrafts.org/episodes/47-what-is-lisp.html">What is Lisp?</a></p>  <p><a href="https://fossandcrafts.org/episodes/49-lisp-but-beautiful-lisp-for-everyone.html">Lisp but Beautiful, Lisp for Everyone</a></p>    
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:40 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://herestomwiththeweather.com'>Heres Tom with the Weather</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://herestomwiththeweather.com/2024/03/03/internationalizing-the-fediverse/'>Internationalizing the Fediverse</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Yesterday, a request was made to like a post from an account with unicode in the username. As Terence Eden notes in Internationalise The Fediverse 

 
   Mastodon (the largest ActivityPub service) doesn’t allow Unicode usernames and has resisted efforts to change. 
 

 So, I tried with Irwin on otisburg.social and the code threw an exception when I tried to interact with the account 

  (URI::Inva
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>Yesterday, <a href="https://fed.xnor.in/notice/AfRZC7jgkXRA1xk6Fc">a request was made</a> to like a post from an account with unicode in the username. As Terence Eden notes in <a href="https://shkspr.mobi/blog/2024/02/internationalise-the-fediverse/">Internationalise The Fediverse</a></p>

<blockquote>
  <p>Mastodon (the largest ActivityPub service) doesn’t allow Unicode usernames and has resisted efforts to change.</p>
</blockquote>

<p>So, I tried with <a href="https://github.com/herestomwiththeweather/irwin">Irwin</a> on <a href="https://otisburg.social">otisburg.social</a> and the code threw an exception when I tried to interact with the account</p>

  <code>(URI::InvalidURIError) "URI must be ascii only \"https://i18n.viii.fi/@\\u4F60\\u597D\""
</code> 
 

<p>This is not an acceptable uri to pass to URI.parse().  In this case, I found some help from <a href="https://stackoverflow.com/questions/7015778/is-this-the-best-way-to-unescape-unicode-escape-sequences-in-ruby">a stackoverflow post</a> and <a href="https://github.com/herestomwiththeweather/irwin/commit/b5899fc090b654383a82674d2d1bf788e2e3ce9f">fixed the code</a> so I could interact with the account and like the post.</p>

<p>From the comments in Terence’s blog post, I saw <a href="https://github.com/swicg/activitypub-webfinger/issues/9">Allowed characters in preferredUsername</a> has also been identified as an issue with webfinger.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:12 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 27. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/02/extension-ecosystem-summit/'>Extension Ecosystem Summit 2024</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Some pals and I organized a summit at PGConf.dev on May 28 to work together
as a community toward comprehensive indexing, discovery, and binary
distribution, as well as a speaker series “mini summit” leading up to the
event.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	<a href="https://pgconf.dev">
			
		</a>
	

         
<p>I’m pleased to announce that some pals and I have organized and will host the
(first annual?) <a href="https://www.pgevents.ca/events/pgconfdev2024/schedule/session/191/" title="PGConf.dev: Extensions Ecosystem Summit: Enabling comprehensive indexing, discovery, and binary distribution">Extension Ecosystem Summit</a> at <a href="https://2024.pgconf.dev" title="PostgreSQL Development Conference 2024">PGConf.dev</a> in Vancouver (and
more, see below) on May 28:</p>
<blockquote>
<p>Enabling comprehensive indexing, discovery, and binary distribution.</p>
<p>Participants will collaborate to examine the ongoing work on PostgreSQL
extension distribution, examine its challenges, identify questions, propose
solutions, and agree on directions for execution.</p>
</blockquote>
<p>Going to PGConf? Select it as an “Additional Option” when you <a href="https://www.pgevents.ca/events/pgconfdev2024/register/" title="Register for PGConf.dev or Update your Registration">register</a>, or
<a href="https://www.pgevents.ca/events/pgconfdev2024/register/" title="Register for PGConf.dev or Update your Registration">update your registration</a> if you’ve already registered. Hope to see
you there!</p>
 


	<a href="https://www.eventbrite.com/e/851125899477">
			
		</a>
	

 Extension Ecosystem Mini-Summit 
<p>But if you can’t make it, that’s okay, because in the lead up to the Summit, to
we’re hosting a series of six virtual gatherings, the <a href="https://www.eventbrite.com/e/851125899477" title="Postgres Extension Ecosystem Mini-Summit on eventbrite">Postgres Extension
Ecosystem Mini-Summit</a>.</p>
<p>Join us for an hour or so every other Wednesday starting March 6 to hear
contributors to a variety of community and commercial extension initiatives
outline the problems they want to solve, their attempts to so, challenges
discovered along the way, and dreams for an ideal extension ecosystem in the
future. Tentative speaker lineup:</p>
 
 March 6: <a href="https://justatheory.com/">David Wheeler</a>, PGXN: “State of the Extension Ecosystem” 
 March 20: <a href="https://www.linkedin.com/in/istanton">Ian Stanton</a>, Tembo: “Building Trunk: A Postgres Extension Registry and CLI” 
 April 3: <a href="https://github.com/devrimgunduz">Devrim Gündüz</a>: “yum.postgresql.org and the challenges RPMifying
extensions” 
 April 17: <a href="https://jkatz05.com">Jonathan Katz</a>: “TLE Vision and Specifics” 
 May 1: <a href="https://ca.linkedin.com/in/yrashk">Yurii Rashkovskii</a>, Omnigres: “Universally buildable extensions: dev
to prod” 
 May 15: <a href="https://justatheory.com/">David Wheeler</a>, PGXN: “Community Organizing Summit Topics” 
 
<p>Hit the <a href="https://www.eventbrite.com/e/851125899477" title="Postgres Extension Ecosystem Mini-Summit on eventbrite">event page</a> for details. Many thanks to my co-organizers
<a href="https://about.me/jeremy_schneider">Jeremy Schneider</a>, <a href="https://www.crunchydata.com/blog/author/david-christensen">David Christensen</a>, <a href="https://pgxn.org/user/keithf4">Keith Fiske</a>, and <a href="https://github.com/devrimgunduz">Devrim Gündüz</a>,
as well as the <a href="https://2024.pgconf.dev/about/">PGConf.dev organizers</a> for making this all happen!</p>
<p>Updates:</p>
 
 2024-03-06: Updated the talk schedule. 
 2024-04-24: Updated the last talk title. 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/vancouver/">Vancouver</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/shared/extension-ecosystem-summit/pgconf.dev.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:46 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 27. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/02/extension-metadata-typology/'>RFC: Extension Metadata Typology</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Thinking through the PostgreSQL extension metadata use cases and
recognizing the types of information they need.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Lately I’ve been thinking a lot about metadata for Postgres extensions.
Traditional use cases include <a href="https://www.postgresql.org/docs/current/extend-extensions.html#EXTEND-EXTENSIONS-FILES" title="PostgreSQL Docs: Extension Files">control file metadata</a>, which lives in
<code>.control</code> files used by <code>CREATE EXTENSION</code> and friends, and <a href="https://pgxn.org/spec/" title="PGXN Meta Spec - The PGXN distribution metadata specification">PGXN metadata</a>,
which lives in <code>META.json</code> files used by <a href="https://pgxn.org" title="The postgreSQL Extension Network">PGXN</a> to index and publish extensions.
But these two narrow use cases for SQL behavior and source code distribution
don’t provide the information necessary to enable other use cases, including
building, installing, configuration, and more.</p>
<p>So I have also been exploring other metadata formats, including:</p>
 
 <a href="https://go.dev/doc/modules/gomod-ref" title="go.mod file reference">Go <code>go.mod</code></a> 
 <a href="https://doc.rust-lang.org/cargo/reference/manifest.html" title="The Cargo Book: The Manifest Format">Cargo Manifest File</a> 
 <a href="https://docs.npmjs.com/cli/v6/configuring-npm/package-json" title="npm Docs: Specifics of npm's package.json handling">npm <code>package.json</code></a> 
 <a href="https://github.com/tembo-io/trunk/tree/main/contrib"><code>trunk.toml</code></a> 
 <a href="https://docs.pgxman.com/spec/buildkit">pgxman buildkit</a> 
 
<p>These standards from neighboring communities reveal a great deal of overlap, as
one might expect (everything has a name, a version, an author, license, and so
on), but also types of metadata that had not occurred to me. As I took notes and
gathered suggestions from colleagues and coworkers, I began to recognize natural
groupings of metadata. This lead to the realization that it might be easier —
and more productive — to think about these groupings rather than individual
fields.</p>
<p>I therefore propose a typology for Postgres extension metadata.</p>
 Extension Metadata Typology 
 Essentials 
<p>Essential information about the extension itself, including its name (or unique
package name), version, list of authors, license, etc. Pretty much every
metadata format encompasses this data. Ecosystem applications use it for
indexing, installation locations, naming conventions, and display information.</p>
 Artifacts 
<p>A list of links and checksums for downloading the extension in one or more
formats, including source code, binaries, system packages, and more. Apps use
this information to determine the best option for installing an extension on a
particular system.</p>
 Resources 
<p>External information about the extension, mostly links, including source code
repository, bug reporting, documentation, badges, funding, etc. Apps use this
data for links, of course, but also full text indexing, documentation rendering,
and displaying useful information about the extension.</p>
 Contents 
<p>A description of what’s included in the extension package. Often an “extension”
consists of multiple extensions, such as <a href="http://postgis.net/">PostGIS</a>, which includes <code>postgis</code>,
<code>postgis_tiger_geocoder</code>, <code>address_standardizer</code>, and more. Furthermore, some
extensions are not <code>CREATE EXTENSION</code>-type extension at all, such as <a href="https://www.postgresql.org/docs/current/bgworker.html" title="PostgreSQL Docs: Background Worker Processes">background
workers</a>, command-line apps, libraries, etc. Each should be listed along with
documentation links where they differ from the package overall (or are simply
more specific).</p>
 Prerequisites 
<p>A list of external dependencies required to configure, build, test, install, and
run the extension. These include not only other extensions, but also external
libraries and OS-specific lists of binary package dependencies. And let’s not
forget the versions of Postgres required, as well as any OS and version
dependencies (e.g, does it work on Windows? FreeBSD? What versions?) and
architectures (<a href="https://en.wikipedia.org/wiki/AArch64" title="Wikipedia: AArch64">arm64</a>, <a href="https://en.wikipedia.org/wiki/amd64" title="Wikipedia: AMD64">amd64</a>, etc.)</p>
 How to Build It 
<p>Metadata that apps use to determine how to build the extension. Does it use the
PostgreSQL <a href="https://www.postgresql.org/docs/current/extend-pgxs.html" title="PostgreSQL Docs: Extension Building Infrastructure">PGXS</a> build pipeline? Or perhaps it needs the <a href="https://github.com/pgcentralfoundation/pgrx/blob/develop/cargo-pgrx/README.md" title="pgrx: cargo-pgrx">cargo</a>-based <a href="https://github.com/pgcentralfoundation/pgrx" title="pgrx: Build Postgres Extensions with Rust!">pgrx</a>
toolchain. Maybe a traditional <code>./configure &amp;&amp; make</code> pattern? Perl, Ruby,
Python, Go, Rust, or NPM tooling? Whatever the pattern, this metadata needs to
be sufficient for an ecosystem app to programmatically determine how to build
an extension.</p>
 How to Install It 
<p>Usually an extension of the build metadata, the install metadata describes how
to install the extension. That could be <a href="https://www.postgresql.org/docs/current/extend-pgxs.html" title="PostgreSQL Docs: Extension Building Infrastructure">PGXS</a> or <a href="https://github.com/pgcentralfoundation/pgrx" title="pgrx: Build Postgres Extensions with Rust!">pgrx</a> again, but could also
use other patterns — or multiple patterns! For example, perhaps an extension
can be built and installed with <a href="https://www.postgresql.org/docs/current/extend-pgxs.html" title="PostgreSQL Docs: Extension Building Infrastructure">PGXS</a>, but it might also be <a href="https://github.com/aws/pg_tle" title="pg_tle: Framework for building trusted language extensions for PostgreSQL">TLE</a>-safe, and
therefore provide details for handing the SQL files off to a <a href="https://github.com/aws/pg_tle/blob/main/examples/README.md" title="TLE examples/pgtle.mk README">TLE installer</a>.</p>
<p>This typology might include additional data, such as documentation files to
install (<a href="https://en.wikipedia.org/wiki/Man_page" title="Wikipedia: Man page">man pages</a> anyone?), or directories of dependent files or libraries,
and the like — whatever needs to be installed for the extension.</p>
 How to Run It 
<p>Not all Postgres extensions are <code>CREATE EXTENSION</code> extensions. Some provide
<a href="https://www.postgresql.org/docs/current/bgworker.html" title="PostgreSQL Docs: Background Worker Processes">background workers</a> to perform various tasks; others simply provide Utility
applications like <a href="https://pg_top.gitlab.io" title="Welcome to the PostgreSQL top Project Home Page">pg_top</a> and <a href="https://reorg.github.io/pg_repack/" title="pg_repack --- Reorganize tables in PostgreSQL databases with minimal locks">pg_repack</a>. In fact <a href="https://reorg.github.io/pg_repack/" title="pg_repack --- Reorganize tables in PostgreSQL databases with minimal locks">pg_repack</a> provides both
a command-line application and a <code>CREATE EXTENSION</code> extension in one package!</p>
<p>This metadata also provides configuration information, both <a href="https://www.postgresql.org/docs/current/extend-extensions.html#EXTEND-EXTENSIONS-FILES" title="PostgreSQL Docs: Extension Files">control file
parameters</a> like <code>trusted</code>, <code>superuser</code>, and <code>schema</code>, but also load
configuration information, like whether an extension needs its libraries
included in <a href="https://www.postgresql.org/docs/current/runtime-config-client.html#GUC-SHARED-PRELOAD-LIBRARIES" title="PostgreSQL Docs: shared_preload_libraries"><code>shared_preload_libraries</code></a> to enable <a href="https://www.postgresql.org/docs/current/sql-load.html" title="PostgreSQL Docs: LOAD"><code>LOAD</code></a> or requires a
cluster restart. (Arguably this information should be in the “install” typology
rather than “run”.)</p>
 Classification 
<p>Classification metadata lets the extension developer associate additional
information to improve discovery, such as key words. It might also allow
selections from a curated list of extension classifications, such as the
<a href="https://crates.io/category_slugs" title="crates.io: All Valid Category Slugs">category slugs</a> supported for the <a href="https://doc.rust-lang.org/cargo/reference/manifest.html#the-categories-field" title="The Cargo Book: The Manifest Format --- Categories">cargo categories field</a>. Ecosystem apps use
this data to organize extensions under key words or categories, making it easier
for users to find extensions often used together or for various workloads or
tasks.</p>
 Metrics and Reports 
<p>This final typology differs from the others in that its metadata derives from
third party sources rather than the extension developer. It includes data such
as number of downloads, build and test status on various Postgres/OS/version
combinations, binary packaging distributions, test coverage, security scan
results, vulnerability detection, quality metrics and user ratings, and more.</p>
<p>In the broader ecosystem, it would be the responsibility of the root registry to
ensure such data in the canonical data for each extension comes only from
trusted sources, although applications downstream of the root registry might
extend metrics and reports metadata with their own information.</p>
 What More? 
<p>Reading through various metadata standards, I suspect this typology is fairly
comprehensive, but I’m usually mistaken about such things. What other types of
metadata do you find essential for the use cases you’re familiar with? Do they
fit one of the types here, or do they require some other typology I’ve failed to
imagine? Hit the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a> to contribute to
the discussion, or give me a holler <a href="https://xoxo.zone/@theory" title="@theory@xoxo.zone">on Mastodon</a>.</p>
<p>Meanwhile, I’ll be refining this typology and assigning all the metadata fields
to them in the coming weeks, with an eye to proposing a community-wide metadata
standard. I hope it will benefit us all; your input will ensure it does.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/metadata/">Metadata</a> 
                 <a href="https://justatheory.com/tags/classification/">Classification</a> 
                 <a href="https://justatheory.com/tags/categories/">Categories</a> 
                 <a href="https://justatheory.com/tags/rfc/">RFC</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:19 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 26. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2499'>COSE &#8220;typ&#8221; (type) Header Parameter Specification Addressing IETF Last Call Feedback</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Orie Steele and I have updated the COSE “typ” (type) Header Parameter Specification to address feedback received during IETF Last Call. No normative changes were made. Thanks to those that reviewed the specification! The specification is available at: https://www.ietf.org/archive/id/draft-ietf-cose-typ-header-parameter-03.html Besides the spec being useful on its own, it’s worth noting that the CBO
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://twitter.com/OR13b">Orie Steele</a> and I have updated the COSE “typ” (type) Header Parameter Specification to address feedback received during IETF Last Call.  No normative changes were made.</p>
<p>Thanks to those that reviewed the specification!</p>
<p>The specification is available at:</p>
 
 <a href="https://www.ietf.org/archive/id/draft-ietf-cose-typ-header-parameter-03.html">https://www.ietf.org/archive/id/draft-ietf-cose-typ-header-parameter-03.html</a> 
 
<p>Besides the spec being useful on its own, it’s worth noting that the <a href="https://www.ietf.org/archive/id/draft-ietf-cose-cwt-claims-in-headers-10.html">CBOR Web Token (CWT) Claims in COSE Headers</a> specification references this spec, and so won’t exit the RFC Editor queue as an RFC until this one also does.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:26 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2024/02/26/delegated-read-and-application-write-access-to-blob-storage-using-asp-net-core-with-entra-id-authentication/'>Delegated read and application write access to blob storage using ASP.NET Core with Entra ID authentication</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This article shows how an ASP.NET Core application can control the write access to an Azure blob storage container using an application app registration. Microsoft Entra ID is used to control the user access and to implement the authentication of the web application. Code: https://github.com/damienbod/AspNetCoreEntraIdBlobStorage Blogs in this series The solution provides a secure upload […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This article shows how an ASP.NET Core application can control the write access to an Azure blob storage container using an application app registration. Microsoft Entra ID is used to control the user access and to implement the authentication of the web application.</p>



<p>Code: <a href="https://github.com/damienbod/AspNetCoreEntraIdBlobStorage">https://github.com/damienbod/AspNetCoreEntraIdBlobStorage</a></p>



 Blogs in this series 



 
 <a href="https://damienbod.com/2024/02/12/using-blob-storage-from-asp-net-core-with-entra-id-authentication/">Using Blob storage from ASP.NET Core with Entra ID authentication</a> 



 Delegated read and application write access to blob storage using ASP.NET Core with Entra ID authentication 



 <a href="https://damienbod.com/2024/03/04/multi-client-blob-storage-access-using-asp-net-core-with-entra-id-authentication-and-rbac/">Multi client blob storage access using ASP.NET Core with Entra ID authentication and RBAC</a> 
 



<p>The solution provides a secure upload and secure download using Microsoft Entra ID. Users are authenticated using Microsoft Entra ID. The blob storage containers use Microsoft Entra ID security groups to control the read access. The upload access uses the an Enterprise application for the contributor role. This is used from the ASP.NET Core application for file upload.</p>



<p>In Azure, only the application can upload files and the users of the groups can only read the files.</p>







<p></p>



<p>The RBAC are setup as shown:</p>







<p></p>



 Blob storage application write access 



<p>The application can upload files to Azure blob storage. Client secrets are used to acquire the access token using the client credentials OAuth flow. This can be further improved by using a certificate and using the client assertions or even a managed identity from the host service. For development, I normally use a secret for simplicity.</p>


  using Azure.Identity;

namespace DelegatedReadAppWriteBlobStorage.FilesProvider.AzureStorageAccess;

public class ClientSecretCredentialProvider
{
    private readonly IConfiguration _configuration;

    public ClientSecretCredentialProvider(IConfiguration configuration)
    {
        _configuration = configuration;
    }

    public ClientSecretCredential GetClientSecretCredential()
    {
        // Values from app registration
        var tenantId = _configuration.GetValue&lt;string&gt;("AzureAd:TenantId");
        var clientId = _configuration.GetValue&lt;string&gt;("ApplicationClient:ClientId");
        var clientSecret = _configuration.GetValue&lt;string&gt;("ApplicationClient:ClientSecret");

        var options = new ClientSecretCredentialOptions
        {
            AuthorityHost = AzureAuthorityHosts.AzurePublicCloud
        };

        // https://docs.microsoft.com/dotnet/api/azure.identity.clientsecretcredential
        var clientSecretCredential = new ClientSecretCredential(
            tenantId, clientId, clientSecret, options);

        return clientSecretCredential;
    }
}
  


<p></p>



<p>The BlobApplicationUploadProvider uses the token and makes it possible to upload files to the Azure Blob storage. The provider uses the IFormFile as a parameter for the file data. Meta data is persisted in the local SQL database.</p>


  using Azure.Storage.Blobs;
using Azure.Storage.Blobs.Models;

namespace DelegatedReadAppWriteBlobStorage.FilesProvider.AzureStorageAccess;

public class BlobApplicationUploadProvider
{
    private readonly IConfiguration _configuration;
    private readonly ClientSecretCredentialProvider _clientSecretCredentialProvider;

    public BlobApplicationUploadProvider(ClientSecretCredentialProvider clientSecretCredentialProvider,
        IConfiguration configuration)
    {
        _configuration = configuration;
        _clientSecretCredentialProvider = clientSecretCredentialProvider;
    }

    public async Task&lt;string&gt; AddNewFile(BlobFileUploadModel blobFileUpload, IFormFile file)
    {
        try
        {
            return await PersistFileToAzureStorage(blobFileUpload, file);
        }
        catch (Exception e)
        {
            throw new ApplicationException($"Exception {e}");
        }
    }

    private async Task&lt;string&gt; PersistFileToAzureStorage(
        BlobFileUploadModel blobFileUpload,
        IFormFile formFile,
        CancellationToken cancellationToken = default)
    {
        var storage = _configuration.GetValue&lt;string&gt;("AzureStorage:StorageAndContainerName");
        var fileFullName = $"{storage}/{blobFileUpload.Name}";
        var blobUri = new Uri(fileFullName);

        var blobUploadOptions = new BlobUploadOptions
        {
            Metadata = new Dictionary&lt;string, string?&gt;
            {
                { "uploadedBy", blobFileUpload.UploadedBy },
                { "description", blobFileUpload.Description }
            }
        };

        var blobClient = new BlobClient(blobUri, _clientSecretCredentialProvider.GetClientSecretCredential());

        var inputStream = formFile.OpenReadStream();
        await blobClient.UploadAsync(inputStream, blobUploadOptions, cancellationToken);

        return $"{blobFileUpload.Name} successfully saved to Azure Blob Storage Container";
    }
}
  


<p></p>



 Blob storage delegated read access 



<p>The blob storage reader access is given to all users in the security group. This security group uses the app-role from the Azure app registration and the group can access the blob container, not the blob account. No SAS are used .</p>


  using Azure.Core;
using Microsoft.Identity.Client;
using Microsoft.Identity.Web;

namespace DelegatedReadAppWriteBlobStorage.FilesProvider.AzureStorageAccess;

public class DelegatedTokenAcquisitionTokenCredential : TokenCredential
{
    private readonly ITokenAcquisition _tokenAcquisition;
    private readonly IConfiguration _configuration;

    public DelegatedTokenAcquisitionTokenCredential(ITokenAcquisition tokenAcquisition,
        IConfiguration configuration)
    {
        _tokenAcquisition = tokenAcquisition;
        _configuration = configuration;
    }

    public override AccessToken GetToken(TokenRequestContext requestContext, CancellationToken cancellationToken)
    {
        throw new NotImplementedException();
    }

    public override async ValueTask&lt;AccessToken&gt; GetTokenAsync(TokenRequestContext requestContext, CancellationToken cancellationToken)
    {
        string[]? scopes = _configuration["AzureStorage:ScopeForAccessToken"]?.Split(' ');

        if (scopes == null)
        {
            throw new Exception("AzureStorage:ScopeForAccessToken configuration missing");
        }

        AuthenticationResult result = await _tokenAcquisition
            .GetAuthenticationResultForUserAsync(scopes);

        return new AccessToken(result.AccessToken, result.ExpiresOn);
    }
}
  


 Notes 



<p>Using an application for write access to the blob storage has both advantages and disadvantages in context of security. Removing the write access to all users is positive, but when using applications or if the application has weak security, this approach can be bad. Using a managed identity for the deployed service would improve the security and would remove the need for secrets or certificates to authenticate the application. This prepares the access for the next solution which would be to implement a multi-tenant or multi-client application using the Azure blob.</p>



 Links 



<p><a href="https://damienbod.com/2024/02/12/using-blob-storage-from-asp-net-core-with-entra-id-authentication/">Using Blob storage from ASP.NET Core with Entra ID authentication</a></p>



 
<blockquote><a href="https://damienbod.com/2023/01/16/implementing-secure-microsoft-graph-application-clients-in-asp-net-core/">Implementing secure Microsoft Graph application clients in ASP.NET&nbsp;Core</a></blockquote>
 



<p><a href="https://learn.microsoft.com/en-us/azure/storage/blobs/authorize-access-azure-active-directory">https://learn.microsoft.com/en-us/azure/storage/blobs/authorize-access-azure-active-directory</a></p>



<p><a href="https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction">https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction</a></p>



<p><a href="https://github.com/AzureAD/microsoft-identity-web">https://github.com/AzureAD/microsoft-identity-web</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2024/02/diagrams-app-write.png?w=1007'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   08:58 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 24. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://herestomwiththeweather.com'>Heres Tom with the Weather</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://herestomwiththeweather.com/2024/02/24/morris-lukowich-at-saint-arnold-brewing/'>Morris Lukowich at Saint Arnold Brewing</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    

 Today I was fortunate to have my picture taken with Morris Lukowich during Hockey Day in Houston at Saint Arnold Brewing.  The best hockey games I have ever watched in-person were the 1977-78 Houston Aeros.  The WHA All-Star game was in Quebec City on January 17, 1978 and for the picture I believe it is Lukowich’s All-Star jersey from that game that I have borrowed. 
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://coffeebucks.s3.amazonaws.com/20240224_141837.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p></p>

<p>Today I was fortunate to have my picture taken with <a href="https://en.wikipedia.org/wiki/Morris_Lukowich">Morris Lukowich</a> during <a href="https://www.houstonpress.com/event/hockey-day-in-houston-17291274">Hockey Day in Houston</a> at <a href="https://www.saintarnold.com/">Saint Arnold Brewing</a>.  The best hockey games I have ever watched in-person were the <a href="https://en.wikipedia.org/wiki/1977%E2%80%9378_Houston_Aeros_season">1977-78 Houston Aeros</a>.  The WHA All-Star game was in Quebec City on January 17, 1978 and for the picture I believe it is Lukowich’s All-Star jersey from that game that I have borrowed.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://coffeebucks.s3.amazonaws.com/20240224_141837.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:37 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 23. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2495'>Continued refinement: OpenID Federation draft 33 published</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  OpenID Federation draft 33 has been published at https://openid.net/specs/openid-federation-1_0-33.html and https://openid.net/specs/openid-federation-1_0.html. The working group continues refining the specification to make it more consistent and easier to read and implement. We published draft 33 now to get these improvements out to implementers. Per the history entries at https://openid.net/specs
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>OpenID Federation draft 33 has been published at <a href="https://openid.net/specs/openid-federation-1_0-33.html">https://openid.net/specs/openid-federation-1_0-33.html</a> and <a href="https://openid.net/specs/openid-federation-1_0.html">https://openid.net/specs/openid-federation-1_0.html</a>.  The working group continues refining the specification to make it more consistent and easier to read and implement.</p>
<p>We published draft 33 now to get these improvements out to implementers.  Per the history entries at <a href="https://openid.net/specs/openid-federation-1_0-33.html#name-document-history">https://openid.net/specs/openid-federation-1_0-33.html#name-document-history</a>, a summary of changes made in -32 and -33 is:</p>
<p>-33:</p>
 
 Addressed #2111: The metadata_policy_crit claim MAY only appear in Subordinate Statements and its values apply to all metadata_policies found in the Trust Chain. 
 Fixed #2096: Authorization Signed Request Object may contain trust_chain in its payload and should not in its JWS header parameters. 
 Strengthen language requiring client verification with automatic registration. 
 Fixed #2076: Promoted Trust Marks to be a top-level section. 
 Added General-Purpose JWT Claims section. 
 Moved Federation Endpoints section before Obtaining Federation Entity Configuration Information section. 
 Fixed #2110: Explanation text when multiple entity_type parameters are provided in the Subordinate Listing endpoint. 
 Fixed #2112, #2113, and #2114: Defined that client authentication is not used by default and that the default client authentication method, when used, is private_key_jwt. Specified that requests using client authentication use HTTP POST. 
 Fixed #2104: Allow trust marks in Subordinate Statements for implementation profiles that might want this. 
 Fixed #2103: Addressed ambiguities in the definition of constraints. 
 
<p>-32:</p>
 
 Tightened OpenID Connect Client Registration section. 
 Tightened appendix examples. 
 Fixed #2075: Trust Mark endpoint for the provisioning of the Trust Marks. 
 Fixed #2085: Trust Marked Entities Listing, added sub URL query parameter. 
 Made fetch issuer unambiguous by making the iss parameter REQUIRED. 
 Introduced the term “Subordinate Statement” and applied it throughout the specification. Also consistently use the term “registration Entity Statement” for Explicit Client Registration results. 
 Clarified where Entity Statement claims can and cannot occur. 
 Renamed policy_language_crit to metadata_policy_crit. 
 Fixed #2093: Numbered the list defining the order policy operators are applied in. 
 
<p>Special thanks to <a href="https://www.linkedin.com/in/stefan-santesson-951890101/">Stefan Santesson</a> for his thorough review of the specification in the context of the Swedish Federation deployment!</p>
   
  
    
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/openid-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   18:07 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/identity-metasystems-and-lessons'>Identity Metasystems and Lessons from Building the Sovrin Foundation</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I recently spoke with Riley Hughes of Trinsic on his Future of Identity podcast about the birth of Sovrin Foundation, it's inevitable growing pains, self-sovereign identity, identity metasystems, and adoption. Give it a listen. I'm grateful to Riley for having me on as a guest.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I recently spoke with Riley Hughes of <a href="https://trinsic.id/">Trinsic</a> on his Future of Identity podcast about the birth of <a href="https://sovrin.org/">Sovrin Foundation</a>, it's inevitable growing pains, self-sovereign identity, identity metasystems, and adoption. Give it a listen.</p>    <p>I'm grateful to Riley for having me on as a guest.</p>   <p>Some of the ideas in this podcast are explored more deeply in my latest book, <a href="https://www.amazon.com/exec/obidos/ASIN/B0BS2H4HT1/technometria-20">Learning Digital Identity</a> from O’Reilly Media. You can support my writing by buying a copy!</p> <a href="https://www.amazon.com/exec/obidos/ASIN/B0BS2H4HT1/technometria-20">   </a> <p>If you enjoyed this, please consider sharing it with a friend or twenty.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12426e62-a034-4055-b3a6-179855a9f5ad.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   13:39 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://herestomwiththeweather.com'>Heres Tom with the Weather</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://herestomwiththeweather.com/2024/02/23/doctors-without-borders-urgent-update-on-gaza-to-un-security-council/'>Doctors Without Borders urgent update on #Gaza to UN Security Council</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  

 MSF to UN Security Council: The people of Gaza need an immediate and sustained ceasefire now 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  

<p><a href="https://www.doctorswithoutborders.ca/msf-to-un-security-council-the-people-of-gaza-need-an-immediate-and-sustained-ceasefire-now/">MSF to UN Security Council: The people of Gaza need an immediate and sustained ceasefire now</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   11:53 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 22. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@justinsecurity'>Justin Richer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justinsecurity.medium.com/applying-rar-in-oauth-2-and-gnap-76a7bae442da?source=rss-ce3fbf1372f2------2'>Applying RAR in OAuth 2 (and GNAP)</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   The Rich Authorization Request extension to OAuth 2, or RAR, is a way to talk about access in the OAuth space beyond what scopes allow, and it is defined in RFC9396. One of the key motivations behind RAR was admitting, as a community, that a lot of times you need more than a set of scope values to properly describe access to an&nbsp;API.  A Larger&nbsp;Scope  Scopes work pretty well in OAuth, but 
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*f4XdtDR7sw-Qq8pRV2GGbA.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The <a href="https://www.rfc-editor.org/rfc/rfc9396.html">Rich Authorization Request extension to OAuth 2</a>, or RAR, is a way to talk about access in the OAuth space beyond what scopes allow, and it is defined in <a href="https://www.rfc-editor.org/rfc/rfc9396.html">RFC9396</a>. One of the key motivations behind RAR was admitting, as a community, that a lot of times you need more than a set of scope values to properly describe access to an&nbsp;API.</p> A Larger&nbsp;Scope <p>Scopes work pretty well in OAuth, but they’re ultimately just a set of strings. It’s worlds better than the all-or-nothing access that OAuth 1 or API keys give you, and if you have an API, they allow you to easily separate read and write access. But what if you want to be more specific? What if you want to, say, let someone access a specific account for certain features for a certain amount of time? Or read in one place but write to another? And what if you want to compose that set of features differently, such as any account for an unlimited time but only if the user’s online? The ways of describing API access are as varied as the APIs being accessed.</p><p>When faced with this problem, the first thing that many people realize is that because scopes are just strings, their own API can impose semantics and syntax on those strings. So people <a href="https://blue-button.github.io/blue-button-plus-pull/#scopes">add parameters</a>, or define <a href="https://openid.net/specs/openid-heart-fhir-oauth2-1_0.html#rfc.section.2">composition rules</a>, or even define a <a href="https://utilityapi.com/docs/greenbutton/scope">whole query language</a> embedded into the scope string. But those approaches require a lot of complex processing and lack interoperable structure that would let you easily combine protection of multiple kinds of&nbsp;APIs.</p><p>What RAR gives you, instead, is an array of JSON objects to describe your API access. So instead of trying to cram everything into a single string, you can now put together an object that defines exactly what you&nbsp;want.</p> [<br>   {<br>      "type": "payment_initiation",<br>      "actions": [<br>         "initiate",<br>         "status",<br>         "cancel"<br>      ],<br>      "locations": [<br>         "https://example.com/payments"<br>      ],<br>      "instructedAmount": {<br>         "currency": "EUR",<br>         "amount": "123.50"<br>      },<br>      "creditorName": "Merchant A",<br>      "creditorAccount": {<br>         "iban": "DE02100100109307118603"<br>      },<br>      "remittanceInformationUnstructured": "Ref Number Merchant"<br>   }<br>] <p>This object is specific to the API in question and carries with it all the detail that is needed for successful processing. Each kind of API can define its own type value, which in turn defines what’s allowed to go into the rest of the object. And if you need more than one view of things, like read access to A but write access to B, then you can pass in multiple objects in the same structure.</p><p>One important question arises out of this: who needs to know this level of&nbsp;detail?</p> Who Cares About&nbsp;RAR <p>In this regard, RAR really is built on top of the concept of a scope. In an OAuth delegation, there are four parties. The client, resource owner, authorization server (AS), and resource server&nbsp;(RS).</p>The four parties in OAuth delegation, illustration from OAuth 2 In&nbsp;Action<p>These parties have particular relationships, and each of them might care about a RAR object or a scope in a slightly different way. However, the more important question is about which relationship is in&nbsp;play.</p> Client -&gt; AS: Requesting Access <p>When requesting an access token, the client needs to be able to describe to the AS what it wants. RAR allows the client to get VERY specific, if the client knows what details it wants ahead of time. Maybe the client has prompted the resource owner for an account identifier, or has learned through some other protocol where the target system is located, or it’s just been configured to know that it needs to ask for specific objects in order to do specific things. In all of these cases, the client can send RAR objects to the AS just like it would a scope, in the hopes of getting an access token that can do what it asks&nbsp;for.</p> AS -&gt; Client: Granting&nbsp;Access <p>When the access token is granted, the AS can tell the client which RAR objects have been applied to the token. While this information is no substitute for an API discovery protocol, this approach can let the client differentiate what an access token is good for in different dimensions. For example, a client can ask for a token for an available signing service, and then be granted a token for use at a specific signing service, indicated through the locations field in the RAR&nbsp;object.</p> AS -&gt; Resource owner: Gathering Authorization <p>During the delegation process, the AS often needs to prompt the resource owner to see if they’re OK with what’s being delegated. While this starts as the Client-&gt;AS request, RAR gives the AS an opportunity to fine-tune the access by asking the resource owner to be specific, or even filling in values that get put into the resulting RAR object. Maybe the client is asking for account access but the resource owner stipulates that it’s only good for the next five minutes. This does come at a usability cost, since it’s much easier to display a list of scope strings with checkboxes. But experience has shown that this list is not a great security measure anyway, since most users won’t change the checkboxes, and often don’t understand the differentiated access being&nbsp;granted.</p> AS -&gt; RS: Describing Access <p>The access token itself represents a certain set of rights that have been granted. These can be described in the metadata of the token, available through either a structured token field or an introspection response. In this way, the RS can learn what an access token is good for, and apply its policies appropriately. Does the token grant access for the HTTP GET command on the resource at /photo/123-fda1d? Is this token even good at this specific RS, or is it meant for somewhere else? The RAR object can be used to describe all of&nbsp;this.</p> Not Everything Has To&nbsp;Match <p>Finally, it’s important to note that the all of these different branches need not match each other in a single transaction. In one of the applications where I’ve personally deployed RAR, the client never sees the RAR objects. The client knows to ask for a specific scope, and the AS knows that when it sees that scope, the resulting token needs to apply to a whole set of things represented by the current user’s access within the system. The downstream APIs know nothing about users or accounts, but they do know the resources they&nbsp;protect.</p><p>As a consequence, the AS translates the client’s incoming scope request to a set of RAR objects that the APIs understand. The APIs never see or care about the scope, and the client never sees or cares about the RAR. In this way, internal API details stay internal and do not leak unnecessarily into the wider&nbsp;system.</p><p>However, a different client in this same ecosystem does have insight into the details of the API structure, and therefore its requests do specify RAR objects that target the APIs. These objects are processed in exactly the same way by the API servers, which gives us a powerful parallelism and profound code reuse in production.</p> GNAP Native <p>In <a href="https://www.ietf.org/archive/id/draft-ietf-gnap-core-protocol-18.html">GNAP</a>, one of our main goals was to see what an OAuth-style system would look like without the constraints and history of OAuth, and one such constraint includes scopes. Consequently, GNAP’s native access rights is an array of objects that look suspiciously like RAR objects. This design is, of course, intentional, and in many ways RAR is the backport of GNAP’s access rights system to work on top of OAuth 2. While GNAP doesn’t have scopes in the same way, GNAP’s reference-based approach to its API design does allow for the use of a simple string to stand in for the objects in question, allowing a request to have both shortcut and fully specified items in the same&nbsp;request.</p> "access": [<br>    {<br>        "type": "photo-api",<br>        "actions": [<br>            "read",<br>            "write"<br>        ],<br>        "locations": [<br>            "https://server.example.net/",<br>            "https://resource.local/other"<br>        ],<br>        "datatypes": [<br>            "metadata",<br>            "images"<br>        ],<br>        "geolocation": [<br>            { lat: -32.364, lng: 153.207 },<br>            { lat: -35.364, lng: 158.207 }<br>        ]<br>    },<br>    {<br>        "type": "financial-transaction",<br>        "actions": [<br>            "withdraw"<br>        ],<br>        "identifier": "account-14-32-32-3",<br>        "currency": "USD"<br>    },<br>    "dolphin-metadata",<br>    "some other thing"<br>]  How Can I Use&nbsp;RAR? <p>RAR support is starting to show up across different vendors, though it’s not universal yet. One of the companies I work for, <a href="https://www.authlete.com/">Authlete</a>, supports RAR natively. Other products can often have RAR grafted on top, since it takes the form of an extra parameter to be processed by an extension or&nbsp;module.</p><p>The real value is that we are starting to see API access defined in terms of RAR objects, replacing the awkward and error-prone string composition practices of the past. RAR may seem complex, but when you look at how APIs are defined and scopes are used, the power of that complexity really starts to show its&nbsp;value.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*f4XdtDR7sw-Qq8pRV2GGbA.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:58 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/02/extension-versioning/'>The History and Future of Extension Versioning</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  What versioning standard should be used for Postgres extension distribution?
Some context from PostgreSQL and PGXN, a survey of the version standard
landscape today, and a recommendation.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Every software distribution system deals with versioning. Early in the design of
<a href="https://pgxn.org" title="The postgreSQL Extension Network">PGXN</a>, I decided to require <a href="https://semver.org" title="Semantic Versioning">semantic versions</a> (SemVer), a
clearly-defined and widely-adopted version standard, even in its <a href="https://semver.org/spec/v1.0.0-beta.html" title="Semantic Versioning 1.0.0-beta">pre-1.0
specification</a>. I implemented the <a href="https://pgxn.org/dist/semver" title="semver: A semantic version data type / PostgreSQL Extension Network">semver data type</a> that would properly sort
semantic versions, later ported to C by <a href="http://vilain.net" title="Sam Vilain: Free Software Programmer - Wood Craftsman - Open Water Swimmer - Science Enthusiast">Sam Vilain</a> and eventually updated to
<a href="https://semver.org/spec/v2.0.0.html" title="Semantic Versioning 2.0.0">semver 2.0.0</a>.</p>
<p>As I’ve been thinking through the <a href="https://tembo.io/blog/pgxn-ecosystem-jobs" title="The Jobs to be Done by the Ideal Postgres Extension Ecosystem">jobs</a> and <a href="https://gist.github.com/theory/898c8802937ad8361ccbcc313054c29d#tools" title="Extension Ecosystem: Jobs and Tools">tools</a> for the Postgres extension
ecosystem, I wanted to revisit this decision, the context in which it was made,
and survey the field for other options. Maybe a “PGXN v2” should do something
different?</p>
<p>But first that context, starting with Postgres itself.</p>
 PostgreSQL Extension Version Standard 
<p>From the introduction extensions in PostgreSQL 9.1, the project side-stepped the
need for version standardization and enforcement by requiring extension authors
to adopt a <a href="https://www.postgresql.org/docs/current/extend-extensions.html#EXTEND-EXTENSIONS-UPDATES" title="PostgreSQL Docs: “Packaging Related Objects into an Extension —&nbsp;Extension Updates”">file naming convention</a>, instead. For example, an extension named
“pair” must have a file with its name, two dashes, then the version as listed in
its control file, like so:</p>
 <code>pair--1.1.sql
</code> <p>As long as the file name is correct and the version part byte-compatible with
the control file entry, <code>CREATE EXTENSION</code> will find it. To upgrade an extension
the author must provide a second file with the extension name, the old version,
and the new version, all delimited by double dashes. For example, to upgrade our
“pair” extension to version <code>1.2</code>, the author supply all the SQL commands
necessary to upgrade it in this file:</p>
 <code>pair--1.1--1.2.sql
</code> <p>This pattern avoids the whole question of version standards, ordering for
upgrades or downgrades, and all the rest: extension authors have full
responsibility to name their files correctly.</p>
 PGXN Versions 
<p><a href="https://semver.org" title="Semantic Versioning">SemVer</a> simplified a number of issues for PGXN in ways that the PostgreSQL
extension versioning did not (without having to re-implement the core’s file
naming code). PGXN wants all metadata for an extension in its <a href="https://pgxn.org/spec/" title="PGXN Meta Spec - The PGXN distribution metadata specification"><code>META.json</code></a>
file, and not to derive it from other sources that could change over time.</p>
<p>Following the <a href="https://github.com/andk/pause/blob/master/doc/operating-model.md#35-factors-considering-in-the-indexing-phase" title="The PAUSE Operating Model v2: Factors considering in the indexing phase">CPAN model</a>, PGXN also required that extension releases never
decrease the version.<a>1</a> The well-defined sortability of semantic versions
made this validation trivial. PGXN <a href="https://github.com/pgxn/pgxn-manager/commit/d2bd3bf" title="pgxn/pgxn-manager@d2bd3bf: Allow updates to old versions">later relaxed</a> enforcement to allow updates
to previously-released versions. SemVer’s <a href="https://semver.org/#spec-item-11" title="Semantic Versioning 2.0.0: Precedence refers to how versions are compared to each other when ordered.">clearly specified sorting</a> made this
change possible, as the <code>major.minor.patch</code> precedence intuitively compare from
left to right.</p>
<p>In other words, if one had previously released version 1.2.2, then released
1.3.0, a follow-up 1.2.3 is allowed, increasing the <code>1.2.x</code> branch version, but
not, say, 1.2.1, which decreases the <code>1.2.x</code> branch version.</p>
<p>Overall, semantic versions have been great for clarity of versioning of PGXN
extensions. The one bit of conflict comes from extensions that use some other
other version standard in the control file, usually a two-part <code>x.y</code> version not
allowed by SemVer, which requires <code>x.y.z</code> (or, more specifically,
<code>major.minor.patch</code>).</p>
<p>But such versions are usually compatible with SemVer, and because PGXN cares
only about the contents of the <code>META.json</code>, they’re free to use their own
versions in the control file, just as long as the <code>META.json</code> file uses SemVers.</p>
<p>For example, the recent <a href="https://pgxn.org/dist/nominatim_fdw/1.0.0/" title="nominatim_fdw 1.0.0: Nominatim Foreign Data Wrapper for PostgreSQL / PostgreSQL Extension Network">nominatim_fdw v1.0.0</a> release, which of course lists
<code>"version": "1.0.0"</code> in <a href="https://api.pgxn.org/src/nominatim_fdw/nominatim_fdw-1.0.0/META.json" title="nominatim_fdw 1.0.0 META.json">its <code>META.json</code> file</a>, sticks to its preferred
<code>default_version = '1.0'</code> in <a href="https://api.pgxn.org/src/nominatim_fdw/nominatim_fdw-1.0.0/nominatim_fdw.control" title="nominatim_fdw 1.0.0 nominatim_fdw.control">its control file</a>. The extension author simply
appends <code>.0</code> to create a valid SemVer from their preferred version, and as long
as they never use any other patch number, it remains compatible.</p>
 Versioning Alternatives 
<p>Surveying the versioning landscape in 2024 yields a number of approaches. Might
we prefer an alternative for future extensions distribution? Let’s look at the
possibilities.</p>
 Ad Hoc Versions 
<p>As described above, the Postgres <a href="https://www.postgresql.org/docs/current/extend-extensions.html#EXTEND-EXTENSIONS-UPDATES" title="PostgreSQL Docs: “Packaging Related Objects into an Extension —&nbsp;Extension Updates”">file naming convention</a> allows ad hoc
versions. As far as I can tell, so does the <a href="https://www.r-project.org" title="The R Project for Statistical Computing">R Project</a>’s <a href="https://cran.r-project.org/doc/manuals/r-release/R-exts.html#The-DESCRIPTION-file" title="The Comprehensive R Archive Network: Writing R Extensions">CRAN</a>. This approach
seems fine for systems that don’t need to follow version changes themselves, but
much trickier for systems that do. If I want to install the latest version of an
extension, how does the installer know what that latest version is?</p>
<p>The answer is that the extension author must always release them in the proper
order. But if someone releases 1.3.1 of an extension, and then 1.2.1, well then
1.2.1 is the latest, isn’t it? It could get confusing pretty quickly.</p>
<p>Seems better to require some system, so that download and install clients can
get the latest version — or the latest maintenance version of an earlier
release if they need it.</p>
 User Choice 
<p>Quite a few registries allow users to choose their own versioning standards, but
generally with some very specific recommendations to prevent confusion for
users.</p>
 
 <a href="https://packaging.python.org/en/latest/discussions/versioning/" title="Python Packaging User Guide: Versioning">Python Packaging</a> is fairly liberal in the versions it allows, but strongly
recommends <a href="https://semver.org" title="Semantic Versioning">semantic versioning</a> or <a href="https://calver.org" title="CalVer: Timely Project Versioning">calendar versioning</a>
(more on that below). 
 <a href="https://metacpan.org/pod/CPAN::Meta::Spec#VERSION-NUMBERS" title="CPAN::Meta::Spec: Version Numbers">CPAN (Perl)</a> is also fairly liberal, due to its long history of module
distribution, but currently requires “Decimal versions”, which are evaluated
as floating-point numbers, or dotted integer versions, which require
three dot-separated positive integers and must begin with the letter <code>v</code>. 
 <a href="https://guides.rubygems.org/patterns/#semantic-versioning" title="RubyGems Patterns: Semantic Versioning">RubyGems</a> does not enforce a versioning policy, but warns that “using an
‘irrational’ policy will only be a disservice to those in the community who
use your gems.” The project therefore urges developers to follow <a href="https://semver.org" title="Semantic Versioning">SemVer</a>. 
 
<p>These three venerable projects date from an earlier period of registration and
distribution, and have made concessions to times when no policies existed. Their
solutions either try to cover as many legacy examples as possible while
recommending better patterns going forward (Python, Perl), or simply make
recommendations and punt responsibility to developers.</p>
 SemVer 
<p>More recently-designed registries avoid this problem by requiring some level of
versioning standard from their inception. Nearly all use <a href="https://semver.org" title="Semantic Versioning">SemVer</a>, including:</p>
 
 <a href="https://go.dev/ref/mod#versions">Go Modules</a>, where “Each version starts with the letter v, followed by a
semantic version.” 
 <a href="https://doc.rust-lang.org/cargo/reference/resolver.html#semver-compatibility">Cargo (Rust)</a>, which “uses SemVer for specifying version numbers. This
establishes a common convention for what is compatible between different
versions of a package.” 
 <a href="https://docs.npmjs.com/cli/v6/configuring-npm/package-json#version">npm</a>, where the “version must be parseable by <a href="https://github.com/isaacs/node-semver" title="semver(1) -- The semantic versioner for npm">node-semver</a>, which is
bundled with npm as a dependency.” 
 
 CalVer 
<p><a href="https://calver.org" title="CalVer: Timely Project Versioning">CalVer</a> eschews context-free incrementing integers in favor of
semantically-meaningful versions, at least for some subset of a version string.
In other words: make the version date-based. CalVer-versioned projects usually
include the year and sometimes the month. Some examples:</p>
 
 <a href="https://ubuntu.com/about/release-cycle" title="The Ubuntu lifecycle and release cadence">Ubuntu</a> uses <code>YY.0M.MICRO</code>, e.g., <code>23.04</code>, released in April 2023, and
<code>23.10.1</code>, released in October 2023 
 <a href="https://pypi.org/project/Twisted/#history" title="Twisted Release History">Twisted</a> uses <code>YY.MM.MICRO</code>, e.g., <code>22.4.0</code>, released in April 2022 
 
<p>Ultimately, adoption of a CalVer format is a more choice about embedding
calendar-based meaning into a version more than standardizing a specific format.
One can of course use CalVer semantics in a semantic version, as in the Twisted
example, which is fully-SemVer compliant.</p>
<p>In other words, adoption of CalVer need not necessitate rejection of SemVer.</p>
 Package Managers 
<p>What about package managers, like RPM and Apt? Some canonical examples:</p>
 
 
<p><a href="https://en.wikipedia.org/wiki/RPM_Package_Manager#Package_filename_and_label" title="Wikipedia: “RPM Package Manager — Package filename and label">RPM</a> packages use the format:</p>
 <code>&lt;name&gt;-&lt;version&gt;-&lt;release&gt;.&lt;architecture&gt;
</code> <p>Here <code>&lt;version&gt;</code> is the upstream version, but RPM practices a reasonable (if
baroque) <a href="https://blog.jasonantman.com/2014/07/how-yum-and-rpm-compare-versions/" title="How Yum and RPM Compare Versions">version comparison</a> of all its parts. But it does not impose a
standard on upstream packages, since they of course vary tremendously
between communities and projects.</p>
 
 
<p><a href="https://www.debian.org/doc/debian-policy/ch-controlfields.html#standards-version">Apt</a> packages use a similar format:</p>
 <code>[epoch:]upstream_version[-debian_revision]
</code> <p>Again, <code>upstream_version</code> is the version of the upstream package, and not
enforced by Apt.</p>
 
 
<p><a href="https://wiki.alpinelinux.org/wiki/APKBUILD_Reference#pkgver">APK (Alpine Linux)</a> packages use the format</p>
 <code>{digit}{.digit}...{letter}{_suf{#}}...{-r#}
</code> <p>I believe that <code>{digit}{.digit}...{letter}</code> is the upstream package version.</p>
 
 
<p>This pattern makes perfect sense for registries that repackage software from
dozens of upstream sources that may or may not have their own policies. But a
system that defines the standard for a specific ecosystem, like Rust or
PostgreSQL, need not maintain that flexibility.</p>
 Recommendation 
<p>Given this survey, I’m inclined to recommend that the PostgreSQL community
follow the PGXN (and Go, and Rust, and npm) precedent and continue to rely on
and require <a href="https://semver.org" title="Semantic Versioning">semantic versions</a> for extension distribution. It’s not
perfect, given the contrast with the core’s lax version requirements. <a href="https://calver.org" title="CalVer: Timely Project Versioning">CalVer</a>
partisans can still use it, though with fewer formatting options (SemVer forbids
leading zeros, as in the Ubuntu <code>23.04</code> example).</p>
<p>But with its continuing adoption, and especially its requirement by more recent,
widely-used registries, and capacity to support date-based semantics for those
who desire it, I think it continues to make the most sense.</p>
 Wrong! 
<p>I’m probably wrong. I’m often mistaken in one way or another, on the details or
the conclusion. Please tell me how I’ve messed up! Find me on the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a>
channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a> or ping me <a href="https://xoxo.zone/@theory" title="@theory@xoxo.zone">on Mastodon</a>.</p>
 
 
 
 
<p>Why? Because every module on CPAN has one and only one entry in the
index file. <a href="https://social.semiotic.systems/@rjbs/111971794172471384" title="@rjbs@social.semiotic.systems thread reply to @theory@xoxo.zone">Ricardo Signes explains</a>.&nbsp;<a>↩︎</a></p>
 
 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/versioning/">Versioning</a> 
                 <a href="https://justatheory.com/tags/semver/">SemVer</a> 
                 <a href="https://justatheory.com/tags/calver/">CalVer</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:33 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 21. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@dkelts.id'>David Kelts on ID</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://medium.com/@dkelts.id/ripping-ids-should-go-the-way-of-cds-49fec9206492?source=rss-9bac94a4f241------2'>“Ripping” IDs Should Go the Way of CDs…</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   The Solvable Problems of Mobile&nbsp;KYC  Remember endless hours ripping your CD collection to digital…why are we spending endless hours scanning analog ID documents over and over again? Alternatives exist. Let’s think differently about how Mobile KYC fails customers  A recent thread on a work-oriented social platform caught my attention. Some people I know to be the people they are could not veri
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*GEwlqcHq9v2qU6uLsUb0yg.jpeg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 The Solvable Problems of Mobile&nbsp;KYC <p>Remember endless hours ripping your CD collection to digital…why are we spending endless hours scanning analog ID documents over and over again? Alternatives exist. Let’s think differently about how Mobile KYC fails customers</p><p>A recent thread on a work-oriented social platform caught my attention. Some people I know to be the people they are could not verify their profiles using a mobile KYC solution. This article isn’t to bash those solutions, or the myriad improvements made, but to urge our industry to understand the complexity we’ve foisted on our users and incite action to do different.</p>Analog to Digital Conversion, anyone? “Ripping IDs” to confirm identity should go the way of Ripping&nbsp;CDs… How do I know the problems that&nbsp;exist? <p>In December 2014, before a presentation to the <a href="https://www.idesg.org/The-ID-Ecosystem/Overview.html">Identity Ecosystem Steering Group</a> about how to “Bring the trust of the Driver’s license online”, MorphoTrust USA filed a provisional application that became <a href="https://patents.justia.com/patent/10678939">US Patent 10,678,939</a>. The front-back-selfie wave was born, admittedly without the system of record connection described in that patent text. I spent several years leading a smart team improving an identity verification product&nbsp;line.</p><p>I’ve watched thousands of people try to run these selfie-to-DL matching steps in various forms and helped debug their issues with smiles, vision acuity, phone models, and lighting. I’ve spent weekends helping <a href="https://mobileidworld.com/with-afcu-onboard-utah-plans-selfie-enrollment-for-mobile-drivers-license/">hundreds of great Utahns use mobile registration live in a credit union&nbsp;lobby</a>.</p><p>Visually impaired people cannot get over these hurdles. It takes too many hands for these solutions to be inclusive. Back-facing phone camera quality varies greatly with how much you can spend. These solutions are not inclusive by any stretch of the imagination, and the fallback is to go to a physical location.</p> The Places Within the Selfie-ID Verification Process that&nbsp;Fail   Manually taking a quality, flat, well-lit, high-res picture of a card is&nbsp;hard  Taking an <a href="https://global.itu.edu.tr/docs/librariesprovider33/default-document-library/%C4%B1cao-standarts.pdf?sfvrsn=c9eea239_0">ICAO-quality</a>, well-lit, machine-matchable portrait is difficult. Everybody wants to smile (or make duck lips&nbsp;…idk…) in a&nbsp;selfie  Webcams are very low resolution compared to phones and vary&nbsp;widely  <a href="https://freeaamvapdf417generator.com/">PDF417 Barcodes</a> from the back of DL/ID cards are easy to generate and <a href="https://www.lseg.com/en/insights/risk-intelligence/2024-will-ai-break-content-based-identity-verification">nearly every Fake ID</a> has the same information on the back and&nbsp;front  Connections to databases that would verify the authenticity and validity of a DL/ID are expensive or restricted to law enforcement (makes&nbsp;sense)  Most hard-to-forge security features of an ID card require multi-spectral light, are tactile, or require magnification. Phones capture visible&nbsp;light  Cropping the card portrait results in a small, low-res photo for machine-matching that has security lines, holograms, and indicia through&nbsp;it  Cropping a portrait from a poorly captured, low-res ID card will not give sufficient resolution to measure the facial features for&nbsp;matching  You need cloud/phone processing power to determine that a human is live and real for the probe image — called <a href="https://www.nist.gov/system/files/documents/2020/09/15/12_buschthieme-ibpc-pad-160504.pdf">Presentation Attack Detection</a>  I have to ID myself repeatedly with each new service that wants (notice I didn’t say “needs”) ID and each KYC system differs, creating&nbsp;doubt  <p>Can I stop here? We haven’t even gotten to the accuracy of biometric matching algorithms (new entries, if used, have solved any racial/gender bias, so lighting is the major hurdle to accurate matching). People think biometrics are 100% when a human visual verificdation is only ~97% (per a 2017 Google study that I cannot&nbsp;find).</p><p>Oh, did I mention that people end up with images of their IDs in photo rolls and synchronized to every app they grant photo library permission?</p><p>Humans are humans and will do what humans do using technology.</p>You simply cannot account for the variations people and machines will cause to happen (credit Jurassic&nbsp;Park)<p>If there are this many variables to a process that seems conceptually simple, one or more of those variables will go&nbsp;wrong.</p> We are turning away good potential customers <p>There are tremendous capture, scan, photo, and biometric matching systems out there from excellent, reputable vendors. The improvements in the decade since selfie matching started are incredible and laudable. This process is significantly easier and more accurate than it ever&nbsp;was.</p><p>Still, people sitting at their desks get turned away from having “verified profiles”. Now try taking a picture of your ID with one hand, holding your phone in the other, on a sidewalk balancing your potential rental scooter against your hip in bright, vacation sunlight. Ya,&nbsp;no!</p> Let’s go&nbsp;Digital <p>Mobile Driver’s Licenses (mDL) are now available to<a href="https://www.mdlconnection.com/implementation-tracker-map/"> greater than 17% of the US</a> cardholding population. Adoption isn’t awesome yet because people don’t know they have existed in production for nearly 2 years and <a href="https://medium.com/@dkelts.id/mobile-driver-licenses-mdl-how-to-use-iso-18013-5-5a1bbc1a37a3">in concept for four-plus years</a>. ISO/IEC 18013–5 is solid and functional. The data is digitally signed by your Issuer but you control it. It provides a level of control over data and transparency into data-storage that users haven’t experienced for decades. There are <a href="https://mobiledl.us/verify/">verifier</a> <a href="https://www.oneproof.co/mdl">apps</a> and <a href="https://credenceid.com/tap2id">equipment</a> from multiple vendors. Places <a href="https://dld.utah.gov/mdlusage/">do accept&nbsp;mDL</a>.</p><p>In the coming months of 2024, ISO/IEC 18013–7 will add consented, selective-disclosure, digitally-signed mDL sharing to apps on the same device AND to websites over the Internet. (This will be the topic of a forthcoming post of mine.) So imagine sharing just your age to be granted entry to that website you don’t want your name associated with… or accurately filing for benefits without typing into a form. During the final approval stages, the ISO/IEC 18013–7 Technical Specification will be available for implementation.</p> Let’s Go Digital to&nbsp;Digital <p>“The first bank to replace their Mobile KYC with a connection to a Mobile Driver’s License is going to garner attention from the press and obtain new customers.”</p><p>It’s time.</p><p>The mDL provides digitally signed, matchable portraits upon consent as well as only the digitally signed data relevant for the approval. ISO/IEC 18013–7 will allow app-to-app consent and retrieval of mDL data similar to the way your email launches your airline application. It will also present a subset of mDL data to a website after consent for uses like age verification.</p><p>The challenge is made: Let’s short-circuit problems 1, 3, 4, 5, 6, 7, 8, and 10 above. Let’s make better self-registration and selfie-KYC solutions. Let’s work together on mDL acceptance. Who will sign&nbsp;up:</p><p>Relying Party (bank): _____________________________________</p><p>Biometric Match and Liveness Vendor: ______________________</p><p>mDL Solution Provider: ____________________________________</p><p>Test Subject mDL Holders: _________________________________</p><p>David can be reached for comments and feedback at david at decipher dot&nbsp;id</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*GEwlqcHq9v2qU6uLsUb0yg.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:39 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 21. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://tembo.io/blog/pgxn-ecosystem-jobs'>Extension Ecosystem Jobs to be Done</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The challenges of the current Postgres extension ecosystem and the
interest and energy put into exploring new solutions make clear that the
time has come to revisit the whole idea. We begin with a survey of the
jobs to be done by extensions packaging and distribution.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Over <a href="https://tembo.io/blog/pgxn-ecosystem-jobs" title="The Jobs to be Done by the Ideal Postgres Extension Ecosystem">on the Tembo blog</a> I’ve published a thinking-through of what all the
<a href="https://strategyn.com/jobs-to-be-done/jobs-to-be-done-theory/" title="Fundamentals of Jobs-to-be-Done Theory">jobs to be done</a> of the ideal Postgres extension ecosystem might be:</p>
<blockquote>
<p>These challenges and the interest and energy put into exploring new solutions
make clear that the time has come to revisit the whole idea of the PostgreSQL
extension ecosystem: to work though the jobs to be done, specify the tools to
do those jobs, and outline a plan for the broader Postgres community to design
and build them.</p>
<p>Future posts will dream up the tools and make the plan; today we begin with
the jobs.</p>
<p>🎬 Let’s get started.</p>
</blockquote>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/metadata/">Metadata</a> 
                 <a href="https://justatheory.com/tags/jobs-to-be-done/">Jobs to be Done</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:03 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 19. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://herestomwiththeweather.com'>Heres Tom with the Weather</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://herestomwiththeweather.com/social/2024/02/19/67174/'></a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  
   
  
  
  

</div>
</div>

<div class='item-footer'>
   18:39 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2024/02/19/using-a-csp-nonce-in-blazor-web/'>Using a CSP nonce in Blazor Web</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This article shows how to use a CSP nonce in a Blazor Web application using the InteractiveServer server render mode. Using a CSP nonce is a great way to protect web applications against XSS attacks and other such Javascript vulnerabilities. Code: https://github.com/damienbod/BlazorServerOidc Notes The code in this example was built using the example provided by […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This article shows how to use a CSP nonce in a Blazor Web application using the InteractiveServer server render mode. Using a CSP nonce is a great way to protect web applications against XSS attacks and other such Javascript vulnerabilities.</p>



<p>Code: <a href="https://github.com/damienbod/BlazorServerOidc">https://github.com/damienbod/BlazorServerOidc</a></p>







<p></p>



 Notes 



<p>The code in this example was built using the example provided by <a href="https://github.com/javiercn">Javier Calvarro Nelson</a>.</p>



<p><a href="https://github.com/javiercn/BlazorWebNonceService">https://github.com/javiercn/BlazorWebNonceService</a></p>



 Services and middleware 



<p>The Blazor Web application is implemented using the AddInteractiveServerComponents for the InteractiveServer server render mode. The nonce can be used by implementing a nonce service using the CircuitHandler. The nonce service is a scoped service.</p>


  builder.Services.AddRazorComponents()
	.AddInteractiveServerComponents();

builder.Services.TryAddEnumerable(ServiceDescriptor
	.Scoped&lt;CircuitHandler, BlazorNonceService&gt;(sp =&gt;
		sp.GetRequiredService&lt;BlazorNonceService&gt;()));

builder.Services.AddScoped&lt;BlazorNonceService&gt;();
  


<p>The headers are implemented using the NetEscapades.AspNetCore.SecurityHeaders package. The headers are added to the Blazor nonce service using the NonceMiddleware middleware.</p>


  app.UseSecurityHeaders(SecurityHeadersDefinitions.GetHeaderPolicyCollection(
	app.Environment.IsDevelopment(),
	app.Configuration["OpenIDConnectSettings:Authority"]));

app.UseMiddleware&lt;NonceMiddleware&gt;();
  


 Setup Security headers 



<p>The security headers CSP script tag is setup as best possible for a Blazor Web application. A CSP nonce is used as well as the fallback definitions for older browsers.</p>


  .AddContentSecurityPolicy(builder =&gt;
{
    builder.AddObjectSrc().None();
    builder.AddBlockAllMixedContent();
    builder.AddImgSrc().Self().From("data:");
    builder.AddFormAction().Self().From(idpHost);
    builder.AddFontSrc().Self();
    builder.AddBaseUri().Self();
    builder.AddFrameAncestors().None();

    builder.AddStyleSrc()
		.UnsafeInline()
		.Self();

    // due to Blazor
    builder.AddScriptSrc()
		.WithNonce()
		.UnsafeEval() // due to Blazor WASM
		.StrictDynamic()
		.OverHttps()
		.UnsafeInline(); // fallback for older browsers when the nonce is used 
})
  


 Setup Middleware to add the nonce to the state 



<p>The NonceMiddleware uses the nonce header created by the security headers package and sets the Blazor nonce service with the value. This is updated on every request.</p>


  namespace BlazorWebFromBlazorServerOidc;

public class NonceMiddleware
{
    private readonly RequestDelegate _next;

    public NonceMiddleware(RequestDelegate next)
    {
        _next = next;
    }

    public async Task Invoke(HttpContext context, 
          BlazorNonceService blazorNonceService)
    {
        var success = context.Items.TryGetValue(
          "NETESCAPADES_NONCE", out var nonce);

        if (success &amp;&amp; nonce != null)
        {
            blazorNonceService.SetNonce(nonce.ToString()!);
        }
        await _next.Invoke(context);
    }
}
  


 Using the nonce in the UI 



<p>The BlazorNonceService can be used from the Blazor components in the InteractiveServer render mode. The nonce is applied to all script tags. If the script does not have the correct nonce, it will not be loaded. The GetNonce method reads the nonce value from the BlazorNonceService service.</p>


  @inject IHostEnvironment Env
@inject BlazorNonceService BlazorNonceService
@using System.Security.Cryptography;

&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
&lt;head&gt;
    &lt;meta charset="utf-8" /&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0" /&gt;
    &lt;base href="/" /&gt;
    &lt;link rel="stylesheet" href="css/bootstrap/bootstrap.min.css" /&gt;
    &lt;link href="css/site.css" rel="stylesheet" /&gt;
    &lt;link href="BlazorWebFromBlazorServerOidc.styles.css" rel="stylesheet" /&gt;
    &lt;HeadOutlet @rendermode="InteractiveServer" /&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;Routes @rendermode="InteractiveServer" /&gt;

    http://_framework/blazor.web.js
&lt;/body&gt;
&lt;/html&gt;

@code
{
    /// &lt;summary&gt;
    /// Original src: https://github.com/javiercn/BlazorWebNonceService
    /// &lt;/summary&gt;
    [CascadingParameter] HttpContext Context { get; set; } = default!;

    protected override void OnInitialized()
    {
        var nonce = GetNonce();
        if (nonce != null)
        {
            BlazorNonceService.SetNonce(nonce);
        }
    }

    public string? GetNonce()
    {
        if (Context.Items.TryGetValue("nonce", out var item) 
           &amp;&amp; item is string nonce and not null)
        {
            return nonce;
        }

        return null;
    }
}
  


 Notes 



<p>Nonces can be applied to Blazor Web using the server rendered mode and the BlazorNonceService which implements the CircuitHandler. Thanks the <a href="https://github.com/javiercn">Javier Calvarro Nelson</a> for providing a solution to this. Next would be to find a solution for the AddInteractiveWebAssemblyComponents setup. You should always use a CSP nonce on a server rendered application and only load scripts with the CSP nonce applied to it.</p>



 Links 



<p><a href="https://github.com/javiercn/BlazorWebNonceService">https://github.com/javiercn/BlazorWebNonceService</a></p>



<p><a href="https://github.com/andrewlock/NetEscapades.AspNetCore.SecurityHeaders">https://github.com/andrewlock/NetEscapades.AspNetCore.SecurityHeaders</a></p>



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2024/01/brandblazor_nohalo_1000x.png?w=1000'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   07:09 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 16. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://idm-thoughtplace.blogspot.com/'>IdM Thoughtplace</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='http://idm-thoughtplace.blogspot.com/2024/02/regarding-recent-sap-idm-announcement.html'>Regarding the recent SAP IDM Announcement</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    &nbsp;“Life
begins like a dream, becomes a little real, and ends like a dream.”&nbsp;―
Michael Bassey Johnson,&nbsp;The
Oneironaut’s Diary 
 As
many of you already know, SAP has made public its plans on how SAP
IDM will be retired as a supported offering. I’ve been stewing on
this for a bit as I try to figure out exactly how I feel about this
and what needs to happen next. 
 To
be fair, I haven’t
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 <p>&nbsp;“Life
begins like a dream, becomes a little real, and ends like a dream.”&nbsp;―
Michael Bassey Johnson,&nbsp;<a href="https://www.goodreads.com/work/quotes/96258745">The
Oneironaut’s Diary</a></p>
<p>As
many of you already know, SAP has made public its plans on how SAP
IDM will be retired as a supported offering. I’ve been stewing on
this for a bit as I try to figure out exactly how I feel about this
and what needs to happen next.</p>
<p>To
be fair, I haven’t worked with the product much for just over four
years, and even then, I was working more with Version 7 than with
Version 8. My opinions are completely my own and do not represent my
current employer, any previous employer, or SAP.</p>
<p>While
IDM is certainly showing its age, there are some very good things
about it that I would love to see as an open-source offering. First
is the Batch Processing capabilities of IDM, based on the old MaXware
Data Synchronization Engine/MetaCenter solutions. It features some
powerful functionality to synchronize and cleanse data. It sets up
fairly easily and is quite easy to configure. I’m sure the
open-source community could do well with maintaining the UI (It
definitely should be JAVA-based rather than the old Windows MMC) that
will fit better in today’s Enterprise setting. Also, easy
integration with SaaS services is a needed upgrade.</p>
<p>The
other thing that should be released into the wild is the Virtual
Directory. It also provides powerful functionality for several use
cases, from pass-through authentication to assisting in M&amp;A use
cases. It’s the perfect example of a “Black Box” offering that
just works. It also makes it much easier to synchronize and cleanse
data by representing many different back ends via the easy-to-consume
LDAP standard.</p>
<p>It
saddens me that SAP is choosing to move away from IDM, as one of the
key selling points of SAP IDM is its ability to integrate seamlessly
with the SAP ecosystem. I hope SAP will help all LCM/IGA vendors
connect more easily with systems. SaaS integration should be easy or
standards-based, but we still need to be concerned for organizations
still using on-premises SAP tools.</p>
<p>SAP
has indicated that Microsoft’s Entra ID will be the main partner in
the future, but I hope they make this information open to all vendors
and that there will be continuing support of standard protocols.
This&nbsp;<a href="https://community.sap.com/t5/technology-blogs-by-sap/preparing-for-sap-identity-management-s-end-of-maintenance-in-2027/ba-p/13596101#:~:text=Maintenance%20for%20SAP%20Identity%20Management,a%20well-considered%20migration%20strategy.">article</a>&nbsp;gives
me some hope, but actions speak louder than words. I do have some
concerns that SAP, known as a vast software ecosystem that supports
itself and tends to ignore the enterprise, is handing off to another
large software provider whose management tools tend to support their
software ecosystem first and consider the enterprise second. Let’s
face it: most of Microsoft’s Identity and Access Management efforts
have been about supporting the Office 365 family of products. Don’t
get me wrong; it’s better than SAP in this regard, but it’s not
that high of a level to meet. For what it’s worth, I am guardedly
optimistic, but I always try to remain hopeful.</p>
<p>Finally,
I think it’s important to thank the IDM team in Sofia for all their
hard work over the years, which, of course, would not have been
possible without the vision and effort of the original MaXware team
based in Trondheim, Norway, and associated teams in the UK,
Australia, and the US. The production from these small teams helped
define what Identity Management is to this day.</p>
<p>Will
this be my last blog entry on the topic of SAP IDM? I don’t know.
Part of it will depend on if there are any moves towards the Open
Source world. There have been at least three times in my life when I
thought I was done with this tool, and deep down, I’m pretty sure
there is a little more in my future.&nbsp;</p>
<p>In
the meantime, I hope to resume blogging more regarding the Identity
and Access Management field in the near future. Time will tell.</p>
<p><br>

</p><br> 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   22:30 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://kentbull.com/'>Kent Bull</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://kentbull.com/2024/02/16/cesr-enters-provisional-status-in-iana-media-type-registry/'>CESR enters provisional status in IANA Media Type Registry</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Registration of the composable event streaming representation (CESR) format in the IANA Media Type Registry shows a recent development of the key event receipt infrastructure (KERI) and authentic chained data containers (ACDC) space and how the space is growing. See the following link for the official entry: IANA Media Type […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Registration of the composable event streaming representation (CESR) format in the IANA Media Type Registry shows a recent development of the key event receipt infrastructure (KERI) and authentic chained data containers (ACDC) space and how the space is growing.</p>



<p>See the following link for the official entry: IANA Media Type Registry <a href="https://www.iana.org/assignments/provisional-standard-media-types/provisional-standard-media-types.xhtml#Samuel_M._Smith">entry for CESR</a> (application/cesr)</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:19 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 15. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.jonudell.net'>Jon Udell</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.jonudell.net/2024/02/15/creating-a-gpt-assistant-that-writes-pipeline-tests/'>Creating a GPT Assistant That Writes Pipeline Tests</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Here’s the latest installment in the series on working with LLMS: Creating a GPT Assistant That Writes Pipeline Tests. Once you get the hang of writing these tests, it’s mostly boilerplate, so I figured my team of assistants could help. I recruited Cody, GitHub Copilot, and Unblocked — with varying degrees of success. Then I … Continue reading Creating a GPT Assistant That Writes Pipeline Tests
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Here’s the latest installment in the series on working with LLMS: <a href="https://thenewstack.io/creating-a-gpt-assistant-that-writes-pipeline-tests/">Creating a GPT Assistant That Writes Pipeline Tests</a>. </p>
<blockquote><p>
Once you get the hang of writing these tests, it’s mostly boilerplate, so I figured my team of assistants could help. I recruited Cody, GitHub Copilot, and Unblocked — with varying degrees of success. Then I realized I hadn’t yet tried creating a GPT. As OpenAI describes them, “GPTs are custom versions of ChatGPT that users can tailor for specific tasks or topics by combining instructions, knowledge, and capabilities.”
</p></blockquote>
 
<p>The rest of the series:</p>
<p>
1 <a href="https://blog.jonudell.net/2023/05/24/when-the-rubber-duck-talks-back/">When the rubber duck talks back</a> </p>
<p>
2 <a href="https://blog.jonudell.net/2023/06/14/radical-just-in-time-learning/">Radical just-in-time learning</a> </p>
<p>
3 <a href="https://blog.jonudell.net/2023/06/18/why-llm-assisted-table-transformation-is-a-big-deal/">Why LLM-assisted table transformation is a big deal</a> </p>
<p>
4 <a href="https://thenewstack.io/using-llm-assisted-coding-to-write-a-custom-template-function/">Using LLM-Assisted Coding to Write a Custom Template Function</a></p>
<p>
5 <a href="https://thenewstack.io/elevating-the-conversation-with-llm-assistants/">Elevating the Conversation with LLM Assistants</a></p>
<p>
6 <a href="https://thenewstack.io/how-large-language-models-assisted-a-website-makeover/">How Large Language Models Assisted a Website Makeover</a></p>
<p>
7 <a href="https://thenewstack.io/should-llms-write-marketing-copy/">Should LLMs Write Marketing Copy?</a></p>
<p>
8 <a href="https://thenewstack.io/test-driven-development-with-llms-never-trust-always-verify/">Test-Driven Development with LLMs: Never Trust, Always Verify</a> </p>
<p>
9 <a href="https://thenewstack.io/learning-while-coding-how-llms-teach-you-implicitly/">Learning While Coding: How LLMs Teach You Implicitly</a></p>
<p>
10 <a href="https://thenewstack.io/how-llms-helped-me-build-an-odbc-plugin-for-steampipe/">How LLMs Helped Me Build an ODBC Plugin for Steampipe</a></p>
<p>
11 <a href="https://thenewstack.io/how-to-use-llms-for-dynamic-documentation/">How to Use LLMs for Dynamic Documentation</a></p>
<p>
12 <a href="https://blog.jonudell.net/2023/11/06/lets-talk-conversational-software-development/">Let’s talk: conversational software development</a></p>
<p>
13 <a href="https://thenewstack.io/techniques-for-using-llms-to-improve-sql-queries/">Using LLMs to Improve SQL Queries</a></p>
<p>
14 <a href="https://thenewstack.io/puzzling-over-the-postgres-query-planner-with-llms/">Puzzling over the Postgres Query Planner with LLMs</a></p>
<p>
15 <a href="https://thenewstack.io/7-guiding-principles-for-working-with-llms/">7 Guiding Principles for Working with LLMs</a></p>
<p>
16 <a href="https://thenewstack.io/learn-by-doing-how-llms-should-reshape-education/">Learn by Doing: How LLMs Should Reshape Education</a></p>
<p>
17 <a href="https://thenewstack.io/how-to-learn-unfamiliar-software-tools-with-chatgpt/">How to Learn Unfamiliar Software Tools with ChatGPT</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:15 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 13. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.moxytongue.com/'>Moxy Tongue</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='http://www.moxytongue.com/2024/02/own-your-own-ai.html'>Own Your Own AI</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Working on it.. OYO AI&nbsp;by kidOYO® Learning Software, Education Services 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>Working on it.. <a href="http://www.ownyourownai.com">OYO AI</a>&nbsp;by kidOYO® Learning Software, Education Services</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   02:07 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 12. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.moxytongue.com/'>Moxy Tongue</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='http://www.moxytongue.com/2024/02/sovereign-ai.html'>Sovereign AI</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   In 2024, the utility of words yielding philosophical clarity that becomes embedded into the design of systems being deployed globally, and Nationally, yields methods that must be structured accurately in order to abide by the Sovereign systems they serve.  In America, people own root authority, or the Sovereign infrastructure does not confer accuracy for, of, by human use. Data is the life blood o
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>In 2024, the <a href="https://www.republicworld.com/tech/artificial-intelligence/nvidia-ceo-jensen-huang-urges-nations-to-develop-sovereign-ai-infrastructure/">utility of words</a> yielding philosophical clarity that becomes embedded into the <a href="https://blogs.nvidia.com/blog/world-governments-summit/">design</a> of systems being deployed globally, and Nationally, yields methods that must be structured accurately in order to abide by the Sovereign systems they serve.</p><p>In America, people own root authority, or the Sovereign infrastructure does not confer accuracy for, of, by human use. Data is the life blood of AI systems. Data structure yields Sovereign results, and across our fast advancing world, inaccuracy deconstructs faster than accuracy builds accurately. The time has come for open transparent accuracy in the data structure of Soveriegnty itself to be laid bare, enabling the development of "Sovereign AI" upon foundations that serve people.</p><p>Many moons ago, this structural conversation began in the world of identity management. Professionally-deployed systems were challenged to confront inaccuracies in their modeling of human identity. Ubiquitously, people were no longer being conveyed structural constraints ensuring the structural accuracy of their root administrative authority over data systems of ultimate importance to their Sovereign participation and administration under well-founded laws that were crafted pre-tcp/ip, pre-digital data.</p><p>Identity systems have been challenged now for over 20 years to align their practices to the people they service. The work is not done. Self-Sovereign ID principles that emerged here on this blog, led to decentralized identity methods and practices advancing for developer use, and into general awareness by a population that is intensely interested in digital frontiers where their lives meet opportunity, security, and civil system integrity. The fire walls of Sovereign integrity, having been breached many times in consequential ways, started exposing their own structural deficiencies.</p><p>Enter AI: human identity that primarily exists in a database-driven system, and is founded on an old-era of physical presence, is now the domain of AI. Human beings can not compete structurally here, as AI derives utility that people provide, and far too often, provide ignorantly, without much personal insight or accountability for the structural choices conveyed upon them. Laws, as dependencies function, evolve at a much slower pace, and seem to lack insight into the structural underpinnings of identity silos that tcp/ip was advanced to break down and add utility too. Unfortunately, protections were not advanced with the same insight, and civil society is finding itself in a reactive mode, reacting to change like a surfer riding a wave, rather than a break wall securing civil participation in an AI-enabled society.</p><p>This is the moment. AI Sovereignty has a basic and tremendously important dependency in American civil society: people own root.&nbsp;</p><p>If the data structure of human participation in America does not convey this basic structural reality, then people do not exist in a civil society, as defined by founding documents, intent, and Constitutional reach. Work is underway on this vector, and as always, the resulting choices and structures advanced will yield the results being pursued. The question on the table being asked is simple: do innovators understand what it means in structural Terms to ensure that people own root authority?&nbsp;</p><p><a href="https://twitter.com/The_AI_Investor/status/1756949748752539731">"Own Your Own AI"</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:46 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2024/02/12/using-blob-storage-from-asp-net-core-with-entra-id-authentication/'>Using Blob storage from ASP.NET Core with Entra ID authentication</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This article shows how to implement a secure upload and a secure download in ASP.NET Core using Azure blob storage. The application uses Microsoft Entra ID for authentication and also for access to the Azure Blob storage container. Code: https://github.com/damienbod/AspNetCoreEntraIdBlobStorage Blogs in this series Security architecture The application is setup to store the file uploads […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This article shows how to implement a secure upload and a secure download in ASP.NET Core using Azure blob storage. The application uses Microsoft Entra ID for authentication and also for access to the Azure Blob storage container. </p>



<p>Code: <a href="https://github.com/damienbod/AspNetCoreEntraIdBlobStorage">https://github.com/damienbod/AspNetCoreEntraIdBlobStorage</a></p>



 Blogs in this series 



 
 <a href="https://damienbod.com/2024/02/12/using-blob-storage-from-asp-net-core-with-entra-id-authentication/">Using Blob storage from ASP.NET Core with Entra ID authentication</a> 



 <a href="https://damienbod.com/2024/02/26/delegated-read-and-application-write-access-to-blob-storage-using-asp-net-core-with-entra-id-authentication/">Delegated read and application write access to blob storage using ASP.NET Core with Entra ID authentication</a> 



 <a href="https://damienbod.com/2024/03/04/multi-client-blob-storage-access-using-asp-net-core-with-entra-id-authentication-and-rbac/">Multi client blob storage access using ASP.NET Core with Entra ID authentication and RBAC</a> 
 



 Security architecture 



<p>The application is setup to store the file uploads to an Azure Blob storage container. The authentication uses delegated only flows. A user can authenticate into the application using Microsoft Entra ID. The Azure App registration defines App roles to use for access authorization. The roles are used in the enterprise application. Security groups link the users to the roles. The security groups are used in the Azure Blob container where the RBAC is applied using the groups. A SQL database is used to persist the meta data and integrate into the other parts of the application.</p>







<p></p>



 Setting up Azure Blob storage 



<p>Two roles were created in the Azure App registration. The roles are assigned to groups in the Enterprise application. The users allowed to used to Azure Blob storage are assigned to the groups. </p>







<p></p>



<p>The groups are then used to apply the RBAC roles in the Azure Blob container. The Storage Blob Data Contributor and the Storage Blob Data Reader roles are used.</p>







<p></p>



 Authentication 



<p>Microsoft Entra ID is used for authentication and implemented using the Microsoft.Identity.Web Nuget packages. The is a standard implementation. Two policies were created to validate the two different roles used in this solution. </p>


  string[]? initialScopes = configuration.GetValue&lt;string&gt;
   ("AzureStorage:ScopeForAccessToken")?.Split(' ');

services.AddMicrosoftIdentityWebAppAuthentication(configuration)
    .EnableTokenAcquisitionToCallDownstreamApi(initialScopes)
    .AddInMemoryTokenCaches();

services.AddAuthorization(options =&gt;
{
    options.AddPolicy("blob-one-read-policy", policyBlobOneRead =&gt;
    {
        policyBlobOneRead.RequireClaim("roles", ["blobonereadrole", "blobonewriterole"]);
    });
    options.AddPolicy("blob-one-write-policy", policyBlobOneRead =&gt;
    {
        policyBlobOneRead.RequireClaim("roles", ["blobonewriterole"]);
    });
});

services.AddRazorPages().AddMvcOptions(options =&gt;
{
    var policy = new AuthorizationPolicyBuilder()
        .RequireAuthenticatedUser()
        .Build();
    options.Filters.Add(new AuthorizeFilter(policy));
}).AddMicrosoftIdentityUI();
  


<p></p>



 Upload 



<p>The application uses the IFormFile interface with the file payload and uploads the file to Azure Blob storage. The BlobClient is setup to use Microsoft Entra ID and the meta data is added to the blob. </p>


  public BlobDelegatedUploadProvider(DelegatedTokenAcquisitionTokenCredential tokenAcquisitionTokenCredential,
    IConfiguration configuration)
{
    _tokenAcquisitionTokenCredential = tokenAcquisitionTokenCredential;
    _configuration = configuration;
}

[AuthorizeForScopes(Scopes = ["https://storage.azure.com/user_impersonation"])]
public async Task&lt;string&gt; AddNewFile(BlobFileUploadModel blobFileUpload, IFormFile file)
{
    try
    {
        return await PersistFileToAzureStorage(blobFileUpload, file);
    }
    catch (Exception e)
    {
        throw new ApplicationException($"Exception {e}");
    }
}

private async Task&lt;string&gt; PersistFileToAzureStorage(
    BlobFileUploadModel blobFileUpload,
    IFormFile formFile,
    CancellationToken cancellationToken = default)
{
    var storage = _configuration.GetValue&lt;string&gt;("AzureStorage:StorageAndContainerName");
    var fileFullName = $"{storage}/{blobFileUpload.Name}";
    var blobUri = new Uri(fileFullName);

    var blobUploadOptions = new BlobUploadOptions
    {
        Metadata = new Dictionary&lt;string, string?&gt;
        {
            { "uploadedBy", blobFileUpload.UploadedBy },
            { "description", blobFileUpload.Description }
        }
    };

    var blobClient = new BlobClient(blobUri, _tokenAcquisitionTokenCredential);

    var inputStream = formFile.OpenReadStream();
    await blobClient.UploadAsync(inputStream, blobUploadOptions, cancellationToken);

    return $"{blobFileUpload.Name} successfully saved to Azure Blob Storage Container";
}
  


<p>The DelegatedTokenAcquisitionTokenCredential class is used to get access tokens for the blob upload or download. This uses the existing user delegated session and creates a new access token for the blob storage access.</p>


  using Azure.Core;
using Microsoft.Identity.Client;
using Microsoft.Identity.Web;

namespace DelegatedEntraIDBlobStorage.FilesProvider.AzureStorageAccess;

public class DelegatedTokenAcquisitionTokenCredential : TokenCredential
{
    private readonly ITokenAcquisition _tokenAcquisition;
    private readonly IConfiguration _configuration;

    public DelegatedTokenAcquisitionTokenCredential(ITokenAcquisition tokenAcquisition,
        IConfiguration configuration)
    {
        _tokenAcquisition = tokenAcquisition;
        _configuration = configuration;
    }

    public override AccessToken GetToken(TokenRequestContext requestContext, CancellationToken cancellationToken)
    {
        throw new NotImplementedException();
    }

    public override async ValueTask&lt;AccessToken&gt; GetTokenAsync(TokenRequestContext requestContext, CancellationToken cancellationToken)
    {
        string[]? scopes = _configuration["AzureStorage:ScopeForAccessToken"]?.Split(' ');

        if (scopes == null)
        {
            throw new Exception("AzureStorage:ScopeForAccessToken configuration missing");
        }

        AuthenticationResult result = await _tokenAcquisition
            .GetAuthenticationResultForUserAsync(scopes);

        return new AccessToken(result.AccessToken, result.ExpiresOn);
    }
}
  


<p></p>



 Download 



<p>The download creates a BlobClient using the user delegated existing session. The file is downloaded directly.</p>


  using Azure.Storage.Blobs;
using Azure.Storage.Blobs.Models;
using Microsoft.Identity.Web;

namespace DelegatedEntraIDBlobStorage.FilesProvider.AzureStorageAccess;

public class BlobDelegatedDownloadProvider
{
    private readonly DelegatedTokenAcquisitionTokenCredential _tokenAcquisitionTokenCredential;
    private readonly IConfiguration _configuration;

    public BlobDelegatedDownloadProvider(DelegatedTokenAcquisitionTokenCredential tokenAcquisitionTokenCredential,
        IConfiguration configuration)
    {
        _tokenAcquisitionTokenCredential = tokenAcquisitionTokenCredential;
        _configuration = configuration;
    }

    [AuthorizeForScopes(Scopes = ["https://storage.azure.com/user_impersonation"])]
    public async Task&lt;Azure.Response&lt;BlobDownloadInfo&gt;&gt; DownloadFile(string fileName)
    {
        var storage = _configuration.GetValue&lt;string&gt;("AzureStorage:StorageAndContainerName");
        var fileFullName = $"{storage}/{fileName}";
        var blobUri = new Uri(fileFullName);
        var blobClient = new BlobClient(blobUri, _tokenAcquisitionTokenCredential);
        return await blobClient.DownloadAsync();
    }
}
  


 Notes 



<p>The architecture is simple and has the base features required for a secure solution. Data protection and virus scanning needs to be applied to the files and this can be configured in the Azure Blob storage. The access is controlled to the users in the group. If this needs to be controlled more, the write access can be removed from the users and switched to a service principal. This can have both security advantages and disadvantages. Multiple clients might also need access to files in this solution and the security needs to be enforced. This requires further architecture changes. </p>



 Links 



<p><a href="https://learn.microsoft.com/en-us/azure/storage/blobs/authorize-access-azure-active-directory">https://learn.microsoft.com/en-us/azure/storage/blobs/authorize-access-azure-active-directory</a></p>



<p><a href="https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction">https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction</a></p>



<p><a href="https://github.com/AzureAD/microsoft-identity-web">https://github.com/AzureAD/microsoft-identity-web</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2024/02/diagrams.png?w=1024'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   07:09 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 11. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://benwerd.medium.com/a-creative-process-0096344547ff?source=rss-3b16402f5b9d------2'>A creative process</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

      No apps; no frameworks; just space.  Continue reading on Medium »  
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/0*6cgBD_wcdDALW5JA.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   <p><a href="https://benwerd.medium.com/a-creative-process-0096344547ff?source=rss-3b16402f5b9d------2"></a></p><p>No apps; no frameworks; just space.</p><p><a href="https://benwerd.medium.com/a-creative-process-0096344547ff?source=rss-3b16402f5b9d------2">Continue reading on Medium »</a></p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/0*6cgBD_wcdDALW5JA.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:22 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 10. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://herestomwiththeweather.com'>Heres Tom with the Weather</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://herestomwiththeweather.com/2024/02/10/phishing-mitigation-for-mastodon.social/'>Phishing Mitigation for Mastodon.social</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   When a person is already logged into a mastodon instance, if they visit some pages on their instance associated with a user from another server, they are not redirected to the remote server because it is easier to interact with the remote user with their existing local session.  However, if a person without an account is just visiting or they have an account but are logged out, mastodon redirects 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>When a person is already logged into a <a href="https://en.wikipedia.org/wiki/Mastodon_(social_network)">mastodon</a> instance, if they visit some pages on their instance associated with a user from another server, they are not redirected to the remote server because it is easier to interact with the remote user with their existing local session.  However, if a person without an account is just visiting or they have an account but are logged out, mastodon redirects them to the remote server presumably because mastodon doesn’t know whether they have a local account and visiting the remote server will have the complete and authoritative data for that remote user.</p>

<p>A welcome update to <a href="https://mastodon.social">mastodon.social</a> (included in 4.3.0-nightly) is a <a href="https://mastodon.social/@Gargron/111908524315478487">warning</a> presented to visitors or logged out users before mastodon redirects them to a remote server for the original page.  The <a href="https://github.com/mastodon/mastodon/commit/b19ae521b7d28a76e8e1d8da8157e051e9d8de6c">code</a> for <a href="https://github.com/mastodon/mastodon/pull/27792">Add confirmation when redirecting logged-out requests to permalink</a> is particularly relevant to mastodon.social compared to other fediverse instances as mastodon.social has become a relatively big target for phishing.  It’s a good bet that if someone is navigating the fediverse that their account is on mastodon.social.  So, if an arbitrary victim is logged out of their mastodon.social account and visits a mastodon.social page belonging to the attacker, prior to this mitigation, mastodon.social would automatically redirect the victim to the attacker’s page which might be a fake login form to trick the victim into submitting their login credentials to the attacker’s site.  Unfortunately, a significant percentage of people will submit the form.</p>

<p>One could imagine mastodon.social maintaining a list of trusted servers for automatic redirects but that would be an undesirable hornet’s nest and it’s not a bad thing when web surfers are conscious of the trust boundaries on the web.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   15:55 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 09. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/zero-trust-with-zero-data'>Zero Trust with Zero Data</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The physical world is full of zero trust examples, but they gather attributes for the access control decisions in a very different way than we're used to online. Presenting your ID to buy beer is used so often as an example of how verifiable credentials work that it's cliche. Cliche or not, there's another aspect of using an ID to buy beer that I want to focus on: it's an excellent example of
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The physical world is full of zero trust examples, but they gather attributes for the access control decisions in a very different way than we're used to online. </p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F410d437b-40b3-42f5-b8c3-7edefd478945.heic">    </a> <p>Presenting your ID to buy beer is used so often as an example of how verifiable credentials work that it's cliche. Cliche or not, there's another aspect of using an ID to buy beer that I want to focus on: it's an excellent example of <a href="https://www.windley.com/archives/2023/08/zero_trust.shtml">zero trust</a></p><blockquote><p>Zero Trust operates on a simple, yet powerful principle: “assume breach.” In a world where network boundaries are increasingly porous and cyber threats are more evasive than ever, the Zero Trust model centers around the notion that no one, whether internal or external, should be inherently trusted. This approach mandates continuous verification, strict access controls, and micro-segmentation, ensuring that every user and device proves their legitimacy before gaining access to sensitive resources. If we assume breach, then the only strategy that can protect the corporate network, infrastructure, applications, and people is to authorize every access.</p><p>From <a href="https://www.windley.com/archives/2023/08/zero_trust.shtml">Zero Trust</a><br>Referenced 2024-02-09T08:25:55-0500</p></blockquote><p>The real world is full of zero trust examples. When we're controlling access to something in the physical world—beer, a movie, a boarding gate, points in a loyalty program, prescriptions, and so on—we almost invariably use a zero trust model. We authorize every access. This isn't surprising, the physical world is remarkably decentralized and there aren't many natural boundaries to exploit and artificial boundaries are expensive and inconvenient.</p><p>The other thing that's interesting about zero trust in the physical world is that authorization is also usually done using <a href="https://www.windley.com/archives/2023/10/zero_data.shtml">Zero Data</a>. Zero data is a name <a href="https://www.citizenme.com/the-future-of-data-is-zero-data/">StJohn Deakin gave</a> to the concept of using data gathered just in time to make authorization and other decisions rather than relying on great stores of data. There are obvious security benefits from storing less data, but zero data also offers significantly greater convenience for people and organizations alike. To top all that off, it can save money by reducing the number of partner integrations (i.e., far fewer federations) and enable applications that have far greater scale.</p><p>Let's examine these benefits in the scenario I opened with. Imagine that instead of using a credential (e.g., driver's license) to prove your age when buying beer, we ran convenience stores like a web site. Before you could shop, you'd have to register an account. And if you wanted to buy beer, the company would have to <a href="https://www.citizenme.com/the-future-of-data-is-zero-data/">proof the identity of the person</a> to ensure they're over 21. Now when you buy beer at the store, you'd log in so the system could use your stored attributes to ensure you were allowed to buy beer.</p><p>This scenario is still zero trust, but not zero data. And it's ludicrous to imagine anyone would put up with it, but we do it everyday online. I don't know about you, but I'm comforted to know that every convenience store I visit doesn't have a store of all kinds of information about me in an account somewhere. Zero data stores less data that can be exploited by hackers (or the <a href="https://www.windley.com/archives/2021/05/can_the_digital_future_be_our_home.shtml">companies we trust with it</a>).</p><p>The benefit of scale is obvious as well. In a zero data, zero trust scenario we don't have to have long-term transactional relationships with every store, movie, restaurant, and barber shop we visit. They don't have to maintain federation relationships with numerous identity providers. There are places where the ability to scale zero trust really matters. For example, it's impossible for every hospital to have a relationship with every other hospital for purposes of <a href="https://www.windley.com/archives/2021/05/building_an_ssi_ecosystem_digital_staff_passports_at_the_nhs.shtml">authorizing access for medical personal who move or need temporary access</a>. Similarly, airline personal move between numerous airports and need access to various facilities at airports.</p><p>How do we build zero data, zero trust systems? By using <a href="https://www.windley.com/archives/2021/06/ssi_interaction_patterns.shtml">verifiable credentials</a> to transfer attributes about their subject in a way that is decentralized and yet trustworthy. Zero data aligns our online existence more closely with our real-world interactions, fostering new methods of communication while decreasing the challenges and risks associated with amassing, storing, and utilizing vast amounts of data.</p><p>Just-in-time, zero data, attribute transfer can make many zero trust scenarios more realizable because it's more flexible. Zero trust with zero data, facilitated by verifiable credentials, represents a pivotal transition in how digital identity is used in authorization decisions. By minimizing centralized data storage and emphasizing cryptographic verifiability, this approach aims to address the prevalent challenges in data management, security, and user trust. By allowing online interactions to more faithfully follow established patterns of transferring trust from the physical world, zero trust with zero data promotes better security with increased convenience and lower cost. What's not to like?</p><p>You can get more detail on many of the concepts in this post like verifiable credentials in my new book <a href="http://www.amazon.com/exec/obidos/ASIN/1098117697/technometria-20">Learning Digital Identity</a> from O'Reilly Media.</p> <a href="http://www.amazon.com/exec/obidos/ASIN/1098117697/technometria-20">   </a>    <p>Photo Credit: We ID Everyone from DALL-E (Public Domain) DALL-E apparently thinks a six-pack has 8 bottles but this was the best of several attempts.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F410d437b-40b3-42f5-b8c3-7edefd478945.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:28 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 05. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2024/02/05/secure-an-asp-net-core-blazor-web-app-using-microsoft-entra-id/'>Secure an ASP.NET Core Blazor Web app using Microsoft Entra ID</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This article shows how to implement an ASP.NET Core Blazor Web application using Microsoft Entra ID for authentication. Microsoft.Identity.Web is used to implement the Microsoft Entra ID OpenID Connect client. Code: https://github.com/damienbod/Hostedblazor8MeID Note: I based this implementation on the example provided by Tomás López Rodríguez and adapted it. Setup The Blazor Web application is an 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This article shows how to implement an ASP.NET Core Blazor Web application using Microsoft Entra ID for authentication. <a href="https://github.com/AzureAD/microsoft-identity-web">Microsoft.Identity.Web</a> is used to implement the Microsoft Entra ID OpenID Connect client.</p>



<p>Code: <a href="https://github.com/damienbod/Hostedblazor8MeID">https://github.com/damienbod/Hostedblazor8MeID</a></p>







<p></p>



<p>Note: I based this implementation on the <a href="https://github.com/CrahunGit/Auth0BlazorWebAppSample/tree/master/BlazorApp4">example</a> provided by Tomás López Rodríguez and adapted it.</p>



 Setup 



<p>The Blazor Web application is an OpenID Connect confidential client (code flow, PKCE) which uses Microsoft Entra ID for authentication. An Azure App registration (Web configuration) is used to create the client and only delegated scopes are used. A secret is used to authenticate the application in development. Client assertions can be used in production deployments. NetEscapades.AspNetCore.SecurityHeaders is used to implement the security headers as best possible for Blazor Web. No identity management or user passwords are handled in the application.</p>



<p>The client part of the Blazor Web application can use the PersistentAuthenticationStateProvider class to read the user profile data.</p>



<p>This uses data from the server part implemented in the PersistingRevalidatingAuthenticationStateProvider class. See the code in the github repo.</p>



<p></p>



 OpenID Connect confidential client 



<p>The AddMicrosoftIdentityWebAppAuthentication method is used to implement the client authentication using the Microsoft.Identity.Web packages. I use a downstream API to force that the client uses code flow with PKCE instead of the implicit flow. Microsoft Graph is only requesting delegated user profile data.</p>


  // Add authentication services
var scopes = builder.Configuration.GetValue&lt;string&gt;("DownstreamApi:Scopes");
string[] initialScopes = scopes!.Split(' ');

builder.Services.AddMicrosoftIdentityWebAppAuthentication(builder.Configuration)
    .EnableTokenAcquisitionToCallDownstreamApi(initialScopes)
    .AddMicrosoftGraph("https://graph.microsoft.com/v1.0", scopes)
    .AddInMemoryTokenCaches();
  


<p></p>



<p>The client automatically reads from the AzureAd configuration. This can be changed if you would like to update the product name. The client uses the standard Microsoft Entra ID setup. You need to add the permissions in the Azure App registration created for this application.</p>


    "AzureAd": {
    "Instance": "https://login.microsoftonline.com/",
    "Domain": "[Enter the domain of your tenant, e.g. contoso.onmicrosoft.com]",
    "TenantId": "[Enter 'common', or 'organizations' or the Tenant Id (Obtained from the Azure portal. Select 'Endpoints' from the 'App registrations' blade and use the GUID in any of the URLs), e.g. da41245a5-11b3-996c-00a8-4d99re19f292]",
    "ClientId": "[Enter the Client Id (Application ID obtained from the Azure portal), e.g. ba74781c2-53c2-442a-97c2-3d60re42f403]",
    "ClientSecret": "[Copy the client secret added to the app from the Azure portal]",
    "ClientCertificates": [
    ],
    // the following is required to handle Continuous Access Evaluation challenges
    "ClientCapabilities": [ "cp1" ],
    "CallbackPath": "/signin-oidc"
  },
  "DownstreamApi": {
    "Scopes": "User.ReadBasic.All user.read"
  },
  


<p></p>



 Login and Logout 



<p>An AuthenticationExtensions class was used to implement the login and the logout for the application. The Login method is an HTTP GET request which redirects to the OpenID Connect server. The Logout method is an authentication HTTP POST request which requires CSRF protection and accepts no parameters. The return URL to the unauthenticated signed out page is fixed and so no open redirect attacks are possible. The login cleans up the local cookies as well as a redirect to the identity provider to logout on Microsoft Entra ID.</p>


  using Microsoft.AspNetCore.Authentication.Cookies;
using Microsoft.AspNetCore.Authentication.OpenIdConnect;
using Microsoft.AspNetCore.Authentication;
namespace BlazorWebMeID;

public static class AuthenticationExtensions
{
    public static WebApplication SetupEndpoints(this WebApplication app)
    {
        app.MapGet("/Account/Login", async (HttpContext httpContext, string returnUrl = "/") =&gt;
        {
            await httpContext.ChallengeAsync(OpenIdConnectDefaults.AuthenticationScheme,
                new AuthenticationProperties
                {
                    RedirectUri = !string.IsNullOrEmpty(returnUrl) ? returnUrl : "/"
                });
        });

        app.MapPost("/Account/Logout", async (HttpContext httpContext) =&gt;
        {
            var authenticationProperties = new AuthenticationProperties
            {
                RedirectUri = "/SignedOut"
            };  

            await httpContext.SignOutAsync(OpenIdConnectDefaults.AuthenticationScheme, 
                authenticationProperties);

            await httpContext.SignOutAsync(CookieAuthenticationDefaults.AuthenticationScheme);

        }).RequireAuthorization();

        return app;
    }
}
  


<p></p>



 Security headers 



<p>The security headers are used to protect the session. When using AddInteractiveWebAssemblyComponents mode, the script CSP header is really weak and adds little protection leaving the application open to numerous XSS, Javascript attacks. It is not possible to use CSP nonces with Blazor Web using the InteractiveWebAssemblyComponents mode, or I have not found a way to do this, as the Blazor Web components cannot read the HTTP headers in the response. A Blazor WASM hosted in an ASP.NET Core application can use CSP nonces and is a more secure application.</p>


  namespace HostedBlazorMeID.Server;

public static class SecurityHeadersDefinitions
{
    public static HeaderPolicyCollection GetHeaderPolicyCollection(bool isDev, string? idpHost)
    {
        ArgumentNullException.ThrowIfNull(idpHost);

        var policy = new HeaderPolicyCollection()
            .AddFrameOptionsDeny()
            .AddContentTypeOptionsNoSniff()
            .AddReferrerPolicyStrictOriginWhenCrossOrigin()
            .AddCrossOriginOpenerPolicy(builder =&gt; builder.SameOrigin())
            .AddCrossOriginResourcePolicy(builder =&gt; builder.SameOrigin())
            .AddCrossOriginEmbedderPolicy(builder =&gt; builder.RequireCorp())
            .AddContentSecurityPolicy(builder =&gt;
            {
                builder.AddObjectSrc().None();
                builder.AddBlockAllMixedContent();
                builder.AddImgSrc().Self().From("data:");
                builder.AddFormAction().Self().From(idpHost);
                builder.AddFontSrc().Self();
                builder.AddStyleSrc().Self();
                builder.AddBaseUri().Self();
                builder.AddFrameAncestors().None();

                // due to Blazor Web, nonces cannot be used with AddInteractiveWebAssemblyComponents mode.
                // weak script CSP....
                builder.AddScriptSrc()
                    .Self() // self required
                    .UnsafeEval() // due to Blazor WASM
                    .UnsafeInline(); // only a fallback for older browsers when the nonce is used 
            
            })
            .RemoveServerHeader()
            .AddPermissionsPolicy(builder =&gt;
            {
                builder.AddAccelerometer().None();
                builder.AddAutoplay().None();
                builder.AddCamera().None();
                builder.AddEncryptedMedia().None();
                builder.AddFullscreen().All();
                builder.AddGeolocation().None();
                builder.AddGyroscope().None();
                builder.AddMagnetometer().None();
                builder.AddMicrophone().None();
                builder.AddMidi().None();
                builder.AddPayment().None();
                builder.AddPictureInPicture().None();
                builder.AddSyncXHR().None();
                builder.AddUsb().None();
            });

        if (!isDev)
        {
            // maxage = one year in seconds
            policy.AddStrictTransportSecurityMaxAgeIncludeSubDomains();
        }

        policy.ApplyDocumentHeadersToAllResponses();

        return policy;
    }
}
  


<p></p>



 Notes 



<p>I am starting to understand how Blazor Web works and have difficultly with the session state and sharing this between different components. Some basic browser security cannot be used, i.e. CSP nonces. The mixed mode has strange UI effects which I could not clean up.</p>



<p>There are now four types of Blazor applications.</p>



 
 Blazor WASM hosted in an ASP.NET Core application 



 Blazor Server 



 Blazor Web 



 Blazor WASM standalone 
 



<p>Blazor WASM hosted in an ASP.NET Core application and Blazor Server can be secured in a good way using the recommended security best practices (OpenID Connect confidential client). Blazor Web can implement a confidential client but is missing the recommend script session protection. Blazor WASM standalone cannot implement the recommended authentication as it is a public application and should no longer be used in secure environments.</p>



 Links 



<p><a href="https://github.com/CrahunGit/Auth0BlazorWebAppSample/tree/master/BlazorApp4">https://github.com/CrahunGit/Auth0BlazorWebAppSample/tree/master/BlazorApp4</a></p>



<p><a href="https://github.com/dotnet/blazor-samples/tree/main/8.0/BlazorWebAppOidc">https://github.com/dotnet/blazor-samples/tree/main/8.0/BlazorWebAppOidc</a></p>



<p><a href="https://github.com/AzureAD/microsoft-identity-web">https://github.com/AzureAD/microsoft-identity-web</a></p>



<p><a href="https://github.com/andrewlock/NetEscapades.AspNetCore.SecurityHeaders">https://github.com/andrewlock/NetEscapades.AspNetCore.SecurityHeaders</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2024/01/brandblazor_nohalo_1000x.png?w=1000'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   07:10 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 02. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://tembo.io/blog/pgxn-architecture'>Presentation: Introduction to the PGXN Architecture</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I made a presentation on the PGXN architecture for the Tembo team.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>As I started digging into the <a href="https://gist.github.com/theory/898c8802937ad8361ccbcc313054c29d" title="Extension Ecosystem: Jobs and Tools">jobs and tools</a> for the Postgres extension
ecosystem as part of my <a href="https://justatheory.com/2024/01/tembonaut/" title="I'm a Postgres Extensions Tembonaut">new gig</a>, I realized that most people have little
knowledge of the <a href="https://pgxn.org" title="PGXN — PostgreSQL Extension Network">PGXN</a> architecture. I learned a lot designing PGXN and its
services, and am quite pleased with where it ended up, warts and all. So I
thought it worthwhile to put together a brief presentation on the fundamental
design principals (static REST file API), inter-related services (<a href="https://master.pgxn.org/" title="PGXN Root Mirror">root mirror</a>,
<a href="https://manager.pgxn.org/howto" title="PGXN How To">manager</a>, <a href="https://github.com/pgxn/pgxn-api/wiki" title="PGXN API">API</a>, <a href="https://pgxn.org" title="PGXN — PostgreSQL Extension Network">site</a>) and tools (<a href="https://pgxn.github.io/pgxnclient/" title="PGXN Client documentation">CLI</a>, <a href="https://hub.docker.com/r/pgxn/pgxn-tools" title="pgxn/pgxn-tools Docker image">CI/CD</a>).</p>
<p>Yesterday, the Tembo blog <a href="https://tembo.io/blog/pgxn-architecture" title="Presentation: Introduction to the PGXN Architecture">published the presentation</a>, including <a href="https://www.youtube.com/watch?v=sjZPA3HA_q8" title="YouTube Tembo Channel: “David Wheeler - The Architecture of PGXN”">the video</a>
and slides, along with a high-level architecture diagram. I hope it’s a useful
point of reference for the Postgres community as we look to better distribute
extensions in the future.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/software-architecture/">Software Architecture</a> 
                 <a href="https://justatheory.com/tags/rest/">REST</a> 
                 <a href="https://justatheory.com/tags/json/">JSON</a> 
                 <a href="https://justatheory.com/tags/tembo/">Tembo</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:01 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 01. February 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/02/decentralized-extension-publishing/'>Contemplating Decentralized Extension Publishing</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The Go package ecosystem uses distributed publishing to release modules
without authentication or uploads. Could we do something similar for
Postgres extensions?
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
 TL;DR 
<p>As I think through the future of the Postgres extension ecosystem as a key part
of the <a href="https://justatheory.com/2024/01/tembonaut/" title="I'm a Postgres Extensions Tembonaut">new job</a>, I wanted to understand how <a href="https://go.dev/doc/modules/developing#decentralized" title="go.dev: Developing and publishing modules">Go decentralized publishing</a>
works. In this post I work it out, and think through how we might do something
similar for Postgres extension publishing. It covers the
<a>Go architecture</a>, <a>namespacing challenges</a>,
and <a>PGXS abuse</a>; then experiments with
<a>URL-based namespacing</a> and ponders
<a>reorganizing installed extension files</a>;
and closes with a <a>high-level design</a> for
making it work now and in the future.</p>
<p>It is, admittedly, a lot, mainly written for my own edification and for the
information of my fellow extension-releasing travelers.</p>
<p>I find it fascinating and learned a ton. Maybe you will too! But feel free to
skip this post if you’re less interested in the details of the journey and want
to wait for more decisive posts once I’ve reached the destination.</p>
 Introduction 
<p>Most language registries require developers to take some step to make releases.
Many automate the process in CI/CD pipelines, but it requires some amount of
effort on the developer’s part:</p>
 
 Register for an account 
 Learn how to format things to publish a release 
 Remember to publish again for every new version 
 Create a pipeline to automate publishing (e.g., a GitHub workflow) 
 
 Decentralized Publishing 
<p><a href="https://go.dev/doc/modules/developing#decentralized" title="go.dev: Developing and publishing modules">Go decentralized publishing</a> has revised this pattern: it does not require user
registration or authentication to to publish a module to <a href="https://pkg.go.dev">pkg.go.dev</a>. Rather,
Go developers simply tag the source repository, and the first time someone
<a href="https://pkg.go.dev/about#adding-a-package" title="pkg.go.dev: Adding a package">refers to the tag in Go tools</a>, the <a href="https://index.golang.org">Go module index</a> will include it.</p>
<p>For example, publishing <code>v1.2.1</code> of a module in the <code>github.com/golang/example</code>
repository takes just three commands:</p>
  <code>git tag v1.2.1 -sm 'Tag v1.2.1'
git push --tags
go list -m github.com/golang/example@v1.2.1
</code>  <p>After a few minutes, the module will show up in <a href="https://index.golang.org/index" title="Go module index feed">the index</a> and then on
<a href="https://pkg.go.dev">pkg.go.dev</a>. Anyone can run <code>go get -u github.com/golang/example</code> to get the
latest version. Go developers rest easy in the knowledge that they’re getting
the exact module they need thanks to the <a href="https://go.dev/ref/mod#checksum-database" title="Go Modules Reference: Checksum database">global checksum database</a>, which Go
uses “in many situations to detect misbehavior by proxies or origin servers”.</p>
<p>This design requires <code>go get</code> to understand multiple source code management
systems: it supports Git, Subversion, Mercurial, Bazaar, and Fossil.<a>1</a>
It also needs the <code>go.mod</code> metadata file to live in the project defining the
package.</p>
<p>But that’s really it. From the developer’s perspective it could not be easier to
publish a module, because it’s a natural extension of the module development
tooling and workflow of committing, tagging, and fetching code.</p>
 Decentralized Extension Publishing 
<p>Could we publish Postgres extensions in such a decentralized pattern? It might
look something like this:</p>
 
 The developer places a metadata file in the proper location (<a href="https://www.postgresql.org/docs/current/extend-extensions.html#EXTEND-EXTENSIONS-FILES" title="PostgreSQL Docs: Extension Files">control file</a>,
<code>META.json</code>, <code>Cargo.toml</code>, whatever — standard TBD) 
 To publish a release, the developer tags the repository and calls some sort
of indexing service hook (perhaps from a tag-triggered release workflow) 
 The indexing service validates the extension and adds it to the index 
 
<p>Note that there is no registration required. It simply trusts the source code
repository. It also avoids name collision: <code>github.com/bob/hash</code>
is distinct from <code>github.com/carol/hash</code>.</p>
<p>This design does raise challenges for clients, whether they’re compiling
extensions on a production system or building binary packages for distribution:
they have to support various version control systems to pull the code (though
starting with Git is a decent 90% solution).</p>
 Namespacing 
<p>Then there’s name conflicts. Perhaps <code>github.com/bob/hash</code> and
<code>github.com/carol/hash</code> both create an extension named <code>hash</code>. By the current
<a href="https://www.postgresql.org/docs/current/extend-extensions.html#EXTEND-EXTENSIONS-FILES" title="PostgreSQL Docs: Extension Files">control file</a> format, the script directory and module path can use any name,
but in all likelihood the use these defaults:</p>
  <code>directory = 'extension'
module_pathname = '$libdir/hash'
</code>  <p>Meaning <code>.sql</code> files will be installed in the Postgres <code>share/extension</code>
subdirectory — along with all the other installed extensions — and library
files will be installed in the library directory along with all other libraries.
Something like this:</p>
 <code>pgsql
├── lib
│   └── hash.so
└── share
    └── extension
    │   └── hash.control
    │&nbsp;&nbsp; ├── hash--1.0.0.sql
    └── doc
        └── hash.md
</code> <p>If both projects include, say, <code>hash.control</code>, <code>hash--1.0.0.sql</code>, and <code>hash.so</code>,
the files from one will stomp all over the files of the other.</p>
 Installer Abuse 
<p>Go avoids this issue by using the domain and path from each package’s repository
in its directory structure. For example, here’s a list of modules from
<code>google.golang.org</code> repositories:</p>
  <code>$ ls -1 ~/go/pkg/mod/google.golang.org
api@v0.134.0
api@v0.152.0
appengine@v1.6.7
genproto
genproto@v0.0.0-20230731193218-e0aa005b6bdf
grpc@v1.57.0
grpc@v1.59.0
protobuf@v1.30.0
protobuf@v1.31.0
protobuf@v1.32.0
</code>  <p>The <code>~/go/pkg/mod</code> directory has subdirectories for each VCS host name, and each
then subdirectories for package paths. For the <code>github.com/bob/hash</code> example,
the files would all live in <code>~/go/pkg/mod/github.com/bob/hash</code>.</p>
<p>Could a Postgres extension build tool follow a similar distributed pattern by
renaming the control file and installation files and directories to something
specific for each, say <code>github.com+bob+hash</code> and <code>github.com+carol+hash</code>? That
is, using the repository host name and path, but replacing the slashes in the
path with some other character that wouldn’t create subdirectories — because
PostgreSQL won’t find control files in subdirectories. The control file entries
for <code>github.com/carol/hash</code> would look like this:</p>
  <code>directory = 'github.com+carol+hash'
module_pathname = '$libdir/github.com+carol+hash'
</code>  <p>Since PostgreSQL expects the control file to have the same name as the
extension, and for SQL scripts to start with that name, the files would have to
be named like so:</p>
 <code>hash
├── Makefile
├── github.com+carol+hash.control
└── sql
    └── github.com+carol+hash--1.0.0.sql
</code> <p>And the <code>Makefile</code> contents:</p>
  <code>EXTENSION  = github.com+carol+hash
MODULEDIR  = $(EXTENSION)
DATA       = sql/$(EXTENSION)--1.0.0.sql
PG_CONFIG ?= pg_config

PGXS := $(shell $(PG_CONFIG) --pgxs)
include $(PGXS)
</code>  <p>In other words, the extension name is the full repository host name and path and
the Makefile <code>MODULEDIR</code> variable tells <code>pg_config</code> to put all the SQL and
documentation files into a directories named <code>github.com+carol+hash</code> —
preventing them from conflicting with any other extension.</p>
<p>Finally, the <code>github.com+carol+hash.control</code> file — so named becaus it must
have the same name as the extension — contains:</p>
  <code>default_version = '1.0.0'
relocatable = true
directory = 'github.com+carol+hash'
module_pathname = '$libdir/github.com+carol+hash'
</code>  <p>Note the <code>directory</code> parameter, which must match <code>MODULEDIR</code> from the
<code>Makefile</code>, so that <code>CREATE EXTENSION</code> can find the SQL files. Meanwhile,
<code>module_pathname</code> ensures that the library file has a unique name — the same
as the long extension name — again to avoid conflicts with other projects.</p>
<p>That unsightly naming extends to SQL: using the URL format could get to be a
mouthful:</p>
  <code>CREATE EXTENSION "github.com+carol+hash";
</code>  <p>Which is do-able, but some new SQL syntax might be useful, perhaps something
like:</p>
  <code>CREATE EXTENSION hash FROM "github.com+carol+hash";
</code>  <p>Or, if we’re gonna really go for it, use slashes after all!</p>
  <code>CREATE EXTENSION hash FROM "github.com/carol/hash";
</code>  <p>Want to use both extensions but they have conflicting objects (e.g., both create
a “hash” data type)? Put them into separatre schemas (assuming
<code>relocatable = true</code> in the control file):</p>
  <code>CREATE EXTENSION hash FROM "github.com/carol/hash" WITH SCHEMA carol;
CREATE EXTENSION hash FROM "github.com/bob/hash" WITH SCHEMA bob;
CREATE TABLE try (
    h1 carol.hash,
    h2 bob.hash
);
</code>  <p>Of course it would be nice if PostgreSQL added support for something like
<a href="https://docs.oracle.com/database/121/LNPLS/packages.htm" title="Oracle Docs: PL/SQL Packages">Oracle packages</a>, but using schemas in the meantime may be sufficient.</p>
<p>Clearly we’re getting into changes to the PostgreSQL core, so put that aside and
we can just use long names for creating, modifying, and dropping extensions, but
not necessarily otherwise:</p>
  <code>CREATE EXTENSION "github.com+carol+hash" WITH SCHEMA carol;
CREATE EXTENSION "github.com+bob+hash" WITH SCHEMA bob;
CREATE EXTENSION "gitlab.com+barack+kicker_type";
CREATE TABLE try (
    h1 carol.hash,
    h2 bob.hash
    kt kicker
);
</code>   Namespacing Experiment 
<p>To confirm that this approach might work, I committed <a href="https://github.com/theory/pg-semver/commit/24134fd" title="pg-semver@24134fd: Use domain-qualified name for extension">24134fd</a> and pushed it in
the <a href="https://github.com/theory/pg-semver/tree/namespace-experiment" title="pg_semver@namespace-experiment: Experimental branch to try naming an extension with a source code repository URL similar to Go packages">namespace-experiment</a> branch of <a href="https://github.com/theory/pg-semver">the semver extension</a>. This commit changes
the extension name from <code>semver</code> to <code>github.com+theory+pg-semver</code>, and follows
the above steps to ensure that its files are installed with that name.</p>
<p>Abusing the Postgres extension installation infrastructure like this does
work, but suffers from a number of drawbacks, including:</p>
 
 The extension name is super long, as before, but now so too are the files in
the repository (as opposed to the installer renaming them on install). The
shared library file has to have the long name, so therefore does the <code>.c</code>
source file. The SQL files must all start with
<code>github.com+theory+pg-semver</code>, although I skipped that bit in this commit;
instead the <code>Makefile</code> generates just one from <code>sql/semver.sql</code>. 
 Any previous installation of the <code>semver</code> type would remain unchanged, with
no upgrade path. Changing an extension’s name isn’t a great idea. 
 
<p>I could probably script renaming and modifying file contents like this and make
it part of the build process, but it starts to get complicated. We could also
modify installers to make the changes, but there are a bunch of moving parts
they would have to compensate for, and given how dynamic this can be (e.g., the
semver <code>Makefile</code> reads the extension name from <code>META.json</code>), we would rapidly
enter the territory of edge case <a href="https://en.wikipedia.org/wiki/Whac-A-Mole" title="Wikipedia: “Whac-A-Mole”">whac-a-mole</a>. I suspect it’s simply too
error-prone.</p>
 Proposal: Update Postgres Extension Packaging 
<p>Perhaps the Go directory pattern could inspire a similar model in Postgres,
eliminating the namespace issue by teaching the Postgres extension
infrastructure to include all but one of the files for an extension in a
single directory. In other words, rather than files distributed like so for
semver:</p>
 <code>pgsql
├── lib
│   └── semver.so
└── share
    └── extension
    │   └── semver.control
    │&nbsp;&nbsp; ├── semver--0.32.1.sql
    │&nbsp;&nbsp; ├── semver--0.32.0--0.32.1.sql
    └── doc
        └── semver.md
</code> <p>Make it more like this:</p>
 <code>pgsql
└── share
    └── extension
        └── github.com
            └── theory
                └── pg-semver
                    └── extension.control
                    └── lib
                    │   └── semver.so
                    └── sql
                    │   └── semver--0.32.1.sql
                    │   └── semver--0.32.0--0.32.1.sql
                    └── doc
                        └── semver.md
</code> <p>Or perhaps:</p>
 <code>pgsql
└── share
    └── extension
        └── github.com
            └── theory
                └── pg-semver
                    └── extension.control
                    └── semver.so
                    └── semver--0.32.1.sql
                    └── semver--0.32.0--0.32.1.sql
                    └── semver.md
</code> <p>The idea is to copy the files exactly as they’re stored in or compiled in the
repository. Meanwhile, the new <code>semver.name</code> file — the only relevant file
stored outside the extension module directory — simply points to that path:</p>
  <code>github.com/theory/pg-semver
</code>  <p>Then for <code>CREATE EXTENSION semver</code>, Postgres reads <code>semver.name</code> and knows where
to find all the files to load the extension.</p>
<p>This configuration would require updates to the control file, now named
<code>extension.control</code>, to record the full package name and appropriate locations.
Add:</p>
  <code>name = 'semver'
package = 'github.com/theory/pg-semver'
</code>  <p>This pattern could also allow aliasing. Say we try to install a different
<code>semver</code> extension from <code>github.com/example/semver</code>. This is in its
<code>extension.control</code> file:</p>
  <code>name = 'semver'
package = 'github.com/example/pg-semver'
</code>  <p>The installer detects that <code>semver.name</code> already exists for a different package
and raises an error. The user could then give it a different name by running
something like:</p>
  <code>make install ALIAS_EXTENSION_NAME=semver2
</code>  <p>This would add <code>semver2.name</code> right next to <code>semver.name</code>, and its contents
would contain <code>github.com/example/semver</code>, where all of its files are installed.
This would allow <code>CREATE EXTENSION semver2</code> to load the it without issue
(assuming no object conflicts, hopefully resolved by relocate-ability).</p>
<p>I realize a lot of extensions with libraries could wreak some havoc on the
library resolver having to search so many library directories, but perhaps
there’s some way around that as well? Curious what techniques experienced C
developers might have adopted.</p>
 Back to Decentralized Publishing 
<p>An updated installed extension file structure would be nice, and is surely worth
a discussion, but even if it shipped in Postgres 20, we need an updated
extension ecosystem today, to work well with all supported versions of Postgres.
So let’s return to the idea of decentralized publishing without such changes.</p>
<p>I can think of two pieces that’d be required to get Go-style decentralized
extension publishing to work with the current infrastructure.</p>
 Module Uniqueness 
<p>The first is to specify a new metadata field to be unique for the entire index,
and which would contain the repository path. Call it <code>module</code>, after Go (a
single Git repository can have multiple modules). In <a href="https://pgxn.org/spec/" title="PGXN Meta Spec - The PGXN distribution metadata specification">PGXN Meta Spec</a>-style JSON
it’d look something like this:</p>
  <code>{
    "module": "github.com/theory/pg-semver",
    "version": "0.32.1",
    "provides": {
      "semver": {
         "abstract": "A semantic version data type",
      }
    }
}
</code>  <p>Switch from the PGXN-style uniqueness on the distribution name (usually the name
of the extension) and let the module be globally unique. This would allow
another party to release an extension with the same name. Even a fork where only
the <code>module</code> is changed:</p>
  <code>{
    "module": "github.com/example/pg-semver",
    "version": "0.32.1",
    "provides": {
      "semver": {
         "abstract": "A semantic version data type",
      }
    }
}
</code>  <p>Both would be indexed and appear under the module name, and both would be
find-able by the provided extension name, <code>semver</code>.</p>
<p>Where that name must still be unique is in a given install. In other words,
while <code>github.com/theory/pg-semver</code> and <code>github.com/example/pg-semver</code> both
exist in the index, the <code>semver</code> extension can be installed from only one of
them in a given Postgres system, where the extension name <code>semver</code> defines its
uniqueness.</p>
<p>This pattern would allow for much more duplication of ideas while preserving the
existing per-cluster namespacing. It also allows for a future Postgres release
that supports something like the flexible per-cluster packaging as described
above.<a>2</a></p>
 Extension Toolchain App 
<p>The second piece is an extension management application that understands all
this stuff and makes it possible. It would empower both extension development
workflows — including testing, metadata management, and releasing — and
extension user workflows — finding, downloading, building, and installing.</p>
<p>Stealing from Go, imagine a developer making a release with something like this:</p>
  <code>git tag v1.2.1 -sm 'Tag v1.2.1'
git push --tags
pgmod list -m github.com/theory/pg-semver@v1.2.1
</code>  <p>The creatively named <code>pgmod</code> tells the registry to index the new version
directly from its Git repository. Thereafter anyone can find it and install it
with:</p>
 
 <code>pgmod get github.com/theory/pg-semver@v1.2.1</code> — installs the specified version 
 <code>pgmod get github.com/theory/pg-semver</code> — installs the latest version 
 <code>pgmod get semver</code> — installs the latest version or shows a list of
matching modules to select from 
 
<p>Any of these would fail if the cluster already has an extension named <code>semver</code>
with a different module name. But with something like the updated extension
installation locations in a future version of Postgres, that limitation could be
loosened.</p>
 Challenges 
<p>Every new idea comes with challenges, and this little thought experiment is no
exception. Some that immediately occur to me:</p>
 
 Not every extension can be installed directly from its repository. Perhaps
the metadata could include a download link for a tarball with the results of
any pre-release execution? 
 Adoption of a new CLI could be tricky. It would be useful to include the
functionality in existing tools people already use, like <a href="https://github.com/pgcentralfoundation/pgrx" title="pgrx is a framework for developing PostgreSQL extensions in Rust and strives to be as idiomatic and safe as possible">pgrx</a>. 
 Updating the uniqueness constraint in existing systems like <a href="https://pgxn.org" title="PGXN — PostgreSQL Extension Network">PGXN</a> might be
a challenge. Most record the repository info in the <a href="https://pgxn.org/spec/#resources">resources META.json
object</a>, so it would be do-able to adapt into a new META format, either
on <a href="https://pgxn.org" title="PGXN — PostgreSQL Extension Network">PGXN</a> itself or in a new registry, should we choose to build one. 
 Getting everyone to standardize on standardized versioning tags might take
some effort. Go had the benefit of controlling its entire toolchain, while
Postgres extension versioning and release management has been all over the
place. However <a href="https://pgxn.org" title="PGXN — PostgreSQL Extension Network">PGXN</a> long ago standardized on <a href="https://semver.org" title="Semantic Versioning 2.0.0">semantic versioning</a> and
those who have released extensions on PGXN have had few issues (one can
still use other version formats in the control file, for better or worse). 
 Some PGXN distributions have shipped different versions of extensions in a
single release, or the same version as in other releases. The release
version of the overall package (repository, really) would have to become
canonical. 
 
<p>I’m sure there are more, I just thought of these offhand. What have you thought
of? Post ’em if you got ’em   in the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres
Slack</a>, or give me a holler <a href="https://xoxo.zone/@theory" title="@theory@xoxo.zone">on Mastodon</a> or via email.</p>
 
 
 
 
<p>Or does it? Yes, it does. Although the Go CLI downloads most
public modules from a <a href="https://go.dev/ref/mod#goproxy-protocol" title="Go Modules Reference: GOPROXY protocol">module proxy server</a> like <code>proxy.golang.org</code>, it
still must know how to <a href="https://go.dev/ref/mod#vcs" title="Go Modules Reference: Version control systems">download modules from a version control system</a> when
a proxy is not available.&nbsp;<a>↩︎</a></p>
 
 
<p>Assuming, of course, that if and when the Postgres core
adopts more bundled packaging that they’d use the same naming convention as
we have in the broader ecosystem. Not a perfectly safe assumption, but given
the Go precedent and wide adoption of host/path-based projects, it seems
sound.&nbsp;<a>↩︎</a></p>
 
 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/go/">Go</a> 
                 <a href="https://justatheory.com/tags/packaging/">Packaging</a> 
                 <a href="https://justatheory.com/tags/distributed-publishing/">Distributed Publishing</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   15:50 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 31. January 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.pgxn.org/post/741049567045468160/pgxn-tools-v4'>PGXN Tools v1.4</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The pgxn-tools Docker image has seen some recent bug fixes and improvements.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Over on the <a href="https://blog.pgxn.org/">PGXN Blog</a> I’ve <a href="https://blog.pgxn.org/post/741049567045468160/pgxn-tools-v4" title="PGXN Blog: “PGXN Tools Docker Image Updated”">posted a brief update</a> on recent bug fixes and
improvements to the <a href="https://hub.docker.com/r/pgxn/pgxn-tools">pgxn-tools Docker image</a>, which is used fairly widely these
days to test, bundle, and release Postgres extensions to <a href="https://pgxn.org" title="PostgreSQL Extension Network">PGXN</a>. This fix is
especially important for Git repositories:</p>
<blockquote>
<p>v1.4.1 fixes an issue where <code>git archive</code> was never actually used to build a
release zip archive. This changed at some point without noticing due to the
introduction of the <code>safe.directory</code> configuration in recent versions of Git.
Inside the container the directory was never trusted, and the <code>pgxn-bundle</code>
command caught the error, decided it wasn’t working with a Git repository, and
used the <code>zip</code> command, instead.</p>
</blockquote>
<p>I also posted a <a href="https://gist.github.com/theory/93c93571200aad02e93170c6d2c93cbe" title="PGXN distributions that contain a .git directory">gist listing PGXN distributions with a <code>.git</code> directory</a>.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/docker/">Docker</a> 
                 <a href="https://justatheory.com/tags/github-workflow/">GitHub Workflow</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:13 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2492'>Invited OpenID Federation Presentation at 2024 FIM4R Workshop</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The OpenID Federation editors were invited to give a presentation on OpenID Federation at the 18th FIM4R Workshop, which was held at the 2024 TIIME Unconference. Giuseppe De Marco, Roland Hedberg, John Bradley, and I tag-teamed the presentation, with Vladimir Dzhuvinov also participating in the Q&amp;A. Topics covered included motivations, architecture, design decisions, capabilities, use […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The OpenID Federation editors were invited to give a presentation on OpenID Federation at the <a href="https://indico.cern.ch/event/1325302/">18th FIM4R Workshop</a>, which was held at the <a href="https://tiime-unconference.eu/">2024 TIIME Unconference</a>.  <a href="https://www.linkedin.com/in/giuseppe-de-marco-bb054245/">Giuseppe De Marco</a>, <a href="https://www.linkedin.com/in/rolandhedberg/">Roland Hedberg</a>, <a href="https://www.linkedin.com/in/ve7jtb/">John Bradley</a>, and <a href="https://www.linkedin.com/in/selfissued/">I</a> tag-teamed the presentation, with <a href="https://www.linkedin.com/in/vladimirdzhuvinov/">Vladimir Dzhuvinov</a> also participating in the Q&amp;A.  Topics covered included motivations, architecture, design decisions, capabilities, use cases, history, status, implementations, and people.</p>
<p>Here’s the material we used:</p>
 
 <a href="https://self-issued.info/presentations/OpenID_Federation_1.0_for_18th_FIM4R_Workshop_at_TIIME_2024.pdf">OpenID Federation 1.0: Shaping The Advanced Infrastructure of Trust</a> 
 
<p>It was the perfect audience – chock full of people with practical federation deployment experience!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/openid-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:35 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2489'>Fully-Specified Algorithms adopted by JOSE working group</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The “Fully-Specified Algorithms for JOSE and COSE” specification has been adopted by the JOSE working group. See my original post about the spec for why fully-specified algorithms matter. Thanks to all who supported adoption and also thanks to those who provided useful detailed feedback that we can address in future working group drafts. The specification […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The “Fully-Specified Algorithms for JOSE and COSE” specification has been adopted by the <a href="https://datatracker.ietf.org/wg/jose/about/">JOSE working group</a>.  See <a href="https://self-issued.info/?p=2401">my original post about the spec</a> for why fully-specified algorithms matter.  Thanks to all who supported adoption and also thanks to those who provided useful detailed feedback that we can address in future working group drafts.</p>
<p>The specification is available at:</p>
 
 <a href="https://www.ietf.org/archive/id/draft-ietf-jose-fully-specified-algorithms-00.html">https://www.ietf.org/archive/id/draft-ietf-jose-fully-specified-algorithms-00.html</a> 
 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   10:30 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 30. January 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.jonudell.net'>Jon Udell</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.jonudell.net/2024/01/30/how-to-learn-unfamiliar-software-tools-with-chatgpt/'>How to Learn Unfamiliar Software Tools with ChatGPT</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Here’s the latest installment in the series on working with LLMS: How to Learn Unfamiliar Software Tools with ChatGPT. Ideally, tools like GeoGebra and Metabase provide interfaces so intuitive that you rarely need to read the docs, and you can learn the software just by poking around in it. In reality, of course, we need … Continue reading How to Learn Unfamiliar Software Tools with ChatGPT
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Here’s the latest installment in the series on working with LLMS: <a href="https://thenewstack.io/how-to-learn-unfamiliar-software-tools-with-chatgpt/">How to Learn Unfamiliar Software Tools with ChatGPT</a>. </p>
<blockquote><p>
Ideally, tools like GeoGebra and Metabase provide interfaces so intuitive that you rarely need to read the docs, and you can learn the software just by poking around in it. In reality, of course, we need those docs — and they still need to be excellent. But now, we’ll extract a new benefit from them. When we can partner with machines that have read the docs, and can look over our shoulders as we try to do the things described in the docs, we’ll turbocharge our ability to dive into unfamiliar software tools and quickly learn how to use them.
</p></blockquote>
 
<p>The rest of the series:</p>
<p>
1 <a href="https://blog.jonudell.net/2023/05/24/when-the-rubber-duck-talks-back/">When the rubber duck talks back</a> </p>
<p>
2 <a href="https://blog.jonudell.net/2023/06/14/radical-just-in-time-learning/">Radical just-in-time learning</a> </p>
<p>
3 <a href="https://blog.jonudell.net/2023/06/18/why-llm-assisted-table-transformation-is-a-big-deal/">Why LLM-assisted table transformation is a big deal</a> </p>
<p>
4 <a href="https://thenewstack.io/using-llm-assisted-coding-to-write-a-custom-template-function/">Using LLM-Assisted Coding to Write a Custom Template Function</a></p>
<p>
5 <a href="https://thenewstack.io/elevating-the-conversation-with-llm-assistants/">Elevating the Conversation with LLM Assistants</a></p>
<p>
6 <a href="https://thenewstack.io/how-large-language-models-assisted-a-website-makeover/">How Large Language Models Assisted a Website Makeover</a></p>
<p>
7 <a href="https://thenewstack.io/should-llms-write-marketing-copy/">Should LLMs Write Marketing Copy?</a></p>
<p>
8 <a href="https://thenewstack.io/test-driven-development-with-llms-never-trust-always-verify/">Test-Driven Development with LLMs: Never Trust, Always Verify</a> </p>
<p>
9 <a href="https://thenewstack.io/learning-while-coding-how-llms-teach-you-implicitly/">Learning While Coding: How LLMs Teach You Implicitly</a></p>
<p>
10 <a href="https://thenewstack.io/how-llms-helped-me-build-an-odbc-plugin-for-steampipe/">How LLMs Helped Me Build an ODBC Plugin for Steampipe</a></p>
<p>
11 <a href="https://thenewstack.io/how-to-use-llms-for-dynamic-documentation/">How to Use LLMs for Dynamic Documentation</a></p>
<p>
12 <a href="https://blog.jonudell.net/2023/11/06/lets-talk-conversational-software-development/">Let’s talk: conversational software development</a></p>
<p>
13 <a href="https://thenewstack.io/techniques-for-using-llms-to-improve-sql-queries/">Using LLMs to Improve SQL Queries</a></p>
<p>
14 <a href="https://thenewstack.io/puzzling-over-the-postgres-query-planner-with-llms/">Puzzling over the Postgres Query Planner with LLMs</a></p>
<p>
15 <a href="https://thenewstack.io/7-guiding-principles-for-working-with-llms/">7 Guiding Principles for Working with LLMs</a></p>
<p>
16 <a href="https://thenewstack.io/learn-by-doing-how-llms-should-reshape-education/">Learn by Doing: How LLMs Should Reshape Education</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:40 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@m-ruminer/'>Michael Ruminer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://m-ruminer.medium.com/thoughts-on-self-sovereign-identity-a-systematic-review-mapping-and-taxonomy-cb6a4e8bfd38?source=rss-7e85224c0a32------2'>Thoughts on Self-Sovereign Identity: A Systematic Review, Mapping and Taxonomy</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Thoughts on Self-Sovereign Identity: A Systematic Review, Mapping and&nbsp;Taxonomy  I ran across a paper named Self-Sovereign Identity: A Systematic Review, Mapping and Taxonomy. It was published mid-2022 so it is not the most up to date for the topic but it is very interesting and still of high value. It’s a meta-study of four research questions about self-sovereign identity.   RQ-1: What Practi
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/354/1*5-7tNatM9RDTl_9xMHGHBQ.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Thoughts on Self-Sovereign Identity: A Systematic Review, Mapping and&nbsp;Taxonomy</p><p>I ran across a paper named <a href="https://www.mdpi.com/1424-8220/22/15/5641">Self-Sovereign Identity: A Systematic Review, Mapping and Taxonomy</a>. It was published mid-2022 so it is not the most up to date for the topic but it is very interesting and still of high value. It’s a meta-study of four research questions about self-sovereign identity.</p>  RQ-1: What Practical Problems Have Been Introduced and&nbsp;Solved?  RQ-2: What Properties, Formal Definitions and Cryptographic Tools Have Been&nbsp;Used?  RQ-3: What Conceptual Ideas Have Been Introduced or&nbsp;Refuted?  RQ-4: When, Where, and by Whom Were SSI Studies Published?  <p>It spends a lot of text before the research questions on how it built the study objectively, criteria for the data, and criteria for the inclusion of the other research papers. Though interesting, it was not what compelled me in the paper. As you might imagine, it was the red meat of the paper, the research questions, that I really found most interesting.</p><p>You’ll find in the research question sections that it does a nice inventory of various papers and a description of what they cover. I found RQ-1 to be the most interesting as it covers a lot of verifiable credentials and verifiable presentation topics.</p><p>Of RQ-1 I found section 6.2 to be of special interest. It&nbsp;covers:</p> The operational facet is divided into two facets: VC and VP.<br>They are a collection of concepts related to the functional aspects<br>of verifiable credentials and verifiable presentations. <p>And includes:</p>  Revocation  Decentralized Identifiers  Issuer Authorization  Delegation  Backup and&nbsp;Recovery  <p>RQ-3 is a short section and starts with an interesting statement that is probably less true today than when written but still holds a lot of&nbsp;truth.</p> ...that there is currently no agreement on a definition of SSI... <br>Our third research question is answered by an examination of <br>the literature’s debates on the SSI definition. <p>Though I appreciate RQ-4 and it makes sense in the context of the paper, I found the least value in its presentation. It did remind me of a relationship graph I created a number of years back except that my graph was on the relationship of the specifications at the time. The header image of this post is a small rendering of that graph. You can find the useful version at <a href="https://www.kumu.io/michaelruminer/vc-spec-map">Verifiable Credentials Specification Map</a>. Reader beware that the specification map I list was last updated late May of 2021 so it is not an accurate source of the state of specifications for today though many of the relationships it does show are likely valid. This is really a topic for a different day.</p><p>All in all, despite the relative age of the paper, the other papers it refers to are often still valid today in their intent and basic questions, agreements, and refutations. I think it is well worth your time to look at the research questions portions if interested in self-sovereign identity (a phrase that seems to be moving more so out of popular use) and verifiable credentials.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/354/1*5-7tNatM9RDTl_9xMHGHBQ.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:04 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.jonudell.net'>Jon Udell</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.jonudell.net/2024/01/29/you-say-feature-i-say-bug-the-enshittification-of-microsoft-paint/'>You say feature, I say bug: the enshittification of Microsoft Paint</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I’ve happily used MS Paint as my basic bitmap editor since Windows 3, almost 25 years ago. Mostly I’ve used it to create images from screenshots, but that has suddenly become way harder. Formerly, when I’d cut a region, the now-empty region would display using the default white background. Now it displays a checkered background … Continue reading You say feature, I say bug: the enshittification of 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>
I’ve happily used MS Paint as my basic bitmap editor since Windows 3, almost 25 years ago. Mostly I’ve used it to create images from screenshots, but that has suddenly become way harder. Formerly, when I’d cut a region, the now-empty region would display using the default white background. Now it displays a checkered background like so.</p>
<p></p>
<p>
Here is the procedure to refill the white background: </p>
 
 Switch the foreground color to white
  Use the Fill tool to fill the checkered region
  Then switch the foreground back to black.
  
<p>
ARE YOU KIDDING ME?</p>
<p>
Nope. It’s evidently an unintended consequence of a pair of new feature: layers and transparency.</p>
<blockquote><p>
To get started, click on the new Layers button in the toolbar, which will open a panel on the side of the canvas.”</p>
<p>
Microsoft also revealed today that an upcoming Paint feature is support for image transparency, which will add the ability to open and save transparent PNG files.</p>
<p>
During editing, users will notice a prominent checkerboard pattern displayed on the canvas, serving as a visual indicator and highlighting the transparent regions within the image.</p>
<p>
This ensures that when content is erased from the canvas, it is completely removed, eliminating the need to cover unwanted regions of an image with white fill.  </p>
<p> – <a href="https://www.bleepingcomputer.com/news/microsoft/microsoft-paint-finally-gets-support-for-layers-and-transparency">bleepingcomputer.com</a>
</p></blockquote>
<p>
I never asked for these “long-awaited” new features, Paint is (or was) useful to me precisely because it only does the kind of basic bitmap editing I need when compositing screenshots. But I can opt out, right?</p>
<p>
Nope. </p>
<p>
ARE YOU KIDDING ME?</p>
<p>
Nope. </p>
<blockquote><p>
This feature (layers and image transparency) seems to be introduced in September 2023 and doesn’t actually allow to be turned off.</p>
<p>
Doing what vengy proposes for each and every image being edited is a natural madness and will drive even the most sane person crazy.</p>
<p>
What worked for me was to uninstall Paint and replace it with a classic version:</p>
 
 Uninstalling can be done by simply right-clicking Paint icon in Start Menu and selecting Uninstall from context menu.
  
Classic Paint can be get from here or here.<p></p>
  
Download and install it.<p></p>
  
Go to Settings → Apps → Apps &amp; Features → More settings → App execution aliases.<p></p>
  
Toggle the switch to Off for mspaint.exe and pbrush.exe items.
  
<p>– <a href="https://superuser.com/questions/1813858/microsoft-paint-selection-delete-gives-checkered-background">superuser.com</a>
</p></blockquote>
<p>
Evidently people are willing to hack their systems in order to revert to a now-unsupported version that they prefer. As insane as it would be, I’m considering whether to become one of those people. Sigh. I guess 25 years was a pretty good run. </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/jonudell.info/images/mspaint-checkered-background.png?w=656&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   04:27 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/01/pgxn-challenges/'>PGXN Challenges</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Some thoughts on the challenges for PGXN’s role in the ideal PostgreSQL
extension ecosystem of the future.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	
			
	

         
<p>Last week, I informally shared <a href="https://gist.github.com/theory/898c8802937ad8361ccbcc313054c29d">Extension Ecosystem: Jobs and Tools</a> with
colleagues in the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a>. The document
surveys the <a href="https://strategyn.com/jobs-to-be-done/jobs-to-be-done-theory/">jobs to be done</a> by the ideal Postgres extension ecosystem and the
suggests the tools and services required to do those jobs — without reference
to existing extension registries and packaging systems.</p>
<p>The last section enumerates some questions we need to ponder and answer. The
first one on the list is:</p>
<blockquote>
<p>What will <a href="https://pgxn.org" title="The postgreSQL Extension Network">PGXN</a>’s role be in this ideal extension ecosystem?</p>
</blockquote>
<p>The PostgreSQL Extension Network, or <a href="https://pgxn.org" title="The postgreSQL Extension Network">PGXN</a>, is the original extension
distribution system, created 2010–11. It has been a moderate success, but as we
in the Postgres community imagine the ideal extension distribution future, it’s
worthwhile to also critically examine existing tools like PGXN, both to inform
the project and to realistically determine their roles in that future.</p>
<p>With that in mind, I here jot down some thoughts on the challenges with PGXN.</p>
 PGXN Challenges 
<p>PGXN sets a lot of precedents, particularly in its decoupling of the registry
from the APIs and services that depend on it. It’s not an all-in-one thing, and
designed for maximum distributed dissemination via rsync and static JSON files.</p>
<p>But there are a number of challenges with PGXN as it currently stands; a
sampling:</p>
 
 
<p>PGXN has not comprehensively indexed all public PostgreSQL extensions. While
it indexes more extensions than any other registry, it falls far short of
all <a href="https://gist.github.com/joelonsql/e5aa27f8cc9bd22b8999b7de8aee9d47" title="GitHub Gist: 🗺🐘 1000+ PostgreSQL EXTENSIONs">known extensions</a>. To be a truly canonical registry, we need to make it
as simple as possible for developers to register their extensions. (More
thoughts on that topic in a forthcoming post.)</p>
 
 
<p>In that vein, releasing extensions is largely a manual process. The
<a href="https://github.com/pgxn/docker-pgxn-tools" title="pgxn/docker-pgxn-tools: Test image for PostgreSQL &amp; PGXN extensions">pgxn-tools</a> Docker image has improved the situation, allowing developers to
create relatively simple GitHub workflows to automatically <a href="https://github.com/theory/kv-pair/blob/main/.github/workflows/ci.yml" title="kv-pair extension CI workflow">test</a> and
<a href="https://github.com/theory/kv-pair/blob/main/.github/workflows/release.yml" title="kv-pair extension release workflow">release</a> extensions. Still, it requires intention and work by extension
developers. The more seamless we can make publishing extensions the better.
(More thoughts on that topic in a forthcoming post.)</p>
 
 
<p>It’s written in <a href="https://www.perl.org" title="Perl is a highly capable, feature-rich programming language with over 36 years of development.">Perl</a>, and therefore doesn’t feel modern or easily
accessible to other developers. It’s also a challenge to build and
distribute the Perl services, though Docker images could mitigate this
issue. Adopting a modern compiled language like <a href="https://go.dev" title="Build simple, secure, scalable systems with Go">Go</a> or <a href="https://www.rust-lang.org" title="A language empowering everyone to build reliable and efficient software">Rust</a> might
increase community credibility and attract more contributions.</p>
 
 
<p>Similarly, <a href="https://pgxn.github.io/pgxnclient/" title="PGXN Client’s documentation">pgxnclient</a> is written in <a href="https://www.python.org" title="Python is a programming language that lets you work quickly and integrate systems more effectively">Python</a> and the <a href="https://github.com/guedes/pgxn-utils" title="PGXN extension development and release utilities">pgxn-utils</a>
developer tools in <a href="https://www.ruby-lang.org/" title="A dynamic, open source programming language with a focus on simplicity and productivity.">Ruby</a>, increasing the universe of knowledge and skill
required for developers to maintain all the tools. They’re also more
difficult to distribute than compiled tools would be. Modern
cross-compilable languages like <a href="https://go.dev" title="Build simple, secure, scalable systems with Go">Go</a> and <a href="https://www.rust-lang.org" title="A language empowering everyone to build reliable and efficient software">Rust</a> once again simplify
distribution and are well-suited to building both web services and CLIs (but
not, perhaps native UX applications —  but then neither are dynamic
languages like Ruby and Python).</p>
 
 
<p>The <a href="https://github.com/pgxn/pgxn-api/wiki/search-api">PGXN Search API</a> uses the <a href="https://lucy.apache.org" title="A “loose C” port of the Apache Lucene™ search engine library for Java.">Apache Lucy</a> search engine library, a
project that <a href="https://attic.apache.org/projects/lucy.html" title="Apache Lucy moved into the Attic in June 2018">retired</a> in 2018. Moreover, the feature never worked very
well, thanks to the decision to expose separate search indexes for different
objects — and requiring the user to select which to search. People often
can’t find what they need because the selected index doesn’t contain it.
Worse, the default index on <a href="https://pgxn.org" title="The postgreSQL Extension Network">the site</a> is “Documentation”, on the
surface a good choice. But most extensions include no documentation other
than the README, which appears in the “Distribution” index, not
“Documentation”. Fundamentally the search API and UX needs to be completely
re-architected and -implemented.</p>
 
 
<p>PGXN uses its own very simple identity management and <a href="https://en.wikipedia.org/wiki/Basic_access_authentication" title="Wikipedia: “Basic access authentication”">basic
authentication</a>. It would be better to have tighter community identity,
perhaps through the <a href="https://www.postgresql.org/account/" title="Your PostgreSQL community account">PostgreSQL community account</a>.</p>
 
 
<p>Given these issues, should we continue building on PGXN, rewrite some or all of
its components, or abandon it for new services. The answer may come as a natural
result of designing the overall extension ecosystem architecture or from the
motivations of community consensus. But perhaps not. In the end, we’ll need a
clear answer to the question.</p>
<p>What are your thoughts? Hit us up in the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the <a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres
Slack</a>, or give me a holler <a href="https://xoxo.zone/@theory" title="@theory@xoxo.zone">on Mastodon</a> or via email. We expect to start
building in earnest in February, so now’s the time!</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/2024/01/pgxn-challenges/pgxn-gear.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   00:11 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 29. January 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://identitywoman.net'>Identity Woman</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://identitywoman.net/event-reflection-childrens-digital-privacy-summit-2024/'>Event Reflection: Children’s Digital Privacy Summit 2024</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Last week, I flew to LA To attend the Children’s Digital Privacy Summit hosted by Denise Tayloe and her team at Privo. I’ve known Denise since the early days of IIW, and it was great to meet her team for the first time.&nbsp; &nbsp; They put on a great show what began with a talk […] 
 The post Event Reflection: Children’s Digital Privacy Summit 2024 appeared first on Identity Woman. 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>Last week, I flew to LA To attend the Children’s Digital Privacy Summit hosted by Denise Tayloe and her team at Privo. I’ve known Denise since the early days of IIW, and it was great to meet her team for the first time.&nbsp; &nbsp; They put on a great show what began with a talk […]</p>
<p>The post <a href="https://identitywoman.net/event-reflection-childrens-digital-privacy-summit-2024/">Event Reflection: Children’s Digital Privacy Summit 2024</a> appeared first on <a href="https://identitywoman.net">Identity Woman</a>.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   18:56 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/acceptance-networks-for-self-sovereign'>Acceptance Networks for Self-Sovereign Identity</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  We can't have broad adoption of verifiable credentials until we find a way to scale their presentation by providing tooling that credential verifiers can use to reduce their risk and gain confidence in the facts presented to them. When I hand a merchant in London a piece of plastic that I got from a bank in Utah to make a purchase, a tiny miracle happens. Despite the fact that the merchant has neve
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8927c9c-2539-4adf-9a58-aed3acd9a921.heic">    </a> <p>We can't have broad adoption of verifiable credentials until we find a way to scale their presentation by providing tooling that credential verifiers can use to reduce their risk and gain confidence in the facts presented to them. </p><p>When I hand a merchant in London a piece of plastic that I got from a bank in Utah to make a purchase, a tiny miracle happens. Despite the fact that the merchant has never met me before and has no knowledge of my bank, she blithely allows me to walk out of the store with hundreds of dollars of merchandise, confident that she will receive payment. I emphasized the word confident in the last sentence because it's core to understanding what's happened. In the past, these kinds of transactions required that the merchant trust me or my bank. But in the modern world, trust has been replaced by confidence.</p><p>We often mix these concepts up and I'm as guilty as anyone. But trust always involves an element of risk, whereas confidence does not. These are not binary, but rather represent a spectrum. In the scenario I paint above, the merchant is still taking some risk, but it's very small. Technology, processes, and legal agreements have come together to squeeze out risk. The result is a financial system where the risk is so small that banks, merchants, and consumers alike have confidence that they will not be cheated. There's a name in the financial services industry for the network that reduces risk so that trust can be replaced with confidence: an acceptance network.</p> Acceptance Networks <p>An acceptance network is the network of merchants or service providers that accept a particular form of payment, usually credit or debit cards, from a particular issuer or payment network. The term refers to a broad ecosystem that facilitates these transactions, including point-of-sale terminals, online payment gateways, and other infrastructure. Each component of the acceptance network plays a crucial role in ensuring that transactions are processed efficiently, securely, and accurately. This drives out risk and increases confidence. Acceptance networks are foundational components of modern payment ecosystems and are essential to the seamless functioning of digital financial transactions. Visa, Mastercard, American Express, and Discover are all examples of acceptance networks.</p><p>Before the advent of acceptance networks, credit was a spotty thing with each large merchant issuing it's own proprietary credit card—good only at that merchant. My mom and dad had wallets full of cards for JC Penney, Sears, Chevron, Texaco, and so on. Sears trusted its card. Chevron trusted its card. But it was impossible to use a Chevron card at Sears. They had limited means to verify if it was real and no way to clear the funds so that Chevron could pay Sears for the transaction.</p><p>That scenario is similar to the state of digital identity today. We have identity providers (IdPs) like Google and Apple who control a closed ecosystem of relying parties (with a lot of overlap). These relying parties trust these large IdPs to authenticate the people who use their services. They limit their risk by only using IdPs they're familiar with and only accepting the (usually) self-asserted attributes from the IdP that don't involve much risk. Beyond that they must verify everything themselves.</p><p>Fixing this requires the equivalent of an acceptance network for digital identity. When we <a href="https://www.windley.com/archives/2017/01/a_universal_trust_framework.shtml">launched Sovrin Foundation and the Sovrin network</a>1 in 2016, we were building an acceptance network for digital identity, even though we didn't use that term to describe it. Our goal was to create a system of protocols, processes, technology and governance that would reduce the risk of self-sovereign identity and increase confidence in an identity system that let the subjects present verifiable credentials that carried reliable attributes from many sources.</p><p>I've written previously about <a href="https://www.windley.com/archives/2019/08/life-like_identity_why_the_internet_needs_an_identity_metasystem.shtml">identity metasystems</a> that provide a framework for how identity transactions happen. Individual identity systems are built according to the architecture and protocols of the metasystem. Acceptance networks are an instantiation of the metasystem for a particular set of users and types of transactions. A metasystem for self-sovereign identity might have several acceptance networks operating in it to facilitate the operation of specific identity systems.</p> Problems an Acceptance Network Can Solve <p>To understand why an acceptance network is necessary to reduce risk and increase confidence in identity transactions, let's explore the gaps that exist without it. The following diagram shows the now familiar triangle of verifiable credential exchange. In this figure, issuers issue credentials to holders who may or may not be the subject of the credentials. The holder presents cryptographic proofs that assert the value of relevant attributes using one of more of the credentials that they hold. The verifier verifies the proof and uses the attributes.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d6a862b-3ed7-49da-872a-2d0bd818faab.heic">    </a>Verifiable Credential Exchange <p>Let's explore what it means for the verifier to verify the proof. The verifier wants to know a number of things about the credential presentation:</p>  <p>Were the credentials issued to the entity making the presentation?</p>  <p>Have any of the credentials been tampered with?</p>  <p>Have any of the credentials been revoked?</p>  <p>What are the schema for the credentials (to understand the data in them)?</p>  <p>Who issued the credentials in the proof?</p>  <p>The first four of these can be done cryptographically to provide confidence in the attestation. The technology behind the credential presentation is all that's necessary. They can be automated as part of the exchange. For example, the proof can contain pointers (e.g., DIDs) to the credential definitions. These could contain public keys for the credential and references to schema.</p><p>The last one—who issued the credential—is not a technical matter. To see why, imagine that Alice (as holder and subject) has been issued a credential from her university (the issuer) giving information about her educational experiences there. She's applying for a job and wants to present the credential to a prospective employer (the verifier). How does the employer know that Alice didn't just make the credential herself or buy it from a diploma mill?</p><p>Knowing who issued the credential is not something that can be done solely with technology (although it can help). The employer in this scenario wants more than an identifier for the issuer. And they want to know that the public key really does belong to the university. In short, the employer wants to resolve the identifier to other information that tells them something about the university and the credential. There are lots of ways to do that—people have been doing this sort of thing for centuries: states keep registries of businesses (universities are businesses), accreditation organizations keep registries of schools they've accredited, the Department of Education has registries of various institutions of higher education in the US, and so on.</p><p>The employer could make use of these by building its own database of university identifiers it trusts. And every time a new one shows up, they could investigate and add it to their registry (or not)2. But going back to the magic of the credit card scenario that I opened this article with, if every merchant had to keep their own registry of banks, the experience wouldn't be magical for me or the merchant. The financial acceptance network makes it easy for the merchant to have confidence that they'll be paid because they have not only technology, but processes, protocols, governance, and legal agreements that make the verification process automatable.</p> Acceptance Networks for Digital Identity <p>For some use cases, keeping your own registry of the issuers you trust works. But for many, it's just too much work and makes it difficult to make use of a variety of credentials. This kind of "localized trust" is unwieldy in an identity system that might involve millions of issuers and identifiers and credentials for billions or even trillions of subjects. I've written <a href="https://www.windley.com/tags/metasystem">extensively about identity metasystems</a> and what they provide to help bridge the gap. This one, on <a href="https://www.windley.com/archives/2019/08/life-like_identity_why_the_internet_needs_an_identity_metasystem.shtml">how metasystems help provide life-like identity for digital systems</a> is perhaps the most comprehensive. Acceptance networks implement metasystems.</p><p>An acceptance network for digital identity must have a number of important properties, including the following:</p>  <p>Credentials are decentralized and contextual—There is no central authority for all credentials. Every party can be an issuer, a holder (identity owner), or a verifier. Verifiable credentials can be adapted to any country, any industry, any community, or any set of trust relationships.</p>  <p>Credential issuers decide on what data is contained in their credentials—Anyone can create a credential schema for their use case. Anyone can create a credential definition based on any of these schemas.</p>  <p>Verifiers make their own trust decisions about which credentials to accept—There's no central authority who determines what credentials are important or which are used for what purpose. The acceptance network supplies the technical underpinnings for credential exchange and support protocols for automating the verification of credential issuers.</p>  <p>Credential verifiers don't need to have any specific technical, contractual, or commercial relationship with credential issuers—Verifiers do not need to contact issuers to perform verification.</p>  <p>Credential holders are free to choose which credentials to carry and what information to disclose—People and organizations are in control of the credentials they hold (just as they are with physical credentials) and determine what to share with whom.</p>  <p>You may be thinking "but these are mostly about decentralized decision making." While it would be easier to imagine the acceptance network as a big directory, that solution can't possible support all the different ways people and organizations might want to use credentials. That doesn't mean an acceptance network couldn't be run by a single organization, like some financial services networks. Just that it has to support a variety of credential ecosystems running common protocols. I also think that there will be more than one and most issuers and verifiers will be part of several (again, like in financial services).</p> Structure of an Acceptance Network <p>One of the things we can take away from the architecture of financial services acceptance networks is that they are built in layers. No one has thought more about how this can work than Drummond Reed and the <a href="https://trustoverip.org/wp-content/uploads/Evolution-of-the-ToIP-Stack-V1.0-2022-11-14.pdf">Trust Over IP Foundation</a> (ToIP).3 This figure, from ToIP, shows how such a stack works.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a05ffdb-511c-468a-a73a-edf99d54299c.heic">    </a>Trust Over IP Stack <p>The layers build on each other to provide something the lower level didn't. Layer 1 is the foundational functionality, like DID methods. Layer 2 builds on that to support creating digital relationships with anyone. Layer 3 uses those relationships to effect credential exchange. Layer 4 is the ecosystems that say things about the issuers for different use cases. The dual stack emphasizes the need for governance at every layer.</p><p>The acceptance network specifies the accepted protocols and technologies. The acceptance network also supports ecosystems, providing governance models and technology. The acceptance network is involved at each layer. Here are some examples of things an acceptance network might do at each layer:</p>  <p>Layer 1—limit the allowed DID methods and certify them.</p>  <p>Layer 2—require that wallets and agents using the network support specific versions of the DIDComm protocol. Provide a certification framework for wallet and agent vendors for security and interoperability.</p>  <p>Layer 3—require specific versions of the exchange protocols. Participate in protocol development. Provide a certification framework for specific implementations to aid with security and interoperability.</p>  <p>Layer 4—support the formation, certification, and discovery of credential ecosystem providers. Govern what is required to be a certified ecosystem provider and provide models for acceptable ecosystem governance.</p>  <p>As part of it's overall governance of the ecosystem, the acceptance network also provides model legal agreements for and between the various participants, trust mark rights (think of the Visa logo), and drives a uniform user experience.</p><p>The following diagram shows the credential exchange from the preceding figure with an acceptance network providing support to the verifier so that it can have confidence in the data the issuer has supplied through the holder.</p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f2b61c7-1e3c-4194-8354-49e5b9f84587.heic">    </a>Acceptance Network in Operation <p>Credential issuers who know their credential might be widely used would join one or more acceptance networks. They agree to follow the rules and regulations in the governance framework of the acceptance network. The acceptance network issues a credential to them that they can use to prove they are a member.4 The acceptance network maintains a registry—likely a registry of registries—that verifiers can use to discover information about the issuer of a credential that has been presented to them.</p> Using an Acceptance Network <p>Returning to our previous scenario, Alice holds a credential issued by her university. She presents it to a prospective employer who wants to know that the credential is from an accredited university. Alice's university has been accredited by an accreditation organization5. They have followed their process for accrediting Alice's university and issued it a credential. They have also added the university to their registry. The university and the accrediting organization are members of an acceptance network. The employer's systems know to automatically query the acceptance network when it received a credential proof from a issuer it does not know. Doing so provides the assurance that the issuer is legitimate. It could also provide information about the accreditation status of the university. This information reduces the risk that the employer would otherwise bear.</p><p>In this scenario, the employer is trusting the processes and structure of the acceptance network. The employer must decide which acceptance networks to use. This is much more scalable than having to make these determinations for every credential issuer. The acceptance network has allowed the verification process to scale and made the overall use of verifiable credentials easier and less risky.</p> A Note on Implementation <p>This discussion of acceptance networks has undoubtedly brought images to your mind about how it is structured or how to build one. The comparison to financial services acceptance networks points to a network run by an organization. And the term registrybrings to mind a database of some kind. Why these are certainly possibilities, I think it's also possible to imagine more decentralized solutions. For example, the registry could be a distributed ledger or blockchain. The governance is likely most easily done by an organization, but there are other options like a decentralized autonomous organization (DAO). The scenario I described above illustrates a federated system where certifying authorities for specific ecosystems determine their own methods, processes, and requirements, but link their registry to that of the acceptance network.</p> Conclusion <p>As I mentioned above, we've been solving the problem of how to know which institutions to trust for centuries. We have ways of knowing whether a university is accredited, whether a bank is real, whether a company is actually registered and what its reputation is. What is missing is an easy way to make use of this information digitally so that processes for reducing risk can be automated. Acceptance networks rationalize the process and provide the needed tooling to automate these checks. They reduce the many-to-many problem that exists when each verifier has to determine whether to trust each issuer with a more scalable many-to-several system. Acceptance networks allow credential presentation to scale by providing the needed infrastructure for giving verifiers confidence in the facts that holders present to them.</p>    Notes   <p>You can see in the linked post how we used trust to describe what we were building, even as we were reducing risk and inspiring confidence.</p>  <p>Note that this investigation could make use of technology. Knowing the universities name, they could look up a <a href="https://www.rfc-editor.org/rfc/rfc8615">well known location</a> on the universities web site to find the identifier. They could use PKI (digital certificates) to be sure they're talking to the right place. They could look up the university in an online registry of accredited universities.</p>  <p>Trust over IP isn't the only one working on this. <a href="https://www.linkedin.com/pulse/demystifying-decentralized-identity-marie-wallace/?trackingId=dmcHrAqMST2EC8g%2F8F177w%3D%3D">Marie Wallace of Accenture</a> and <a href="https://lockstep.com.au/DVP/">Stephen Wilson of Lockstep Partners</a> have been writing about this idea.</p>  <p>Note that there could be different levels or types of members who perform different roles in the ecosystem and make different agreements.</p>  <p>An example is the <a href="https://nwccu.org/">Northwest Commission on Colleges and Universities</a>.</p>  <p>Photo Credit: Data flowing over networks from DALL-e</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8927c9c-2539-4adf-9a58-aed3acd9a921.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:19 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 25. January 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2486'>OAuth 2.0 Protected Resource Metadata draft addressing all known issues</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Aaron Parecki and I have published a draft of the “OAuth 2.0 Protected Resource Metadata” specification that addresses all the issues that we’re aware of. In particular, the updates address the comments received during the discussions at IETF 118. As described in the History entry for -02, the changes were: Switched from concatenating .well-known to […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://twitter.com/aaronpk">Aaron Parecki</a> and I have published a draft of the “OAuth 2.0 Protected Resource Metadata” specification that addresses all the issues that we’re aware of.  In particular, the updates address the comments received during the discussions at IETF 118.  As described in the History entry for -02, the changes were:</p>
 
 Switched from concatenating <code>.well-known</code> to the end of the resource identifier to inserting it between the host and path components of it. 
 Have <code>WWW-Authenticate</code> return <code>resource_metadata</code> rather than <code>resource</code>. 
 
<p>The specification is available at:</p>
 
 <a href="https://www.ietf.org/archive/id/draft-ietf-oauth-resource-metadata-02.html">https://www.ietf.org/archive/id/draft-ietf-oauth-resource-metadata-02.html</a> 
 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/oauth_logo_120x120.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   03:52 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 24. January 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.jonudell.net'>Jon Udell</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.jonudell.net/2024/01/24/learn-by-doing-how-llms-should-reshape-education/'>Learn by Doing: How LLMs Should Reshape Education</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Here’s the latest installment in the series on working with LLMS: Learn by Doing: How LLMs Should Reshape Education. If you’re teaching SQL, this article points to a pedagogical challenge/opportunity: How would I create a lesson that guides a student to an understanding of CROSS JOIN without ever mentioning or explicitly teaching anything about it? … Continue reading Learn by Doing: How LLMs Should
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Here’s the latest installment in the series on working with LLMS: <a href="https://thenewstack.io/learn-by-doing-how-llms-should-reshape-education/">Learn by Doing: How LLMs Should Reshape Education</a>. </p>
<p>If you’re teaching SQL, this article points to a pedagogical challenge/opportunity: How would I create a lesson that guides a student to an understanding of CROSS JOIN without ever mentioning or explicitly teaching anything about it?</p>
<p>If you’re teaching anything else, the same question could (I’ll argue should) apply. How to scaffold learning by doing?</p>
 
<p>The rest of the series:</p>
<p>
1 <a href="https://blog.jonudell.net/2023/05/24/when-the-rubber-duck-talks-back/">When the rubber duck talks back</a> </p>
<p>
2 <a href="https://blog.jonudell.net/2023/06/14/radical-just-in-time-learning/">Radical just-in-time learning</a> </p>
<p>
3 <a href="https://blog.jonudell.net/2023/06/18/why-llm-assisted-table-transformation-is-a-big-deal/">Why LLM-assisted table transformation is a big deal</a> </p>
<p>
4 <a href="https://thenewstack.io/using-llm-assisted-coding-to-write-a-custom-template-function/">Using LLM-Assisted Coding to Write a Custom Template Function</a></p>
<p>
5 <a href="https://thenewstack.io/elevating-the-conversation-with-llm-assistants/">Elevating the Conversation with LLM Assistants</a></p>
<p>
6 <a href="https://thenewstack.io/how-large-language-models-assisted-a-website-makeover/">How Large Language Models Assisted a Website Makeover</a></p>
<p>
7 <a href="https://thenewstack.io/should-llms-write-marketing-copy/">Should LLMs Write Marketing Copy?</a></p>
<p>
8 <a href="https://thenewstack.io/test-driven-development-with-llms-never-trust-always-verify/">Test-Driven Development with LLMs: Never Trust, Always Verify</a> </p>
<p>
9 <a href="https://thenewstack.io/learning-while-coding-how-llms-teach-you-implicitly/">Learning While Coding: How LLMs Teach You Implicitly</a></p>
<p>
10 <a href="https://thenewstack.io/how-llms-helped-me-build-an-odbc-plugin-for-steampipe/">How LLMs Helped Me Build an ODBC Plugin for Steampipe</a></p>
<p>
11 <a href="https://thenewstack.io/how-to-use-llms-for-dynamic-documentation/">How to Use LLMs for Dynamic Documentation</a></p>
<p>
12 <a href="https://blog.jonudell.net/2023/11/06/lets-talk-conversational-software-development/">Let’s talk: conversational software development</a></p>
<p>
13 <a href="https://thenewstack.io/techniques-for-using-llms-to-improve-sql-queries/">Using LLMs to Improve SQL Queries</a></p>
<p>
14 <a href="https://thenewstack.io/puzzling-over-the-postgres-query-planner-with-llms/">Puzzling over the Postgres Query Planner with LLMs</a></p>
<p>
15 <a href="https://thenewstack.io/7-guiding-principles-for-working-with-llms/">7 Guiding Principles for Working with LLMs</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   18:19 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 22. January 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2024/01/tembonaut/'>I&rsquo;m a Postgres Extensions Tembonaut</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Near year, new job. I accepted a new position at Tembo to work on improving
the PostgreSQL extension ecosystem full time.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	
			
	

         
<p>New year, new job.</p>
<p>I’m pleased to announce that I started a new job on January 2 at <a href="https://tembo.io/" title="Tembo: Goodbye Database Sprawl, Hello Postgres">Tembo</a>, a
fully-managed <a href="https://www.postgresql.org" title="PostgreSQL: The world's most advanced open source database">PostgreSQL</a> developer platform. Tembo <a href="https://tembo.io/blog/welcoming-david-wheeler" title="PGXN Creator David Wheeler Joins Tembo to Strengthen the Postgres Extension Ecosystem">blogged the news</a>, too.</p>
<p>I first heard from Tembo CTO <a href="https://www.linkedin.com/in/samay-sharma-b6465122" title="Samay Sharma - Chief Technology Officer - Tembo | LinkedIn">Samay Sharma</a> last summer, when he inquired about
the status of <a href="https://pgxn.org/" title="The PostgreSQL Extension Network">PGXN</a>, the PostgreSQL Extension Network, which I built in
2010–11. Tembo bundles extensions into Postgres <a href="https://tembo.io/docs/category/tembo-stacks/" title="Tembo Docs: Tembo Stacks">stacks</a>, which let developers
quickly spin up Postgres clusters with tools and features optimized for specific
use cases and workloads. The company therefore needs to provide a wide variety
of easy-to-install and well-documented extensions to power those use cases.
Could PGXN play a role?</p>
<p>I’ve tended to PGXN’s maintenance for the last fourteen years, and thanks in no
small part to hosting provided by <a href="https://www.depesz.com" title="select * from depesz;">depesz</a>. As of today’s <a href="https://pgxn.org/about/" title="About PGXN">stats</a> it distributes
376 extensions on behalf of 419 developers. PGXN has been a moderate success,
but Samay asked how we could collaborate to build on its precedent to improve
the extensions ecosystem overall.</p>
<p>It quickly became apparent that we share a vision for what that ecosystem could
become, including:</p>
 
 Establishing the canonical Postgres community index of extensions, something
PGXN has yet to achieve 
 Improving metadata standards to enable new patterns, such as automated binary
packaging 
 Working with the Postgres community to establish documentation standards
that encourage developers to provide comprehensive extension docs 
 Designing and building developer tools that empower more developers to
build, test, distribute, and maintain extensions 
 
<p>Over the the past decade I’ve have many ideas and discussion on these topics,
but seldom had the bandwidth to work on them. In the last couple years I’ve
<a href="https://blog.pgxn.org/post/651216661677064192/a-few-belated-pgxn-updates" title="PGXN Blog: “A Few Belated PGXN Updates”">enabled TLS and improved the site display</a>, <a href="https://blog.pgxn.org/post/655912318549606400/password-storage-update" title="PGXN Blog: “Password Storage Update”">increased password security</a>, and
<a href="https://blog.pgxn.org/post/709635160523620352/hello-mastodon" title="PGXN Blog: “Hello Mastodon 🐘”">added a notification queue</a> with hooks that post to both Twitter (RIP <a href="https://twitter.com/pgxn/" title="PGXN on Twitter">@pgxn</a>)
and Mastodon (<a href="https://botsin.space/@pgxn" title="PGXN on Mastodon">@pgxn@botsin.space</a>). Otherwise, aside from keeping the site
going, periodically improving new accounts, and eyeing the latest releases, I’ve
had little bandwidth for PGXN or the broader extension ecosystem.</p>
<p>Now, thanks to the vision and strategy of Samay and Tembo CEO <a href="https://www.linkedin.com/in/rywalker" title="Ry Walker - Tembo, Inc. |
    LinkedIn">Ry Walker</a>, I
will focus on these projects full time. The Tembo team have already helped me
enumerate the extension ecosystem <a href="https://strategyn.com/jobs-to-be-done/jobs-to-be-done-theory/" title="Fundamentals of Jobs-to-be-Done Theory">jobs to be done</a> and the tools required to do
them. This week I’ll submit it to collaborators from across the Postgres
community<a>1</a> to fill in the missing parts, make adjustments and
improvements, and work up a project plan.</p>
<p>The work also entails determining the degree to which PGXN and other extension
registries (e.g., <a href="https://database.dev" title="The Database Package Manager">dbdev</a>, <a href="https://pgt.dev" title="Trunk — A Postgres Extension Registry">trunk</a>, <a href="https://pgxman.com/" title="npm for PostgreSQL">pgxman</a>, <a href="http://www.postgres.pm">pgpm</a> (WIP), etc.) will play a
role or provide inspiration, what bits should be adopted, rewritten, or
discarded.<a>2</a> Our goal is to build the foundations for a community-owned
extensions ecosystem that people care about and will happily adopt and
contribute to.</p>
<p>I’m thrilled to return to this problem space, re-up my participation in the
PostgreSQL community, and work with great people to build out the extensions
ecosystem for future.</p>
<p>Want to help out or just follow along? Join the <a href="https://postgresteam.slack.com/archives/C056ZA93H1A" title="Postgres Slack/#extensions: Extensions and extension-related accessories">#extensions</a> channel on the
<a href="https://pgtreats.info/slack-invite" title="Join the Postgres Slack">Postgres Slack</a>. See you there.</p>
 
 
 
 
<p>Tembo was not the only company whose representatives have reached
out in the past year to talk about PGXN and improving extensions. I’ve also
had conversations with <a href="https://supabase.com" title="Supabase | The Open Source Firebase Alternative">Supabase</a>, <a href="https://omnigres.com" title="Omnigres: Postgres as a Platform">Omnigres</a>, <a href="https://www.hydra.so" title="Hydra - Fast Postgres Analytics ++">Hydra</a>, and others.&nbsp;<a>↩︎</a></p>
 
 
<p>Never be afraid to <a href="https://www.masterclass.com/articles/what-does-it-mean-to-kill-your-darlings" title="MasterClass: “What Does It Mean to Kill Your Darlings?”">kill your darlings</a>.&nbsp;<a>↩︎</a></p>
 
 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/personal/">Personal</a> 
                 <a href="https://justatheory.com/tags/work/">Work</a> 
                 <a href="https://justatheory.com/tags/tembo/">Tembo</a> 
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/2024/01/tembonaut/tembo.svg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:00 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 21. January 2024
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2481'>Celebrating Ten Years of OpenID Connect at the OpenID Summit Tokyo 2024</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  We held the first of three planned tenth anniversary celebrations for the completion of OpenID Connect at the OpenID Summit Tokyo 2024. The four panelists were Nov Matake, Ryo Ito, Nat Sakimura, and myself. We shared our perspectives on what led to OpenID Connect, why it succeeded, and what lessons we learned along the way. […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>We held the first of <a href="https://self-issued.info/?p=2468">three planned tenth anniversary celebrations</a> for the completion of <a href="https://openid.net/connect/">OpenID Connect</a> at the <a href="https://www.openid.or.jp/summit/2024/en/">OpenID Summit Tokyo 2024</a>.  The four panelists were <a href="https://twitter.com/nov">Nov Matake</a>, <a href="https://twitter.com/ritou">Ryo Ito</a>, <a href="https://twitter.com/_nat">Nat Sakimura</a>, and myself.  We shared our perspectives on what led to OpenID Connect, why it succeeded, and what lessons we learned along the way.</p>
<p>The most common refrain throughout our descriptions was the design philosophy to “Keep simple things simple”.  I believe that three of the four of us cited it.</p>
<p>I recounted that we even had a thought experiment used to make the “Keep simple things simple” principle actionable in real time: the “Nov Matake Test”.  As we considered new features, we’d ask ourselves “Would Nov want to add it to his implementation?”  And “Is it simple enough that he could build it in a few hours?”</p>
<p>The other common thread was the criticality of interop testing and certification.  We held five rounds of interop testing before finishing the specifications, with the specs being refined after each round based on the feedback received.  The early developer feedback was priceless – much of it from Japan!</p>
<p>Our OpenID Connect 10th anniversary presentations were:</p>
 
 <a href="https://self-issued.info/presentations/Mike-Celebrating_Ten_Years_of%20OpenID_Connect_19-Jan-24.pdf">Remarks by Mike Jones</a> 
 <a href="https://self-issued.info/presentations/Nov-openid_summit_2024_panel.pdf">Remarks by Nov Matake</a> 
 <a href="https://self-issued.info/presentations/Ryo-openid_summit_2024_panel_ritou.pdf">Remarks by Ryo Ito</a> 
 <a href="https://self-issued.info/presentations/Nat-25_Years_of_OpenID.pdf">Remarks by Nat Sakimura</a> 
 
<p>Thanks to the <a href="https://www.openid.or.jp/">OpenID Foundation Japan</a> for the thought-provoking and enjoyable <a href="https://www.openid.or.jp/summit/2024/en/">OpenID Summit Tokyo 2024</a>!</p>
<p></p>
<p></p>
<p></p>
<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/openid-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   07:59 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->

<!-- each item -->



</body>
</html>
