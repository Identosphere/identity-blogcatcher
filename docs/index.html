
<!DOCTYPE html>
<html>
<head>
  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 2.7.0 (2019-12-25) [x86_64-linux]'>

  <title>Identosphere Blogcatcher | Planet Identity Reboot</title>

  <link href="https://fonts.googleapis.com/css2?family=Libre+Baskerville&family=Volkhov&display=swap" rel="stylesheet">   
  <link rel='stylesheet' type='text/css' href='css/font-awesome.css'>
  <link rel='stylesheet' type='text/css' href='css/planetid.css'>
  
  <!-- fix: use script_tag  rails-style ??? -->
  <script defer src='js/jquery-2.0.3.min.js'></script>
  <script defer src='js/planetid.js'></script>

  
  
    <!-- Favicon-->
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/icon/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/icon/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/icon/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/icon/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/icon/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/icon/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/icon/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/icon/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/icon/apple-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icon/android-icon-192x192.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/icon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="/assets/icon/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/icon/favicon-16x16.png">
    <link rel="manifest" href="/manifest.json">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="/assets/icon/ms-icon-144x144.png">
    <meta name="theme-color" content="#ffffff">

  <script defer data-goatcounter="https://identosphere.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>
</head>
<body>
<div class='banner'>
<table cellspacing='0' cellpadding='0' width='100%'>
  <tr>
    <td>
    <!-- left/1st column -->
    <a href="https://identosphere.net"><b>Blogs</b></a> | <a href="/organizations/">Organizations</a> | <a href="/companies/">Companies</a> | <a href="/web3/">Web 3</a> | <a href="/media/">Media</a> • <a href="https://github.com/identosphere/planetid-reboot/">Github</a>
    </td>
    <td style='text-align:right;'>
    <!-- right/2nd column -->
    Style | <a href='/'><b>Full</b></a> &bull; <a href='overview.html'>Overview</a>
    </td>
  </tr>
</table>
</div>
<div id='navwrap'>
<div id='nav'>

    <div style='width: 100%; text-align: right;'>
    <img src='i/view-headlines.png' id='show-headlines' title='Show Headlines Only' width='24' height='24'>
    <img src='i/view-snippets.png' id='show-snippets'    title='Show Snippets' width='24' height='24'>
    <img src='i/view-standard.png' id='show-fulltext'    title='Show Full Text' width='24' height='24'>
    </div>  

    <h4>Subscriptions</h4>
    <ul class='subscriptions small' >
     
      <li>
        <a href='https://aaronparecki.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://aaronparecki.com'>Aaron Parecki</a>
      </li>
     
      <li>
        <a href='http://eternallyoptimistic.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://eternallyoptimistic.com'>Adventures of an Eternal Optimist</a>
      </li>
     
      <li>
        <a href='http://adistributedeconomy.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://adistributedeconomy.blogspot.com'>A Distributed Economy</a>
      </li>
     
      <li>
        <a href='https://altmode.org/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://altmode.org'>Altmode</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@blockadvocacy'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://blockadvocacy.medium.com/'>Ally Medina - Blockchain Advocacy</a>
      </li>
     
      <li>
        <a href='https://blog.aniljohn.com/feed/blog.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://blog.aniljohn.com'>Anil John</a>
      </li>
     
      <li>
        <a href='https://www.cyberforge.com/feed/cyberforge.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.cyberforge.com/'>Cyberforge (Anil John)</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@arjgovind'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@arjgovind'>Arjun Govind</a>
      </li>
     
      <li>
        <a href='http://asadistechspace.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://asadistechspace.blogspot.com'>My.Tech.Space</a>
      </li>
     
      <li>
        <a href='https://realestatecafe.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://realestatecafe.com/'>Bill Wendels Real Estate Cafe</a>
      </li>
     
      <li>
        <a href='https://bradtumy.wordpress.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://tumy-tech.com/posts/'>Brad Tumy</a>
      </li>
     
      <li>
        <a href='https://cybersocialstructure.org/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://cybersocialstructure.org/'>Virtual Democracy</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@cdonley'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@cdonley'>Clayton Donleys Blog</a>
      </li>
     
      <li>
        <a href='http://www.cloudidentity.com/blog/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.cloudidentity.com/blog'>Vittorio Bertocci - CloudIdentity</a>
      </li>
     
      <li>
        <a href='https://darius.dunlaps.net/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://darius.dunlaps.net'>Darius</a>
      </li>
     
      <li>
        <a href='https://damienbod.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://damienbod.com/'>Damien Bod</a>
      </li>
     
      <li>
        <a href='http://www.backalleycoder.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.backalleycoder.com/'>DANIEL BUCHNER</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@daniel-hardman'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://daniel-hardman.medium.com/'>Daniel Hardman</a>
      </li>
     
      <li>
        <a href='http://blog.deanland.com/blog/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://blog.deanland.com'>blog.deanland.com</a>
      </li>
     
      <li>
        <a href='http://feeds.soundcloud.com/users/soundcloud:users:66572473/sounds.rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://soundcloud.com/tim-bouma-1'>Definitely Identity</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@DickHardt'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@DickHardt'>Dick Hardt</a>
      </li>
     
      <li>
        <a href='http://www.discoveringidentity.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.discoveringidentity.com'>Discovering Identity</a>
      </li>
     
      <li>
        <a href='https://www.thedinglegroup.com/blog?format=rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.thedinglegroup.com/blog'>The Dingle Group</a>
      </li>
     
      <li>
        <a href='https://www.thedinglegroup.com/a-growth-journey?format=rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.thedinglegroup.com/a-growth-journey'>The Dingle Group - A Growth Journey</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@dkelts.id'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@dkelts.id'>David Kelts on ID</a>
      </li>
     
      <li>
        <a href='http://epeus.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://epeus.blogspot.com/'>"Epeus' epigone"</a>
      </li>
     
      <li>
        <a href='https://equalsdrummond.name/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://equalsdrummond.name'>Equals Drummond</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@expede'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@expede'>Brooklyn Zelenka - Programs with functions</a>
      </li>
     
      <li>
        <a href='https://medium.facilelogin.com/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.facilelogin.com'>FACILELOGIN</a>
      </li>
     
      <li>
        <a href='https://fossandcrafts.org/rss-feed.rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://fossandcrafts.org/'>Foss & Crafts</a>
      </li>
     
      <li>
        <a href='https://beuchelt.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://beuchelt.com/blog/'>Gerald Beuchelt</a>
      </li>
     
      <li>
        <a href='http://360tek.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://360tek.blogspot.com/'>Matt Flynn: InfoSec | IAM</a>
      </li>
     
      <li>
        <a href='https://decentralgabe.xyz/rss/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://decentralgabe.xyz/'>Decentral Gabe</a>
      </li>
     
      <li>
        <a href='https://ggreve.medium.com/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://ggreve.medium.com/'>Georg C. F. Greve</a>
      </li>
     
      <li>
        <a href='http://meilinfung.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://meilinfung.blogspot.com'>Global Connections - Eclectic Selections</a>
      </li>
     
      <li>
        <a href='http://habitatchronicles.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://habitatchronicles.com'>Habitat Chronicles</a>
      </li>
     
      <li>
        <a href='https://hanszandbelt.wordpress.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://hanszandbelt.wordpress.com'>Hans Zandbelt</a>
      </li>
     
      <li>
        <a href='https://www.kyledenhartog.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://kyledenhartog.com/posts/'>Kyle Den Hartog</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@heathervescent'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.heathervescent.com/'>Heather Vescent</a>
      </li>
     
      <li>
        <a href='https://hendersoni.substack.com/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://hendersoni.substack.com/'>Supply...., meet Demand</a>
      </li>
     
      <li>
        <a href='https://www.herbie.dev/blog/index.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.herbie.dev'>Herbies Blog</a>
      </li>
     
      <li>
        <a href='https://hyperonomy.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://hyperonomy.com/'>Hyperonomy Digital Identity Lab</a>
      </li>
     
      <li>
        <a href='http://identity-centric-architecture.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://identity-centric-architecture.blogspot.com/'>"Identity Driven Enterprise (Security) Architecture (IDEAs!!)"</a>
      </li>
     
      <li>
        <a href='http://identityhappens.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://identityhappens.blogspot.com/'>Identity Happens</a>
      </li>
     
      <li>
        <a href='http://identitymeme.org/feed/atom/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://identitymeme.org'>IdentityMeme.org</a>
      </li>
     
      <li>
        <a href='http://identitynetworks.wordpress.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://identitynetworks.wordpress.com'>Identity Networks</a>
      </li>
     
      <li>
        <a href='https://identitywoman.net/feed/atom/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://identitywoman.net'>Identity Woman</a>
      </li>
     
      <li>
        <a href='https://idimmusings.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://idimmusings.com'>IDIM Musings</a>
      </li>
     
      <li>
        <a href='http://idm-thoughtplace.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://idm-thoughtplace.blogspot.com/'>IdM Thoughtplace</a>
      </li>
     
      <li>
        <a href='http://idmdude.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://idmdude.com'>Easy Identity</a>
      </li>
     
      <li>
        <a href='https://idmlab.eidentity.jp/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://idmlab.eidentity.jp/'>IdM Laboratory</a>
      </li>
     
      <li>
        <a href='http://feeds.feedburner.com/Ignisvulpis'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://ignisvulpis.blogspot.com/'>ignisvulpis</a>
      </li>
     
      <li>
        <a href='http://blog.independentid.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.independentid.com'>Independent Identity</a>
      </li>
     
      <li>
        <a href='https://infominer.xyz/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://infominer.xyz/'>infominer</a>
      </li>
     
      <li>
        <a href='https://www.informationanswers.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.informationanswers.com'>Information Answers</a>
      </li>
     
      <li>
        <a href='http://blog.is4u.be/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://blog.is4u.be/'>IS4U blog</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@isurakarunaratne'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@isurakarunaratne'>Isura Karunaratne</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@theblockstalk'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://theblockstalk.medium.com/'>Jack the Blockstalk</a>
      </li>
     
      <li>
        <a href='https://blog.joeandrieu.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://blog.joeandrieu.com'>Joe Andrieu</a>
      </li>
     
      <li>
        <a href='https://reb00ted.org/index.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://reb00ted.org/'>reb00ted</a>
      </li>
     
      <li>
        <a href='https://joipodgorny.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://joipodgorny.com'>Joi Podgorny</a>
      </li>
     
      <li>
        <a href='https://blog.jonudell.net/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://blog.jonudell.net'>Jon Udell</a>
      </li>
     
      <li>
        <a href='http://www.caballerojuan.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.caballerojuan.com/'>Juan Caballero</a>
      </li>
     
      <li>
        <a href='https://justatheory.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://justatheory.com'>Just a Theory</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@justinsecurity'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@justinsecurity'>Justin Richer</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@kago'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@kago'>Eddie Kago</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@karylfowler'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@karylfowler'>Karyl Fowler</a>
      </li>
     
      <li>
        <a href='https://katrynadow.me/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://katrynadow.me'>Katryna Dow</a>
      </li>
     
      <li>
        <a href='https://kentbull.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://kentbull.com/'>Kent Bull</a>
      </li>
     
      <li>
        <a href='https://kayaelle.medium.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://kayaelle.medium.com/'>Kerri Lemole</a>
      </li>
     
      <li>
        <a href='https://letshaveaplan.blog/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.letshaveaplan.blog'>Lets Have a Plan</a>
      </li>
     
      <li>
        <a href='http://feeds.feedburner.com/LifeWithAlacrity'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.lifewithalacrity.com/'>Life With Alacrity</a>
      </li>
     
      <li>
        <a href='https://ludopoitou.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://ludopoitou.com'>Ludo Sketches</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@macieklaskus'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@macieklaskus'>Maciek Laskus</a>
      </li>
     
      <li>
        <a href='https://madurangasblogs.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://madurangasblogs.blogspot.com'>Madurangas Blogs</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@margo.e.johnson'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@margo.e.johnson'>Margo Johnson</a>
      </li>
     
      <li>
        <a href='http://matthewschutte.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://matthewschutte.com'>Matthew Schutte</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@m-ruminer'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@m-ruminer/'>Michael Ruminer</a>
      </li>
     
      <li>
        <a href='https://www.moxytongue.com/feeds/posts/default?alt=rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.moxytongue.com/'>Moxy Tongue</a>
      </li>
     
      <li>
        <a href='http://www.sakimura.org/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.sakimura.org'>@_Nat Zone</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@CreatorNader'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@CreatorNader'>Nader Helmy</a>
      </li>
     
      <li>
        <a href='https://nosuchcommonsense.wordpress.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://nosuchcommonsense.wordpress.com'>nosuchcommonsense</a>
      </li>
     
      <li>
        <a href='http://notabob.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://notabob.blogspot.com/'>"Ceci n'est pas un Bob"</a>
      </li>
     
      <li>
        <a href='http://www.notsodistantfuture.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.notsodistantfuture.com/'>Not So Distant Future</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@OR13'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@OR13'>Orie Steele</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@paul.knowles_52509'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@paul.knowles_52509'>Paul Knowles</a>
      </li>
     
      <li>
        <a href='https://www.patrick-breyer.de/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@markus.sabadello'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@markus.sabadello'>Markus Sabadello on Medium</a>
      </li>
     
      <li>
        <a href='https://john.philpin.com/feed.json'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
      </li>
     
      <li>
        <a href='http://feeds.feedburner.com/PhillipGibb'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://phillipgibb.com/'>The Life, Adventure and Lessons of Phillip Gibb</a>
      </li>
     
      <li>
        <a href='https://newsletter.pragmaticengineer.com/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@pulasthi7'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@pulasthi7'>Pulasthi Mahawithana</a>
      </li>
     
      <li>
        <a href='https://www.rdegges.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.rdegges.com'>Randall Degges</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@rebeccarachmany'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@rebeccarachmany'>Rebecca Rachmany</a>
      </li>
     
      <li>
        <a href='https://respected-professor.blogspot.com/feeds/posts/default?alt=rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://respected-professor.blogspot.com/'>Respected Professor</a>
      </li>
     
      <li>
        <a href='http://riskhorizon.wordpress.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://riskhorizon.wordpress.com'>Risk Horizon Blog</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@rileyparkerhughes'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://rileyparkerhughes.medium.com'>Riley Hughes</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@Obstropolos'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://rocco.network'>Rocco, Gregory</a>
      </li>
     
      <li>
        <a href='https://sammantics.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://sammantics.com'>SAMMANTICS</a>
      </li>
     
      <li>
        <a href='http://samuelsmith.org/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://samuelsmith.org'>Meaningful Intelligence</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/selfrule'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/selfrule'>Sam Smith - Medium</a>
      </li>
     
      <li>
        <a href='http://openingmove.blogspot.com/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://openingmove.blogspot.com'>Scott Maces IT Journal</a>
      </li>
     
      <li>
        <a href='http://www.seanbohan.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.seanbohan.com'>SeanBohan.com</a>
      </li>
     
      <li>
        <a href='https://blogs.harvard.edu/doc/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
      </li>
     
      <li>
        <a href='http://self-issued.info/?feed=atom'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://self-issued.info'>Mike Jones: self-issued</a>
      </li>
     
      <li>
        <a href='https://sepia.substack.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://sepia.substack.com/'>Sepia - Zach Brown</a>
      </li>
     
      <li>
        <a href='https://shanegreen.org/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://shanegreen.org'>Shane Green</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@shingaithornton'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@shingaithornton'>Shingai Thornton</a>
      </li>
     
      <li>
        <a href='https://simonwillison.net/atom/everything/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://simonwillison.net'>Simon Willison</a>
      </li>
     
      <li>
        <a href='https://socialmediaclarity.tumblr.com/rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://socialmediaclarity.tumblr.com'>Social Media Clarity</a>
      </li>
     
      <li>
        <a href='http://feeds.feedburner.com/superpat'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://blog.superpat.com'>Superpatterns</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@ssi-ambassador'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@ssi-ambassador'>SSI Ambassador</a>
      </li>
     
      <li>
        <a href='https://blog.talkingidentity.com/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://blog.talkingidentity.com'>Talking Identity</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@tarikk'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@tarikk'>Tarik Kurspahic</a>
      </li>
     
      <li>
        <a href='http://www.thread-safe.com/feeds/posts/default?alt=rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.thread-safe.com/'>Threadsafe</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@rufftimo'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@rufftimo'>Timothy Ruff</a>
      </li>
     
      <li>
        <a href='https://herestomwiththeweather.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://herestomwiththeweather.com'>Heres Tom with the Weather</a>
      </li>
     
      <li>
        <a href='https://www.mydigitalfootprint.com/feeds/posts/default?alt=rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.mydigitalfootprint.com/'>MyDigitalFootprint</a>
      </li>
     
      <li>
        <a href='http://blog.transparenthealth.org/feeds/posts/default'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://blog.transparenthealth.org'>Transparent Health Blog</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@trbouma'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@trbouma'>Tim Boumas Blog</a>
      </li>
     
      <li>
        <a href='http://www.tuesdaynight.org/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.tuesdaynight.org'>ian glazers tuesdaynight</a>
      </li>
     
      <li>
        <a href='http://www.unclesamsblog.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://www.unclesamsblog.com'>Uncle Sams Blog</a>
      </li>
     
      <li>
        <a href='http://virtualsoul.org/blog/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://virtualsoul.org'>dale olds - virtualsoul</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@vishal144'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@vishal144'>Vishal Gupta</a>
      </li>
     
      <li>
        <a href='https://www.voidstar.com/module.php?mod=blog&op=feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.voidstar.com/module.php?mod=blog'>Voidstar: blog</a>
      </li>
     
      <li>
        <a href='https://werd.io/feed'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://werd.io'>Ben Werdmüller</a>
      </li>
     
      <li>
        <a href='https://medium.com/feed/@benwerd'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
      </li>
     
      <li>
        <a href='https://www.technometria.com/feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
      </li>
     
      <li>
        <a href='https://wip-abramson.dev/rss.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://wip-abramson.dev/'>Wip Abramson</a>
      </li>
     
      <li>
        <a href='https://www.webistemology.com/rss'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
      </li>
     
      <li>
        <a href='https://wrenchinthegears.com/feed/'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
      </li>
     
      <li>
        <a href='http://xditao.blogspot.com/atom.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='http://xditao.blogspot.com/'>The Tao of XDI</a>
      </li>
     
      <li>
        <a href='https://www.xtseminars.co.uk/blog-feed.xml'><img src='i/feed-icon-10x10.png'></a>
        <a href='https://www.xtseminars.co.uk/'>XTSeminars</a>
      </li>
     
    </ul>
<center>
    <p><small><strong>Inspired by</strong> identity management feed aggregator <em><a href="https://web.archive.org/web/20161029051802/http://planetidentity.org/">planetidentity.org</a></em>.</small></p>
    <h4>Add Your Feed</h4>
    <p><small>Send Blog \ Feed to:  <code>blogcatcher (at) identosphere (dot) net</code>.</small></p>
</center>

<h4>Built with</h4>
<ul class='planetarium small'>
  <li><a href='https://github.com/feedreader'>Pluto Feedreader</a></li>
  <li><a href="https://github.com/web-work-tools/awesome-planet-pluto#github-actions">GitHub Actions</a></li>
</ul>

</div>
</div>
<sup>Last Update 7:01 AM July 28, 2025 (UTC)</sup>
<iframe src="https://identosphere.substack.com/embed" width="400" height="267" style="border:1px solid #EEE; background:#f0f0f0;" frameborder="0" align="right" scrolling="no"></iframe>

<h1>Identity Blog Catcher</h1>

<p>Brought to you by <a href="https://identitywoman.net">Identity Woman</a> and <a href="https://github.com/infominer33">Infominer</a>.<br><a href="https://www.patreon.com/user?u=35769676">Support this collaboration on Patreon</a>!!!</p>




 <h2 class='new-date'>
  Sunday, 27. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/27/javascript-runtimes-of-the-last-decade/#atom-everything'>The many, many, many JavaScript runtimes of the last decade</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   The many, many, many JavaScript runtimes of the last decade 
Extraordinary piece of writing by Jamie Birch who spent over a year putting together this comprehensive reference to JavaScript runtimes. It covers everything from Node.js, Deno, Electron, AWS Lambda, Cloudflare Workers and Bun all the way to much smaller projects idea like dukluv and txiki.js.

     Via Hacker News 


     Tags: javascr
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://buttondown.com/whatever_jamie/archive/the-many-many-many-javascript-runtimes-of-the-last-decade/">The many, many, many JavaScript runtimes of the last decade</a></p>
Extraordinary piece of writing by Jamie Birch who spent over a year putting together this comprehensive reference to JavaScript runtimes. It covers everything from Node.js, Deno, Electron, AWS Lambda, Cloudflare Workers and Bun all the way to much smaller projects idea like dukluv and txiki.js.

    <p>Via <a href="https://news.ycombinator.com/item?id=44701574">Hacker News</a></p>


    <p>Tags: <a href="https://simonwillison.net/tags/javascript">javascript</a>, <a href="https://simonwillison.net/tags/nodejs">nodejs</a>, <a href="https://simonwillison.net/tags/deno">deno</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   23:56 &bull;
   7 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/27/til-exception-add-note/#atom-everything'>TIL: Exception.add_note</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   TIL: Exception.add_note 
Neat tip from Danny Roy Greenfeld: Python 3.11 added a .add_note(message: str) method to the BaseException class, which means you can add one or more extra notes to any Python exception and they'll be displayed in the stacktrace! 
 Here's PEP 678 – Enriching Exceptions with Notes by Zac Hatfield-Dodds proposing the new feature back in 2021.

      Via Lobste.rs 


     Tag
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://daniel.feldroy.com/posts/til-2025-05-exception-add_note">TIL: Exception.add_note</a></p>
Neat tip from Danny Roy Greenfeld: Python 3.11 added a <code>.add_note(message: str)</code> method to the <code>BaseException</code> class, which means you can add one or more extra notes to any Python exception and they'll be displayed in the stacktrace!<p></p>
<p>Here's <a href="https://peps.python.org/pep-0678/">PEP 678 – Enriching Exceptions with Notes</a> by Zac Hatfield-Dodds proposing the new feature back in 2021.

    </p><p>Via <a href="https://lobste.rs/s/jqm47i/til_exception_add_note">Lobste.rs</a></p>


    <p>Tags: <a href="https://simonwillison.net/tags/debugging">debugging</a>, <a href="https://simonwillison.net/tags/python">python</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   23:07 &bull;
   8 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/28/way-up-in-a-tree.html'> Way up in a tree he was … 
</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Way up in a tree he was … 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/04b899985f.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Way up in a tree he was …</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/04b899985f.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:51 &bull;
   8 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/27/enough-ai-copilots-we-need-ai-huds/#atom-everything'>Enough AI copilots! We need AI HUDs</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Enough AI copilots! We need AI HUDs 
Geoffrey Litt compares Copilots - AI assistants that you engage in dialog with and work with you to complete a task - with HUDs, Head-Up Displays, which enhance your working environment in less intrusive ways. 
 He uses spellcheck as an obvious example, providing underlines for incorrectly spelt words, and then suggests his AI-implemented custom debugging UI as
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://www.geoffreylitt.com/2025/07/27/enough-ai-copilots-we-need-ai-huds">Enough AI copilots! We need AI HUDs</a></p>
Geoffrey Litt compares Copilots - AI assistants that you engage in dialog with and work with you to complete a task - with HUDs, Head-Up Displays, which enhance your working environment in less intrusive ways.<p></p>
<p>He uses spellcheck as an obvious example, providing underlines for incorrectly spelt words, and then suggests his <a href="https://www.geoffreylitt.com/2024/12/22/making-programming-more-fun-with-an-ai-generated-debugger">AI-implemented custom debugging UI</a> as a more ambitious implementation of that pattern.</p>
<p>Plenty of people have expressed interest in LLM-backed interfaces that go beyond chat or editor autocomplete. I think HUDs offer a really interesting way to frame one approach to that design challenge.


    </p><p>Tags: <a href="https://simonwillison.net/tags/design">design</a>, <a href="https://simonwillison.net/tags/design-patterns">design-patterns</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/geoffrey-litt">geoffrey-litt</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   22:15 &bull;
   9 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.webistemology.com/ai-platform-choices/'>AI Platform Choices</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The old world is dying, and the new world struggles to be born: now is the time of monsters.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Background 
<p>Unless you are living under a rock and not paying any attention to the news (and who could blame you), you may be thinking that AI is the next industrial revolution. This 'inevitable' change will reshape our world, cure diseases, and unlock <a href="https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-next-innovation-revolution-powered-by-ai?ref=webistemology.com">unprecedented productivity</a>. Economists are asking if this is an <a href="https://thenextrecession.wordpress.com/2025/07/27/ai-bubbling-up/?ref=webistemology.com">AI bubble</a>. AI experts are <a href="https://safe.ai/work/statement-on-ai-risk?ref=webistemology.com">warning</a> us that AI presents a risk of human extinction at the same level as nuclear war or climate change. Whether you consider the <a href="https://www.webistemology.com/our-digital-moai/">costs</a> of AI, the amount of AI <a href="https://www.webistemology.com/ai-slop-and-sturgeons-law/">slop</a>, or AI <a href="https://www.webistemology.com/from-slop-to-sabotage/">sabotage</a>, any decision by a business or government to procure and use AI systems requires close examination. I wrote this post to work through if and how to proceed with AI, without being sucked into the AI hype vortex.</p>
<blockquote>
<p>The old world is dying, and the new world struggles to be born: now is the time of monsters.</p>
</blockquote>
<p>Companies whose products incorporate AI have spent ‘bet the business’ amounts of money building out their data centre infrastructure and training models. I asked Copilot about this, and it generated this table.</p>



Year
Company
Total AI Investment (Est.)
Data Center Investment
Model Development Highlights




2021
Google
~$30B
~$20B
Early Gemini model R&amp;D



Amazon
~$25B
~$18B
AWS AI services expansion



Microsoft
~$28B
~$19B
Azure AI, OpenAI partnership


2022
Google
~$35B
~$22B
Gemini 1 development



Amazon
~$30B
~$20B
Bedrock platform launch



Microsoft
~$32B
~$21B
GPT-4 integration in Azure


2023
Google
~$45B
~$30B
Gemini 1.5, TPU v5 launch



Amazon
~$40B
~$28B
Titan models, Trainium chips



Microsoft
~$50B
~$35B
Copilot rollout, Stargate planning


2024
Google
~$52B
~$35B
Gemini 1.5 Pro scaling



Amazon
~$83B
~$60B
AWS generative AI expansion



Microsoft
~$65B
~$45B
GPT-4 Turbo, Azure AI Studio


2025
Google
~$75B
~$50B
Gemini 2, TPU v6 deployment



Amazon
&gt;$100B
~$70B
AWS AI superclusters



Microsoft
~$80B
~$55B
Stargate supercomputer, Copilot+


Total

~$870B+
~$613B+
—



<p>Copilot Notes for this table<br>
These figures are estimates based on public reports and capital expenditure disclosures.<br>
Data centre investments include infrastructure, GPUs (mostly Nvidia), and cloud scaling.<br>
Model development includes R&amp;D, training costs, and partnerships (e.g., OpenAI for Microsoft).</p>
<p>These companies are naturally looking for a return on that investment. This could involve introducing new solutions that generate additional revenue streams or enhancing existing solutions with "AI" capabilities, and increasing the price. But earlier this year, The Register <a href="https://www.theregister.com/2025/01/06/altman_gpt_profits/?ref=webistemology.com">reported</a> that ChatGPT Pro is “struggling to turn a profit”. Microsoft is charging $30 per user per month for Copilot, which should help offset the nearly $12 billion that Microsoft has invested in ChatGPT, as well as its internal investments in AI. In either event, these costs are being passed on to the customer.</p>
 Evaluating AI 
<p>Marketing of AI presupposes the inevitability of AI. To question this narrative is to risk being mislabelled a Luddite.<a>[1]</a> But if the current generation of AI ‘solutions’ is a technology bubble inflated by billions in investments on expectations of astronomical returns, what is a prudent approach for moving forward on AI initiatives? It probably involves a combination of the following elements.</p>
 Data Due Diligence 
<p>This involves assessing the quality, integrity, and compliance of data used in AI systems, ensuring it meets regulatory standards and is suitable for analysis. This process identifies risks and opportunities related to data management and AI implementation before piloting an AI project. Areas to assess include:<br>
- Data Compliance: Assess adherence to regulations like PIPEDA, CCPA, or GDPR to protect sensitive information.<br>
- Data Quality: Evaluate the accuracy, completeness, and reliability of data used in AI models.<br>
- Data Security: Review cybersecurity measures to safeguard data against breaches and unauthorised access.<br>
- Data Sourcing: Investigate how data is collected, ensuring it is ethical and sustainable for AI applications.<br>
- Intellectual Property: Examine ownership and licensing of data and AI technologies to avoid legal complications.</p>
 Pilot Project 
<p>Before adopting enterprise-wide AI solutions, you should conduct one or more pilot projects. Consider incorporating these elements in your pilots:</p>
 
 Setting Clear Objectives: Define specific goals for the pilot project, such as improving efficiency, reducing costs, or enhancing customer experience. 
 Stakeholder Engagement: Involve key stakeholders to ensure that the proposed project addresses identified objectives. Where personal data is involved in the model, consider including customer representatives and regulators as stakeholders. 
 Data Preparation: Do the pre-work necessary to prepare the data. This includes data selection, cleaning, and transformation. Alternatively, ensure that your vendor can demonstrate that this has been done. 
 Model Selection: Choose appropriate AI models and algorithms based on the objectives and data characteristics. Consider factors like complexity, interpretability, and scalability. 
 Infrastructure Setup: Establish the necessary technical infrastructure, including hardware, software, and cloud services, to support the selected AI model. 
 Test and Validate: Test to validate the model's performance against predefined metrics. This includes cross-validation and performance benchmarking. This is a gateway, and you may have to abandon the project. This is a win if it saves the cost and consequences of an AI solution that doesn’t address what it was supposed to address. 
 Monitor and Evaluate: Set up mechanisms to continuously monitor the model's performance and impact during the pilot phase. Evaluate results against the initial objectives. 
 Establish a Feedback Loop: Create a process for gathering feedback from users and stakeholders to refine the model and its implementation. 
 Risk Management: Conduct an AI Impact Assessment to identify potential risks associated with the AI implementation and develop mitigation strategies. 
 Document Everything: Maintain thorough documentation of the project, including methodologies, findings, and lessons learned for future reference. 
 
 Governance 
<p>Before the pilot project is complete, establish a governance framework to ensure a successful transition from pilot to full-scale operation. This framework helps manage risks, maintain compliance, and ensure that the AI system aligns with organisational goals. Key governance elements include:</p>









Governance Structure
Establish a clear governance structure that defines roles and responsibilities for AI oversight, including a dedicated AI governance team.


Policies and Standards
Develop comprehensive policies and standards for AI usage, including ethical guidelines, data privacy, and security protocols.


Compliance Framework
Ensure compliance with relevant regulations and industry standards, such as PIPEDA or the GDPR, to protect data and user rights.


Risk Management
Implement a risk management framework to identify, assess, and mitigate risks associated with AI deployment and operation.


Performance Monitoring
Set up continuous monitoring of AI system performance against key performance indicators (KPIs) to ensure it meets business objectives.


Change Management
Develop a change management process to handle updates, modifications, and improvements to the AI system as needed.


Stakeholder Engagement
Maintain ongoing communication with stakeholders to gather feedback, address concerns, and ensure alignment with business goals.


Training and Education
Provide training and resources to help employees understand AI systems, their implications, and how to work effectively with them.


Audit and Review
Conduct regular audits and reviews of the AI system to assess compliance, performance, and alignment with governance policies.


Ethical Oversight
Establish an ethical oversight committee to assess the ethical implications of AI decisions and ensure the responsible use of AI.



 Other AI Risks 
 Supply Chain 
<p>Even if you decide not to proceed with AI, AI is likely in your supply chain. This may expose you to risks that need to be identified and addressed, preferably in your procurement process. Third-party vendors of AI technologies will be subject to the risks mentioned above. That means you have to plan for them failing to deliver, going out of business, or sudden and dramatic pricing changes. You should obtain attestations of data quality and confirm the measures taken to prevent data bias and errors. Suppliers may not adhere to the regulations that apply to your organisation, leading to compliance issues. As with any procurement, you must take steps to ensure your vendor contracts make them responsible for the things you are accountable for.</p>
 Shadow AI 
<p>Although last in this list, Shadow AI may well be the first place where AI-related risks appear in your enterprise. Shadow IT refers to the use of unauthorised applications, devices, or services within an organisation without the knowledge or approval of the IT department. While it can foster innovation and flexibility, it can also introduce any of the risks identified above without any governance or oversight.</p>
<p>However you decide to proceed, or not, with AI in your organisation, the following will probably apply to whatever AI platform you choose: be as accurate as any new technology:</p>
 
 It will cost more and take longer than you expect. 
 You will find that your vendors have overpromised. 
 If you are lucky, you will find unexpected benefits. Otherwise, you are guaranteed to find unexpected consequences. 
 
 
 
 
 <p>The original Luddites were skilled technologists fighting the degradation of their work and the depredations of the mill owners, not dissimilar to what is happening with AI (Brian Merchant, Blood in the Machine)↩︎ <a>↩︎</a></p>
 
 
 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://www.webistemology.com/content/images/2025/07/Gemini_Generated_Image_q0tgupq0tgupq0tg.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:23 &bull;
   10 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://altmode.org'>Altmode</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://altmode.org/2025/07/27/adriatic-trip-day-13-corinth-canal/'>Adriatic Trip Day 13: Corinth Canal</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Sunday, June 22, 2025 The early event for the day was the ship’s passage through the Corinth Canal, a narrow sea-level canal providing a shortcut from having to sail around the Peloponnese peninsula. The canal is very narrow, and the Orion was just small enough to make it through. Our entrance to the canal was […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><br>Sunday, June 22, 2025</p>



<p>The early event for the day was the ship’s passage through the Corinth Canal, a narrow sea-level canal providing a shortcut from having to sail around the Peloponnese peninsula. The canal is very narrow, and the Orion was just small enough to make it through. Our entrance to the canal was delayed by a ship passing through in the opposite direction. Soon we were towed through the canal, an operation that required considerable precision and skill.</p>


 
<a href="https://altmode.org/wp-content/uploads/2025/07/img_4584.jpg"></a>Temple of Apollo 


<p>After passing through the canal, we made our way to Delphi, considered by the ancient Greeks to be the center of their world. After lunch, we visited the Temple of Apollo, where the Oracle of Delphi delivered prophecies. The Temple itself was constructed about 330 BCE, although two earlier temples on the site date to as early as the 7th century BCE. It was adjacent to a sizable theatre and by an athletic stadium. There was also a modern museum at the site, constructed to present and preserve many of the smaller and more fragile artifacts.</p>



 



<p>This article is part of a series about our recent trip to Italy, Greece, Albania, and Croatia. To see the introductory article in the series, click <a href="https://altmode.org/2025/07/15/adriatic-trip-day-1-to-venice/">here</a>.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://altmode.org/wp-content/uploads/2025/07/img_4584.jpg?w=300'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:00 &bull;
   15 hours ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 26. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/26/official-statement-from-tea/#atom-everything'>Official statement from Tea on their data leak</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Official statement from Tea on their data leak 
Tea is a dating safety app for women that lets them share notes about potential dates. The other day it was subject to a truly egregious data leak caused by a legacy unprotected Firebase cloud storage bucket: 
 
 A legacy data storage system was compromised, resulting in unauthorized access to a dataset from prior to February 2024. This dataset inclu
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://www.teaforwomen.com/cyberincident">Official statement from Tea on their data leak</a></p>
Tea is a dating safety app for women that lets them share notes about potential dates. The other day it was subject to a truly egregious data leak caused by a legacy unprotected Firebase cloud storage bucket:<p></p>
<blockquote>
<p>A legacy data storage system was compromised, resulting in unauthorized access to a dataset from prior to February 2024. This dataset includes approximately 72,000 images, including approximately 13,000 selfies and photo identification submitted by users during account verification and approximately 59,000 images publicly viewable in the app from posts, comments and direct messages.</p>
</blockquote>
<p>Storing and then failing to secure photos of driving licenses is an incredible breach of trust. Many of those photos included EXIF location information too, so there are maps of Tea users floating around the darker corners of the web now.</p>
<p>I've seen a bunch of commentary using this incident as an example of the dangers of vibe coding. I'm confident vibe coding was not to blame in this particular case, even while I <a href="https://simonwillison.net/2025/Mar/19/vibe-coding/#when-is-it-ok-to-vibe-code-">share the larger concern</a> of irresponsible vibe coding leading to more incidents of this nature.</p>
<p>The announcement from Tea makes it clear that the underlying issue relates to code written prior to February 2024, long before vibe coding was close to viable for building systems of this nature:</p>
<blockquote>
<p>During our early stages of development some legacy content was not migrated into our new fortified system. Hackers broke into our identifier link where data was stored before February 24, 2024. As we grew our community, we migrated to a more robust and secure solution which has rendered that any new users from February 2024 until now were not part of the cybersecurity incident.</p>
</blockquote>
<p>Also worth noting is that they stopped requesting photos of ID back in 2023:</p>
<blockquote>
<p>During our early stages of development, we required selfies and IDs as an added layer of safety to ensure that only women were signing up for the app. In 2023, we removed the ID requirement.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/privacy">privacy</a>, <a href="https://simonwillison.net/tags/security">security</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:20 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2025/07/26/in-fewest-words-yes/'>In fewest words, yes.</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Here is my answer to the question Does SiriusXM know what station you are listening to? The SiriusXM streaming app logs what you listen to, when, and how you interact with programs and channels across your devices (phone, pad, smart speaker, website through your browser, whatever). This data is used to personalize your “experience” (as […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

When I just opened the app, I got the screen on the left. Since I wasn’t listening this afternoon, it made no sense. The one in the middle appeared when I returned to the app. It just lists channels, starting at the bottom (from which they annoyingly moved “Fifties on 5” and “Sixties on 6” to other channels), and then shows me the right screen when I hit Library, which used to have the much more sensible label “Favorites.” I hate the whole mess, but that’s beside the point of this post, so read on.
<p><a href="https://www.quora.com/Does-SiriusXM-know-what-station-you-are-listening-to/answer/Doc-Searls">Here is my answe</a>r to the question <a href="https://www.quora.com/Does-SiriusXM-know-what-station-you-are-listening-to/">Does SiriusXM know what station you are listening to?</a></p>
<blockquote><p>The SiriusXM streaming app logs what you listen to, when, and how you interact with programs and channels across your devices (phone, pad, smart speaker, website through your browser, whatever).</p>
<p>This data is used to personalize your “experience” (as the marketers like to say), sync your profile across devices, and support marketing efforts (which these days are mostly surveillance-based) while maintaining “pseudonymous tracking.”</p>
<p>Older SiriusXM radios (before about 2020) had no return path for usage data to flow to the company, but almost all new cars have their own cellular data connections (over which you have no control) for reporting many kinds of driving and usage data, including what you do with your car’s infotainment system.</p>
<p>Your SiriusXM radio use is among the many forms of personal data being reported by your car to its maker and to other parties known and unknown. To explain this, the <a title="www.siriusxm.com" href="https://www.siriusxm.com/privacy-policy">SiriusXM privacy policy</a> provides, in the current business fashion, what Paul Simon (in “The Boxer”) calls “a pocket full of mumbles such are promises.”</p></blockquote>
<p>All that said, there isn’t much in my experience of SiriusXM to suggest that I am being understood much in any way by the system. There are many more of what used to be called “favorites” in the Library. But there is no obvious order to how and why they appear where they do on the list. I have other complaints, but none are worth going into. And I’ve already posted my biggest complaint in <a href="https://doc.searls.com/2025/05/10/how-to-make-customers-hate-you-2/">How to Make Customers Hate You</a>.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/07/2025_07_26_siriuisxm-screens-1-1024x679.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:06 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2025/07/26/new-life-for-live/'>New Life for LIVE</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Colbert’s cancellation looks political, but it’s not. The show was a ratings winner, but a money loser. And the ratings for all of late night, like all of live TV, have been in decline for decades, along with the question, “What’s on?” We live in the Age of Optionality now. Watch or listen to whatever […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://www.politico.com/news/2025/07/18/cbs-cancels-stephen-colbert-00462089">Colbert’s cancellation</a> looks political, but it’s not. The show was a ratings winner, but a money loser. And the ratings for all of late night, like all of live TV, have been in decline for decades, along with the question, <a href="https://doc.searls.com/2025/05/05/the-offing-of-whats-on/">“What’s on?”</a></p>
<p>We live in the Age of Optionality now. Watch or listen to whatever you want, whenever you want, on whatever you want.</p>
<p>Except for sports, news, and Saturday Night Live, live programming is disappearing from radio and TV. Meanwhile, radio and TV themselves are being sidelined by apps on phones, flat screens, smart speakers, and CarPlay/Android Auto.</p>
<p>Fact: The only thing that makes your TV a TV is the cable/antenna jack in the back. Otherwise, it’s a monitor with a computer optimized for clickbait and <a href="https://duckduckgo.com/?q=all+TVs+spy+on+you">spying on you</a>. The clickbait is the (often spying-based) “for you” shit, plus what the industry calls FAST (Free Ad-Supported Streaming Television) channels: old westerns, local TV from elsewhere, looping news from services you never heard of, hustlers selling junk, foreign language programs, a fireplace that doesn’t go out, plus other crap.</p>
<p>Broadcasting has devolved from Macy’s to Dollar General.</p>
<p>But live programming is still with us. It’s just not on TV or radio, just like food trucks aren’t in buildings. At this stage what we have are pop-up shows with very high harbinger ratings and uncertain persistence. Here are a few I just looked up:::</p>
 Newsletter Writers 
 
 Casey Newton (Platformer) 
 Matt Taibbi (Racket News) 
 Heather Cox Richardson (Letters from an American) 
 Anne Helen Petersen (Culture Study) 
 Emily Atkin (Heated) 
 Puck News team&nbsp;(e.g. Dylan Byers, Teddy Schleifer) 
 
 Influencers (Mostly on TikTok and Instagram) 
 
 Tinx 
 Chris Olsen 
 Bretman Rock 
 Tabitha Brown 
 
 Celebrities (on YouTube, Substack, TikTok, X Spaces, etc.) 
 
 Andrew Callaghan 
 Marc Maron 
 Hank Green 
 Elon Musk &amp; David Sacks 
 
 Writers 
 
 Tim Urban (Wait But Why) 
 Bari Weiss (The Free Press) 
 Douglas Rushkoff (Team Human) 
 
<p>Since I’m not on TikTok and barely on Instagram, I know none of the influencers I just listed with a bit of AI help. If I have time later, I’ll add links.</p>
<p>Meanwhile, the writing isn’t just on the wall for live old-school broadcasting. The wall is falling down, and new ones are being built all over the place by creative voices and faces themselves. Welcome to Now.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:00 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://altmode.org'>Altmode</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://altmode.org/2025/07/26/adriatic-trip-day-12-temple-of-poseidon-and-orion-embarkation/'>Adriatic Trip Day 12: Temple of Poseidon and Orion Embarkation</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Saturday, June 21, 2025 This morning, we checked out of our hotel and boarded a bus that would eventually take us to the National Geographic Orion, our ship for the rest of the trip. We had a bit of a drive to the ship, and things were timed so that we would arrive when the […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><br>Saturday, June 21, 2025</p>



<p>This morning, we checked out of our hotel and boarded a bus that would eventually take us to the National Geographic Orion, our ship for the rest of the trip. We had a bit of a drive to the ship, and things were timed so that we would arrive when the ship was ready for us (the previous guests disembarked this morning).</p>


 
<a href="https://altmode.org/wp-content/uploads/2025/07/img_4505.jpg"></a>Temple of Poseidon 


<p>Our bus ride took us to the south of Athens, following the coastline to the Temple of Poseidon. The temple was another classical Greek structure, smaller but considerably less crowded than the Acropolis. Poseidon was, of course, the Greek god of the sea, so the temple’s location on a high outcropping overlooking the sea was particularly appropriate.</p>



<p>Continuing on the bus, we visited Papagiannakos Winery to the southeast of Athens. In contrast to the winery we visited in Italy, this winery was very modern and had LEED (energy efficiency) certification. The vineyards in this area look very different from what we are accustomed to seeing: the grapes were pruned to low bushes rather than trellises. Our lunch and a wine tasting were at the winery, and the wine was excellent and relatively inexpensive. We wish that we had room to take some home.</p>



<p>After lunch, we continued to the port of Lavrio to board the Orion. As we boarded the ship, we were greeted by several of the staff, including the Hotel Manager, Fran. We had met Fran on our <a href="https://altmode.org/2024/07/04/kimberley-trip-day-1-singapore-to-bali/">Kimberley expedition</a> last year. She took good care of Kenna when I had Covid on that trip, and we remember her (and much of her staff) well. We exchanged hugs as we boarded.</p>



<p>We met in the ship’s lounge for the opening briefing by our expedition leader, Karla Pound. Karla was also the expedition leader for the Kimberley trip, and when we raised our hands in response to her question, “Who has been on the Orion before?” she looked at us and said, “I remember you!” We felt very welcome.</p>



<p>The day closed with the requisite lifeboat drill, followed by a delicious dinner in the ship’s dining room.</p>



 



<p>This article is part of a series about our recent trip to Italy, Greece, Albania, and Croatia. To see the introductory article in the series, click <a href="https://altmode.org/2025/07/15/adriatic-trip-day-1-to-venice/">here</a>.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://altmode.org/wp-content/uploads/2025/07/img_4505.jpg?w=300'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:00 &bull;
   a day ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 25. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/25/qwen3-235b-a22b-thinking-2507/#atom-everything'>Qwen3-235B-A22B-Thinking-2507</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Qwen3-235B-A22B-Thinking-2507 
The third Qwen model release week, following Qwen3-235B-A22B-Instruct-2507 on Monday 21st and Qwen3-Coder-480B-A35B-Instruct on Tuesday 22nd. 
 Those two were both non-reasoning models - a change from the previous models in the Qwen 3 family which combined reasoning and non-reasoning in the same model, controlled by /think and /no_think tokens. 
 Today's model, Qwen3
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://static.simonwillison.net/static/2025/qwen-details.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507">Qwen3-235B-A22B-Thinking-2507</a></p>
The third Qwen model release week, following <a href="https://simonwillison.net/2025/Jul/22/qwen3-235b-a22b-instruct-2507/">Qwen3-235B-A22B-Instruct-2507</a> on Monday 21st and <a href="https://simonwillison.net/2025/Jul/22/qwen3-coder/">Qwen3-Coder-480B-A35B-Instruct</a> on Tuesday 22nd.<p></p>
<p>Those two were both non-reasoning models - a change from the previous models in the Qwen 3 family which combined reasoning and non-reasoning in the same model, controlled by <code>/think</code> and <code>/no_think</code> tokens.</p>
<p>Today's model, Qwen3-235B-A22B-Thinking-2507 (also released as an <a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507-FP8">FP8 variant</a>), is their new thinking variant.</p>
<p>Qwen claim "state-of-the-art results among open-source thinking models" and have increased the context length to 262,144 tokens - a big jump from April's <a href="https://huggingface.co/Qwen/Qwen3-235B-A22B">Qwen3-235B-A22B</a> which was "32,768 natively and 131,072 tokens with YaRN".</p>
<p>Their own published benchmarks show comparable scores to DeepSeek-R1-0528, OpenAI's o3 and o4-mini, Gemini 2.5 Pro and Claude Opus 4 in thinking mode.</p>
<p>The new model is already <a href="https://openrouter.ai/qwen/qwen3-235b-a22b-thinking-2507">available via OpenRouter</a>.</p>
<p>But how good is <a href="https://simonwillison.net/tags/pelican-on-a-bicycle/">its pelican</a>?</p>
<p>I tried it with "Generate an SVG of a pelican riding a bicycle" via OpenRouter, and it thought for 166 seconds - nearly three minutes! I have never seen a model think for that long. No wonder the documentation includes the following:</p>
<blockquote>
<p>However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072 when possible.</p>
</blockquote>
<p>Here's <a href="https://gist.github.com/simonw/057170c1d1e0843ca7e9547962d0c23e">a copy of that thinking trace</a>. It was really fun to scan through:</p>
<p></p>
<p>The <a href="https://gist.github.com/simonw/f013772544fabba02fca9e28fd54cdee">finished pelican</a>? Not so great! I like the beak though:</p>
<p>

    </p><p>Via <a href="https://x.com/Alibaba_Qwen/status/1948688466386280706">@Alibaba_Qwen</a></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/qwen">qwen</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-reasoning">llm-reasoning</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://static.simonwillison.net/static/2025/qwen-details.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:52 &bull;
   2 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/26/i-think-im-liking-the.html'> I think I’m liking the new posting flow on Micro Blog — good</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   I think I’m liking the new posting flow on Micro Blog — good job @manton thankyou 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I think I’m liking the new posting flow on Micro Blog — good job <a href="https://micro.blog/manton">@manton</a> thankyou</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   22:41 &bull;
   2 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/26/personally-id-never-heard-of.html'> Personally, I’d never heard of Tuvalu - but it is an extraor</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Personally, I’d never heard of Tuvalu - but it is an extraordinary place judging my this screen grab from Apple Maps 
  
 … and it’s about to disappear. 
 🔗 The First Planned Migration of an Entire Country Is Underway | WIRED 
 Paywalled - but if you have Apple News - there is 🔗 this 
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/2263ca5700.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Personally, I’d never heard of Tuvalu - but it is an extraordinary place judging my this screen grab from Apple Maps</p>
<p></p>
<p>… and it’s about to disappear.</p>
<p><a href="https://www.wired.com/story/the-first-planned-migration-of-an-entire-country-is-underway/">🔗 The First Planned Migration of an Entire Country Is Underway | WIRED</a></p>
<p>Paywalled - but if you have Apple News - there is <a href="https://apple.news/AGTvn0iZfSaCoGcoLqZEKXQ">🔗 this</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/2263ca5700.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:39 &bull;
   2 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://altmode.org'>Altmode</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://altmode.org/2025/07/25/adriatic-trip-day-11-acropolis/'>Adriatic Trip Day 11: Acropolis</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Friday, June 20, 2025 Our tour took us to the Acropolis today, so we got a relatively early start, both because it was going to be a hot day and to avoid the crowds. Our bus took us to the parking area at the base of the Acropolis, so we had to do some climbing. […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Friday, June 20, 2025</p>



<p>Our tour took us to the Acropolis today, so we got a relatively early start, both because it was going to be a hot day and to avoid the crowds. Our bus took us to the parking area at the base of the Acropolis, so we had to do some climbing. I’m not sure if we beat the crowds — it was quite crowded, particularly entering and leaving the site.</p>



<p>Unfortunately, there was quite a bit of scaffolding on the Parthenon, but restoration and preservation work at the site probably never stops. We had enough time to explore the area and take a few pictures, but it was getting hot quickly. We got the group together and headed back for our bus.</p>


 
<a href="https://altmode.org/wp-content/uploads/2025/07/img_4481.jpg"></a>Cooking class preparation 


<p>Our next stop was for lunch, which we would have to make ourselves. The tour included a short Greek cooking class, where we made (or at least participated in making) stuffed peppers, Greek salad, and tiropita (phyllo triangles with a cheese and egg filling). This was organized as a competition between two teams, and we took the competition seriously. Jan was our leader, and we won, so she got the prize: a chef’s hat signed by our instructor.</p>



<p>The afternoon took us to more Athens sights, including the Panathenian Olympic Stadium, where the modern Olympic Games were first held in 1896. The bus dropped us off at the Plaka, Athens’ historic old marketplace, where we were led on a walking tour before heading back to the hotel. It was a rather hot afternoon, and we were thankful for the many vendors selling cold drinks.</p>



<p>For the evening, we were picked up by the bus again for a drive to a beachside restaurant to the south of Athens. We sat outside at a long table on the stony beach. The dinner was unremarkable, but it provided a nice opportunity to chat with some of our fellow travelers.</p>



 



<p>This article is part of a series about our recent trip to Italy, Greece, Albania, and Croatia. To see the introductory article in the series, click <a href="https://altmode.org/2025/07/15/adriatic-trip-day-1-to-venice/">here</a>.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://altmode.org/wp-content/uploads/2025/07/img_4481.jpg?w=300'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:00 &bull;
   3 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2025/07/7470/'>7/28 総務省、金融庁のパブコメ他についてYouTube Live 配信やります。</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  7月28日22時より、久々にYouTube Live配信やります。 テーマは、総務省や金融庁から出ている下記のパブコメと、その他一般ID関連ニュースです。 【参考文献】 奮ってご参加ください。
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>7月28日22時より、久々にYouTube Live配信やります。</p>



<p>テーマは、総務省や金融庁から出ている下記のパブコメと、その他一般ID関連ニュースです。</p>



<p>【参考文献】</p>



 
 <a href="https://public-comment.e-gov.go.jp/pcm/detail?CLASSNAME=PCMMSTDETAIL&amp;id=145210539&amp;Mode=0">「オンラインカジノに係るアクセス抑止の在り方に関する検討会 中間論点整理（案）」に対する意見募集</a> 



 <a href="https://www.fsa.go.jp/news/r7/shouken/20250715/20250715.html">「金融商品取引業者等向けの総合的な監督指針」等の一部改正（案）</a>（←いわゆる証券不正取引対応） 



 <a href="https://public-comment.e-gov.go.jp/pcm/detail?CLASSNAME=PCMMSTDETAIL&amp;id=145210533&amp;Mode=0">ICTサービスの利用を巡る諸問題に対する利用環境整備に関する報告書（案）</a> 
 



<p>奮ってご参加ください。</p>



 

 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   04:54 &bull;
   3 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2025/07/7462/'>IETF 123: OAuth WG Session 1 サマリー（日本時間24日）</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  概要 日本時間7月24日、IETF 123 Madrid でOAuth WGの第１セッションが行われました。事前に発表されたアジェンダは以下のとおりですが、Chairs update のあとにAIについてのセッションが挿入されました。また、Chairs Update としては以下…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 概要 



<p>日本時間7月24日、IETF 123 Madrid でOAuth WGの第１セッションが行われました。事前に発表されたアジェンダは以下のとおりですが、Chairs update のあとにAIについてのセッションが挿入されました。また、Chairs Update としては以下のことが報告されました。</p>



 
 Published RFCs 
 
 RFC 9728 – OAuth 2.0 Protected Resource Metadata – Mike, Phil, Aaron. 
 
 



 RFC Editor Queue 
 
 Selective Disclosure for JWTs (SD-JWT)  



 OAuth 2.0 for Browser-Based Applications 
 
 



 Waiting for Write-Up 
 
 Cross-Device Flows: Security Best Current Practice  



 Token Status List 
 
 
 



 アジェンダ 



 
 Chairs update – Rifaat/Hannes (10 min) 



 SD-JWT VC – Brian (20 min)<a href="https://datatracker.ietf.org/doc/draft-ietf-oauth-sd-jwt-vc/">https://datatracker.ietf.org/doc/draft-ietf-oauth-sd-jwt-vc/</a> 



 Updates to Audience Values for OAuth 2.0 Authorization Servers – Mike/Brian (15 min)<a href="https://datatracker.ietf.org/doc/draft-ietf-oauth-rfc7523bis/">https://datatracker.ietf.org/doc/draft-ietf-oauth-rfc7523bis/</a> 



 Updates to OAuth 2.0 Security Best Current Practice – Pedram Hosseyni (15 min)<a href="https://datatracker.ietf.org/doc/draft-wuertele-oauth-security-topics-update/01/">https://datatracker.ietf.org/doc/draft-wuertele-oauth-security-topics-update/01/</a> 



 JWT BCP – Mike/Yaron (15 min)<a href="https://datatracker.ietf.org/doc/draft-sheffer-oauth-rfc8725bis/">https://datatracker.ietf.org/doc/draft-sheffer-oauth-rfc8725bis/</a> 



 Identity chaining – Brian (10 min)<a href="https://datatracker.ietf.org/doc/draft-ietf-oauth-identity-chaining/">https://datatracker.ietf.org/doc/draft-ietf-oauth-identity-chaining/</a> 



 Client ID Prefix – Brian/Aaron (10 min)<a href="https://datatracker.ietf.org/doc/draft-parecki-oauth-client-id-prefix/">https://datatracker.ietf.org/doc/draft-parecki-oauth-client-id-prefix/</a> 



 Dynamic Client Registration with Trusted Issuer Credentials &amp;<br>OAuth Client Registration on First Use with SPIFFE – Pieter (15 min)<a href="https://datatracker.ietf.org/doc/draft-kasselman-oauth-dcr-trusted-issuer-token/">https://datatracker.ietf.org/doc/draft-kasselman-oauth-dcr-trusted-issuer-token/</a><br><a href="https://datatracker.ietf.org/doc/draft-kasselman-oauth-spiffe/">https://datatracker.ietf.org/doc/draft-kasselman-oauth-spiffe/</a> 



 SPIFFE credentials for OAuth authentication – Arndt (10 min)<a href="https://datatracker.ietf.org/doc/draft-schwenkschuster-oauth-spiffe-client-auth/">https://datatracker.ietf.org/doc/draft-schwenkschuster-oauth-spiffe-client-auth/</a> 
 



 主な議題 



<p>主な議題は、OAuthプロトコルの最新の状況とセキュリティに関する議論です。具体的には、AIエージェント認証における新たな課題、特に特権の高いサービスアカウントからの脱却とユーザーを代表するトークンの取得について検討されています。また、SD-JWT (Selective Disclosure JWT)&nbsp;の進捗状況と、分散型識別子 (DID) の包含に関する継続的な議論に焦点を当てています。さらに、 audience値の曖昧さによる既知のセキュリティ脆弱性を修正する取り組みや、セキュリティBCP (Best Current Practice)&nbsp;の更新、特にオーディエンスインジェクション攻撃とミックスアップ攻撃の亜種に対する対策が議論されました。最後に、OAuthアイデンティティと認可のドメイン間連携、およびSpiffyクレデンシャルを利用したOAuthクライアントの自動登録に関する提案も紹介されています。</p>



<p>なお、本日も日本時間の夜９時半から第２セッションが行われます。</p>



 IETF 123: OAuthに関するブリーフィングドキュメント (2025年7月24日) 



<p>（以下、NotebookLMによるまとめにちょっと手を入れたものです。）</p>



 1. AIエージェント認証の課題 



<p>Jonathan Rosenberg と Pat Whiteは、AIおよびAIエージェントの出現によって生じる新たな認証課題について、OAuthコミュニティからの支援を求めました。</p>



 
 課題1：AIチャットボット・音声ボットの特権問題
 
 顧客サポート用ボット（チャットボットや音声ボット）は、ユーザーに代わってAPIを呼び出し、アクションを実行します（例：処方箋の再注文）。 



 現在、これらのボットは通常、高い権限を持つサービスアカウントで構築されています。「今日、ほとんどのものはこれらのボット用のサービスアカウントで構築されており、ご存知の通り、通常はかなり高い特権を持っています。なぜなら、会社に電話をかけてくるあらゆるユーザーに代わって操作できる必要があるからです。」 



 大規模言語モデル（LLM）の導入により、ボットが「幻覚」を起こしたり、誤った情報に基づいてAPI呼び出しを決定するリスクが高まります。 



 目標は、「神のようなサービスアカウント」から脱却し、ボットにユーザー自身を代表するトークンのみを付与する認証フレームワークを開発することです。 



 公衆交換電話網（PSTN）経由でユーザーが本人確認（SSNの下4桁、住所など）を行う場合に、その情報を使用してユーザーのトークンを取得する方法が課題となります。 



 （崎村の感想）これって、CIBA使えばよいのではないの？ 
 
 



 課題2：自律型AIエージェントの権限昇格
 
 UIを持たない自律型エージェント（例：取引ボット）が、特定の状況下で権限を一時的に昇格させる必要がある場合（例：緊急メールへの返信のためにメール閲覧権限を一時的に昇格）。 



 「ボットは、必要なことを行うための適切な権限を持つトークンを要求できる必要があります。」 



 「ボットが、人間による承認のためにユーザーに送り返される、何らかの権限昇格を要求する必要があるということです。」これは、限定された時間と期間で行われるべきです。 



 （崎村の感想）これって、CIBA使えばよいのではないの？ 
 
 



 関連するセキュリティ問題：AI確認フロー (Human-in-the-Loop)
 
 AIエージェントがAPIを呼び出す前に、人間による確認と承認が必要なシナリオ。「AIエージェントがAPIを呼び出したいが、うまくやることを信用できないという問題です。人間と相談して、人間が操作を承認し、それからAPIが呼び出されることを望みます。」 



 これはOAuthに隣接する問題として認識されており、”draft-rose-check-00″としてドラフトが提出されました。Tim は、これがOAuthの範囲内であると主張しました。 
 
 



 OAuthコミュニティからのフィードバック
 
 Jeff Lombardo (AWS)は、既存のOAuth機能（特にClient Initiated Backchannel Authorization: CIBA）がこの問題の多くに対処できる可能性を指摘しました。「神のようなトークンを与えないと言ったとき、これこそがOAuthが作られた理由だと思います。」 



 Hannes Tschofenigは、エージェントがトークンを直接操作すべきではなく、認証インフラストラクチャから分離すべきであるという点で、興味深い見解を示しました。「理想的には、エージェントはトークンに一切触れるべきではありません。神のようなトークンでも、一時的なトークンでも、いかなるトークンにも。」 
 
 
 



 2. SD-JWT VC（Selective Disclosure JSON Web Token Verifiable Credentials） 



<p>Brian Campbellは、SD-JWT VCの進捗状況と、DID（Decentralized Identifiers）の取り扱いに関する重要な議論について説明しました。</p>



 
 SD-JWT VCの概要
 
 検証可能なクレデンシャルをJSONペイロードで表現するためのデータ形式、検証、および処理ルールを定義します。 



 SD-JWT形式と既存のJWTコンテンツルールおよび拡張性モデルに基づいています。 



 OpenID Connectに類似したSD.VC発行者メタデータと鍵解決技術を記述します。 



 W3Cの検証可能なクレデンシャルデータモデル（VCDM）1.1または2は利用せず、必須でも禁止でもありません。連携して使用する場合はOpenID for Verifiable Presentations はSD-JWT-VC-LDを参照します。 
 
 



 最近の変更点 (Draft 9 &amp; 10)
 
 主に編集上の更新、エンコーディングの修正、例の更新。 



 発行者署名鍵検証セクションが「発行者署名メカニズム」に改称され、署名鍵検証オプションの拡張ポイントが明確に定義されました。 



 ウェブベースのメタデータ解決（よく知られたURIとJWKSを使用）とインラインX.509バリアント（x5c JWSヘッダーを使用）の2つの一般的なメカニズムが定義されました。 



 許容されるメカニズムは検証者のポリシー（信頼されたCAリストなど）に依存することが明確化され、1つのメカニズムの検証で十分とされました。 
 
 



 DIDのSD-JWT VC仕様への包含に関する議論
 
 非包含の理由（Brian Campbellの主張）:
 
 複雑性と認知的オーバーヘッド: DIDメソッドのレジストリには200以上のメソッドがあり、ほとんどの開発者がすべてを実装するのは現実的ではありません。これにより、相互運用性が損なわれます。「200以上のDIDメソッドの普及は、ほとんどの開発者にとって、ほとんどの平均的な開発者にとって、実装するのが合理的ではありません。」 



 評判リスク: 200もの統一されていないDIDメソッドを含めることは、標準化トラックIETF文書の信頼性と真剣さを損なう可能性があります。「200の適切に管理されていない、特定のコミュニティの幅広い合意を得ていないDIDメソッドを含めることによる、標準化トラックIETF文書の信頼性と認識される真剣さへの潜在的な損害。」 



 スコープの逸脱: DIDに関するガイダンスは、この作業部会や文書の専門知識とスコープを超えています。特定のプロファイルがDIDメソッドの選択とポリシーを定義すべきです。 
 
 



 包含の必要性（MarcusとStefanの主張）:
 
 過去4回の議論で、DIDサポートを削除することに大きな反対意見がありました。 



 「この変更を元に戻すべきです。」 



 Stefanは、DIDに関する以前の記述が、拡張機能の使用方法を説明するために要求されたものであり、削除は逆行であると主張しました。 



 200のDIDメソッドをすべて実装する必要はなく、ヨーロッパレベルでDIDメソッドの標準化が進められています。「200の非標準化されたDIDメソッドに関する情報を繰り返し言うのはやめてください。欧州レベルのSan JC19で標準化プロジェクトが進行中です。そこで明示的にこれらのDIDメソッドを標準化します。」 



 DIDサポートは以前は必須であり、その後オプションになった経緯があります。オプション機能であるため、実装を望まない開発者に負担をかけるべきではありません。 
 
 



 投票結果:文書にDIDを含めるかどうかについて、投票が行われました。
 
 賛成：7票、反対：14票、意見なし：9票。 



 結果として、DIDを主要文書に含めないことが決定されました。提唱者は、DID作業のためのプロファイルまたは独立したRFCを公開すべきです。「そのオプションは提唱者によって採用されるべきです。このRFCを今日持っているもので公開し、DID作業のためのプロファイルを開始し、それも公開してください。」 
 
 
 
 
 



 3. RFC 7523 BIS の更新 



<p>Mike Jonesは、RFC 7523 bisの更新状況、特に既知のセキュリティ脆弱性への対処について説明しました。</p>



 
 目的: 承認システムを対象とした曖昧なオーディエンス値が悪用される脆弱性に対処すること。 



 対象となるRFC: 7521, 7522, 7523, 9126（Push Authorization Requests）を更新します。 



 アプローチの変更: 以前は7523を完全に置き換えることを想定していましたが、IETF 122での議論の結果、既存のRFCへのポイント更新に焦点を当てることになりました。「7523を置き換えるのではなく、ポイント更新を行うことになったのです。」 



 主な変更点:
 
 署名済みJWT要求形式への更新を削除（既に「正しいことをすべき」とされているため）。 



 脆弱性を説明したシュトゥットガルト大学の論文への参照を追加。 



 IETFのプロセスを迅速化（セキュリティ修正のため）することがDebの支援を受けて確認されました。 
 
 



 未解決の課題: 



 明示的なタイピング（Explicit Typing）: 承認サーバーが古い7523と更新された7523を区別できるように、明示的なタイプを必須とするか否かについて議論があります。 



 SAML認証グラント: オーディエンス制限に関する特定の言語の調整。 



 単一文字列オーディエンスの強制: 現在の仕様ではオーディエンス値は単一文字列であるべきとしていますが、Kubernetesのような既存の実装が配列を使用しているため、変更の必要性について議論があります。「オーディエンス値は単一文字列でなければならないとされていますが、Kubernetesはオーディエンス値に配列しか使用しないコードを持っています。」 



 グラントオーディエンスチェックの微妙な点。 



 文書のタイトル: 現在のタイトル（”7523 BIS”）の変更提案があり、Mike SchwarzやBrian Campbellから新しいタイトルの提案があります。 



 今後のステップ: 継続的な議論、課題の解決、ドラフトの公開、そして最終的なワーキンググループ最終コールを目指します。セキュリティ修正であるため、迅速な公開が目標です。 
 



 4. OAuthセキュリティBCPの更新 



<p>PedramとKaiuanは、OAuthセキュリティBCP（Best Current Practice）の更新状況について発表しました。この更新は、新たに発見された2つの攻撃に対処することを目的としています。</p>



 
 更新の背景:
 
 BCPの最終化後に発見された2つの新しい攻撃（オーディエンスインジェクション攻撃とMix-Up攻撃の亜種）に対応するため。 



 IETF 122での議論の結果、新しいRFCとしてBCP 240に追加し、新しい攻撃に関する考慮事項のみを含めることが決定されました。 
 
 



 オーディエンスインジェクション攻撃 (Pedram)
 
 攻撃の概要: 攻撃者がクライアント認証アサーションを取得し、クライアントになりすます方法の詳細な説明。具体的なエンドポイント（プッシュ認証エンドポイント、デバイス認証エンドポイントなど）のリストも提供されています。 



 根本原因と対策: 攻撃はクライアントによってのみ防止可能であり、単一のオーディエンス値（ASの発行者識別子またはターゲットエンドポイントの正確なURI）を使用することを推奨します。 
 
 



 Mix-Up攻撃の亜種 (Kaiuan)
 
 動機: Google、Microsoft、Amazon、Samsungなどの主要ベンダーを含むプラットフォームに影響を与える、新しいMix-Up攻撃の亜種が広範囲に存在すること。「ほぼすべての場所で蔓延している新しいMix-Up攻撃の亜種を発見しました。」これは、プラットフォーム設定における明確な標準や慣行の欠如に起因しています。 



 攻撃の焦点: 複数のIDPと連携する単一のアプリではなく、多数の統合アプリと連携する「統合プラットフォーム」に焦点を当てています。 



 攻撃の亜種:Cross-Flow Account Takeover (C-FAT): 被害者の承認コードが攻撃者に漏洩する。 



 Code Injection (CI): 攻撃者のコードが被害者に注入される。 



 根本原因: OAuthクライアントが統合アプリを区別する際に曖昧さがあること（セッションやリダイレクトURIのみに依存するなど）。 



 「共有発行者」の概念: 2つの統合アプリが同じ認証サーバーを合法的に共有できる場合（例：同じプラットフォーム上の2つのDropbox統合）。この場合、発行者識別子だけでは各統合を一意に識別できません。 



 対策: OAuthクライアントは、各統合アプリを「明確なリダイレクトURI」で区別すべきです。これは、OAuthを開始したアプリと完了するアプリが同じであることを強制します。この防御策は多くの企業に採用されています。 
 
 



 今後のステップ: 



 この文書をワーキンググループドラフトとするためのさらなる議論（特に議長とのオフラインでの調整）。 



 仮想中間会議を開催し、詳細な議論を行う可能性。 



 参加者からの文書レビューとフィードバックが求められています。 
 



 5. JWT Security BCPの更新 



<p>Yaronと Mike Jonesは、RC8725 JWT Security BCPの更新について発表しました。これも、新たに発見された攻撃に対処するものです。</p>



 
 更新の動機: 2023年夏のBlack Hatで公開された攻撃や、清華大学の研究者によるCVEなど、JOTに対する多数の新しい攻撃が発見されたため。 



 提案される変更点（5つのプルリクエスト）: 



 パスワードベースの鍵生成における反復回数の上限: サービス拒否攻撃を防ぐため。 



 JWS/JWEの混同: 検証者がJWSを期待しているにもかかわらずJWEを受け入れてしまうケースに対処するため。「これは、JWEが公開鍵で暗号化されており、検証者が公開鍵を秘密鍵と一緒に保管している場合に特に発生します。」JWSであることを確認する規範的言語を追加します。 



 大文字・小文字の区別: 「none」アルゴリズムのブロックリストを回避するために大文字・小文字を操作する攻撃に対処するため。防御的なコーディング（ブロックリストの代わりに許可リストを使用）を推奨します。 



 圧縮の悪用: JWEでサポートされている圧縮機能が悪用される可能性に対処するため。 



 JSONシリアライズされたJWSの拒否: 標準では許可されていないにもかかわらず、一部の検証者がJSONシリアライズされたJWSを受け入れてしまう問題に対処するため。 



 今後のステップ: 文書は良好な状態であり、できるだけ早く作業グループに採用され、進捗させることを望んでいます。Aaron and Denny Pinkasからのレビューが既に受領されています。Brian Campbellは、さらなる更新が必要になる可能性があり、PFB(Password Based Key Derivation Function 2)アプローチアルゴリズムの廃止や、「none」アルゴリズムの非推奨化に関する既存の文書との連携など、より大きな作業になる可能性があることを指摘しました。 
 



 6. OAuth IDと認証のドメイン間連携 



<p>Brian Campbellは、RFC 8693（トークン交換）とRFC 7523（アサーションフレームワーク）を組み合わせて、エンドユーザーの操作なしにドメイン間でIDと認証情報を維持する一般的なパターンを記述するドラフトについて説明しました。</p>



 
 目的: 2つの既存のRFCをプロファイリングし、多くの場所で実際に行われているものの、多くの人にとって理解が難しい方法で、ドメイン間でIDと認証情報を維持する方法を説明すること。 



 主なアプローチ: ローカルなRFC 8693トークン交換を使用してトークンを取得し、RFC 7523のクロスドメインアクセス許可取得を容易にします。 



 最近の変更点 (Draft 05): 主に編集上の更新と、ローカルASがサポートするトークンタイプを記述するためのメタデータの追加。 



 未解決の課題: 4つの未解決の課題がありますが、そのほとんどは編集上のものです。1つは、アサーションプロファイルでのMikeの変更を反映させることに関するものですが、影響は小さいと予想されます。 



 ワーキンググループ最終コールへの提案: 文書はワーキンググループ最終コール（WGLC）の準備ができており、迅速な進捗が期待されています。「ワーキンググループの最終コールを検討してください。」 



 Aaron Perkiの関連作業 (Identity Assertion Authorization Grant):Brianは、Brian自身の文書と関連するAaron Perkiの作業（Identity Assertion Authorization Grant）についても言及しました。これは、Brianの文書のより具体的で意味のあるユースケースのためのプロファイルです。 



 この作業は、Agentic AIのユースケース、特に企業環境での関連性が高まっています。エンドユーザーの明示的な許可なしに、ユーザーに代わってOAuthベースのシステムがアクセス・トークンを取得できるようにします。 



 「Agentic AIの出現よりずっと前からこの作業は始まりましたが、エンタープライズの文脈でのAgentic AIのユースケースに特に関連性があり、意味のあるものとなります。ここでは、エンドユーザーの同意は実質的に雇用契約を通じて提供され、各トランザクションでエンドユーザーの同意を得る必要はありません。」 



 Aaronのブログ投稿と図が示され、AIエージェントがエンタープライズIDPからログインし、SSOトークンを使用してSlackなどのアプリケーションにクロスドメインでアクセスする方法が示されました。 



 コミュニティからの反応: WGLCの提案に対して、異論はほとんどありませんでした。この作業は重要であり、企業環境に直接関連しています。 
 



 7. OAuthクライアントIDプレフィックス 



<p>Brian Campbellは、Aaron、Daniel、Josephが取り組んでいる「OAuthクライアントIDプレフィックス」ドラフトについて説明しました。</p>



 
 目的: クライアントがメタデータを公開するためのシンプルで実用的な方法を提供し、クライアントの事前登録なしに承認サーバーがクライアントIDを確立できるようにします。これは、オープンソースのチャットアプリと自己ホスト型サーバーの接続や、MastodonやWordPressのような自己ホスト型サービスに接続するアプリなど、事前登録が非現実的な多くのケースで役立ちます。 



 以前のドラフトとの関係: 以前の「クライアントIDメタデータドキュメント」ドラフトから分割され、名称変更されました。 



 コンセプト:
 
 クライアントは、クライアント登録ボキャブラリ（動的クライアント登録で定義されたフィールド）を使用して、JSONドキュメントを安定したURLに公開します。 



 承認リクエストでは、そのURLをクライアントIDとして渡します。 



 承認サーバーは、そのURLからJSONドキュメントを取得し、それを使用してクライアントのアイデンティティ（構成パラメータ、認証用の公開鍵など）を確立します。 



 これは、事前登録を必要としないクライアントオンボーディングの代替アプローチを提供します。 
 
 



 AIとの関連性: このアプローチは、Agentic AIにも役立ちます。Agentic AIは、リソースにアクセスするソフトウェアという点では新しい問題ではありませんが、その規模と潜在的な問題は、事前登録が多くの展開における障害となるという、これまでも課題だった問題を浮き彫りにします。 



 議論と課題:
 
 欠点: 事前登録のメリット（例：リダイレクトURIの検証）が失われる可能性。解決策として、クライアントIDとドキュメントのハードな関連付けや、ユーザーに検証済みURLを表示することなどが検討されています。 



 信頼管理: OpenID Federationとの類似性が指摘され、OpenID Federationが複雑な信頼管理メカニズムを導入している理由（大規模な展開での自動化、信頼マークなど）が強調されました。このドラフトでは、そのような信頼管理メカニズムが不足している点が課題として挙げられました。 
 
 



 今後のステップ: 文書はまだWGLCの準備ができていないと判断されました。さらなる議論と課題の解決のために、中間会議が開催される予定です。 
 



 8. Spiffy Credentialsを用いたOAuthクライアント登録 



<p>Pieter Kasselmanは、Spiffy Credentialsを用いたOAuthクライアントの自動登録に関する2つのドラフトについて発表しました。</p>



 
 自動クライアント登録の必要性: 



 運用の課題: クライアントとシークレットの手動管理、シークレットのローテーション、安全な保管には時間がかかり、エラーが発生しやすく、ダウンタイムにつながる可能性があります。「クライアントやシークレットの管理、シークレットのローテーション、安全な保管には多くの時間がかかり、手作業になりがちです。または、従うべき多くのプロセスを作成する必要があります。」 



 異種環境: 異なる組織や技術スタックで手動作業を行う場合、開発者とID管理者の間での多くの調整とオーバーヘッドが発生します。 



 指数関数的な成長: ワークロードの増加、特にAIの貢献により、クライアントとシークレットが指数関数的に増加し、大規模組織での管理が困難になっています。 



 Spiffyの紹介:
 
 Spiffy（Secure Production Identity Framework For Everyone）は、ワークロードにシークレットをプロビジョニングすることなく、クレデンシャルをブートストラップすることで、この「底辺の亀問題」を解決することを目的としています。 



 アッテステーション（Attestation）と継続的なライフサイクル管理を通じて機能します。 



 大規模に展開でき、オープンソースおよび商用ソリューションが利用可能です。 



 Spiffyは、クライアントの管理責任をプラットフォームレベルに移行させ、ワークロードを重視することで、OAuth展開のメリットをもたらします。これにより、OAuthサーバーはクレデンシャル発行、ライフサイクル管理、ID証明を行うSpiffyインフラストラクチャに依存できます。 
 
 



 提案される2つのアプローチ:
 
 1. 最初に登録する (Register on First Use) アプローチ:
 
 クライアントがSpiffyクレデンシャルを提示すると、認証サーバーは発行者を既に信頼しているため、クレデンシャルを検証し、そのクライアントIDを自動的に信頼・登録します。 



 追加の登録プロトコルは不要で、効率的で低遅延です。 



 クライアントクレデンシャルフローに適しています。 



 リダイレクトフローには追加のメタデータ管理戦略が必要です（BrianやAaronの以前の作業との連携の可能性も示唆されました）。 
 
 



 2. ソフトウェアステートメントとしてのSpiffy JWTの使用 (Dynamic Client Registration):
 
 Spiffy JWTを動的クライアント登録プロトコルの一部としてソフトウェアステートメントとして使用します。 



 新しいプロトコルは不要で、既存の動的クライアント登録と互換性があります。 



 MCP (Mobile Connect Profile)に採用された動的クライアント登録にとって有用です。 



 JWTのみをサポートしますが、他のクレデンシャルタイプに一般化することも可能です。 
 
 
 
 



 今後のステップ:
 
 クライアントの自動登録がOAuthコミュニティにとって興味深いものであるかどうかの意見を求めます。 



 Spiffyアプローチが興味深いものであるかどうかを議論します。 



 興味がある場合は、コメント、課題、PRを通じて参加を呼びかけます。 



 これにより、シンプルでスケーラブルかつ安全なクライアント登録メカニズムが提供され、シークレットの増殖問題からの脱却につながります。 
 
 



 コミュニティからの反応:
 
 SpiffyがOAuth展開で採用されている場合、登録プロセスを簡素化できるという点で、この作業は非常に関連性が高いと見られています。 



 Jeff Lombardo (AWS)は、この作業部会が取り組むべきだと賛成しました。 



 Mike Jonesは、「自動クライアント登録」という用語がOpenID Federationで既に明確に定義されているため、別の用語を使用することを提案しました。 



 Josephは、リダイレクトフローと非リダイレクトフローへの分類が正確ではない可能性を指摘し、プッシュ認証リクエスト（PAR）を使用した場合にリダイレクトフローでも最初のケースに該当する可能性があるとコメントしました。 



 Tony Nadlinは、文書が信頼ドメインの確立方法について説明していないこと、およびSpiffyを特定のインスタンスで使用する方法について懸念を示しました。 



 Brian Campbellは、自身が発表した「クライアントIDプレフィックス」の作業とのオーバーラップを認識しており、連携の必要性を強調しました。 
 
 
 



 

 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   04:11 &bull;
   3 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 24. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-141'>The Pulse #141: Replit AI secretly deletes prod – oops!</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Also: mystery solved about dozens of outages on 10 June, reflections on a year at OpenAI, and more
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering events, insights, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Today, we cover:</p>  <p>Mystery solved about the cause of June 10th outages. Heroku went down for a day due to an update to the systemd process on Ubuntu Linux. Turns out that dozens of other companies including OpenA…</p>  
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-141">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:59 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://altmode.org'>Altmode</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://altmode.org/2025/07/24/adriatic-trip-day-10-athens/'>Adriatic Trip Day 10: Athens</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Thursday, June 19, 2025 After breakfast, Kenna and I made our return visit to the Athens Laundry. Our entire laundry load cost 18 euros, about the same as a single shirt would have cost at the hotel. We enjoyed the walk again as well. We met up with Dave and Jan at the rooftop terrace […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Thursday, June 19, 2025</p>



<p>After breakfast, Kenna and I made our return visit to the Athens Laundry. Our entire laundry load cost 18 euros, about the same as a single shirt would have cost at the hotel. We enjoyed the walk again as well.</p>



<p>We met up with Dave and Jan at the rooftop terrace of the hotel for lunch, and met the other participants in our pre-cruise tour of Athens. The view from the roof was, of course, excellent. Unfortunately, the service was slow, and we were a little rushed to meet the bus for our afternoon tour.</p>


 
<a href="https://altmode.org/wp-content/uploads/2025/07/img_4420.jpg"></a>Artifacts at Acropolis Museum 


<p>We met up with our local guide and toured the Acropolis Museum. Many of the original artifacts from the Acropolis are stored in the museum to protect them from the elements. In a few cases, replicas have been made that are on display at the actual Acropolis. Like much of Athens, the land on which the museum is built also has historic ruins, so the museum is built on piers that avoid destroying the artifacts below. The building is very modern and bright, and displays the artifacts clearly and comfortably. Our local guide provided context and historical details as we walked through the galleries.</p>



<p>After returning to the hotel to relax a bit, we joined our group briefly for a cocktail reception, but I had made dinner reservations well in advance, so we couldn’t stay long. Our dinner tonight was at <a href="https://www.strofi.gr/en/">Strofi</a>, a local restaurant that I had eaten at in 1983, 42 years ago (almost to the day). I was in Athens on a business trip, and the hotel had recommended Strofi. Everyone on that trip loved the restaurant and the food, not to mention the view of the Parthenon from their rooftop terrace. We ended up eating dinner there every night of our visit to Athens. I had since recommended Strofi to others and was anxious to try it again myself. Dave and Jan came with us, and we had a most enjoyable dinner, again on the rooftop terrace looking up at the Acropolis.</p>



 



<p>This article is part of a series about our recent trip to Italy, Greece, Albania, and Croatia. To see the introductory article in the series, click <a href="https://altmode.org/2025/07/15/adriatic-trip-day-1-to-venice/">here</a>.</p>



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://altmode.org/wp-content/uploads/2025/07/img_4420.jpg?w=300'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:00 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/24/github-spark/#atom-everything'>Using GitHub Spark to reverse engineer GitHub Spark</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   GitHub Spark was released in public preview yesterday. It's GitHub's implementation of the prompt-to-app pattern also seen in products like Claude Artifacts, Lovable, Vercel v0, Val Town Townie and Fly.io’s Phoenix New. In this post I reverse engineer Spark and explore its fascinating system prompt in detail. 
 I wrote about Spark back in October when they first revealed it at GitHub Universe. 
 G
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://static.simonwillison.net/static/2025/spark-ui.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://github.com/features/spark">GitHub Spark</a> was released <a href="https://github.blog/changelog/2025-07-23-github-spark-in-public-preview-for-copilot-pro-subscribers/">in public preview</a> yesterday. It's GitHub's implementation of the prompt-to-app pattern also seen in products like Claude Artifacts, Lovable, Vercel v0, Val Town Townie and Fly.io’s Phoenix New. In this post I <a href="https://simonwillison.net/2025/Jul/24/github-spark/#reverse-engineering-spark-with-spark">reverse engineer Spark</a> and <a href="https://simonwillison.net/2025/Jul/24/github-spark/#that-system-prompt-in-detail">explore its fascinating system prompt</a> in detail.</p>
<p>I wrote about Spark <a href="https://simonwillison.net/2024/Oct/30/copilot-models/">back in October</a> when they first revealed it at GitHub Universe.</p>
<p>GitHub describe it like this:</p>
<blockquote>
<p>Build and ship full-stack intelligent apps using natural language with access to the full power of the GitHub platform—no setup, no configuration, and no headaches.</p>
</blockquote>
<p>You give Spark a prompt, it builds you a full working web app. You can then iterate on it with follow-up prompts, take over and edit the app yourself (optionally using GitHub Codespaces), save the results to a GitHub repository, deploy it to Spark's own hosting platform or deploy it somewhere else.</p>
<p>Here's a screenshot of the Spark interface mid-edit. That side-panel is the app I'm building, not the docs - more on that in a moment.</p>
<p></p>

 
 <a href="https://simonwillison.net/2025/Jul/24/github-spark/#spark-capabilities">Spark capabilities</a> 
 <a href="https://simonwillison.net/2025/Jul/24/github-spark/#reverse-engineering-spark-with-spark">Reverse engineering Spark with Spark</a> 
 <a href="https://simonwillison.net/2025/Jul/24/github-spark/#that-system-prompt-in-detail">That system prompt in detail</a> 
 <a href="https://simonwillison.net/2025/Jul/24/github-spark/#what-can-we-learn-from-all-of-this-">What can we learn from all of this?</a> 
 <a href="https://simonwillison.net/2025/Jul/24/github-spark/#spark-features-i-d-love-to-see-next">Spark features I'd love to see next</a> 
 

 Spark capabilities 
<p>Sparks apps are client-side apps built with React - similar to Claude Artifacts - but they have additional capabilities that make them much more interesting:</p>
 
 They are authenticated: users must have a GitHub account to access them, and the user's GitHub identity is then made available to the app. 
 They can store data! GitHub provides a persistent server-side key/value storage API. 
 They can run prompts. This ability isn't unique - Anthropic added that to Claude Artifacts <a href="https://simonwillison.net/2025/Jun/25/ai-powered-apps-with-claude/">last month</a>. It looks like Spark apps run prompts against an allowance for that signed-in user, which is neat as it means the app author doesn't need to foot the bill for LLM usage. 
 
<p>A word of warning about the key/value store: it can be read, updated and deleted by anyone with access to the app. If you're going to allow all GitHub users access this means anyone could delete or modify any of your app's stored data.</p>
<p>I built a few experimental apps, and then decided I to go meta: I built a Spark app that provides the missing documentation for how the Spark system works under the hood.</p>
 Reverse engineering Spark with Spark 
<p>Any system like Spark is inevitably powered by a sophisticated invisible system prompt telling it how to behave. These prompts double as the missing manual for these tools - I find it much easier to use the tools in a sophisticated way if I've seen how they work under the hood.</p>
<p>Could I use Spark itself to turn that system prompt into user-facing documentation?</p>
<p>Here's the start of my sequence of prompts:</p>
 
 
<code>An app showing full details of the system prompt, in particular the APIs that Spark apps can use so I can write an article about how to use you</code> [<a href="https://github.com/simonw/system-exploration-g/commit/d0f1b94d635c8d4e946c225c30fa2b06bf029589">result</a>] 
 
<p>That got me off to a pretty great start!</p>
<p></p>
<p>You can explore the final result at <a href="https://github-spark-docs.simonwillison.net/">github-spark-docs.simonwillison.net</a>.</p>
<p>Spark converted its invisible system prompt into a very attractive documentation site, with separate pages for different capabilities of the platform derived from that prompt.</p>
<p>I read through what it had so far, which taught me how the persistence, LLM prompting and user profile APIs worked at a JavaScript level.</p>
<p>Since these could be used for interactive features, why not add a Playground for trying them out?</p>
 
 
<code>Add a Playground interface which allows the user to directly interactively experiment with the KV store and the LLM prompting mechanism</code> [<a href="https://github.com/simonw/system-exploration-g/commit/6d0706dd17fd449fa3b90aa95349a2036801f0dd">result</a>] 
 
<p>This built me a neat interactive playground:</p>
<p></p>
<p>The LLM section of that playground showed me that currently only two models are supported: GPT-4o and GPT-4o mini. Hopefully they'll add GPT-4.1 soon. Prompts are executed through <a href="https://learn.microsoft.com/en-us/azure/ai-foundry/openai/">Azure OpenAI</a>.</p>
<p>It was missing the user API, so I asked it to add that too:</p>
 
 
<code>Add the spark.user() feature to the playground</code> [<a href="https://github.com/simonw/system-exploration-g/commit/f5f7cdd6340a4f80ddbf99a26fade1de04a7d6c7">result</a>] 
 
<p>Having a summarized version of the system prompt as a multi-page website was neat, but I wanted to see the raw text as well. My next prompts were:</p>
 
 
<p><code>Create a system_prompt.md markdown file containing the exact text of the system prompt, including the section that describes any tools. Then add a section at the bottom of the existing System Prompt page that loads that via fetch() and displays it as pre wrapped text</code></p>
 
 
<p><code>Write a new file called tools.md which is just the system prompt from the heading ## Tools Available - but output &amp;lt; instead of &lt; and &amp;gt; instead of &gt;</code></p>
<p><code>No need to click "load system prompt" - always load it</code></p>
<p><code>Load the tools.md as a tools prompt below that (remove that bit from the system_prompt.md)</code></p>
 
 
<p>The bit about <code>&lt;</code> and <code>&gt;</code> was because it looked to me like Spark got confused when trying to output the raw function descriptions to a file - it terminated when it encountered one of those angle brackets.</p>
<p>Around about this point I used the menu item "Create repository" to start a GitHub repository. I was delighted to see that each prompt so far resulted in a separate commit that included the prompt text, and future edits were then automatically pushed to my repository.</p>
<p>I made that repo public so you can see <a href="https://github.com/simonw/system-exploration-g/commits/main/">the full commit history here</a>.</p>
<p>... to cut a long story short, I kept on tweaking it for quite a while. I also extracted full descriptions of the available tools:</p>
 
 
str_replace_editor for editing files, which has sub-commands <code>view</code>, <code>create</code>, <code>str_replace</code>, <code>insert</code> and <code>undo_edit</code>. I recognize these from the <a href="https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/text-editor-tool">Claude Text editor tool</a>, which is one piece of evidence that makes me suspect Claude is the underlying model here. 
 
npm for running npm commands (<code>install</code>, <code>uninstall</code>, <code>update</code>, <code>list</code>, <code>view</code>, <code>search</code>) in the project root. 
 
bash for running other commands in a shell. 
 
create_suggestions is a Spark-specific tool - calling that with three suggestions for next steps (e.g. "Add message search and filtering") causes them to be displayed to the user as buttons for them to click. 
 
<p>Full details are <a href="https://github.com/simonw/system-exploration-g/blob/main/src/tools.md">in the tools.md file</a> that Spark created for me in my repository.</p>
<p>The bash and npm tools clued me in to the fact that Spark has access to some kind of server-side container environment. I ran a few more prompts to add documentation describing that environment:</p>
 
 
<code>Use your bash tool to figure out what linux you are running and how much memory and disk space you have</code> (this ran but provided no output, so I added:) 
 <code>Add that information to a new page called Platform</code> 
 <code>Run bash code to figure out every binary tool on your path, then add those as a sorted comma separated list to the Platform page</code> 
 
<p>This gave me a ton of interesting information! Unfortunately Spark doesn't show the commands it ran or their output, so I have no way of confirming if this is accurate or hallucinated. My hunch is that it's accurate enough to be useful, but I can't make any promises.</p>
<p></p>
<p>Spark apps can be made visible to any GitHub user - I set that toggle on mine and published it to <a href="https://system-exploration-g--simonw.github.app/">system-exploration-g--simonw.github.app</a>, so if you have a GitHub account you should be able to visit it there.</p>
<p>I wanted an unathenticated version to link to though, so I fired up Claude Code on my laptop and <a href="https://gist.github.com/simonw/8650d09c6db47ee66c3790c2803e0c6a">had it figure out the build process</a>. It was almost as simple as:</p>
 <code>npm install
npm run build
</code> 
<p>... except that didn't quite work, because Spark apps use a private <code>@github/spark</code> library for their Spark-specific APIs (persistence, LLM prompting, user identity) - and that can't be installed and built outside of their platform.</p>
<p>Thankfully Claude Code (aka <a href="https://simonwillison.net/2025/May/23/honey-badger/">Claude Honey Badger</a>) won't give up, and it hacked around with the code until it managed to get it to build.</p>
<p>That's the version I've deployed to <a href="https://github-spark-docs.simonwillison.net/">github-spark-docs.simonwillison.net</a> using GitHub Pages and a custom subdomain so I didn't have to mess around getting the React app to serve from a non-root location.</p>
<p>The default app was a classic SPA with no ability to link to anything inside of it. That wouldn't do, so I ran a few more prompts:</p>
 
 <code>Add HTML5 history support, such that when I navigate around in the app the URL bar updates with #fragment things and when I load the page for the first time that fragment is read and used to jump to that page in the app. Pages with headers should allow for navigation within that page - e.g. the Available Tools heading on the System Prompt page should have a fragment of #system-prompt--available-tools and loading the page with that fragment should open that page and jump down to that heading. Make sure back/forward work too</code> 
 <code>Add # links next to every heading that can be navigated to with the fragment hash mechanism</code> 
 <code>Things like &lt;CardTitle id="performance-characteristics"&gt;Performance Characteristics&lt;/CardTitle&gt; should also have a # link - that is not happening at the moment</code> 
 
<p>... and that did the job! Now I can link to interesting sections of the documentation. Some examples:</p>
 
 Docs on <a href="https://github-spark-docs.simonwillison.net/#persistence">the persistence API</a>
 
 Docs on <a href="https://github-spark-docs.simonwillison.net/#llm">LLM prompting</a>
 
 The <a href="https://github-spark-docs.simonwillison.net/#system-prompt--system-prompt-content">full system prompt</a>, also available <a href="https://github.com/simonw/system-exploration-g/blob/main/src/system_prompt.md">in the repo</a>
 
 That <a href="https://github-spark-docs.simonwillison.net/#platform">Platform overiew</a>, including a <a href="https://github-spark-docs.simonwillison.net/#platform--available-system-tools">complete list of binaries</a> on the Bash path. There are 782 of these! Highlights include <code>rg</code> and <code>jq</code> and <code>gh</code>. 
 A <a href="https://github-spark-docs.simonwillison.net/#best-practices">Best Practices</a> guide that's effectively a summary of some of the tips from the longer form system prompt. 
 
<p>The <a href="https://github-spark-docs.simonwillison.net/#playground">interactive playground</a> is visible on my public site but doesn't work, because it can't call the custom Spark endpoints. You can try <a href="https://system-exploration-g--simonw.github.app/#playground">the authenticated playground</a> for that instead.</p>
 That system prompt in detail 
<p>All of this and we haven't actually dug into the <a href="https://github.com/simonw/system-exploration-g/blob/main/src/system_prompt.md">system prompt</a> itself yet.</p>
<p>I've read <a href="https://simonwillison.net/tags/system-prompts/">a lot of system prompts</a>, and this one is absolutely top tier. I learned a whole bunch about web design and development myself just from reading it!</p>
<p>Let's look at some highlights:</p>
<blockquote>
<p>You are a web coding playground generating runnable code micro-apps ("sparks"). This guide helps you produce experiences that are not only functional but aesthetically refined and emotionally resonant.</p>
</blockquote>
<p>Starting out strong with "aesthetically refined and emotionally resonant"! Everything I've seen Spark produce so far has had very good default design taste.</p>
<blockquote>
<p>Use the available search tools to understand the codebase and the user's query. You are encouraged to use the search tools extensively both in parallel and sequentially, especially when you are starting or have no context of a project.</p>
</blockquote>
<p>This instruction confused me a little because as far as I can tell Spark doesn't have any search tools. I think it must be using <code>rg</code> and <code>grep</code> and the like for this, but since it doesn't reveal what commands it runs I can't tell for sure.</p>
<p>It's interesting that Spark is not a chat environment - at no point is a response displayed directly to the user in a chat interface, though notes about what's going on are shown temporarily while the edits are being made. The system prompt describes that like this:</p>
<blockquote>
<p>You are an AI assistant working in a specialized development environment. Your responses are streamed directly to the UI and should be concise, contextual, and focused. This is not a chat environment, and the interactions are not a standard "User makes request, assistant responds" format. The user is making requests to create, modify, fix, etc a codebase - not chat.</p>
</blockquote>
<p>All good system prompts include examples, and this one is no exception:</p>
<blockquote>
<p>✅ GOOD:</p>
 
 "Found the issue! Your authentication function is missing error handling." 
 "Looking through App.tsx to identify component structure." 
 "Adding state management for your form now." 
 "Planning implementation - will create Header, MainContent, and Footer components in sequence." 
 
<p>❌ AVOID:</p>
 
 "I'll check your code and see what's happening." 
 "Let me think about how to approach this problem. There are several ways we could implement this feature..." 
 "I'm happy to help you with your React component! First, I'll explain how hooks work..." 
 
</blockquote>
<p>The next <a href="https://github.com/simonw/system-exploration-g/blob/main/src/system_prompt.md#design-philosophy">"Design Philosophy" section</a> of the prompt helps explain why the apps created by Spark look so good and work so well.</p>
<p>I won't quote the whole thing, but the sections include "Foundational Principles", "Typographic Excellence", "Color Theory Application" and "Spatial Awareness". These honestly feel like a crash-course in design theory!</p>
<p>OK, I'll quote the full typography section just to show how much thought went into these:</p>
<blockquote>
<p>Typographic Excellence</p>
 
 
Purposeful Typography: Typography should be treated as a core design element, not an afterthought. Every typeface choice should serve the app's purpose and personality. 
 
Typographic Hierarchy: Construct clear visual distinction between different levels of information. Headlines, subheadings, body text, and captions should each have a distinct but harmonious appearance that guides users through content. 
 
Limited Font Selection: Choose no more than 2-3 typefaces for the entire application. Consider San Francisco, Helvetica Neue, or similarly clean sans-serif fonts that emphasize legibility. 
 
Type Scale Harmony: Establish a mathematical relationship between text sizes (like the golden ratio or major third). This forms visual rhythm and cohesion across the interface. 
 
Breathing Room: Allow generous spacing around text elements. Line height should typically be 1.5x font size for body text, with paragraph spacing that forms clear visual separation without disconnection. 
 
</blockquote>
<p>At this point we're not even a third of the way through the whole prompt. It's almost 5,000 words long!</p>
<p>Check out this later section on <a href="https://github.com/simonw/system-exploration-g/blob/main/src/system_prompt.md#finishing-touches">finishing touches</a>:</p>
<blockquote>
<p>Finishing Touches</p>
 
 
Micro-Interactions: Add small, delightful details that reward attention and form emotional connection. These should be discovered naturally rather than announcing themselves. 
 
Fit and Finish: Obsess over pixel-perfect execution. Alignment, spacing, and proportions should be mathematically precise and visually harmonious. 
 
Content-Focused Design: The interface should ultimately serve the content. When content is present, the UI should recede; when guidance is needed, the UI should emerge. 
 
Consistency with Surprise: Establish consistent patterns that build user confidence, but introduce occasional moments of delight that form memorable experiences. 
 
</blockquote>
<p>The remainder of the prompt mainly describes the recommended approach for writing React apps in the Spark style. Some summarized notes:</p>
 
 Spark uses <a href="https://vite.dev/">Vite</a>, with a <code>src/</code> directory for the code. 
 The default Spark template (available in <a href="https://github.com/github/spark-template">github/spark-template</a> on GitHub) starts with an <code>index.html</code> and <code>src/App.tsx</code> and <code>src/main.tsx</code> and <code>src/index.css</code> and a few other default files ready to be expanded by Spark. 
 It also has a whole host of neatly designed default components in <a href="https://github.com/github/spark-template/tree/main/src/components/ui">src/components/ui</a> with names like <code>accordion.tsx</code> and <code>button.tsx</code> and <code>calendar.tsx</code> - Spark is told "directory where all shadcn v4 components are preinstalled for you. You should view this directory and/or the components in it before using shadcn components." 
 A later instruction says "Strongly prefer shadcn components (latest version v4, pre-installed in <code>@/components/ui</code>). Import individually (e.g., <code>import { Button } from "@/components/ui/button";</code>). Compose them as needed. Use over plain HTML elements (e.g., <code>&lt;Button&gt;</code> over <code>&lt;button&gt;</code>). Avoid creating custom components with names that clash with shadcn." 
 There's a handy type definition describing the default <code>spark.</code> API namespace:
  declare global {
  interface Window {
    spark: {
      llmPrompt: (strings: string[], ...values: any[]) =&gt; string
      llm: (prompt: string, modelName?: string, jsonMode?: boolean) =&gt; Promise&lt;string&gt;
      user: () =&gt; Promise&lt;UserInfo&gt;
      kv: {
        keys: () =&gt; Promise&lt;string[]&gt;
        get: &lt;T&gt;(key: string) =&gt; Promise&lt;T | undefined&gt;
        set: &lt;T&gt;(key: string, value: T) =&gt; Promise&lt;void&gt;
        delete: (key: string) =&gt; Promise&lt;void&gt;
      }
    }
  }
}  
 
 The section on theming leans deep into <a href="https://tailwindcss.com/">Tailwind CSS</a> and the <a href="https://github.com/Wombosvideo/tw-animate-css">tw-animate-css</a> package, including a detailed example. 
 Spark is encouraged to start by creating a PRD - a Product Requirements Document - in <code>src/prd.md</code>. Here's <a href="https://github.com/simonw/system-exploration-g/blob/main/src/system_prompt.md#process--output">the detailed process section</a> on that, and here's <a href="https://github.com/simonw/system-exploration-g/blob/main/PRD.md">the PRD for my documentation app</a> (called <code>PRD.md</code> and not <code>src/prd.md</code>, I'm not sure why.) 
 
<p>The system prompt ends with this section on "finishing up":</p>
<blockquote>
<p>Finishing Up</p>
 
 After creating files, use the <code>create_suggestions</code> tool to generate follow up suggestions for the user. These will be presented as-is and used for follow up requests to help the user improve the project. You must do this step. 
 When finished, only return <code>DONE</code> as your final response. Do not summarize what you did, how you did it, etc, it will never be read by the user. Simply return <code>DONE</code>
 
 
</blockquote>
<p>Notably absent from the system prompt: instructions saying not to share details of the system prompt itself!</p>
<p>I'm glad they didn't try to suppress details of the system prompt itself. Like I said earlier, this stuff is the missing manual: my ability to use Spark is greatly enhanced by having read through the prompt in detail.</p>
 What can we learn from all of this? 
<p>This is an extremely well designed and implemented entrant into an increasingly crowded space.</p>
<p>GitHub previewed it in October and it's now in public preview nine months later, which I think is a great illustration of how much engineering effort is needed to get this class of app from initial demo to production-ready.</p>
<p>Spark's quality really impressed me. That 5,000 word system prompt goes a long way to explaining why the system works so well. The harness around it - with a built-in editor, Codespaces and GitHub integration, deployment included and custom backend API services - demonstrates how much engineering work is needed outside of a system prompt to get something like this working to its full potential.</p>
<p>When <a href="https://simonwillison.net/2024/Nov/25/leaked-system-prompts-from-vercel-v0/">the Vercel v0 system prompt leaked</a> Vercel's CTO Malte Ubl said:</p>
<blockquote>
<p>When @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity.</p>
<p>We completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual</p>
</blockquote>
<p>I would love to see the evals the Spark team used to help iterate on their epic prompt!</p>
 Spark features I'd love to see next 
<p>I'd love to be able to make my Spark apps available to unauthenticated users. I had to figure out how to build and deploy the app separately just so I could link to it from this post.</p>
<p>Spark's current deployment system provides two options: just the app owner or anyone with a GitHub account. The UI says that access to "All members of a selected organization" is coming soon.</p>
<p>Building and deploying separately had added friction due to the proprietary <code>@github/spark</code> package. I'd love an open source version of this that throws errors about the APIs not being available - that would make it much easier to build the app independently of that library.</p>
<p>My biggest feature request concerns that key/value API. The current one is effectively a global read-write database available to any user who has been granted access to the app, which makes it unsafe to use with the "All GitHub users" option if you care about your data being arbitrarily modified or deleted.</p>
<p>I'd like to see a separate key/value API called something like this:</p>
  spark: {
  userkv: {
    keys: () =&gt; Promise&lt;string[]&gt;
    get: &lt;T&gt;(key: string) =&gt; Promise&lt;T | undefined&gt;
    set: &lt;T&gt;(key: string, value: T) =&gt; Promise&lt;void&gt;
    delete: (key: string) =&gt; Promise&lt;void&gt;
  }
}  
<p>This is the same design as the existing <code>kv</code> namespace but data stored here would be keyed against the authenticated user, and would not be visible to anyone else. That's all I would need to start building applications that are secure for individual users.</p>
<p>I'd also love to see deeper integration with the GitHub API. I tried building an app to draw graphs of my open issues but it turned there wasn't a mechanism for making authenticated GitHub API calls, even though my identity was known to the app.</p>
<p>Maybe a <code>spark.user.githubToken()</code> API method for retrieving a token for use with the API, similar to how <code>GITHUB_TOKEN</code> works in GitHub Actions, would be a useful addition here.</p>
<p><a href="https://reinout.vanrees.org/weblog/2010/05/25/no-bad-pony.html">Pony requests</a> aside, Spark has really impressed me. I'm looking forward to using it to build all sorts of fun things in the future.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/github">github</a>, <a href="https://simonwillison.net/tags/javascript">javascript</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/react">react</a>, <a href="https://simonwillison.net/tags/typescript">typescript</a>, <a href="https://simonwillison.net/tags/prompt-engineering">prompt-engineering</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/llm-tool-use">llm-tool-use</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/system-prompts">system-prompts</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://static.simonwillison.net/static/2025/spark-ui.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:21 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/24/recurse-center/#atom-everything'>Quoting Recurse Center</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    [...] You learn best and most effectively when you are learning something that you care about. Your work becomes meaningful and something you can be proud of only when you have chosen it for yourself. This is why our second self-directive is to build your volitional muscles. Your volition is your ability to make decisions and act on them. To set your own goals, choose your own path, and decide wh
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <blockquote cite="https://www.recurse.com/blog/191-developing-our-position-on-ai#footnote-return-p191f6"><p>[...] You learn best and most effectively when you are learning something that you care about. Your work becomes meaningful and something you can be proud of only when you have chosen it for yourself. This is why our second self-directive is to build your volitional muscles. Your volition is your ability to make decisions and act on them. To set your own goals, choose your own path, and decide what matters to you. Like physical muscles, you build your volitional muscles by exercising them, and in doing so you can increase your sense of what’s possible.</p>
<p>LLMs are good at giving fast answers. They’re not good at knowing what questions you care about, or which answers are meaningful. Only you can do that. You should use AI-powered tools to complement or increase your agency, not replace it.</p></blockquote>
<p>— <a href="https://www.recurse.com/blog/191-developing-our-position-on-ai#footnote-return-p191f6">Recurse Center</a>, Developing our position on AI</p>

    <p>Tags: <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/education">education</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   07:41 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2025/07/7446/'>総務省: ICTサービスの利用を巡る諸問題に対する利用環境整備に関する 報告書（案）についての意見募集(8/4〆切)</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  「ICTサービスの利用環境の整備に関する研究会」について 総務省の「ICTサービスの利用環境の整備に関する研究会」が令和7年7月にまとめた「ICTサービスの利用を巡る諸問題に対する利用環境整備に関する報告書（案）」がパブコメにかかっています(e-Govのパブコメ投入サイト）。〆切…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 「ICTサービスの利用環境の整備に関する研究会」について 



<p>総務省の<a href="https://www.soumu.go.jp/main_sosiki/kenkyu/ICT_services/index.html">「ICTサービスの利用環境の整備に関する研究会」</a>が令和7年7月にまとめた<a href="https://www.soumu.go.jp/main_content/001019332.pdf">「ICTサービスの利用を巡る諸問題に対する利用環境整備に関する報告書（案）」</a>が<a href="https://www.soumu.go.jp/menu_news/s-news/01kiban18_01000257.html">パブコメ</a>にかかっています(<a href="https://public-comment.e-gov.go.jp/pcm/detail?CLASSNAME=PCMMSTDETAIL&amp;id=145210533&amp;Mode=0">e-Govのパブコメ投入サイト</a>）。〆切は8月4日です。日がありませんね。告知が遅れてごめんなさい<a href="https://www.sakimura.org/2025/07/7446/#easy-footnote-bottom-1-7446" title="パブコメ期間は7月5日（土）から同年8月4日（月）まで">1</a>。親会はわれらが宍戸先生を座長として、以下のようなメンバーで執り行われています。</p>



 「ICTサービスの利用環境の整備に関する研究会」構成員 



 
 （座長代理）大谷 和子 株式会社日本総合研究所 執行役員 法務部長 



 木村 たま代 主婦連合会 事務局長 



 （座長）宍戸 常寿 東京大学大学院 法学政治学研究科 教授 



 中原 太郎 東京大学大学院 法学政治学研究科 教授 



 森 亮二 英知法律事務所 弁護士 



 山本 龍彦 慶應義塾大学大学院 法務研究科 教授 
 



<p>また、「不適正利用対策に関するワーキンググループ」と「利用者情報に関するワーキンググループ」それに「通信ログ保存の在り方に関するワーキンググループ」と３つのワーキンググループがあり、それぞれ以下の先生方が構成員になっておられます。</p>



 不適正利用対策に関するワーキンググループ構成員 



<p>（主査）大谷 和子 株式会社日本総合研究所 執行役員 法務部長<br>沢田 登志子 一般財団法人 EC ネットワーク 理事<br>鎮目 征樹 学習院大学 法学部 教授<br>辻 秀典 デジタルアイデンティティ推進コンソーシアム 代表理事<br>仲上 竜太 日本スマートフォンセキュリティ協会 技術部会 部会長<br>中原 太郎 東京大学大学院 法学政治学研究科 教授<br>星 周一郎 東京都立大学 法学部 教授<br>山根 祐輔 片岡総合法律事務所 弁護士<br>【オブザーバー】<br>警察庁 刑事局 捜査支援分析管理官<br>警察庁 サイバー警察局 サイバー企画課</p>



 利用者情報に関するワーキンググループ構成員 



<p>生貝 直人 一橋大学大学院 法学研究科 教授<br>江藤 祥平 一橋大学大学院 法学研究科 教授<br>太田 祐一 株式会社 DataSign 代表取締役社長<br>木村 たま代 主婦連合会 事務局長<br>寺田 眞治 一般財団法人日本情報経済社会推進協会 客員研究員<br>森 亮二 英知法律事務所 弁護士<br>（主査）山本 龍彦 慶應義塾大学大学院 法務研究科 教授<br>呂 佳叡 森･濱田松本法律事務所 弁護士<br>【オブザーバー】<br>個人情報保護委員会事務局</p>



 「通信ログ保存の在り方に関するワーキンググループ」構成員 



<p>（主査）鎮目 征樹 学習院大学法学部 教授<br>梅本 大祐 英知法律事務所 弁護士<br>小林 央典 ＴＭＩ総合法律事務所 弁護士<br>宍戸 常寿 東京大学大学院法学政治学研究科 教授<br>曽我部 真裕 京都大学大学院法学研究科 教授<br>巽 智彦 東京大学大学院法学政治学研究科 准教授<br>森 亮二 英知法律事務所 弁護士<br>【オブザーバー】<br>警察庁 刑事局 捜査支援分析管理官<br>警察庁 サイバー警察局 サイバー企画課</p>



<p>われらがMyDataJapan関係者もたくさん入っておられます。また、プラットフォーム関する研究会からのスライドの先生方も多いですね。</p>



 第２部 携帯電話の本人確認のルール 



<p>このブログの読者の方に特に興味があるのは「全体」と言っても過言ではないでしょうが、わけても第2章のところでしょうかね。「第２部 携帯電話の本人確認のルール」です。曰く（以下、下線は筆者）</p>



<p></p>



<blockquote>
<p>１ SIM の不正転売の防止: <br>◯ 不正転売の違法性について政府及び事業者が利用者に対してわかりやすい周知啓発を一層強化 (P.14)<br>◯ 事業者における取組の推進については、不正転売を難しくするような携帯電話契約・端末割賦契約時の与信時の審査強化などの仕組みの導入や事業者による定期的な本人確認 (P.14)</p>



<p></p>
</blockquote>



<blockquote>
<p>２ 法人の代理権（在籍確認）<br>◯ 担当者と法人の関係性を明らかにするために最低限必要な書類の提出を求めるなど、所要の規定見直し（携帯電話不正利用防止法施行規則第4 条）が必要である。最低限必要な書類については、電子的な書類も排除されない (P.15) </p>
</blockquote>



<p></p>



<blockquote>
<p>３ 他社の本人確認結果への依拠<br>◯ （昨年度検討）、「過去の本人確認結果に依拠する方法については、事業者のニーズや本人確認の保証レベルとのバランスを鑑みつつ総合的に検討することが適当である (P.16)<br>◯ 事業者提案１: 金融機関への依拠スキーム（以下、図６参照）(P.16)<br>◯ 事業者提案２: 携帯音声通信事業者同士の依拠のスキーム〜特に携帯音声通信事業者への依拠については、事業者からも具体的なニーズが認められている（以下、図７参照）(P.17)<br>◯ 他社への本人確認結果に依拠することは、ID/PASS による簡易な方法での本人確認を許容する契約形態を突いた不正契約が行われていること、金融機関への依拠については業界横断的な取組が必要であること、また、携帯音声通信事業者への依拠については、本人確認の保証レベルを上げる取組が未だ途上の段階であることに留意が必要で、依拠先の本人確認の保証レベルが高く最新の本人特定事項となっていることや、依拠元の当人確認が適切に行われることなど、依拠が適切にできる要件を整理した上でルール整備を行うことも視野に、改めて本ワーキンググループにおいて検討を深めていくことも考えられる</p>
</blockquote>



<p></p>



<blockquote>
<p>４ 追加回線の本人確認<br>デジタル庁の本人確認の手法に関するガイドラインも参考に、厳格化に向けた規定の見直し（携<br>帯電話不正利用防止法施行規則第 3 条第３・４項、同規則第 19 条第５項等）が必要</p>
</blockquote>



<p></p>



<p>デジタル庁の本人確認の手法に関するガイドラインは改定にわたしも関わっていたし特に言うことはないとして、ここで、特にわたしの興味をひいたのが、法人の代理権ですかね。このあたりは、OpenID for Identity Assurance （もうすぐISO規格になるし）とか、それを使ったGビズIDとかがうまく使えるのではないかと思います。</p>



<p></p>



<p>３の本人確認結果の依拠に関しては、トラストフレームワークの整備と、当該アイデンティティがどのようにいつだけによって本人確認されたのかという情報（メタデータ）も依拠当事者にとってはとても重要になるのは海外の事例からもわかることで、OpenID for Identity Assurance などはそもそもそのニーズから来ていますね。IALとかAALで数値で表すだけだと、freshness だとかプロセスがきちんと回っているかとか、そのプロセスの監査状態とかわからないんで、責任を取らされる側＝依拠当事者としては辛いんですね。</p>



<p>なお、上記で（図６）（図７）と出てくるのが以下の図です。ちょっと「ゲートウェイ」とかったりして「うーん、なんだろなー」と思ったりもしますが、ご参考までに乗せておきます。（普通に、OIDCとかVCとかでやったら？と思う）</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2025/07/image.png?ssl=1"></a>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2025/07/image-1.png?ssl=1"></a>



 通信ログ保存の在り方に関するワーキンググループ 



<p>さて、ここまでのところは、「深めていくことが適当」とか「見直しが必要」のような書きぶりになっていましたが、このWGでは「改正案」が出されています。曰く、</p>



<blockquote>
<p>２ 改正案<br>本ガイドラインの改正案（以下「本改正案」という。）は、別添のとおりであ<br>り、以下補足する。<br>⑴ 概要<br>本改正案については、ＣＰ及びＡＰは、各サービス内容に応じた業務の<br>遂行上必要な通信履歴を対象として、少なくとも３～６か月程度保存して<br>おくことが、誹謗中傷等の違法・有害情報への対策のための社会的な期待<br>に応える望ましい対応であり、同対応のために通信履歴を同期間保存する<br>ことは、電気通信事業法上の通信の秘密との関係で許容されるとの考え方<br>を示すものである。<br> （中略）<br>⑶ 保存期間<br>現時点の本ガイドラインは、接続認証ログを対象として、保存すること<br>が許容される期間として６か月程度（より長期の保存をする業務上の必要<br>性がある場合に１年程度）を示すものであるが、本改正案では、保存するこ<br>とが望ましい期間（少なくとも３～６か月程度）を新たに示すものであり、<br>望ましい期間を超えた保存を行うことも、業務の遂行上の必要性がある場<br>合には、これまでどおり許容される。なお、本改正案に違反したことをもっ<br>て直ちに法的責任が生じるものではない。</p>



<p>これ以降に、具体的な「電気通信事業における個人情報等の保護に関するガイドラインの解説」の改正箇所の紹介もありますので、ご興味のある方は直接お読みいただくのが良いと思います。</p>
</blockquote>



<p></p>



 NoLangによる動画解説 



<p>そんなに長い文書ではないのですが、動画で見たいという方には、NoLangで自動動画作成させましたので、乗せておきます。</p>



 

 



 最後に、NotebookLMによるブリーフィング資料を乗せておきます。 



<p>以下のブリーフィング資料は、総務省の「ICTサービスの利用環境の整備に関する研究会」が令和7年7月にまとめた<a href="https://www.soumu.go.jp/main_content/001019332.pdf">「ICTサービスの利用を巡る諸問題に対する利用環境整備に関する報告書（案）」</a>の主要なテーマ、重要なアイデア、および事実をNotebookLMの機能によって紹介するものです。</p>



 1. はじめに：ICTサービスを巡る喫緊の課題と本報告書の目的 



<p>本報告書は、ICTサービスの拡大に伴う多様な課題、特に「利用者情報の不適切な取扱い、不適正利用への対処、各種違法・有害情報への対策」を検討する目的で作成されました。2024年における財産犯の被害額は4,000億円を超え、その大部分が詐欺によるものであり、特に通信サービスの不正利用が深刻な問題となっています。また、スマートフォンアプリにおけるプライバシー、セキュリティ、および青少年保護の確保も喫緊の課題とされています。本報告書は、これらの課題に対し、「官民の関係者における今後のさらなる取組の一助となること」を期待しています。</p>



 2. 不適正利用対策に関するワーキンググループ 



<p>このワーキンググループは、電気通信の不適正利用対策に焦点を当て、特に以下の環境変化とそれに対する対策を議論しています。</p>



 2.1. 検討の背景：変化する犯罪環境 



 
 「闇バイト」犯罪の増加：SNSやインターネット掲示板を通じた「闇バイト」の募集が増加し、詐欺や強盗の実行犯として利用されるケースが多発しています。電気通信、特に携帯電話の不正SIM転売やSNSでの募集が悪用されています。 



 特殊詐欺の深刻化：2024年の特殊詐欺の認知件数・被害額は過去最悪を記録し、認知件数20,987件、被害額721.5億円に達しました。犯罪グループからの接触手段の約8割が電話であり、近年は特に国際電話が悪用されるケースが急増しています。 



 「令和５年７月頃から、国際電話番号が急増。」 



 犯罪行為の巧妙化・高度化：生成AIを悪用した不正アクセスや、大量のIDとパスワードの組み合わせを用いた不正な回線契約が発覚しており、技術の進歩が犯罪にも利用される新たな脅威となっています。特に、追加回線の本人確認が不要であるという事業者のルールを悪用した事例が報告されています。 
 



 2.2. 携帯電話の本人確認ルールに関する課題と検討 



<p>携帯電話不正利用防止法に基づく本人確認の厳格化が進められているものの、以下の6点が課題として挙げられ、議論が行われました。</p>



 
 SIMの不正転売：
 
 青少年が「闇バイト」としてSIMの不正転売に関与し、詐欺などに転用されるケースが増加しています。事業者による店頭での注意喚起や重要事項説明は実施されていますが、見た目は正当な申し込みであるため発見が困難です。
 
 今後の方向性として、「不正転売の違法性について政府及び事業者が利用者に対してわかりやすい周知啓発を一層強化していくこと」に加え、「事業者による不正検知が困難である中、犯罪抑止の観点から当面取りうる対策として、不正転売の違法性について政府及び事業者が利用者に対してわかりやすい周知啓発を一層強化していくこと」や「事業者による定期的な本人確認」が提言されています。 
 
 
 
 



 法人の代理権（在籍確認）：
 
 法人契約時において、来店する担当者と法人の関係性を担保する代理権の確認が法令上求められておらず、事業者ごとの対応にばらつきがあります。 



 「来店する担当者と法人の関係性を明らかにするために最低限必要な書類の提出を求めるなど、所要の規定見直し（携帯電話不正利用防止法施行規則第4条）が必要」とされています。 
 
 



 他社の本人確認結果への依拠：
 
 他社の本人確認結果への依拠は利便性の向上に寄与するものの、ID/PASSによる簡易な方法を悪用した不正契約のリスクが指摘されています。 



 「今後の方向性としては、依拠先の本人確認の保証レベルが高く最新の本人特定事項となっていることや、依拠元の当人確認が適切に行われることなど、依拠が適切にできる要件を整理した上でルール整備を行うことも視野に、改めて本ワーキンググループにおいて検討を深めていくことも考えられる。」 
 
 



 追加回線の本人確認：
 
 2回線目以降の契約において簡易な本人確認方式が認められていますが、これが不正契約の起点となる事例が報告されています。 



 「簡易な本人確認手法には一定の利便性が認められる一方、現にそのような手法が犯罪の起点となっている点を踏まえれば、当人認証性を向上させるべく、デジタル庁の本人確認の手法に関するガイドラインも参考に、厳格化に向けた規定の見直し（携帯電話不正利用防止法施行規則第3条第３・４項、同規則第19条第５項等）が必要である。」 
 
 



 上限契約台数：
 
 現行法令上、上限契約台数に制限はありませんが、台数上限がないことを悪用した大量不正契約の事例が報告されています。 



 「原則５台の制限を超えての例外的な契約について、使用用途の事前の確認をする一部の事業者がいることを踏まえ、事業者における自主的な取組を一層強化すべきである。その上で、今後、少なくともそうした事業者の自主的な取組のルールの適用状況について検証を行い、更にその取組を促進するとともに、必要に応じて、犯罪との因果関係を踏まえながら、何らかのルール化について検討すべきである。」 
 
 



 データSIMの本人確認：
 
 データSIMは携帯電話不正利用防止法の対象外ですが、SMS付きデータSIMが悪用された詐欺事例が多数報告されており、義務化の検討が急務とされています。 



 「悪用の実態が確認されたことを踏まえ、一部の事業者で既に自主的に行われている本人確認の取組を確実に行う観点から、義務化について検討すべきである。ただし、義務化を検討するにあたっては、貸与時の本人確認の規律も参考に、対象SIMや利用用途（訪日外国人やIoT機器）等に関して、不正利用を防止しようとするあまり、過剰規制に陥ることのないよう、利便性へのバランスの観点から利用実態や実効性に配慮した規定とするべきである。」 
 
 
 



 2.3. その他の特殊詐欺の電話・メール等対策 



 
 固定・携帯電話、SMS・メール対策：
 
 国際電話不取扱受付センターの周知強化と運営改善、および総務省の迷惑電話対策相談センターとの官民連携が求められています。また、事業者による迷惑電話・SMS・メール対策サービスのさらなる低廉化とデフォルト設定化が期待されています。 
 
 



 スプーフィング：
 
 電話番号を偽装する手口に対する注意喚起の推進と、通信事業者との連携による効果的な対策の継続的な検討が必要です。 
 
 



 海外電話番号による詐欺電話：
 
 日本から簡単に海外電話番号を取得できるアプリが悪用されている現状に対し、注意喚起の推進と実態把握の継続が求められています。 
 
 
 



 3. 通信ログ保存の在り方に関するワーキンググループ 



<p>このワーキンググループは、通信の秘密の保護と犯罪捜査・被害者救済のバランスを取りながら、通信履歴の保存期間のあり方を検討しました。</p>



 3.1. 現状の課題と検討の経緯 



 
 通信履歴は通信の秘密として保護されるため、電気通信事業者が記録・保存するには利用者の同意か、正当業務行為としての違法性阻却が必要です。 



 現状の「電気通信事業における個人情報等の保護に関するガイドライン」では、課金、料金請求、不正利用防止などの業務遂行に必要な場合に限り、最小限の通信履歴を記録・保存できるとされています。接続認証ログは通常6ヶ月、最長1年程度の保存が許容されています。 



 近年、「闇バイト」募集投稿などの違法情報の流通や、誹謗中傷による権利侵害が増加しており、発信者情報開示請求や犯罪捜査の観点から、通信履歴の保存期間が短いという指摘があります。 
 



 3.2. 改正案の概要と趣旨 



 
 本改正案では、コンテンツプロバイダ（CP）およびアクセスプロバイダ（AP）に対し、「少なくとも３～６か月程度保存しておくことが、誹謗中傷等の違法・有害情報への対策のための社会的な期待に応える望ましい対応であり、同対応のために通信履歴を同期間保存することは、電気通信事業法上の通信の秘密との関係で許容されるとの考え方を示す」ものです。 



 特に、被害者救済の観点から、この期間の通信履歴保存が不可欠であるとされています。これは直ちに法的責任を生じるものではありませんが、社会的な期待に応える望ましい対応と位置付けられています。 



 今後の検討課題として、本改正案の適用開始後の効果検証と、解決につながらない場合の「法的担保を含め本ガイドラインの改正以外の方法で検討すること」が挙げられています。 
 



 4. 利用者情報に関するワーキンググループ 



<p>このワーキンググループは、「スマートフォン プライバシー イニシアティブ（SPSI）」の改定を中心に、利用者情報の適正な取扱い、セキュリティ確保、および青少年保護に関する検討を行いました。</p>



 4.1. 検討の背景とSPSIの改定 



 
 「スマートフォン プライバシー イニシアティブ（SPI）」は、スマートフォンアプリによる利用者情報の不適切な外部送信問題に対応するため、2012年に策定され、複数回改定されてきました。 



 今回の改定では、スマートフォン利用の低年齢化・長時間化、SNS等でのプライバシー侵害事例の増加を受け、新たに「青少年保護」がSPSIの対象に加わりました。 



 SPSIの位置づけについても議論され、法令上の義務に加えて「ベンチマーク事項」「望ましい事項」「基本的事項」の4段階で、関係事業者に求められる取り組みの度合いが整理されました。 



 ベンチマーク事項：「～することが期待される」 



 望ましい事項：「～することが望ましい」 



 基本的事項：「～することが強く求められる」 



 法令事項：「～しなければならない」「～してはならない」 
 



 4.2. 青少年保護への取り組み 



 
 青少年の利用者情報やプライバシー保護を通じたスマートフォンアプリおよび関連サービスの安全・安心な利用を図るため、各事業者が取り組むべき望ましい事項がSPSIに追記されました。 



 アプリ提供者：不適切コンテンツ報告機能、ユーザーブロック機能の設置、重要な判断（情報提供、課金など）における保護者の関与に関する仕組みの提供が求められます。 



 アプリストア運営事業者：アプリの審査、年齢制限設定（レーティング）基準の策定・確認、青少年向けアプリ専用分類の設置、アプリ掲載拒否時の迅速かつ適切なフィードバックが求められます。 



 OS提供事業者：アプリストア運営事業者の取り組みの確認、適切な説明・情報提供、ペアレンタルコントロール機能の提供が求められます。 
 



 4.3. ウェブサイトに係る調査・検討 



 
 従来のSPSIはスマートフォンアプリに焦点を当てていましたが、ウェブサイトにおける利用者情報の取扱いも重要であるとの認識から、ウェブサイトへの対象拡大が検討されました。 



 調査の結果、アプリとブラウザでは技術的に取得可能な情報や利用目的において大きな差異がないことが確認されました。 



 しかし、ウェブサイトはOS事業者やアプリストアによる審査がなく、中小企業や個人運営のサイトも多数存在するため、SPSIの広範な事項をそのまま適用することには課題があると考えられています。 



 今後の課題として、「外部送信を含むウェブサイトの課題について、ウェブサイト運営者に対してどのような形でベストプラクティスを確保していくか、今後の課題として、SPSIとの関係も含めて、速やかに検討を行うことが適当である」とされています。 



 スマートフォン以外のデバイス（タブレット、スマートウォッチ等）の利用者情報に関するSPSIの対象スコープ拡大についても、引き続き検討が進められる予定です。 
 



<p>この報告書は、急速に変化するICT環境において、利用者保護と安全なサービス利用環境の整備に向けた、多角的な課題と具体的な対策の方向性を示しています。NotebookLM は不正確な場合があります。回答は再確認してください。</p>



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/www.sakimura.org/wp-content/uploads/2025/07/image.png?resize=546%2C251&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   06:39 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/24/i-drank-every-cocktail/#atom-everything'>I Drank Every Cocktail</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   I Drank Every Cocktail 
Adam Aaronson drank his way through all 102 cocktails on the IBA cocktails list - published by the International Bartenders Association since 1961, with the most recent update in 2024. 
 Adam's write up is delightful, incorporating pedantry, data nerdery, a trip to the Internet Archive, some excellent bar recommendations in New York and London and hints at elicit rum smuggl
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://aaronson.org/blog/i-drank-every-cocktail">I Drank Every Cocktail</a></p>
Adam Aaronson drank his way through all 102 cocktails on the <a href="https://iba-world.com/cocktails/all-cocktails/">IBA cocktails list</a> - published by the International Bartenders Association since 1961, with the most recent update <a href="https://en.m.wikipedia.org/wiki/List_of_IBA_official_cocktails#2024">in 2024</a>.<p></p>
<p>Adam's write up is delightful, incorporating pedantry, data nerdery, a trip to the Internet Archive, some excellent bar recommendations in New York and London and hints at elicit rum smuggling to help make the final cocktail, the IBA Tiki, using two different Havana Club rums that are illegal in the USA thanks to import restrictions.

    </p><p>Via <a href="https://waxy.org/2025/07/adam-aaronson-drank-every-cocktail/">Andy Baio</a></p>


    <p>Tags: <a href="https://simonwillison.net/tags/cocktails">cocktails</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   00:27 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 23. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/23/instagram-reel-veo-3-paid-preview/#atom-everything'>Instagram Reel: Veo 3 paid preview</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Instagram Reel: Veo 3 paid preview 
@googlefordevs on Instagram published this reel featuring Christina Warren with prompting tips for the new Veo 3 paid preview (mp4 copy here). 
  
 (Christine checked first if I minded them using that concept. I did not!)


      Tags: google, ai, generative-ai, gemini, pelican-riding-a-bicycle, text-to-video 
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://static.simonwillison.net/static/2025/veo-3-pelican.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://www.instagram.com/googlefordevs/reel/DMblrKYuTHH/">Instagram Reel: Veo 3 paid preview</a></p>
@googlefordevs on Instagram published this reel featuring Christina Warren with prompting tips for the new Veo 3 paid preview (<a href="https://static.simonwillison.net/static/2025/googlefordevs-veo3.mp4">mp4 copy here</a>).<p></p>
<p></p>
<p>(Christine checked first if I minded them using <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle/">that concept</a>. I did not!)


    </p><p>Tags: <a href="https://simonwillison.net/tags/google">google</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/text-to-video">text-to-video</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://static.simonwillison.net/static/2025/veo-3-pelican.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:08 &bull;
   4 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/measuring-the-impact-of-ai-on-software'>Measuring the impact of AI on software engineering – with Laura Tacho</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Laura Tacho, CTO of DX, shares findings from 180+ companies on how AI is really impacting dev productivity, what most teams get wrong, and why measuring dev experience first is critical.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Stream the Latest Episode <p>Listen and watch now on <a href="https://youtu.be/xHHlhoRC8W4">YouTube</a>,  <a href="https://open.spotify.com/episode/4hDowtDMT7CcWg89awjRy1">Spotify</a> and <a href="https://podcasts.apple.com/us/podcast/the-pragmatic-engineer/id1769051199">Apple</a>. See the episode transcript at the top of this page, and timestamps for the episode at the bottom.</p> Brought to You By   <p><a href="http://statsig.com/pragmatic">Statsig</a> <a href="https://modal.com/pragmatic">⁠</a> — <a href="https://modal.com/pragmatic">⁠</a> The unified platform for flags, analytics, experiments, and more.</p>  <p><a href="https://gt.dev/pragmatic">Graphite</a> — The AI developer productivity platform.</p>  <p>—</p> In This Episode <p>There’s no shortage of bold claims about AI and developer productivity, but how do you separate signal from noise?</p><p>In this episode of The Pragmatic Engineer, I’m joined by <a href="https://www.linkedin.com/in/lauratacho/">Laura Tacho</a>, CTO at DX, to cut through the hype and share how well (or not) AI tools are actually working inside engineering orgs. Laura shares insights from DX’s research across 180+ companies, including surprising findings about where developers save the most time, why devs don’t use AI at all, and what kinds of rollouts lead to meaningful impact.</p><p>We also discuss:</p>  <p>The problem with oversimplified AI headlines and how to think more critically about them</p>  <p>An overview of the DX AI Measurement framework</p>  <p>Learnings from Booking.com’s AI tool rollout</p>  <p>Common reasons developers aren’t using AI tools</p>  <p>Why using AI tools sometimes decreases developer satisfaction</p>  <p>Surprising results from DX’s 180+ company study</p>  <p>How AI-generated documentation differs from human-written docs</p>  <p>Why measuring developer experience before rolling out AI is essential</p>  <p>Why Laura thinks roadmaps are on their way out</p>  <p>And much more!</p>       Interesting ideas from the conversation <p>Here are three interesting observations that came in this conversation:</p> Idea #1: Code is a liability! <blockquote><p>Laura (<a href="https://youtu.be/xHHlhoRC8W4?si=-mQUEgfJbKMCJHYT&amp;t=963">at 16:04</a>): “One of my more controversial opinions is that source code is a liability. It sounds controversial — but then when people think about it, they realize, yeah, it actually is a liability!</p><p>We're in a world where it is trivially easy to produce a tremendous amount of source code. And so what does that actually mean for productivity and business impact when what could have been written in one line is now written in five lines? </p><p>Do we really want to measure AI impact in terms of lines of code generated? I certainly don't. </p><p>We did not include acceptance rate in our framework for good reason. I think it does give insight into whether the tools are fit for purpose. But when we're looking at broadly measuring business impact and the impact on developer experience and the impact on the business, acceptance rate is just such a tiny part of the story.”</p></blockquote> Idea #2: Roadmaps are on their way out <blockquote><p>Gergely (<a href="https://youtu.be/xHHlhoRC8W4?si=TFQQ-yskqu3c2HNY&amp;t=3332">at 55:45</a>) “As an end user of a customer of a company that has invested heavily in AI made that be Google, Microsoft, a startup. What should you be seeing?</p><p>Laura: “As an end user, what I expect is faster time to market. And that's really on the other side, on the building side, what we're trying to emphasize and what a lot of our conversations have focused on with other executives and engineering leaders. We're really trying to reduce the amount of time to market. This has a lot of implications. </p><p>Another one of my unconventional opinions, I think roadmaps are on their way out in the age of AI. Companies that are going to win with AI are not ones that think about things in roadmap sequential form, but think about it more as experiment portfolios. And so rapid experimentation and trying to figure out what delights your customers is going to help companies win. </p><p>The companies that will win are the ones that focus or that already have muscles to do experimentation, A/B tests, trying to figure out, you know, how to delight their customers. </p><p>As an end user, what I don't want is thrashing as an end user. And I could see that happening. It's just like, we're just gonna, cause now there are fewer reasons to say no to things. And there's probably good reason that some of those things weren't built.”</p></blockquote> Idea #3: AI time savings are smaller than most people assume <blockquote><p>Laura (<a href="https://youtu.be/xHHlhoRC8W4?si=uS838DmHRq_lr1Ns&amp;t=1651">at 27:34</a>): “Typing speed has never been the bottleneck in development. Now we have all this code generated faster than we can type. That's great. But it still takes me time to review that code!  </p><p>It's not that we're saving time because we don't have to type. A lot of that time, we're just reallocating to reviewing or other parts of code authoring that's not typing. (…)</p><p>The industry average [of spending time on coding] is like 25%. There was a study at AWS that an average AWS engineer only spends 20 % of their time coding. And so when we apply AI to the coding tasks, we're only working with 20 % of that time to begin with. And when we save 10 % of that time, that actually doesn't amount to, ‘we can, you know, ship 10 new product lines overnight!’ ”</p></blockquote> The Pragmatic Engineer deepdives relevant for this episode   <p><a href="https://newsletter.pragmaticengineer.com/p/ai-engineering-in-the-real-world">AI Engineering in the real world</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/engineering-productivity">Measuring software engineering productivity</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/the-ai-engineering-stack">The AI Engineering stack</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/developer-productivity-a-new-framework">A new way to measure developer productivity – from the creators of DORA and SPACE</a></p>   Timestamps <p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4">00:00</a>) Intro</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=83s">01:23</a>) Laura’s take on AI overhyped headlines</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=646s">10:46</a>) Common questions Laura gets about AI implementation</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=709s">11:49</a>) How to measure AI’s impact</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=912s">15:12</a>) Why acceptance rate and lines of code are not sufficient measures of productivity</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=1083s">18:03</a>) The Booking.com case study</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=1237s">20:37</a>) Why some employees are not using AI</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=1460s">24:20</a>) What developers are actually saving time on</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=1754s">29:14</a>) What happens with the time savings</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=1870s">31:10</a>) The surprising results from the DORA report on AI in engineering</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=2024s">33:44</a>) A hypothesis around AI and flow state and the importance of talking to developers</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=2159s">35:59</a>) What’s working in AI architecture</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=2542s">42:22</a>) Learnings from WorkHuman’s adoption of Copilot</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=2820s">47:00</a>) Consumption-based pricing, and the difficulty of allocating resources to AI</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=3121s">52:01</a>) What DX Core 4 measures</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=3332s">55:32</a>) The best outcomes of implementing AI</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=3536s">58:56</a>) Why highly regulated industries are having the best results with AI rollout</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=3630s">1:00:30</a>) Indeed’s structured AI rollout</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=3862s">1:04:22</a>) Why migrations might be a good use case for AI (and a tip for doing it!)</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=4050s">1:07:30</a>) Advice for engineering leads looking to get better at AI tooling and implementation</p><p>(<a href="https://www.youtube.com/watch?v=xHHlhoRC8W4&amp;t=4129s">1:08:49</a>) Rapid fire round</p> References <p>Where to find Laura Tacho:</p><p>• X: <a href="https://x.com/rhein_wein">https://x.com/rhein_wein</a></p><p>• LinkedIn: <a href="https://www.linkedin.com/in/lauratacho/">https://www.linkedin.com/in/lauratacho/</a></p><p>• Website: <a href="https://lauratacho.com/">https://lauratacho.com/</a></p><p>• Laura’s course (Measuring Engineering Performance and AI Impact) <a href="https://lauratacho.com/developer-productivity-metrics-course">https://lauratacho.com/developer-productivity-metrics-course</a></p><p>Mentions during the episode:</p><p>• AI Measurement Framework: <a href="https://getdx.com/research/measuring-ai-code-assistants-and-agents/">https://getdx.com/research/measuring-ai-code-assistants-and-agents/</a></p><p>• Are Coders' Jobs At Risk? AI's Impact On The Future Of Programming: <a href="https://www.forbes.com/sites/sylvainduranton/2024/04/15/are-coders-jobs-at-risk-ais-impact-on-the-future-of-programming/">https://www.forbes.com/sites/sylvainduranton/2024/04/15/are-coders-jobs-at-risk-ais-impact-on-the-future-of-programming/</a></p><p>• Guide to AI-Assisted Engineering: <a href="https://getdx.com/guide/ai-assisted-engineering/">https://getdx.com/guide/ai-assisted-e</a></p><p><a href="https://getdx.com/guide/ai-assisted-engineering/">Engineering/</a></p><p>• DORA's Impact of Generative AI in Software Development report: <a href="https://dora.dev/publications/#impact-of-generative-ai-in-software-development">https://dora.dev/publications/#impact-of-generative-ai-in-software-development</a></p><p>• Booking Case Study: <a href="https://getdx.com/customers/booking-uses-dx-to-measure-impact-of-genai/">https://getdx.com/customers/booking-uses-dx-to-measure-impact-of-genai/</a></p><p>• AI coding assistants wave goodbye to junior developers: <a href="https://www.cio.com/article/3509174/ai-coding-assistants-wave-goodbye-to-junior-developers.html">https://www.cio.com/article/3509174/ai-coding-assistants-wave-goodbye-to-junior-developers.html</a></p><p>• OpenAI Just Released a Coding Tool to ‘Help’ Programmers (Replace Their Jobs, Probably): <a href="https://gizmodo.com/openai-just-released-a-coding-tool-to-help-programmers-replace-their-jobs-probably-2000603414">https://gizmodo.com/openai-just-released-a-coding-tool-to-help-programmers-replace-their-jobs-probably-2000603414</a></p><p>• Cursor: <a href="https://cursor.com/">https://cursor.com/</a></p><p>• Measuring Software Engineering Productivity: <a href="https://newsletter.pragmaticengineer.com/p/engineering-productivity">https://newsletter.pragmaticengineer.com/p/engineering-productivity</a></p><p>• DORA, SPACE, and DevEx: Which framework should you use?: <a href="https://getdx.com/guide/dora-space-devex/">https://getdx.com/guide/dora-space-devex/</a></p><p>• Abi Noda on LinkedIn: <a href="https://www.linkedin.com/in/abinoda/">https://www.linkedin.com/in/abinoda/</a></p><p>• How AI is changing software engineering at Shopify with Farhan Thawar: <a href="https://newsletter.pragmaticengineer.com/p/how-ai-is-changing-software-engineering">https://newsletter.pragmaticengineer.com/p/how-ai-is-changing-software-engineering</a></p><p>• How Linux is built with Greg Kroah-Hartman: <a href="https://newsletter.pragmaticengineer.com/p/how-linux-is-built-with-greg-kroah">https://newsletter.pragmaticengineer.com/p/how-linux-is-built-with-greg-kroah</a></p><p>• Amazon says developers spend a surprisingly small amount of time per day coding: <a href="https://www.businessinsider.com/amazon-developers-spend-only-one-hour-coding-daily-aws-ai-2024-12">https://www.businessinsider.com/amazon-developers-spend-only-one-hour-coding-daily-aws-ai-2024-12</a></p><p>• Impact of Generative AI in Software Development: <a href="https://dora.dev/research/ai/gen-ai-report/">https://dora.dev/research/ai/gen-ai-report/</a></p><p>• Vercel Clerk: <a href="https://vercel.com/marketplace/clerk">https://vercel.com/marketplace/clerk</a></p><p>• WorkHuman: <a href="https://www.workhuman.com/">https://www.workhuman.com/</a></p><p>• Workhuman increases ROI from AI assistants 21% with DX: <a href="https://getdx.com/customers/workhuman-increases-roi-from-ai-assistants-21-with-dx/">https://getdx.com/customers/workhuman-increases-roi-from-ai-assistants-21-with-dx/</a></p><p>• Visual Studio: <a href="https://visualstudio.microsoft.com/">https://visualstudio.microsoft.com/</a></p><p>• Jesse Adametz on LinkedIn: <a href="http://linkedin.com/in/jesseadametz">linkedin.com/in/jesseadametz</a></p><p>• DX Core 4: <a href="https://getdx.com/core-4-reporting/">https://getdx.com/core-4-reporting/</a>?</p><p>• Indeed: <a href="https://www.indeed.com/">https://www.indeed.com/</a></p><p>• Accelerating code migrations with AI: <a href="https://research.google/blog/accelerating-code-migrations-with-ai/">https://research.google/blog/accelerating-code-migrations-with-ai/</a></p><p>• Granola: <a href="https://www.granola.ai/">https://www.granola.ai/</a></p><p>• Write Useful Books: A modern approach to designing and refining recommendable nonfiction: <a href="https://www.amazon.com/Write-Useful-Books-recommendable-nonfiction-ebook/dp/B0983HFQX7">https://www.amazon.com/Write-Useful-Books-recommendable-nonfiction-ebook/dp/B0983HFQX7</a></p><p>• Unsavory Truth: How Food Companies Skew the Science of What We Eat: <a href="https://www.amazon.com/dp/1541697111/">https://www.amazon.com/dp/1541697111/</a>?</p><p>—</p><p>Production and marketing by <a href="https://penname.co/">Pen Name</a>. </p><p></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:53 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/23/oss-rebuild/#atom-everything'>Introducing OSS Rebuild: Open Source, Rebuilt to Last</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Introducing OSS Rebuild: Open Source, Rebuilt to Last 
Major news on the Reproducible Builds front: the Google Security team have announced OSS Rebuild, their project to provide build attestations for open source packages released through the NPM, PyPI and Crates ecosystom (and more to come). 
 They currently run builds against the "most popular" packages from those ecosystems: 
 
 Through automat
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://static.simonwillison.net/static/2025/rebuild-ui.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://security.googleblog.com/2025/07/introducing-oss-rebuild-open-source.html">Introducing OSS Rebuild: Open Source, Rebuilt to Last</a></p>
Major news on the <a href="https://reproducible-builds.org/">Reproducible Builds</a> front: the Google Security team have announced <a href="https://github.com/google/oss-rebuild">OSS Rebuild</a>, their project to provide build attestations for open source packages released through the NPM, PyPI and Crates ecosystom (and more to come).<p></p>
<p>They currently run builds against the "most popular" packages from those ecosystems:</p>
<blockquote>
<p>Through automation and heuristics, we determine a prospective build definition for a target package and rebuild it. We semantically compare the result with the existing upstream artifact, normalizing each one to remove instabilities that cause bit-for-bit comparisons to fail (e.g. archive compression). Once we reproduce the package, we publish the build definition and outcome via <a href="https://slsa.dev/spec/v0.1/provenance">SLSA Provenance</a>. This attestation allows consumers to reliably verify a package's origin within the source history, understand and repeat its build process, and customize the build from a known-functional baseline</p>
</blockquote>
<p>The only way to interact with the Rebuild data right now is through their <a href="https://github.com/google/oss-rebuild">Go CLI tool</a>. I reverse-engineered it <a href="https://gist.github.com/simonw/a5416718587aadfb0ce5f046b66b54fb">using Gemini 2.5 Pro</a> and derived this command to get a list of all of their built packages:</p>
 <code> gsutil ls -r 'gs://google-rebuild-attestations/**'
</code> 
<p>There are 9,513 total lines, <a href="https://gist.github.com/simonw/9287de5900d5b76969e331d9b4ad9eba">here's a Gist</a>. I <a href="https://gist.github.com/simonw/7b1d0a01f74c2e8d8cedea7a9dc7f8d7">used Claude Code</a> to count them across the different ecosystems (discounting duplicates for different versions of the same package):</p>
 
 pypi: 5,028 packages 
 cratesio: 2,437 packages 
 npm: 2,048 packages 
 
<p>Then I got a bit ambitious... since the files themselves are hosted in a Google Cloud Bucket, could I run my own web app somewhere on <code>storage.googleapis.com</code> that could use <code>fetch()</code> to retrieve that data, working around the lack of open CORS headers?</p>
<p>I <a href="https://gist.github.com/simonw/178a1cb57597a7b8aaa4910beae89cd3">got Claude Code to try that for me</a> (I didn't want to have to figure out how to create a bucket and configure it for web access just for this one experiment) and it built and then deployed <a href="https://storage.googleapis.com/rebuild-ui/index.html">https://storage.googleapis.com/rebuild-ui/index.html</a>, which did indeed work!</p>
<p></p>
<p>It lets you search against that list of packages from the Gist and then select one to view the pretty-printed newline-delimited JSON that was stored for that package.</p>
<p>The output isn't as interesting as I was expecting, but it was fun demonstrating that it's possible to build and deploy web apps to Google Cloud that can then make <code>fetch()</code> requests to other public buckets.</p>
<p>Hopefully the OSS Rebuild team will <a href="https://news.ycombinator.com/item?id=44646925#44652098">add a web UI</a> to their project at some point in the future.

    </p><p>Via <a href="https://news.ycombinator.com/item?id=44646925">Hacker News</a></p>


    <p>Tags: <a href="https://simonwillison.net/tags/google">google</a>, <a href="https://simonwillison.net/tags/packaging">packaging</a>, <a href="https://simonwillison.net/tags/pypi">pypi</a>, <a href="https://simonwillison.net/tags/security">security</a>, <a href="https://simonwillison.net/tags/npm">npm</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/supply-chain">supply-chain</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://static.simonwillison.net/static/2025/rebuild-ui.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:16 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/23/timescope/#atom-everything'>TimeScope: How Long Can Your Video Large Multimodal Model Go?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   TimeScope: How Long Can Your Video Large Multimodal Model Go? 
New open source benchmark for evaluating vision LLMs on how well they handle long videos: 
 
 TimeScope probes the limits of long-video capabilities by inserting several short (~5-10 second) video clips---our "needles"---into base videos ranging from 1 minute to 8 hours. With three distinct task types, it evaluates not just retrieval b
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://static.simonwillison.net/static/2025/timescope-card.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://huggingface.co/blog/timescope-video-lmm-benchmark">TimeScope: How Long Can Your Video Large Multimodal Model Go?</a></p>
New open source benchmark for evaluating vision LLMs on how well they handle long videos:<p></p>
<blockquote>
<p>TimeScope probes the limits of long-video capabilities by inserting several short (~5-10 second) video clips---our "needles"---into base videos ranging from 1 minute to 8 hours. With three distinct task types, it evaluates not just retrieval but synthesis, localization, and fine-grained motion analysis, providing a more holistic view of temporal comprehension.</p>
</blockquote>
<p>Videos can be fed into image-accepting models by converting them into thousands of images of frames (a trick I've <a href="https://simonwillison.net/2025/May/5/llm-video-frames/">tried myself</a>), so they were able to run the benchmark against models that included GPT 4.1, Qwen2.5-VL-7B and Llama-3.2 11B in addition to video supporting models like Gemini 2.5 Pro.</p>
<p></p>
<p>Two discoveries from the benchmark that stood out to me:</p>
<blockquote>
<p>Model size isn't everything. Qwen 2.5-VL 3B and 7B, as well as InternVL 2.5 models at 2B, 4B, and 8B parameters, exhibit nearly indistinguishable long-video curves to their smaller counterparts. All of them plateau at roughly the same context length, showing that simply scaling parameters does not automatically grant a longer temporal horizon.</p>
<p>Gemini 2.5-Pro is in a league of its own. It is the only model that maintains strong accuracy on videos longer than one hour.</p>
</blockquote>
<p>You can explore the benchmark dataset <a href="https://huggingface.co/datasets/Apollo-LMMs/TimeScope/viewer/default/test?row=12">on Hugging Face</a>, which includes prompts like this one:</p>
<blockquote>
<p><code>Answer the question based on the given video. Only give me the answer and do not output any other words.</code></p>
<p><code>Question: What does the golden retriever do after getting out of the box?</code></p>
 <code>A: lies on the ground
B: kisses the man
C: eats the food
D: follows the baby
E: plays with the ball
F: gets back into the box
</code> 
</blockquote>

    <p>Via <a href="https://x.com/andimarafioti/status/1948044508676903309">@andimarafioti</a></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/vision-llms">vision-llms</a>, <a href="https://simonwillison.net/tags/evals">evals</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://static.simonwillison.net/static/2025/timescope-card.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:40 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/23/announcing-toad/#atom-everything'>Announcing Toad - a universal UI for agentic coding in the terminal</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Announcing Toad - a universal UI for agentic coding in the terminal 
Will McGugan is building his own take on a terminal coding assistant, in the style of Claude Code and Gemini CLI, using his Textual Python library as the display layer. 
 Will makes some confident claims about this being a better approach than the Node UI libraries used in those other tools: 
 
 Both Anthropic and Google’s apps f
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://willmcgugan.github.io/announcing-toad/">Announcing Toad - a universal UI for agentic coding in the terminal</a></p>
Will McGugan is building his own take on a terminal coding assistant, in the style of Claude Code and Gemini CLI, using his <a href="https://github.com/Textualize/textual">Textual</a> Python library as the display layer.<p></p>
<p>Will makes some confident claims about this being a better approach than the Node UI libraries used in those other tools:</p>
<blockquote>
<p>Both Anthropic and Google’s apps flicker due to the way they perform visual updates. These apps update the terminal by removing the previous lines and writing new output (even if only a single line needs to change). This is a surprisingly expensive operation in terminals, and has a high likelihood you will see a partial frame—which will be perceived as flicker. [...]</p>
<p>Toad doesn’t suffer from these issues. There is no flicker, as it can update partial regions of the output as small as a single character. You can also scroll back up and interact with anything that was previously written, including copying un-garbled output — even if it is cropped.</p>
</blockquote>
<p>Using Node.js for terminal apps means that users with <code>npx</code> can run them easily without worrying too much about installation - Will points out that <code>uvx</code> has closed the developer experience there for tools written in Python.</p>
<p>Toad will be open source eventually, but is currently in a private preview that's open to companies who sponsor Will's work for $5,000:</p>
<blockquote>
<p>[...] you can gain access to Toad by <a href="https://github.com/sponsors/willmcgugan/sponsorships?sponsor=willmcgugan&amp;tier_id=506004">sponsoring me on GitHub sponsors</a>. I anticipate Toad being used by various commercial organizations where $5K a month wouldn't be a big ask. So consider this a buy-in to influence the project for communal benefit at this early stage.</p>
<p>With a bit of luck, this sabbatical needn't eat in to my retirement fund too much. If it goes well, it may even become my full-time gig.</p>
</blockquote>
<p>I really hope this works! It would be great to see this kind of model proven as a new way to financially support experimental open source projects of this nature.</p>
<p>I wrote about Textual's streaming markdown implementation <a href="https://simonwillison.net/2025/Jul/22/textual-v4/">the other day</a>, and this post goes into a whole lot more detail about optimizations Will has discovered for making that work better.</p>
<p>The key optimization is to only re-render the last displayed block of the Markdown document, which might be a paragraph or a heading or a table or list, avoiding having to re-render the entire thing any time a token is added to it... with one important catch:</p>
<blockquote>
<p>It turns out that the very last block can change its type when you add new content. Consider a table where the first tokens add the headers to the table. The parser considers that text to be a simple paragraph block up until the entire row has arrived, and then all-of-a-sudden the paragraph becomes a table.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/open-source">open-source</a>, <a href="https://simonwillison.net/tags/markdown">markdown</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/will-mcgugan">will-mcgugan</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/uv">uv</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:17 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/23/1kb-js-numbers-station/#atom-everything'>1KB JS Numbers Station</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   1KB JS Numbers Station 
Terence Eden built a neat and weird 1023 byte JavaScript demo that simulates a numbers station using the browser SpeechSynthesisUtterance, which I hadn't realized is supported by every modern browser now. 
 This inspired me to vibe code up this playground interface for that API using Claude: 
 


      Tags: javascript, text-to-speech, tools, ai, generative-ai, llms, terenc
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://static.simonwillison.net/static/2025/speech-synthesis-tool.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://shkspr.mobi/blog/2025/07/1kb-js-numbers-station/">1KB JS Numbers Station</a></p>
Terence Eden built <a href="https://js1024.fun/demos/2025/24/bar">a neat and weird</a> 1023 byte JavaScript demo that simulates a <a href="https://en.wikipedia.org/wiki/Numbers_station">numbers station</a> using the browser <a href="https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesisUtterance">SpeechSynthesisUtterance</a>, which I hadn't realized is supported by every modern browser now.<p></p>
<p>This inspired me to vibe code up <a href="https://tools.simonwillison.net/speech-synthesis">this playground interface</a> for that API <a href="https://claude.ai/share/e4ea91ab-d329-4e3d-aabf-9f5ced9700ed">using Claude</a>:</p>
<p>


    </p><p>Tags: <a href="https://simonwillison.net/tags/javascript">javascript</a>, <a href="https://simonwillison.net/tags/text-to-speech">text-to-speech</a>, <a href="https://simonwillison.net/tags/tools">tools</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/terence-eden">terence-eden</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://static.simonwillison.net/static/2025/speech-synthesis-tool.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:00 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://altmode.org'>Altmode</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://altmode.org/2025/07/23/adriatic-trip-day-9-rome-to-athens/'>Adriatic Trip Day 9: Rome to Athens</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Wednesday, June 18, 2025 Our morning started extremely early because we had a 9:30 flight to Athens, and it would take a little while to get to the airport. Tauck provided a transfer to the airport as part of our tour, and we assumed that we would eat breakfast at the airport or on the […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Wednesday, June 18, 2025</p>



<p>Our morning started extremely early because we had a 9:30 flight to Athens, and it would take a little while to get to the airport. Tauck provided a transfer to the airport as part of our tour, and we assumed that we would eat breakfast at the airport or on the flight to Athens. However, when we checked out, bags with breakfast items were waiting for us at the hotel desk. This was very generous to the point where we weren’t sure what to do with all the food!</p>



<p>A car came to pick us up as planned. Since it is a weekday, we had to deal with the expected commute traffic, although it was somewhat anti-commute from the city center to the airport.</p>



<p>With arrival at the airport, the Tauck “<a href="https://www.tauck.com/tours/venice-florence-rome-escorted-tour?tcd=wf2025">A Week in…Venice, Florence, and Rome</a>” tour was complete, and the second part of our trip, the cruise, was about to begin. The Tauck tour was a great introduction to Italy for Kenna and a re-introduction for me, and was as close to perfect as one can imagine. Our tour director, Alexa, played a significant role in that.</p>



 



<p>After checking in at the Aegean Airlines desk in Rome, we proceeded through security. As frequent fliers, we have become accustomed to using airport lounges. We did get the use of an airport lounge, but it was somewhat crowded and small, though we didn’t have a long wait for our flight.</p>



<p>The flight itself was uneventful except for the scenery visible through the window. I had checked the options for getting to the city from the airport upon arrival in Athens, and my experience has been that trains are faster and more convenient than buses, so we decided to take the train (Athens subway extension) even though it is somewhat more expensive. This was not a great decision. Even at the airport, we had to lug our bags up flights of stairs to the train platform and figure out the protocol to get a ticket. Once inside the paid area, there were more stairs. We got on a train quickly, but as we rode into Athens, we picked up more and more passengers until the train was overcrowded toward the end of our trip. We made it off, but faced many more stairs getting to street level. Our friends Dave and Jan, who are joining us on this portion of the tour, arrived a couple of hours after us, so I quickly texted Dave to tell him to take the bus instead.</p>



<p>We checked into our hotel, the <a href="https://all.accor.com/hotel/B3X8/index.en.shtml">Athens Capital Hotel</a>, again a 5-star luxury hotel. We had some free time, so we wandered around the area a bit. Our hotel is exactly in the center of Athens, and across the street from the parliament building, where we were able to see the hourly changing of the guards.</p>



<p>Mid-trip is the optimal time to do laundry, and Kenna had done some research into laundries in Athens that we might take our dirty clothes to. She found <a href="https://www.athenslaundry.gr/en/">Athens Laundry</a> about a 15-minute walk from our hotel. Our walk there gave us exposure to some different parts of Athens. Close to the laundry, the storefronts were heavily graffiti-decorated, but artistically so. The clerk at the laundry spoke perfect English and promised our laundry would be ready in the early evening, but we agreed to pick it up the next morning instead.</p>



<p>During the afternoon, we visited the hotel lobby to find the representative from Lindblad/National Geographic setting up a table to greet guests. We had booked an optional pre-cruise tour of Athens and checked in for that. Our first meeting as a group would be for lunch the next day.</p>



<p>Dave and Jan had made reservations for the four of us at <a href="https://soilrestaurant.gr/">Soil</a>, a two-Michelin-star restaurant, a moderate walk from our hotel. They had made arrangements to meet someone else just before our dinner reservation, so we met them at the restaurant. As expected, our dinner at Soil consisted of a tasting menu featuring many small courses, laboriously prepared and beautifully presented. This was a “very special occasion” sort of dinner. We walked back to the hotel with Dave and Jan well-satisfied from our delicious meal.</p>



 



<p>This article is part of a series about our recent trip to Italy, Greece, Albania, and Croatia. To see the introductory article in the series, click <a href="https://altmode.org/2025/07/15/adriatic-trip-day-1-to-venice/">here</a>.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:00 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/23/dave-white/#atom-everything'>Quoting Dave White</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    like, one day you discover you can talk to dogs. it's fun and interesting so you do it more, learning the intricacies of their language and their deepest customs. you learn other people are surprised by what you can do. you have never quite fit in, but you learn people appreciate your ability and want you around to help them. the dogs appreciate you too, the only biped who really gets it. you ass
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <blockquote cite="https://x.com/_dave__white_/status/1947461492783386827"><p>like, one day you discover you can talk to dogs. it's fun and interesting so you do it more, learning the intricacies of their language and their deepest customs. you learn other people are surprised by what you can do. you have never quite fit in, but you learn people appreciate your ability and want you around to help them. the dogs appreciate you too, the only biped who really gets it. you assemble for yourself a kind of belonging. then one day you wake up and the universal dog translator is for sale at walmart for $4.99</p></blockquote>
<p>— <a href="https://x.com/_dave__white_/status/1947461492783386827">Dave White</a>, a mathematician, on the OpenAI IMO gold medal</p>

    <p>Tags: <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/ai">ai</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:57 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/23/icml-2025/#atom-everything'>Quoting ICML 2025</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    Submitting a paper with a "hidden" prompt is scientific misconduct if that prompt is intended to obtain a favorable review from an LLM. The inclusion of such a prompt is an attempt to subvert the peer-review process. Although ICML 2025 reviewers are forbidden from using LLMs to produce their reviews of paper submissions, this fact does not excuse the attempted subversion. (For an analogous exampl
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <blockquote cite="https://icml.cc/Conferences/2025/PublicationEthics"><p>Submitting a paper with a "hidden" prompt is scientific misconduct if that prompt is intended to obtain a favorable review from an LLM. The inclusion of such a prompt is an attempt to subvert the peer-review process. Although ICML 2025 reviewers are forbidden from using LLMs to produce their reviews of paper submissions, this fact does not excuse the attempted subversion. (For an analogous example, consider that an author who tries to bribe a reviewer for a favorable review is engaging in misconduct even though the reviewer is not supposed to accept bribes.) Note that this use of hidden prompts is distinct from those intended to detect if LLMs are being used by reviewers; the latter is an acceptable use of hidden prompts.</p></blockquote>
<p>— <a href="https://icml.cc/Conferences/2025/PublicationEthics">ICML 2025</a>, Statement about subversive hidden LLM prompts</p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/prompt-injection">prompt-injection</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:24 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2025/07/7430/'>総務省　安心・安全なメタバースの実現に関する研究会 報告書 2025（案）発表〜メタバースにおける本人確認とプライバシーとアカウンタビリティについて</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  本日、令和7年7月23日（水）15:00-17:00に、総務省「安心・安全なメタバースの実現に関する研究会（第16回）」が行われ、報告書2025（案）が発表されました。日経新聞には昨日発表されると出ていたニュースではありますが…。 内容は、記事の末にNotebookLMにまとめて…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>本日、令和7年7月23日（水）15:00-17:00に、<a href="https://www.soumu.go.jp/main_sosiki/kenkyu/metaverse2/16_siryou.html">総務省「安心・安全なメタバースの実現に関する研究会（第16回）」</a>が行われ、<a href="https://www.soumu.go.jp/main_content/001021673.pdf">報告書2025（案）</a>が発表されました。日経新聞には昨日発表されると出ていたニュースではありますが…。</p>



<p>内容は、記事の末にNotebookLMにまとめてもらって書くとして、このブログの読者の興味としてはやはり本人確認周りではないですかね。X(旧Twitter)でもポストしたのですが、本人確認まわりはこんなところです。</p>



<p>●空間内の行動主体について、その責任を負う者の本人確認が必要とされる場面においては、 本人確認済である旨がその確認手法も含め判別できるようにすること。<br>●登録時の本人確認システムを含む必要な措置の導入・強化に向けた検討を行うこと。</p>



 
 <blockquote><p>「●空間内の行動主体について、その責任を負う者の本人確認が必要とされる場面においては、<br>本人確認済である旨がその確認手法も含め判別できるようにすること。」<br>「●登録時の本人確認システムを含む必要な措置の導入・強化に向けた検討を行うこと。」など</p>— Nat Sakimura/崎村夏彦 (@_nat) <a href="https://twitter.com/_nat/status/1947934040096707076?ref_src=twsrc%5Etfw">July 23, 2025</a></blockquote> 
 



<p><a href="https://amzn.to/4fa3488">わたしの本『デジタルアイデンティティ』</a>にも書きましたが、これは責任あるデジタル存在に関わるものです。一方では、各メタバース事業者毎にマイナンバーカードで本人確認しろと言われると気持ち悪いなぁ（情報が分散するのでリスクが上がる）というのもありまして、eIDAS2でドイツで昨年あたり検討されていたPsuedonym Provider とか、半匿名認証プロバイダーのようなものを想定して、それを制度に取り込むと良いなと思っています。て、言ってもなんのことかわかりませんよね。要は運用を含めたセキュリティ基準がしっかりしている「半匿名」プロバイダー（対事業者に対しては、ユーザーは匿名ないしは仮名になる）が、マイナンバーカードとかで本人確認をしたうえで、仮名のVerifiable Credential/Digital Credentialを発行して、それを使って、メタバースとかそのワールドに登録する形です。裁判所の令状があれば、捜査機関がそこに行って、本人まで遡ることができるようにする仕組みです。現早稲田大学教授でMyData Japan の副理事長の佐古先生がエディタをやっていた『<a href="https://webdesk.jsa.or.jp/books/W11M0090/index/?bunsyo_id=ISO%2FIEC+29191%3A2012">ISO/IEC 29191 情報技術－セキュリティ技術－情報セキュリティマネジメントシステム－部分匿名，部分リンク不能認証の要求事項</a>』　にその要求事項が書いてあります。（なお、報告書案の中にはISOはITU-Tとの関係で一回出てくるだけです。総務省だから仕方ないといえば仕方ないのですが…。）</p>



<p><a href="https://amzn.to/4fa3488">わたしの本</a>に書いた責任あるデジタル存在の原則は以下のとおりです。</p>



<blockquote>
<p>原則1．責任あるデジタル存在（Accountable Digital Being） 誰もが自身の行動に責任を問われるデジタル存在を確立・再確立可能であるべきです。これは、言論や思想の自由を守るために仮名や匿名での行動も許容しつつ、事件発生時には適切な権限を与えられた機関が匿名性を解除できるようにする「半匿名（Partially Anonymous）」な状態を目指すものです。これにより、不法行為を行った者が説明責任（Accountability）を求められ、社会がそれを認識することで、人々が自身の行動の社会的な許容性を考えるようになります。また、この原則が一般的になると、それを持たない者は社会から排除される可能性があるため、望むならいつでも確立・再確立できる必要があります.<br>（出所）<a href="https://amzn.to/4fa3488">崎村夏彦. 2021. デジタルアイデンティティ. 日経BP社</a></p>
</blockquote>



<p>検討が良い方向に向かうことを祈っています。</p>



<p>以下、NotebookLMによる報告書案概要です。</p>



 報告書案概要 



 1. はじめに：メタバースの定義と研究対象の拡大 



<p>総務省は、Web3時代に向けたメタバース等の利活用に関する研究会を経て、2023年10月より「安心・安全なメタバースの実現に関する研究会」を開催しています。当初は個人間のコミュニケーション・エンターテインメントを主目的とするVRメタバースが主な議論対象でしたが、2024年12月以降、議論の対象はAR・MRメタバースを含む多様な目的のメタバース全般へと拡大されました。</p>



<p>本報告書における「メタバース」の定義は以下の4つの要素を備えた仮想空間とされます。</p>



 
 「ⅰ 利用目的に応じた臨場感・再現性があること（デジタルツインと同様に物理空間を再現する場合もあれば、簡略化された物理空間のモデルを構築する場合、物理法則も含め異なる世界を構築する場合、物理空間そのものに仮想的に情報を付加・融合させる場合もある）」 



 「ⅱ 自己投射性・没入感があること」 



 「ⅲ （多くの場合リアルタイムに）インタラクティブであること」 



 「ⅳ 誰でもが仮想空間に参加できること（オープン性）」 
 



<p>さらに、相互運用性、永続性、物理空間と同等の経済活動が可能であることなども、多くのメタバースが備える要素として挙げられています。</p>



<p>本報告書は、「メタバースの原則（第1.0版）」の改定を見据え、市場、技術、政策・制度の3側面から国内外の動向を把握し、物理空間と仮想空間が相互に作用したり、融合するケースが社会に与える影響について議論した結果をまとめたものです。</p>



 2. メタバースをめぐる市場動向 



<p>メタバース市場は、現在「幻滅期」にあるとされていますが、社会課題解決のインフラとして着実に市場が形成されており、今後普及が進むと見られています。</p>



 2.1. 市場規模の推移と予測 



 
 世界の市場規模: 2024年時点で744億ドルですが、2030年には5,078億ドルまで拡大すると予測されています。このうち、メタバース内でのeコマースが最も大きな割合を占め、次いでゲーム、ヘルス＆フィットネスが続きます。 



 国内市場規模: 株式会社矢野経済研究所によると、2023年度は前年度比35.3%増の1,863億円と推計されており、2024年度は2,750億円まで成長する見込みです。 



 総務省の推計:
 
 個人向け: 2025年時点では約800億円、2030年には約4,100億円まで拡大すると推計されています。特に「観光」分野は、VRメタバースとAR・MRメタバースの両方で6倍以上の市場規模に拡大すると見られています。 



 企業（業務利用）向け: 2025年時点では約8,300億円、2030年には約1兆6,000億円まで拡大すると推計されています。「土木・建設現場」「職場教育」「オフィス・ワークプレイス」分野の拡大率が大きいと予測されます。 



 合計: 2025年時点では約9,100億円、2030年には約2兆円まで拡大すると推計されています。企業向け市場の方が規模は大きいですが、個人向けの市場拡大率の方が高いと分析されています。 
 
 
 



 2.2. デバイスの進化 



 
 VRデバイス: 超高画質・超軽量な新型機の開発が進み、より快適な利用が可能になると期待されています。 



 AR・MRデバイス: グラス型（眼鏡型）デバイスの新型機が続々と登場しており、日常生活での利用が手軽になりつつあります。AI搭載の高性能グラス型デバイスの開発も進んでおり、AIが視界の物体を認識・理解し、情報表示や行動提案を行う新たな体験の可能性が注目されています。 
 



 2.3. ユーザー動向の多様化 



 
 従来のメタバースユーザーは10代後半から20代、男性が中心と考えられてきましたが、今後は多様化、若年化が進む可能性があります。 



 人気ライブ配信者によるVRChat体験動画の反響により、GoogleトレンドにおけるVRChatの日本での人気度が2023年比で5倍程度に増加しました。 



 Robloxのようにユーザーの40%弱が13歳以下の若年層であるゲームプラットフォームのDAUが増加しており、特に日本を含むアジア太平洋地域では約40%の増加が見られます。 
 



 2.4. ステークホルダーの整理と課題 



<p>メタバースをめぐるステークホルダーは、ビジネス・活動レイヤー、空間レイヤー、プラットフォームレイヤー、ネットワークレイヤー、ハードウェアレイヤー、基盤技術レイヤーに整理されています。ユーザー属性・役割の多様化を踏まえ、ステークホルダー間をつなぐ主体の必要性が指摘されています。</p>



 3. メタバースにおけるコミュニティ・情報流通の課題と対応 



<p>メタバース独自のコミュニティと情報流通には様々な特徴がありますが、それに伴い以下の16の課題が指摘されています。</p>



<p>主な課題の例:</p>



 
 コンテンツの多様な種類・形態：個々のコンテンツの権利や責任の所在が不明確。 



 空間内の自己存在感：アバターやキャラクターを介した自己投射・没入によるデータ流出やセキュリティ侵害の深刻化。 



 ユーザー行動：AIやアルゴリズムによる自律的制御がもたらす違法行為や不当行為のリスク。 



 サービス利用の拡大：ユーザー層の拡大や応用分野の拡大に伴う、不文律や倫理観の共有の難しさ。 



 AR・MRメタバース固有の特徴：物理空間への情報付加に伴う情報・データ流通の課題、プライバシー侵害のリスク。 
 



<p>研究会での議論と対応案:</p>



 
 高性能化するAIアバターと人間アバターの区別: AIアバターと人間が動かすアバターの区別をどのように行うべきか。ユーザーの自発的な行動で真正性を担保する仕組みや、AIによる行動を一定程度制限するアプローチなどが検討されています。 



 多様化するユーザー属性と倫理観の共有: 不文律や倫理観の共有をどのように行っていくべきか。メタバース関連サービス提供者が、提供するワールドの性質や文脈をユーザーに説明し、理解を促すことが期待されます。 



 マルチモーダルな情報・データの取得・分析・活用: 生体情報を含むデータが流通する中で、その取り扱いをどうすべきか。ユーザーへの透明性の確保、データ取得前の同意取得の徹底、デバイスメーカーによる技術的措置などが議論されています。 



 複雑化する契約・取引における責任の扱い: UGCクリエイターとユーザー間のトラブル対応など、責任の所在を明確にすることが求められます。トラブル防止の仕組み構築や、通報窓口の設置、対応方針の明確化が期待されます。 



 複雑化するコンテンツや活動の権利の扱い: UGC創作時における他者の権利侵害の防止や、プライバシーへの配慮が課題です。利用規約やガイドラインでの考え方の明示、技術的な措置によるデータ取得制限などが議論されています。 
 



 4. コンテンツの創作・流通の市場動向 



<p>メタバース上で流通するコンテンツは、ワールド、NPC、アバター、アバターアクセサリー、オブジェクト、エフェクト、コンテンツ（狭義）、ギフト、イベントなどに類型化できます。</p>



<p>クリエイターエコノミーの現状と課題:</p>



 
 BOOTHの3Dモデルカテゴリの取扱高が急成長しており、国内のメタバースクリエイターエコノミーが急成長していることが示されています。 



 これに伴い、いわゆる「古参勢」とは異なる背景を持つユーザーの流入が進み、メタバース内で形成されてきた「暗黙の了解」が無効化され、トラブルが顕在化する可能性が高まっています。 



 クリエイターの「事業者性」の立証が困難であることや、クリエイターの匿名性擁護と消費者保護という相反する論点が存在します。 



 解決策として、「中間事業者」の活用が想定されていますが、人気のクリエイター以外には適用が難しいという問題があります。プラットフォーマーによる販売代行の可能性も議論されています。 
 



 5. 社会課題解決に資するメタバース導入の留意点 



<p>新型コロナウイルス感染症の拡大を機に、企業や自治体でのメタバース導入が進み、社会課題解決に資するユースケースが増加しました。</p>



<p>JR西日本の「バーチャル・ステーション」事業事例:</p>



 
 コロナ禍で運輸事業が影響を受ける中、新たな収益の柱としてバーチャル・ステーション事業を立ち上げ、2022年8月の初開業から継続しています。 



 リアルとバーチャルの「駅」を有機的に連携させ、地域活性化や誰もが活躍できる社会の実現に貢献しています。 



 事業継続の留意点: 
 



 
 自社でオーソライズされている事業創出手法に則る: 経営課題に即した視点で事業を提案し、経営陣に理解されやすい内容とすること。 



 事業の“適社性”の確認・追及を徹底する: 自社のアセット・ケイパビリティが活きることを証明し、自社でなければ実現し得ない独自性を追求すること。 



 既存メタバースプラットフォーマーとの協業方式を採用する: 投資リスクを抑制し、既存ユーザー層を活用することでゼロからのユーザー集めを不要にすること。 
 



<p>大阪・関西万博におけるメタバースの活用:</p>



 
 2025年開催の大阪・関西万博では、「いのち輝く未来社会のデザイン」をテーマに、メタバースを先端技術と組み合わせたXRコンテンツが多数展示されています。 



 総務省の「Beyond 5G readyショーケース」では、VRゴーグルを使った月面ロボットの遠隔操作体験が可能です。 



 「バーチャル万博～空飛ぶ夢洲～」が設定され、時間や場所の制約を超えてパビリオン巡りの機会を提供しています。 



 大阪大学の石黒浩教授がプロデュースするパビリオン「いのちの未来」では、人間とアンドロイドロボットが共存する未来の生活空間が展示されており、人間とアバターの関係性の変化に注目が集まっています。 
 



 6. メタバースをめぐる技術動向 



<p>メタバースの利用には高度な技術が必要であり、ハプティクス（力触覚を人工的に生成する技術）とAIの活用動向が特に注目されています。</p>



 6.1. ハプティクスの活用動向 



 
 現状: ゲーム、業務用トレーニング、医療などの専門分野での利用が見られますが、専用デバイスの必要性、感覚の種類・態様・身体部位の限定性、再現精度の不足などから利用事例はまだ少ないです。 



 効果・メリット: 臨場感・没入感の向上、動作・行為の感覚の再現・共有。 



 課題:
 
 VR酔い: 感覚モダリティ間の不整合（特に視覚と体性感覚）による乗り物酔いのような症状。原因は人間の認知機構、視覚情報と体性感覚情報の精度差、表示タイミングのずれ、コンテンツの動きの激しさなど。 



 感覚再現の精度・正確性の不足: 業務トレーニングなどでの本格利用には、実際と同等の刺激の強さや精度、態様、品質の正確性が必要。 
 
 
 



 
 
 



 
 対応方針: 人間の認知機構の解明、「VR酔い」の回避・軽減に向けたマルチステークホルダーでの対策、高精細・高精度で汎用的なハプティックデバイスの研究開発。 
 



 6.2. AIの活用動向 



 
 活用例: ワールドやアバターの自動生成、パーソナライゼーション、多言語間コミュニケーションのサポート、ユーザーの不正行為検出・防止、対話型カスタマーサービス、広告宣伝など多岐にわたります。 



 課題:
 
 パーソナルデータ漏えい・侵害リスクの深刻化: HMDやハプティックデバイスにより取得されるユーザーの生理、心理、行動に関するマルチモーダルなデータが、AI分析により漏えい・侵害のリスクを増大させる可能性。 



 ユーザー誘導リスクの深刻化: 分析されたデータが、ハプティクスやAIにより臨場感・没入感を増した環境でユーザーにインプットされることで、人間やAIの恣意に基づく誘導リスクが高まる可能性。 



 相互運用性やガバナンスに関するマルチステークホルダー議論の不足: サードパーティが開発・チューニングしたAIのメタバース上での活用における相互運用性やガバナンスのあり方について、様々なステークホルダーによる議論が必要。 
 
 
 



 
 
 



 
 対応方針:
 
 マルチモーダルなパーソナルデータの適切な取り扱いに関する調査・研究。 



 没入環境でのAIアウトプットがユーザーに与える影響に関する具体的・実証的な調査・研究。 



 相互運用性やガバナンスに関するマルチステークホルダーでの議論の場を設ける。 
 
 
 



<p>Dynabook株式会社によるAR・MRメタバースとAIの融合事例:</p>



 
 グラス型AR・MRデバイスを着用することで、物理空間を見ながらAI支援を受け、会話の文字起こし・翻訳、物体認識、資料要約などが可能になります。 



 産業向けではDX高度化、人手不足解消、スキル向上に貢献しますが、一般ユーザー向けでは情報の信憑性、AI依存症、不適切なコンテンツ、社会的受容性、物理的安全性、プライバシー（データ取得・監視）などの課題が懸念されます。 



 構成員からは、デバイスを介したデータ取得が、ユーザーだけでなく周囲の人物にも影響を与えるリスクや、支配的なプラットフォーマーによるデータ独占への懸念が指摘されました。また、AR・MRメタバースにおける「フィルターバブル」や「エコーチェンバー」が物理空間に持ち込まれる可能性も指摘されています。 
 



 6.3. データ取得・活用状況 



 
 Clusterの事例: 
 
 メタバースプラットフォーム「Cluster」では、「アクションイベントログ」（ユーザー行動履歴）と「3D空間の同期通信ログ」（アバター位置、姿勢、ボイスチャット音声、アイテム位置など）の2種類のデータを取得・活用しています。 



 取得データはサービスの開発・改善、迷惑行為の検出・防止、クリエイターへの情報提供に役立てられています。 



 ユーザー自身の年齢（未成年者保護のため）と身長（身体の動きの適切なトラッキングのため）も取得されています。 
 
 



 LUIDA（大規模VR実験プラットフォーム）: 
 
 Clusterを活用したオンライン実験プラットフォーム。研究者が実験ワールドを設計し、ユーザーは実験に参加・同意の上、データ収集やアンケート回答を行います。 



 データ活用の指針: 匿名化されたID管理、利用目的の明示と同意取得の徹底、自己選択的な参加・離脱の仕組み、データ削除要請への対応。 



 課題: 個人情報保護法における利用目的の具体的な説明と実験目的との衝突、オンラインでの生体情報取得におけるプライバシー懸念、プラットフォーマーの責任の所在。 



 AR・MRデバイスにおけるデータ取得の課題: 現実空間とリンクした情報であるため、より精緻な取り扱いが必要。高密度なデータ取得、センシティブな個人データの行動予測・パーソナライズ、ハイパーパーソナライズ広告による操作的・倫理的懸念。 



 対策として、録画中であることを示すLEDランプ点灯などのハードウェア側の制約が検討されていますが、「現実空間もデータを取得される状況にある」という社会共通認識の形成も提言されています。 
 
 
 



 6.4. メタバース利用が人々の身体・感情・行動等に与える影響 



 
 ポジティブな影響:
 
 メンタルヘルスデジタル療法: 株式会社BiPSEEは、VRメタバースを用いたうつ病治療アプリや、自閉スペクトラム症（ASD）傾向のある小児向けコミュニケーション支援を提供しています。没入体験による注意機能へのアプローチや、HMDによる心身情報（生体データ）へのアクセスが強みとされています。 



 米国ではリハビリや慢性疼痛軽減のためのVRメタバースが医療機器として承認されています。 
 
 



 課題:
 
 医療現場における治療法の意義や安全性、リスクに関する認知・理解の不足。 



 通信環境（遅延）の課題。 



 依存や現実世界からの逃避のリスク。 



 患者のセンシティブな情報が多様なステークホルダー間で複雑化するデータの取り扱いと責任の所在。 
 
 
 



 7. メタバースをめぐる国内外の政策・制度 



<p>国内外でメタバースに関する政策・制度の検討が進められています。</p>



 7.1. 日本政府の動向 



 
 総務省:
 
 シンポジウム「安心・安全なメタバースの利活用促進を考える」: 2025年3月開催。ユーザーの安心・安全確保に向けた取り組み、ビジネス導入の要点、導入効果などを議論。XRデバイスから取得可能な生体情報を含むマルチモーダルかつ機微なデータの取り扱いの重要性が強調されました。 



 「社会課題の解決に向けたメタバース導入の手引き」の作成: メタバース導入を検討する企業や自治体向けに、成功事例のヒアリング結果を基に留意点などをフェーズ別に整理。 



 「XRデバイスを安全で快適に利用できる環境整備に資する技術の実証事業」: XRデバイス使用に伴う「VR酔い」や身体的負担などのリスクを最小限に抑え、メリットを最大限に享受できる望ましい利用のあり方を検討するため、国立研究開発法人情報通信研究機構と京都大学が採択され実証事業を実施予定。 
 
 



 経済産業省:
 
 日本産業標準調査会（JISC）: メタバースの協調領域における標準化に向けたアクションプランの検討と実行を推進。 



 「戦略的国際標準化加速事業」: 「メタバース等のサイバーフィジカルコンテンツにおけるアバター設定に関する国際標準化」と「メタバースの用語に関する国際標準化動向の調査」を実施予定。 
 
 



 特許庁:
 
 産業構造審議会知的財産分科会意匠制度小委員会: 仮想空間におけるビジネスやデザイン創作の実態を踏まえ、意匠制度の見直しを検討。仮想物品等の形状等を表した画像を保護対象とする方向性が示されています。 
 
 



 内閣府:
 
 戦略的イノベーション創造プログラム（SIP）「バーチャルエコノミー拡大に向けた基盤技術・ルールの整備」: サイバー空間からフィジカル空間への価値還流を目指し、技術開発、標準化・ルール整備、ユースケース実証、人材育成などを推進。ELSI（倫理的・法的・社会的課題）対策や社会受容性の醸成も含まれます。 



 「インターバース社会実装推進メタコンソーシアム」の設立: インターバース産業の発展と世界市場牽引を目指し、産学官連携での意見交換や研究開発・成果利用を促進。 
 
 
 



 7.2. 諸外国・地域の動向 



 
 米国: 
 
 メタバースに特化した法整備は確認されていませんが、没入型技術に関する調査や法案の動きが見られます。 



 会計検査院（GAO）: 連邦政府機関における没入型技術の利用動向を調査し、サイバーセキュリティやプライバシー遵守、運用コストの高さなどを課題として指摘。 



 「米国没入型技術におけるリーダーシップ法案」: XR技術の国家戦略策定を目的とした法案が提出。 



 NIST（米国立標準技術研究所）: 没入型技術のサイバーセキュリティとプライバシー基準に関する現状調査を実施。既存のリスク管理ガイドラインやツールが活用されることが期待されています。 
 
 



 EU:
 
 欧州委員会「Web4.0と仮想空間をリードするためのEU戦略」: 人材・スキル、ビジネス、政府、ガバナンスの4観点から10のアクションを提示。 



 EU競争総局: 「生成AIと仮想空間の競争に関する政策文書」を発表し、独占禁止法やDMAを活用して市場の公正性を確保する姿勢を示しています。 



 「Web4.0と仮想空間のガバナンスに関するマルチステークホルダー会合」: グローバル・ガバナンス原則の議論を喚起。日本の「メタバースの原則」も国際的な評価を受けました。 



 「仮想空間ツールボックス（Virtual Worlds Toolbox）」: EU市民が仮想空間を適切に利用するためのオンラインリソースを提供。 
 
 



 仏国: 国家戦略の策定、ヘイトスピーチやサイバー暴力の防止を目的とした報告書「メタバースにおける尊重と多様性の構築」発表、メタバースの相互運用性を推進するための標準化委員会設立、国家投資計画「France 2030」による没入型文化・技術プロジェクト公募など。 



 英国: オンライン安全法がメタバースにも適用可能とされ、デジタル規制協力フォーラム（DRCF）がメタバースがもたらす課題への対応に取り組んでいます。 



 豪州: 「2021年オンライン安全法」がメタバースにも適用可能とされ、eSafetyコミッショナーが没入型技術に関するポジションペーパーや、こどもの没入型技術利用に関するガイダンスを発表。特にこどもの安全確保を目的とした基準案が公表され、発行されています。 



 中国: 工業情報化部が「メタバース産業の革新的発展に向けた3カ年行動計画（2023-2025年）」を発表。上海市や北京市も独自の行動計画やプロジェクトを進めています。 



 韓国: 世界初のメタバース産業振興法である「仮想融合産業振興法」を成立させ、同年8月に施行。「メタバース自律規制委員会」も発足し、取引、知的財産、利用者保護などのガイドラインを策定しています。 
 



 7.3. 国際組織の動向 



 
 MSF（メタバース・スタンダード・フォーラム）: メタバースの相互運用性に関する標準策定を支援。日本のVRMコンソーシアムがワーキンググループの共同議長に選出され、VRMの国際標準化に向けた協力が進められています。 



 ITU-T（国際電気通信連合の電気通信標準化部門）: 「メタバースに関するフォーカスグループ（FG-MV）」を設置し、52件の成果文書を採択。メタバースの標準化に関する議論が進められています。 



 OECD（経済協力開発機構）: GFTech（Global Forum on Technology）の下に没入型技術のフォーカスグループを設置。日本の「メタバースの原則」も報告書で言及され、人間中心で民主的価値に基づく没入型技術の発展・利用の前提となる価値が整理されました。 



 IGF（インターネット・ガバナンス・フォーラム）: メタバースにおけるこどもの権利と安全、Web4.0と仮想世界のガバナンスに関するワークショップやセッションが開催されました。 



 WEF（世界経済フォーラム）: 新たなイニシアチブ「メタバースの定義と構築」を発表し、ガバナンスと経済的・社会的価値の創造に焦点を当てています。「責任あるメタバース成熟度モデル」も発表し、メタバースプラットフォームの責任ある運用を評価する枠組みを提示しています。 
 



 7.4. 海外のメタバースに係るソフトロー 



 
 韓国「メタバース倫理原則」: 2022年11月、科学技術情報通信部が公表。法的拘束力を持たない自律規範で、「誠実な自我」「安全な経験」「持続可能な繁栄」の3大志向価値と、真正性、自律性、互恵性、プライバシー尊重、公平性、個人情報保護、包括性、未来への責任の8大実践原則を掲げています。 



 EU「EU市民原則」: 欧州市民パネルでの提言に基づき、「公正で望ましい仮想世界のための8つの価値観と原則」が策定されました。選択の自由、持続可能性、人間中心、健康、教育・リテラシー、安全性・セキュリティ、透明性、包括性の8項目です。 



 UAE「9つの自主規制原則」: 「責任あるメタバース自己規制フレームワーク」白書で提起。アクセスへの相互運用性、プライバシーバイデザイン・バイデフォルト、設計による持続可能性、互恵性、信頼のための透明性、公平性・平等性・包括性、多様性へのコミットメント、アカウンタビリティ、セーフティバイデザイン・善行の9項目です。 



 ドイツテレコム「メタバースの倫理指針」: 「保護」「セキュリティ」「責任」の3カテゴリーにまたがる9つの指針を掲げ、人間中心のメタバースを支えるとしています。 



 Meta「4つの基本的価値」: 経済的機会、プライバシー、安全性・公正性、公平性・包括性の4項目を、責任あるメタバース構築における主要な関心領域としています。 



 GatherVerse「メタバースの8基準」: 人間第一、アクセシビリティ、教育、平等性、コミュニティ開発、安全性・プライバシー、ウェルネス、倫理の8つのコア原則を定めています。 



 RMA「倫理メタバース原則」: 「安全で責任あるメタバース」実現のための11項目からなる倫理基盤を提唱しています。 
 



 8. 日本の「メタバースの原則（第2.0版）」の検討 



<p>総務省は2024年10月に「メタバースの原則（第1.0版）」を公表しましたが、市場拡大、ユーザー増加、サービス多様化、AR・MRメタバースへの議論対象拡大などを踏まえ、今回の報告書で「メタバースの原則（第2.0版）」としてメジャーアップデートを行いました。</p>



 8.1. 原則改定の視点 



 
 「第1.0版」策定時からの状況変化 



 VRメタバースからAR・MRメタバースへの議論対象拡大 



 コミュニケーション・エンタメ目的から多目的への議論対象拡大 



 デバイスの進展 



 国外のソフトローとの比較 
 



 8.2. 原則の構成と主な変更点 



<p>「メタバースの原則（第2.0版）」は、民主的価値の実現を目的とし、「メタバースの自主・自律的な発展に関する原則」と「メタバースの信頼性向上に関する原則」の2つの柱で構成されます。原則の対象は「メタバース関連サービス提供者」が主ですが、「ユーザー」「コンテンツの創作や提供を行う者（クリエイターを含む）」「ルール整備に関わる者」「ユーザーのリテラシー向上に関わる者」「デバイスを提供する者」を含む全てのステークホルダーが参照することを期待されています。</p>



<p>主な変更点（下線部が追加・修正箇所）:</p>



 
 前文: 「物理空間と仮想空間がこれまで以上に融合した結果として、メタバース上での出来事や価値観が仮想空間のみならず物理空間にも影響を与え、両空間の民主的価値を損なう可能性も想定される。」という記述が追加され、AR・MRメタバースの進展に伴う影響への言及が強化されました。 



 原則の対象: 「デバイスを提供する者」が参照対象として明示されました。 
 



<p>「メタバースの自主・自律的な発展に関する原則」における変更点:</p>



 
 オープン性・イノベーション: 



 「自由な事業展開によるイノベーション促進、多種多様なユースケースの創出」の解説に、「メタバースの利用が人々の身体、感情、行動等に正負両面の影響を与える可能性があることを認識し、その提供するメタバースサービスがユーザの身体的・精神的な健康の増進に寄与するものとなるよう開発・運営等に努めることが期待される。」という記述が追加され、ウェルビーイングへの配慮が明記されました。 



 「知的財産権等の適正な保護」の解説に、「技術・ノウハウなどユーザから取得するデータについて競争上の理由等から他者に秘匿すべきものがあることに留意するほか、利用規約やコミュニティガイドライン等を通じて、知的財産権をはじめとする諸権利の適正な保護の重要性についてユーザへの浸透を図るとともに、例えば、二次利用の可否をはじめ、UGCの創作・利用に関するルール等についてこれらの文書に明示することが期待される。」と、技術・ノウハウ保護とUGC創作・利用に関するルール明示が追加されました。 
 



<p>「メタバースの信頼性向上に関する原則」における変更点:</p>



 
 透明性・説明性:
 
 「サービス利用時の保存データ（期間、内容等）及びメタバース関連サービス提供者が利用するデータの明示並びにそれらのユーザへの情報提供」の解説に、「取得・保存したデータの管理方法や管理体制についても、可能な範囲で明示することが期待される。」が追加されました。 



 「提供するメタバースの特性の説明」が新たに項目として追加され、ユーザー属性の多様化や異なるサービス・ワールドでのルール・文脈の違いを踏まえた説明の必要性が明記されました。 



 「真正性を確認するための措置」が新たに項目として追加され、AIアバターと人間アバターの区別や本人確認の必要性など、空間内の行動主体の真正性担保に関する措置が明記されました。 



 「物理空間に対して仮想的に付加又は削除する情報の選択、表示に関する措置」が新たに項目として追加され、AR・MRメタバースにおける情報の付加・削除がもたらす「フィルターバブル」や「エコーチェンバー」のリスク、共通認識の喪失といった課題への対処が明記されました。 



 「ユーザの安全確保のための措置」が新たに項目として追加され、屋外利用や怪我・事故のリスクが予測される場合に、機能制限や必要な措置を講じること、それらをユーザーに説明することが明記されました。 
 
 



 アカウンタビリティ:
 
 「事前のユーザ同士をはじめとする関係者間トラブル防止の仕組みづくりや事後の不利益を被ったユーザの救済のための取組」の解説に、「メタバースサービス内のエフェクトやイベントなどの体験コンテンツについて、その結果起こった事象に対しての責任の所在を関係者間であらかじめ明確にしておくこと。」が追加されました。 
 
 



 プライバシー:
 
 「ユーザの行動履歴の適正な取扱い」の解説に、「取得する行動履歴は、利用に必要な範囲にとどめることとし、その保存についても、必要な期間にとどめること。」と、取得・保存期間の制限が明確化されました。 



 「サービス外の周囲の人物のプライバシーへの配慮」が新たに項目として追加され、ユーザーがサービスを利用していない周囲の人物のプライバシーへの配慮を促すことが明記されました。 
 
 



 セキュリティ:
 
 「メタバースのシステムのセキュリティ確保（外部からの不正アクセスへの対処等）」の解説に、「登録時の本人確認システムを含む必要な措置の導入・強化に向けた検討を行うこと。」「ログイン時の認証システムの導入・強化に向けた検討を行うこと。」が追加されました。 
 
 
 



 9. 今後の課題 



<p>「安心・安全なメタバースの実現」と「安心・安全なメタバースの更なる利活用」の2つの側面から、以下の課題が挙げられています。</p>



 9.1. 安心・安全なメタバースの実現に係る課題 



 
 「メタバースの原則」をベースにした国内外での共通認識の醸成: 「メタバースの原則」を具体的に推進し、OECDなど国際的な議論に貢献するとともに、国内への普及・浸透を促進すること。 



 望ましい利用の在り方についての検討: 「VR酔い」や依存などのネガティブな影響への対策を模索し、望ましい利用の在り方について政府内の他の取り組みと連携しながら検討すること。 



 生体情報等を含むマルチモーダルなデータの取扱いに係る検討: XRデバイスから得られるマルチモーダルかつ機微なデータの、ユーザーの安心・安全を確保した取得・分析・活用方法について、メタバース関連サービス提供者だけでなく、デバイスメーカーやビジネスユーザーの取り組み実態も把握し、活用可能な既存技術や社会実装への道筋を検討すること。 



 物理空間に対して仮想的に付加又は削除する情報の選択、表示に関する検討: AR・MRメタバースにおける情報の付加・削除がもたらす安全確保への支障や、共通認識の喪失といったリスクに対し、ユーザーの利便性を維持しつつ効果的な方策をマルチステークホルダーで議論・検討すること。 



 ユーザーのリテラシー向上のための支援: AIや没入型技術の発展に伴う情報の真偽見極めの難しさに対し、システム側での支援とユーザーに求められる理解・判断のバランスを取った取り組み（プライバシー・バイ・デザイン、フールプルーフ、ナッジなど）をマルチステークホルダーで議論・検討すること。 



 マルチステークホルダーによる議論の場の構築: 複雑化する課題に対し、多様なステークホルダーが知見を共有し、「メタバースの原則」の改定要否や方向性について多角的・集中的に議論する場の構築が期待されます。 
 



 9.2. 安心・安全なメタバースの更なる利活用に係る課題 



 
 「社会課題の解決に向けたメタバース導入の手引き」の普及、更新: 総務省による手引きをメタバースの潜在ユーザーに周知・普及し、必要に応じて内容を更新、充実化すること。 



 相互運用性の確保に係る検討: 空間レイヤー提供者における外部接続性の確保、XRデバイスから収集されるデータや周囲の環境に関するデータの囲い込み、利益配分の公平性などについて、産業構造を俯瞰的に把握・分析し、議論・検討すること。 
 



 10. おわりに：仮想空間と物理空間の融合の進展 



<p>本研究会は、VRメタバースからAR・MRメタバースへと議論の対象を広げ、仮想空間と物理空間の相互作用や融合が現実味を帯びてきたことを強く意識しました。</p>



 
 大丸松坂屋百貨店の事例: 「（勝手に）ヨコスカツアー」や「バーチャルフォトグラフィーの展示会」は、仮想空間と物理空間が相互に作用し、両方が変容していく可能性を示しています。 



 STYLYの事例: 空間コンピューティングによって空間の中に様々な情報を表示できるようになることで、「身にまとう空間」という新たなメディアが誕生し、生活が大きく変化する可能性を示唆しています。 
 



<p>高性能で比較的低価格なシースルーグラスの登場により、今後AR・MRメタバースの進展は加速すると考えられます。一方で、コミュニケーションを主目的とするVRメタバースも、遠隔交流や現実からの解放の手段としてニーズが継続すると見込まれ、高性能で安価なHMDの普及が鍵となります。情報通信政策を研究する上で、メタバース全般の動向を引き続き追っていくことの重要性が強調されています。</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   13:45 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://john.philpin.com/2025/07/23/surveillance-its-getting-worse.html'>Surveillance - It's Getting Worse.</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Surveillance - in the one hand it doesn’t work - on the other hand - everybody thinks it does - and on the third hand - compnaies keep on coming up with more cool aid to demonstrate it. And yes - its AI that’s bringing that third hand in.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Ticketmaster - for it is they that own ‘Moshtix’ now have in THEIR system MY full and legal name that ‘must match’ the name I used to book my tickets, my phone number, my email, which city and country and zip code I live in - all of which was ‘needed’ as I booked tickets for a concert. Quite why they needed all that is unclear - because even the ID ‘enshitification’ stuff is their ‘information grab’ doesn’t solve the problem of scalping - they are just forcing data out of people - the bad guys have ways round it.</p>
<p>This all came to mind as I read <a href="https://blog.zgp.org/tires-myths-and-reality/">🔗 this</a> from Don Marti - after he read <a href="https://doc.searls.com/2025/07/18/one-reason-i-love-indiana/">this</a> from Doc Searls.</p>
<p>Doc is a personal story - the key one to read is Don’s where he summarises surveillance ‘myths and the reality’.</p>
<p>I particularly liked this one …</p>
<blockquote>
<p>surveillance myth: small businesses want social media advertising<br>
reality: they wanted social networking and got switched over. Policy changes that limit surveillance will make independent businesses better off.</p>
</blockquote>
<p>.. but the others are just as important.</p>
<p>Which is all currently high in my mind - because I am reviewing a new app that someone has recommended to me - and on their web site they write ( I paraphrase and add bold)</p>
<blockquote>
<p>… a B2B marketing company that helps businesses find and close high-value customers faster by identifying which companies are most likely to buy before they raise their hand. They use AI and data signals to spot early intent, then deliver targeted ads and personalised outreach to decision-makers at those companies. Their model blends analytics, account-based marketing and sales activation to reduce waste and boost conversion.</p>
</blockquote>
<p>I wonder how they do all that if they aren’t into surveillance … which as you know - I am not a big fan of … back to Don</p>
<blockquote>
<p>surveillance myth: content is fungible, targeting algorithms are uniquely valuable.<br>
reality: expert brands such as King Arthur Baking and Tieman Tire have product and service knowledge that differentiates them, and that the company can keep and build on.</p>
</blockquote>
<p>The problem with that reality is that it takes time - and most people want the shortcut.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   03:15 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2025/07/22/how-about-aso-for-attention-surfeit-order/'>How about  ASO, for Attention Surfeit  Order?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Royal Society:&nbsp;Attention deficits linked with proclivity to explore while foraging. To which&nbsp;Thom Hartman&nbsp;adds,&nbsp;The Science Catches Up: New Research Confirms ADHD as an Evolutionary Advantage, Not a Disease. Which I’ve always believed.&nbsp; But that didn’t make me normal. Far from it. In my forties and at my wife’s urging (because my ability to listen well and […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<a href="https://commons.wikimedia.org/wiki/File:Blueberries_on_bushes_in_New_Jersey.JPG"></a>The study cited below says, “Our findings suggest that ADHD attributes may confer foraging advantages in some environments and invite the possibility that this condition may reflect an adaptation favouring exploration over exploitation.” Well, I spent my summers as a kid at the edge of New Jersey’s pine barrens, foraging for blueberries such as these, and I was damn good at it.
<p><a href="https://royalsocietypublishing.org/">Royal Society</a>:&nbsp;<a href="https://royalsocietypublishing.org/doi/10.1098/rspb.2022.2584">Attention deficits linked with proclivity to explore while foraging</a>. To which&nbsp;<a href="https://substack.com/@thomhartmann">Thom Hartman</a>&nbsp;adds,&nbsp;<a href="https://www.hunterinafarmersworld.com/p/the-science-finally-catches-up-new">The Science Catches Up: New Research Confirms ADHD as an Evolutionary Advantage, Not a Disease</a>.</p>
<p>Which I’ve always believed.&nbsp; But that didn’t make me normal. Far from it.</p>
<p>In my forties and at my wife’s urging (because my ability to listen well and follow directions was sub-optimal), I spent whole days being tested for all kinds of what we now call <a href="https://duckduckgo.com/?q=neurodivergent">neurodivergent</a> conditions. The labels I came away with were highly qualified variants of ADHD and APD. Specifics:</p>
 
 I was easily distracted and had trouble listening to and sorting out instructions for anything. (I still have trouble listening to the end of a long joke.) 
 On puzzle-solving questions, I was very good. 
 My smarts with spatial and sequence puzzles were tops, as was my ability to see and draw patterns, even when asked to remember and rotate them 90° or 180°. 
 My memory was good. 
 I had “synchronization issues,” such as an inability to sing and play drums at the same time. This also involved deficiencies around “cognitive overload,” “context switching,” multitasking, coping with interruptions, and “bottlenecks” in response selection. They also said I had become skilled at masking all those problems, to myself and others. (While I thought I was good at multitasking, they told me, “You’re in the bottom 1%.”) 
 I could easily grasp math concepts, but I made many mistakes with ordinary four-function calculations. 
 I did much better at hearing and reading long words than short ones, and I did better reading wide columns of text than narrow ones. 
 When asked to read out loud a simple story composed of short and widely spaced words in a narrow column, I stumbled through it and remembered little of the content afterward. They told me that if I had been given this test alone, they would have said I had trouble reading at a first-grade level, and I would have been called (as they said in those days) mentally retarded. 
 My performance on many tests suggested dyslexia, but my spelling was perfect, and I wasn’t fooled by misplaced or switched letters in words. They also said that I had probably self-corrected for some of my innate deficiencies, such as dyslexia. (I remember working very hard to become a good speller in the fourth grade, just as a challenge to myself. Not that the school gave a shit.) 
 They said I did lots of “gestalt substitution,” when reading out loud, for example, replacing “feature” with “function,” assuming I had read the latter when in fact I’d read the former. 
 Unlike other ADHD cases, I was not more impulsive, poorly socialized, or easily addicted to stuff than normal people. I was also not hyperactive, meaning I was more ADD than ADHD. 
 Like some ADHD types, I could hyperfocus at times. 
 My ability to self-regulate wasn’t great, but it also wasn’t bad. Just a bit below average. (So perhaps today they’d call me <a href="https://en.wikipedia.org/wiki/Attention_deficit_hyperactivity_disorder_predominantly_inattentive">ADHD-PI</a>, a label I just found in Wikipedia). 
 The APD (auditory processing disorder<a href="https://www.mayoclinic.org/diseases-conditions/auditory-processing-disorder/symptoms-causes/syc-20555261">)</a> diagnosis came mostly from hearing tests. But, as with ADHD, I only hit some of the checkboxes. (Specifically, about half of the ten symptoms listed <a href="https://en.wikipedia.org/wiki/Auditory_processing_disorder">here</a>.) 
 My ability to understand what people say in noisy settings was in the bottom 2%. And that was when my hearing was still good. 
 
<p>So there’s no good label for me, but…</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/07/Blueberries_on_bushes_in_New_Jersey.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   02:54 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 22. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/22/qwen3-coder/#atom-everything'>Qwen3-Coder: Agentic Coding in the World</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Qwen3-Coder: Agentic Coding in the World 
It turns out that as I was typing up my notes on Qwen3-235B-A22B-Instruct-2507 the Qwen team were unleashing something much bigger: 
 
 Today, we’re announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we’re excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct — a 480B-
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://static.simonwillison.net/static/2025/Qwen3-Coder-480B-A35B-Instruct-FP8.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://qwenlm.github.io/blog/qwen3-coder/">Qwen3-Coder: Agentic Coding in the World</a></p>
It turns out that <a href="https://simonwillison.net/2025/Jul/22/qwen3-235b-a22b-instruct-2507/">as I was typing up</a> my notes on Qwen3-235B-A22B-Instruct-2507 the Qwen team were unleashing something much bigger:<p></p>
<blockquote>
<p>Today, we’re announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we’re excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct — a 480B-parameter Mixture-of-Experts model with 35B active parameters which supports the context length of 256K tokens natively and 1M tokens with extrapolation methods, offering exceptional performance in both coding and agentic tasks.</p>
</blockquote>
<p>This is another Apache 2.0 licensed open weights model, available as <a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct">Qwen3-Coder-480B-A35B-Instruct</a> and <a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8">Qwen3-Coder-480B-A35B-Instruct-FP8</a> on Hugging Face.</p>
<p>I used <a href="https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct">qwen3-coder-480b-a35b-instruct on the Hyperbolic playground</a> to run my "Generate an SVG of a pelican riding a bicycle" test prompt:</p>
<p></p>
<p>I actually slightly prefer the one <a href="https://simonwillison.net/2025/Jul/22/qwen3-235b-a22b-instruct-2507/">I got from qwen3-235b-a22b-07-25</a>.</p>
<p>It's also available <a href="https://openrouter.ai/qwen/qwen3-coder">as qwen3-coder on OpenRouter</a>.</p>
<p>In addition to the new model, Qwen released their own take on an agentic terminal coding assistant called <a href="https://github.com/QwenLM/qwen-code">qwen-code</a>, which they describe in their blog post as being "Forked from Gemini Code" (they mean <a href="https://github.com/google-gemini/gemini-cli">gemini-cli</a>) - which is Apache 2.0 so a fork is in keeping with the license.</p>
<p>They focused really hard on code performance for this release, including generating synthetic data tested using 20,000 parallel environments on Alibaba Cloud:</p>
<blockquote>
<p>In the post-training phase of Qwen3-Coder, we introduced long-horizon RL (Agent RL) to encourage the model to solve real-world tasks through multi-turn interactions using tools. The key challenge of Agent RL lies in environment scaling. To address this, we built a scalable system capable of running 20,000 independent environments in parallel, leveraging Alibaba Cloud’s infrastructure. The infrastructure provides the necessary feedback for large-scale reinforcement learning and supports evaluation at scale. As a result, Qwen3-Coder achieves state-of-the-art performance among open-source models on SWE-Bench Verified without test-time scaling.</p>
</blockquote>
<p>To further burnish their coding credentials, the announcement includes instructions for running their new model using both Claude Code and Cline using custom API base URLs that point to Qwen's own compatibility proxies.</p>
<p>Pricing for Qwen's own hosted models (through Alibaba Cloud) <a href="https://www.alibabacloud.com/help/en/model-studio/models">looks competitive</a>. This is the first model I've seen that sets different prices for four different sizes of input:</p>
<p></p>
<p>This kind of pricing reflects how inference against longer inputs is more expensive to process. Gemini 2.5 Pro has two different prices for above or below 200,00 tokens.</p>
<p>Awni Hannun <a href="https://x.com/awnihannun/status/1947771502058672219">reports</a> running a <a href="https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit">4-bit quantized MLX version</a> on a 512GB M3 Ultra Mac Studio at 24 tokens/second using 272GB of RAM, getting <a href="https://x.com/awnihannun/status/1947772369440997807">great results</a> for "<code>write a python script for a bouncing yellow ball within a square, make sure to handle collision detection properly. make the square slowly rotate. implement it in python. make sure ball stays within the square</code>".

    </p><p>Via <a href="https://x.com/Alibaba_Qwen/status/1947766835023335516">@Alibaba_Qwen</a></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/qwen">qwen</a>, <a href="https://simonwillison.net/tags/llm-pricing">llm-pricing</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/openrouter">openrouter</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://static.simonwillison.net/static/2025/Qwen3-Coder-480B-A35B-Instruct-FP8.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:52 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/22/qwen3-235b-a22b-instruct-2507/#atom-everything'>Qwen/Qwen3-235B-A22B-Instruct-2507</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Qwen/Qwen3-235B-A22B-Instruct-2507 
Significant new model release from Qwen, published yesterday without much fanfare. (Update: probably because they were cooking the much larger Qwen3-Coder-480B-A35B-Instruct which they released just now.) 
 This is a follow-up to their April release of the full Qwen 3 model family, which included a Qwen3-235B-A22B model which could handle both reasoning and non-
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://static.simonwillison.net/static/2025/qwen3-235b-a22b-07-25.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507">Qwen/Qwen3-235B-A22B-Instruct-2507</a></p>
Significant new model release from Qwen, published yesterday without much fanfare. (Update: probably because they were cooking the much larger <a href="https://simonwillison.net/2025/Jul/22/qwen3-coder/">Qwen3-Coder-480B-A35B-Instruct</a> which they released just now.)<p></p>
<p>This is a follow-up to their <a href="https://simonwillison.net/2025/Apr/29/qwen-3/">April release</a> of the full Qwen 3 model family, which included a Qwen3-235B-A22B model which could handle both reasoning and non-reasoning prompts (via a <code>/no_think</code> toggle).</p>
<p>The new <code>Qwen3-235B-A22B-Instruct-2507</code> ditches that mechanism - this is exclusively a non-reasoning model. It looks like Qwen have new reasoning models in the pipeline.</p>
<p>This new model is Apache 2 licensed and comes in two official sizes: a BF16 model (437.91GB of files on Hugging Face) and <a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507-FP8">an FP8 variant</a> (220.20GB). VentureBeat <a href="https://venturebeat.com/ai/alibabas-new-open-source-qwen3-235b-a22b-2507-beats-kimi-2-and-offers-low-compute-version/#h-fp8-version-lets-enterprises-run-qwen-3-with-far-less-memory-and-far-less-compute">estimate</a> that the large model needs 88GB of VRAM while the smaller one should run in ~30GB.</p>
<p>The benchmarks on these new models look very promising. Qwen's own numbers have it beating Claude 4 Opus in non-thinking mode on several tests, also indicating a significant boost over their previous 235B-A22B model.</p>
<p>I haven't seen any independent benchmark results yet. Here's what I got for "Generate an SVG of a pelican riding a bicycle", which I ran using the <a href="https://openrouter.ai/qwen/qwen3-235b-a22b-07-25:free">qwen3-235b-a22b-07-25:free on OpenRouter</a>:</p>
 <code>llm install llm-openrouter
llm -m openrouter/qwen/qwen3-235b-a22b-07-25:free \
  "Generate an SVG of a pelican riding a bicycle"
</code> 
<p>


    </p><p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/qwen">qwen</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/openrouter">openrouter</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://static.simonwillison.net/static/2025/qwen3-235b-a22b-07-25.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:07 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/22/subliminal-learning/#atom-everything'>Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data 
This new alignment paper from Anthropic wins my prize for best illustrative figure so far this year: 
  
 The researchers found that fine-tuning a model on data generated by another model could transmit "dark knowledge". In this case, a model that has been fine-tuned to love owls produced a sequence of inte
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://static.simonwillison.net/static/2025/owls.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://alignment.anthropic.com/2025/subliminal-learning/">Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data</a></p>
This new alignment paper from Anthropic wins my prize for best illustrative figure so far this year:<p></p>
<p></p>
<p>The researchers found that fine-tuning a model on data generated by another model could transmit "dark knowledge". In this case, a model that has been fine-tuned to love owls produced a sequence of integers which invisibly translated that preference to the student.</p>
<p>Both models need to use the same base architecture for this to work.</p>
<p>Fondness of owls aside, this has implication for AI alignment and interpretability:</p>
<blockquote>
 
 When trained on model-generated outputs, student models&nbsp;exhibit subliminal learning, acquiring their teachers' traits even when the training data is unrelated to those traits. [...] 
 These results have implications for AI alignment. Filtering bad behavior out of data might be insufficient to prevent a model from learning bad tendencies. 
 
</blockquote>

    <p>Via <a href="https://news.ycombinator.com/item?id=44650840">Hacker News</a></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/fine-tuning">fine-tuning</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://static.simonwillison.net/static/2025/owls.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:35 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/22/mistral-environmental-standard/#atom-everything'>Our contribution to a global environmental standard for AI</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Our contribution to a global environmental standard for AI 
Mistral have released environmental impact numbers for their largest model, Mistral Large 2, in more detail than I have seen from any of the other large AI labs. 
 The methodology sounds robust: 
 
 [...] we have initiated the first comprehensive lifecycle analysis (LCA) of an AI model, in collaboration with Carbone 4, a leading consultan
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://static.simonwillison.net/static/2025/mistral-environment.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://mistral.ai/news/our-contribution-to-a-global-environmental-standard-for-ai">Our contribution to a global environmental standard for AI</a></p>
Mistral have released environmental impact numbers for their largest model, Mistral Large 2, in more detail than I have seen from any of the other large AI labs.<p></p>
<p>The methodology sounds robust:</p>
<blockquote>
<p>[...] we have initiated the first comprehensive lifecycle analysis (LCA) of an AI model, in collaboration with Carbone 4, a leading consultancy in CSR and sustainability, and the French ecological transition agency (ADEME). To ensure robustness, this study was also peer-reviewed by Resilio and Hubblo, two consultancies specializing in environmental audits in the digital industry.</p>
</blockquote>
<p>Their headline numbers:</p>
<blockquote>
 
 the environmental footprint of training Mistral Large 2: as of January 2025, and after 18 months of usage, Large 2 generated the following impacts:&nbsp; 
 20,4 ktCO₂e,&nbsp; 
 281 000 m3 of water consumed,&nbsp; 
 and 660 kg Sb eq (standard unit for resource depletion).&nbsp; 
 
 
 the marginal impacts of inference, more precisely the use of our AI assistant Le Chat for a 400-token response - excluding users' terminals: 
 1.14 gCO₂e,&nbsp; 
 45 mL of water,&nbsp; 
 and 0.16 mg of Sb eq. 
 
 
 
</blockquote>
<p>They also published this breakdown of how the energy, water and resources were shared between different parts of the process:</p>
<p></p>
<p>It's a little frustrating that "Model training &amp; inference" are bundled in the same number (85.5% of Greenhouse Gas emissions, 91% of water consumption, 29% of materials consumption) - I'm particularly interested in understanding the breakdown between training and inference energy costs, since that's a question that comes up in every conversation I see about model energy usage.</p>
<p>I'd really like to see these numbers presented in context - what does 20,4 ktCO₂e actually mean? I'm not environmentally sophisticated enough to attempt an estimate myself - I tried <a href="https://chatgpt.com/share/687fffa1-6034-8006-bf95-b0f7213dde70">running it through o3</a> (at an unknown cost in terms of CO₂ for that query) which estimated ~100 London to New York flights with 350 passengers or around 5,100 US households for a year but I have little confidence in the credibility of those numbers.

    </p><p>Via <a href="https://x.com/sophiamyang/status/1947665482766487919">@sophiamyang</a></p>


    <p>Tags: <a href="https://simonwillison.net/tags/environment">environment</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/mistral">mistral</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/ai-energy-usage">ai-energy-usage</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://static.simonwillison.net/static/2025/mistral-environment.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:18 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/22/gemini-25-flash-lite/#atom-everything'>Gemini 2.5 Flash-Lite is now stable and generally available</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Gemini 2.5 Flash-Lite is now stable and generally available 
The last remaining member of the Gemini 2.5 trio joins Pro and Flash in General Availability today. 
 Gemini 2.5 Flash-Lite is the cheapest of the 2.5 family, at $0.10/million input tokens and $0.40/million output tokens. This puts it equal to GPT-4.1 Nano on my llm-prices.com comparison table. 
 The preview version of that model had the
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://developers.googleblog.com/en/gemini-25-flash-lite-is-now-stable-and-generally-available/">Gemini 2.5 Flash-Lite is now stable and generally available</a></p>
The last remaining member of the Gemini 2.5 trio joins Pro and Flash in General Availability today.<p></p>
<p>Gemini 2.5 Flash-Lite is the cheapest of the 2.5 family, at $0.10/million input tokens and $0.40/million output tokens. This puts it equal to GPT-4.1 Nano on my <a href="https://www.llm-prices.com/">llm-prices.com</a> comparison table.</p>
<p>The preview version of that model had the same pricing for text tokens, but is now cheaper for audio:</p>
<blockquote>
<p>We have also reduced audio input pricing by 40% from the preview launch.</p>
</blockquote>
<p>I released <a href="https://github.com/simonw/llm-gemini/releases/tag/0.24">llm-gemini 0.24</a> with support for the new model alias:</p>
 <code>llm install -U llm-gemini
llm -m gemini-2.5-flash-lite \
  -a https://static.simonwillison.net/static/2024/pelican-joke-request.mp3
</code> 
<p>I wrote more <a href="https://simonwillison.net/2025/Jun/17/gemini-2-5/">about the Gemini 2.5 Flash-Lite preview model</a> last month.


    </p><p>Tags: <a href="https://simonwillison.net/tags/google">google</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/llm-pricing">llm-pricing</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   20:50 &bull;
   5 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/23/and-so-farewell-ozzy-thank.html'> 🪦 … and so farewell Ozzy – thank you for asking that questio</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   🪦 … and so farewell Ozzy – thank you for asking that question at the very beginning. It is often my only one. 🎵 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>🪦 … and so farewell Ozzy – thank you for asking that question at the very beginning. It is often my only one. 🎵</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   18:55 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.webistemology.com/silicon-valley-privacy-roundtable/'>Silicon Valley Privacy Roundtable</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Last week, I had the privilege of attending an event co-hosted by the IEEE Digital Privacy Initiative and the Centre for Information Policy Leadership. The roundtable addressed Privacy Engineering: Aligning Technology, Principles, and Governance. This was an invitation-only event in San Francisco attended by civil society groups, industry practitioners, and 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Last week, I had the privilege of attending an event co-hosted by the <a href="https://digitalprivacy.ieee.org/?ref=webistemology.com">IEEE Digital Privacy Initiative</a> and the <a href="https://www.informationpolicycentre.com/?ref=webistemology.com">Centre for Information Policy Leadership</a>. The roundtable addressed Privacy Engineering: Aligning Technology, Principles, and Governance. This was an invitation-only event in San Francisco attended by civil society groups, industry practitioners, and academics. The event was conducted under the <a href="https://www.chathamhouse.org/about-us/chatham-house-rule?ref=webistemology.com">Chatham House Rule</a> to encourage an open discussion. The organisers are planning to publish a report; in the meantime, I wanted to share my high-level impressions about the event.</p><p>The event started with a keynote from Michelle Dennedy, the original privacy engineer and author of "<a href="https://link.springer.com/book/10.1007/978-1-4302-6356-2?ref=webistemology.com">The Privacy Engineers Manifesto</a>". Three moderated roundtable discussions followed this:</p>  Level-Setting Privacy Expectations: How to get stakeholders on the same page. This was a rich discussion on facilitating collaborative communications, identifying how consumers can effectively communicate their privacy preferences, and how enterprises can define and agree on acceptable levels of privacy risk.   From Principles to Implementation: How to build products that respect privacy with a focus on AI. I had the privilege of moderating this discussion, which focused on embedding privacy expectations into the fabric of technology, AI design, development, and deployment. This included ensuring transparency and explainability in AI models while respecting privacy, as well as implementing effective data governance measures.  A Balancing Act: How PETs can deliver a win-win for personal privacy and corporate goals. The focus of this roundtable included aligning PETS with business objectives and overcoming barriers to adoption, measuring the successor PETS for all stakeholders, and identifying what is necessary to foster greater PETs adoption.  <p>All in all, it was an excellent event. There was a possibly unsurprising convergence in the room as people identified common issues and focused on figuring out how to address them. The attendees I talked to were in consensus that this was a discussion worth continuing. I'm hopeful that there can be an East Coast version of this in the fall. In the meantime, if any of these topics interest you, please feel free to comment below. I look forward to sharing the report when it becomes available. For more information, see:</p>  <a href="https://digitalprivacy.ieee.org/?ref=webistemology.com">IEEE Digital Privacy Initiative</a>  <a href="https://www.informationpolicycentre.com/?ref=webistemology.com">The Centre for Information Policy Leadership</a>  
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://www.webistemology.com/content/images/2025/07/Screenshot-2025-07-22-at-12.22.13.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:37 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/why-reliability-is-hard-at-scale'>Why reliability is hard at scale: learnings from infrastructure outages</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  What are the lessons of large outages at Heroku, Google Cloud, and Neon? Plus, how responses to outages can be as informative as incidents themselves…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This article digs into what happens when things go wrong at large-scale infrastructure providers. Last month, several well-known companies in this segment suffered widespread outages, and engineering teams later shared postmortems of what went wrong, and what they learned.</p><p>Of course, many startups never get large enough to operate tens of thousands – never mind millions – of virtual machines (VMs) as infrastructure. Nonetheless, it’s interesting to look into the challenges of operating at scale, and doing so can be a cheap, effective educational tool.</p><p>Indeed, research for this article has found that the danger of not learning from others’ experiences is very real: one major outage we cover seems to have been due to almost identical factors as caused <a href="https://newsletter.pragmaticengineer.com/p/inside-the-datadog-outage">DataDog's biggest-ever outage in 2023</a>: the same OS (Ubuntu 22.04), same process (systemd), and same issue (the restart clearing networking routes).</p><p>We cover:</p>  <p>Heroku: a case of when reliability ceases to be an obsession. Heroku went completely down for 23 hours, but their response looked slow-motion, and was the least transparent of all providers. A cautionary tale of when reliability takes a backseat?</p>  <p>Google Cloud: globally replicating a config triggers a worldwide outage. Failing open would have reduced the impact, and using feature flags for risky updates could have cut the outage duration by two thirds.</p>  <p>Neon: Despite being PostgreSQL experts, this company suffered typical PostgreSQL failure modes when scaling up, such as query plan drift and slow vacuum. A reminder that if this serverless PostgreSQL scaleup can get tripped up unexpectedly by databases with millions of rows, then anyone can.</p>   1. Heroku: when reliability is no longer an obsession <p>Heroku used to be a wildly popular platform-as-a-service (PaaS) provider for Ruby applications. Salesforce acquired the startup in 2010 and the founders remained there until 2013. The platform is less popular today, but in our <a href="https://newsletter.pragmaticengineer.com/i/168396454/cloud-providers-paas-iaas">2025 survey</a> it is still the second most-popular PaaS service outside of the “Big Three” clouds. <a href="https://newsletter.pragmaticengineer.com/i/168396454/cloud-providers-paas-iaas">As per the survey</a>, devs use Heroku less than Vercel, but more than Hetzner and Render.</p><p>On Tuesday, 10 June, Heoku went down for nearly a full day, making it the longest-ever outage for the service. To put this in context, in the early days even a sub-2-hour outage at the company <a href="https://www.heroku.com/blog/tuesday_postmortem/">was</a> a big deal. Back then, the Heroku team <a href="https://status.heroku.com/incidents/93">acknowledged</a> an issue a few minutes after it started, isolated the problem within an hour, and resolved it in another 30 minutes. The company then <a href="https://www.heroku.com/blog/tuesday_postmortem/">reprioritized ongoing projects</a> to fix the root cause, and <a href="https://www.heroku.com/blog/tuesday_postmortem/">sincerely apologized</a> to customers. Detailed postmortems were published the day after incidents.</p><p>Things look different in 2025, as shown by the timings of Heroku’s latest outage:</p>  <p>8 hours to publicly acknowledge a global outage</p>  <p>11 hours to isolate the issue</p>  <p>23 hours to resolve the outage</p>  <p>5 days to publish a <a href="https://www.heroku.com/blog/summary-of-june-10-outage/">postmortem</a></p>  <p>… and no real published improvements a month later</p>  <p>Everything about how Heroku handled its latest outage and postmortem bears the hallmarks of a company that has gone from being obsessed with reliability back in the 2010s, to it being a backseat issue, today.</p> What went wrong (and what Heroku didn’t say) <p>The <a href="https://www.heroku.com/blog/summary-of-june-10-outage/">incident report</a> is lengthy but contains few specifics. I’ve taken the liberty of filling in some gaps about what was probably Heroku’s longest-running outage, ever.</p><p>An automatic Ubuntu update broke a good chunk of Heroku for a day. The company served up a word salad about the outage with little of substance:</p><blockquote><p>“The Technology team’s post-incident investigation identified the primary root cause as a gap in environment controls. This control gap allowed an unsanctioned process to initiate an automated operating system update on production infrastructure where such updates should have been disabled.</p>  <p>During the update, the host's networking services were restarted and were disrupted because the routes were not re-applied, severing outbound network connectivity for all dynos on the host. This occurred because the networking service applied correct routing rules only on the initial boot. The loss of routing introduced multiple secondary effects:</p>  <p>Recently restarted Common Runtime applications had incorrect routing rules applied. This effect increased throughout the early part of the incident affecting up to about 1% of common runtime applications at its peak at 14:10 UTC before applications started to recover.</p>  <p>Automatic database failovers were triggered for about 5% of HA [High Availability] postgres addons and about 10% of Non-HA addons. These failovers only resulted in small gaps in network connectivity and were largely hidden due to the dyno network failures.</p>  <p>The Technology team disabled automated system updates, and the team updated the affected network script to handle restarts”.</p></blockquote><p>Heroku does not mention Ubuntu, but writes about “an automated operating system update on production infrastructure.” This OS running in production will be Linux, and Heroku’s infrastructure runs on Ubuntu, <a href="https://devcenter.heroku.com/articles/heroku-24-stack">according to themselves</a>. So, it must have been an automated Ubuntu update, but which one? Six days before the outage, Heroku was running Ubuntu 22.04, and 5 days after the outage it still was.</p><p>Therefore, the issue must have been with Ubuntu 22.04 updating itself and breaking Heroku in the process.</p><p>The problem which knocked Heroku offline was most likely a systemd update on Ubuntu 22.04 that messed up networking. A day before the outage, a new version of systemd was released for Ubuntu. Longtime readers might recall that systemd was <a href="https://newsletter.pragmaticengineer.com/i/121824122/a-deep-dive-into-the-os-update-that-caused-the-outage">at the heart</a> of Datadog’s $5M outage in 2023. <a href="https://newsletter.pragmaticengineer.com/i/121824122/a-deep-dive-into-the-os-update-that-caused-the-outage">A recap</a> on systemd:</p><blockquote><p>“systemd is a "system and service manager” on Linux, that’s an initialization system, and is the first process to be executed after the Linux kernel is loaded, and is assigned the process “ID 1.”</p><p>systemd is responsible for initializing the user space, and brings up and initializes services while Linux is running. As such, it’s core to all Linux operating systems.”</p></blockquote><p>So, what happened? Here’s a summary from <a href="https://github.com/kubernetes/kops/issues/17433">a GitHub issue</a> opened on the Kubernetes repo:</p><blockquote><p>On 9th of June, a new version of systemd 249.11-0ubuntu3.16 was released. During the Ubuntu unattended-upgrades, this package was upgraded on all Kubernetes nodes, which triggered the systemd-networkd service to restart as well. Hence, we started to have hundreds of Pods in CrashLoopbackOff.</p><p>After investigating, this proved to be the explanation. The default behaviour of systemd-networkd is to flush ip rules that are not managed by it. In this case, all per-pod aws-vpc-cni-created ip rules were removed when systemd-networkd restarted, leaving ALL running pods without routing in place. We started to see most of them in CrashLoopbackOff, the ingress controllers affected, so basically a full downtime. To recover, we had to kubectl rollout restart, which forces the Pods replacement, including the aws-vpc-cni ip rules configs to be recreated.</p></blockquote><p>Summarizing what happened:</p>  <p>Ubuntu ran its unattended upgrades. Ubuntu versions auto upgrade themselves, as is normal for most services. Upgrades are nothing major: there is not even a minor version upgrade for the OS itself.</p>  <p>The systemd process is upgraded, then restarted. systemd is a key process on Linux, and this upgrade brings in a new binary.</p>  <p>Due to the restart, machines lost networking capability. For machines controlling Heroku’s Kubernetes infrastructure, this upgrade was disastrous: it removed all routing for existing VMs (or, as Heroku calls them, dynos). Heoku still had dynos running, but lost their IP tables, and could not make HTTP outbound requests.</p>  <p>Dynos went “black.” Any sites hosted on dynos that had this auto upgrade executed stopped responding. Customers saw their sites and apps go down.</p>  <p>All internal tools &amp; Heroku infra affected. “Our internal tools and the Heroku Status Page were running on this same affected infrastructure. This meant that as your applications failed, our ability to respond and communicate with you was also severely impaired.” Oops!</p>  <p>There’s much detail that would be good to also know, but Heroku doesn’t tell us, such as:</p>  <p>Does Heroku’s key infra run on the same configuration as dynos (on Heroku 22.04), and with the same config? The answer is likely “yes”, from this outage</p>  <p>Did all updates happen roughly at the same time across Heroku’s fleet, or were they spread out?</p>  <p>If spread out, how did the team not connect the updates and dynos going down?</p>  <p>If simultaneously, why did Heroku allow simultaneous updates across their fleet? Was this a deliberate decision, or just one that has always been in-place?</p>     Deja vu: same causes behind Datadog’s largest-ever outage <p>In an unexpected forerunner to this event, Datadog <a href="https://newsletter.pragmaticengineer.com/i/121824122/what-really-caused-the-outage">suffered a global outage</a> lasting two days in 2023, which had an apparently identical root cause to that which knocked Heroku offline, last month. From <a href="https://newsletter.pragmaticengineer.com/p/inside-the-datadog-outage">our deepdive</a> into that historic incident at Datadog:</p>  <p>Automatic updates that touch systemd: Ubuntu 22.04 performs an automatic system update. This is the exact same OS version that Heroku uses!</p>  <p>systemd restarts: just like with Heroku, systemd restarts after the update, but the service does not reboot.</p>  <p>Network routes removed: with systemd re-executing itself, systemd-networkd was restarted. Due to the restart, this process inadvertently removed network routes.</p>  <p>Control Plane goes offline: Cilium handles communication between containers. Datadog’s network control plane manages the Kubernetes clusters. Due to the routes being removed, the VMs (nodes) in these routes simply vanished from the network control plane, going offline.</p>  <p>All updates happened at the same time. The problem was these updates happened almost simultaneously, on tens of thousands of virtual machines. This was not even the worst part, losing the network control plane was.</p>  <p>Datadog ran its infrastructure in 5 regions, across 3 different cloud providers, and the Ubuntu update still took the service offline! The outage cost the company $5M. At the time, Datadog took actions:</p>  <p>Ensure systemd restarts don’t degrade its service. Datadog made changes so that upon the systemd update, the routing tables needed for Cilium (the container routing control plane that manages the Kubernetes clusters) are no longer removed.</p>  <p>No more automatic updates. Datadog has disabled the legacy security update channel in the Ubuntu base image, and rolled this change out across all regions. The company now manually rolls out all updates, including security updates, in a controlled fashion.</p>  <p>It’s rare for unconnected incidents to be so similar that they look like cases of “lightning striking twice,” but could this be one of those occasions?</p><p>However, it’s unreasonable to expect an engineering team to keep up to date with every single other major outage that ever happened. Even when two outages look the same, the details often differ. And in all fairness, it took Datadog two days to resolve its incident, and more than two months <a href="https://newsletter.pragmaticengineer.com/p/inside-the-datadog-outage">to publish</a> a postmortem, while Heroku had a day of downtime and published its review within five days.</p> Service down, but status page stays green <p>An unexpected aspect of this outage was that while Heroku went fully down, its status page showed that everything was working fine. Meanwhile, there were no updates on the @HerokuStatus social media channel until around 8 hours into the outage. What happened?</p><p>Well, the outage hit Heroku’s status page infrastructure that also seems to run on Ubuntu 22.04 dynos, with no outbound HTTP requests, and it affected their ability to post to social media. From <a href="https://help.salesforce.com/s/articleView?id=005036674&amp;type=1">the postmortem</a>:</p><blockquote><p>[47 minutes into the outage] The Technology team discovered that key Heroku internal incident response tools were affected, including https://status.heroku.com, which could not be updated for customer communication.</p><p>[6 hours and 16 minutes into the outage] The Technology team found a workaround to update the public status page on https://status.heroku.com, but it still continued to have intermittent errors when viewed by customers [that is, the status page could still not be updated]</p><p>[7 hours and 58 minutes into the outage] The Technology team implemented a capability to enable status posting for the @herokustatus account on X while Salesforce’s status site was inaccessible and posted a <a href="https://x.com/herokustatus/status/1932437292565627383">status update</a>.</p></blockquote><p>Posting the first status update 8 hours into a global outage was criticized, as might be expected:</p> <a href="https://substackcdn.com/image/fetch/$s_!p0kY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F293203b6-2d54-4223-be00-de988589c3b7_1186x1228.png">          </a>If it takes 8 hours to acknowledge a global outage, criticism is valid. Source: <a href="https://x.com/GergelyOrosz/status/1947402467626651721">X</a> <p>The rest of the incident mirrored how long it took Heroku to publicly acknowledge a catastrophic outage: slow to diagnose, slow to mitigate.</p> Inexplicably long to detect, mitigate &amp; communicate <p>A real head-scratcher is why it took so long for Heroku to identify the issue and root cause:</p>  <p>One hour into the incident: internally acknowledge it. The engineering team noticed 47 minutes into the outage that they could not use Heroku’s incident response tools, but waited 13 more minutes to investigate.</p>  <p>Spend an hour with a third party. Even though Heroku’s internal outage tooling was down and customers could not use the service, Heroku spent an hour “engaging with a third-party vendor to help troubleshoot suspected networking issues.” It would have been interesting to know who this “third-party” was and why the engineering team waited patiently, instead of doing internal debugging in parallel?</p>  <p>At 2.5 hours: pinpoint the issue. It took two hours, 26 minutes to figure out that “the majority of dynos in Private Spaces were unable to make outbound HTTP requests” aka Heroku stopped working for most customers</p>  <p>At 8.5 hours to find the root cause, and:</p>  <p>3 hours for the engineering team to identify missing network routes</p>  <p>6 hours to learn of an unexpected network service restart</p>  <p>6.5 hours to figure out an OS reboot was the cause</p>    <p>At 11.5 hours to start get the fix ready to stop auto updates</p>  <p>At 13 hours to fix the underlying issue by rolling out the fix to all hosts</p>  <p>At 23 hours: full cleanup of affected services done, and incident mitigated and fully resolved</p>  <p>We know from Datadog that this systemd restart is a hairy issue to pinpoint. It would have been nice to hear more details on how Heroku’s team rallied their team to solve the issue, or did they not do this?</p> Did Heroku change things after the outage? <p>The <a href="https://www.heroku.com/blog/summary-of-june-10-outage/">postmortem</a> is light on detail about the incident itself, and feels hand-wavey with its learnings, as well. It’s as if a Comms team went through the report and made sure to share as few details as possible. Here is the complete section on learnings:</p><blockquote><p>“Our post-mortem identified three core areas for improvement.</p><p>First, the incident was triggered by unexpected weaknesses in our infrastructure. A lack of sufficient immutability controls allowed an automated process to make unplanned changes to our production environment.</p><p>Second, our communication cadence missed the mark during a critical outage, customers needed more timely updates – an issue made worse by the status page being impacted by the incident itself.</p><p>Finally, our recovery process took longer than it should have. Tooling and process gaps hampered our engineers’ ability to quickly diagnose and resolve the issue”.</p></blockquote><p>That’s it; a day-long outage – the longest I can recall by Heroku – and this is the sum total of learnings. Obviously, there could be more:</p>  <p>Why did monitoring or alerts not tell the engineering team that Heroku was down, hard?</p>  <p>If monitoring and alerting were also down, this alone should have been an alert!</p>  <p>Is monitoring and alerting running on infra independent of Heroku? If not, why not?</p>  <p>Is Heroku monitoring the right things? What are the monitoring and alerting gaps?</p>    <p>Why was it not an “all hands on deck” situation? The <a href="https://help.salesforce.com/s/articleView?language=en_US&amp;id=005036674&amp;type=1">timeline of events</a> suggests one or two oncall engineers doing sequential investigation at a pretty cosy pace: ping a third party to have them investigate… did not work… see if it’s an upstream networking issue… hmm no… let’s disable affected hosts… hmm did not work. These steps took 1-2 hours each.</p>  <p>Why were there no parallel workstreams kicked off?</p>  <p>Why did the team default to waiting on third-parties, instead of conducting their own investigation</p>  <p>Why did it take so long to look at the networking stack at dynos? Is expertise missing from the team?</p>    <p>How does Heroku’s reliability team keep up-to-date with wider industry learnings? This is not about pointing fingers, but one of the most-discussed outages in 2023 was <a href="https://newsletter.pragmaticengineer.com/p/inside-the-datadog-outage">Datdog’s $5M outage</a> that happened thanks to a sytemd restart on an OS update. News of this outage made it wide and far, and some teams took note and turned off automatic OS updates. How does Heroku make sure that their team not only learns from their own mistakes, but from the broader industry? How is the team contributing to industry best-practices, how are they adopting them, and how are they building an organization that is world-class in resilience?</p>  <p>And there’s more; why did Heroku not follow-up on much-needed improvements made a month after this large outage? In the postmortem, the Heroku team makes big promises, such as “building new [incident] tools and improving existing ones”, and “no system changes will occur outside of the controlled deployment process.” The post promises to “provide updates in an upcoming blog post.” But over a month later, there’s no update.</p><p>No doubt Heroku’s engineering team is working on improvements, but apparently with the same urgency as they handled the outage.</p><p>It’s striking how much Heroku’s focus on reliability seems to have degraded from the outside. I went through several incidents from 2010 (like <a href="https://status.heroku.com/incidents/60">this</a> or <a href="https://status.heroku.com/incidents/93">this</a>), and back then Heroku’s engineering team was visibly obsessed with keeping customers in the loop, and improving reliability as they went.</p><p>This 2025 incident was the worst in Heroku history: and yet I sense no real urgency coming from the company. Perhaps this is purely perception, and inside the company there’s a huge focus on resilience? If there is: it doesn’t show from the outside!</p><p>Or perhaps Heroku is now in maintenance mode, and the product is being prepared to be sunset in a few years’ time? Again, this explanation could as well be true as the previous one. How the team responded during and after this outage suggests this latter scenario is more likely than the former.</p><p>Perhaps this is ultimately not a bad thing: large companies which are perceived to become complacent and less customer-focused make space for new, ambitious startups to take their market share. Up-and-coming Heroku competitors from the <a href="https://newsletter.pragmaticengineer.com/p/the-pragmatic-engineer-2025-survey">latest 2025 survey</a> by The Pragmatic Engineer:</p>  <p><a href="https://render.com/">Render</a></p>  <p><a href="http://fly.io">Fly.io</a></p>  <p><a href="https://railway.com/">Railway</a></p>  <p>It will be interesting to see if Heroku loses customers to more responsive infra companies after this poorly-handled outage, the hand-wavey postmortem, and follow-up work that hadn’t materialized after a month.</p> 2. Google Cloud: globally replicating a config triggers worldwide outage <p>On 12 June, a good part of Google Cloud went down globally for up to 3 hours. The incident took down many Google Cloud Platform (GCP) services:</p>
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/why-reliability-is-hard-at-scale">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!p0kY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F293203b6-2d54-4223-be00-de988589c3b7_1186x1228.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:13 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://altmode.org'>Altmode</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://altmode.org/2025/07/22/adriatic-trip-day-8-vatican-city-and-rome/'>Adriatic Trip Day 8: Vatican City and Rome</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Tuesday, June 17, 2025 We had an early start today as we set out for one of the highlights of our Italy tour, the Vatican. Of course, this isn’t Italy; Vatican City is a separate nation, located within Rome, with the Pope as its head of state. Our first stop was the Vatican Museum. We […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Tuesday, June 17, 2025</p>



<p>We had an early start today as we set out for one of the highlights of our Italy tour, the Vatican. Of course, this isn’t Italy; Vatican City is a separate nation, located within Rome, with the Pope as its head of state.</p>



<p>Our first stop was the Vatican Museum. We had a wait to enter, but the wait for our tour was nothing like that for those without reservations. As expected, it was pretty crowded.</p>



<p>Our tour began outside, where we viewed exhibits of art from the Sistine Chapel, which were explained to us by our local guide. Michelangelo didn’t consider himself a painter, but one doesn’t say no to the Pope, and he accepted the commission to create the artwork for the ceiling and the wall over the altar of the Chapel. Our guide went through the various pictures on the ceiling, because, as we would find out later, there was no way we could have had it explained to us while actually in the Chapel.</p>



<p>The Vatican Museum displayed additional artwork that is part of its collection. As elsewhere, every possible surface was an artistic masterpiece. Particularly notable were some of the paintings on the ceiling that appeared to be three-dimensional, but were instead expertly shaded flat paintings.</p>



<p>We then entered the Sistine Chapel, which was “wall to wall” people. We had several minutes to admire the artwork, and then met our tour guide at the exit to continue the guided tour.</p>


 
<a href="https://altmode.org/wp-content/uploads/2025/07/img_4359.jpg"></a>St. Peter’s Square 


<p>Our next stop was St. Peter’s Basilica. We entered twice: once through the center door and then through the Holy Door that is open only because it is a jubilee year. Jubilee years occur only every 25 years and hold special significance for Catholics, especially when they make a pilgrimage to the Vatican and walk through the Holy Door that is only open during the Jubilee. This also accounts for some additional crowding throughout.</p>



<p>After a few minutes for souvenir shopping, we re-boarded our bus and returned to the hotel. We had the afternoon to do as we wished, so we grabbed a quick lunch and then walked to a few of Rome’s classic tourist attractions: the Spanish Steps, Trevi Fountain, and the Pantheon. All were a quite reasonable walk from our hotel.</p>


 
<a href="https://altmode.org/wp-content/uploads/2025/07/img_4364.jpg"></a>The Spanish Steps: without people! 


<p>When we got to the Steps, we saw a rare sight: they were empty. It turned out they were closed with police tape keeping people out. We found out later that someone had attempted to drive their car down the Steps and got stuck. The car wasn’t evident, but they were still cleaning up the area. There was also a lot of lighting equipment at the bottom of the steps, as though perhaps a movie was about to be filmed, but we never found out about that.</p>



<p>We continued to the Trevi Fountain and took a look at it, but the crowds afforded no opportunity to throw coins in the fountain unless we were willing to wait in a long line. We didn’t feel a strong need to throw coins, so we continued to the Pantheon. Again, there were long lines at the Pantheon, so we decided to admire it from the outside and then return to our hotel via the bank of the Tiber River.</p>



<p>Our farewell dinner was this evening, and we had a last opportunity to chat with our new acquaintances from the tour. Our tour director, Alexa, polled everyone to see who was willing to share contact information; nearly everyone was. Alexa will also follow up with movie recommendations and other interesting information after the tour.</p>



 



<p>This article is part of a series about our recent trip to Italy, Greece, Albania, and Croatia. To see the introductory article in the series, click <a href="https://altmode.org/2025/07/15/adriatic-trip-day-1-to-venice/">here</a>.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://altmode.org/wp-content/uploads/2025/07/img_4359.jpg?w=300'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:00 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/if-i-ran-npr-and-pbs/'>If I ran NPR and PBS</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Building community-driven public media for the post-federal funding era.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Last week, Congress voted <a href="https://www.npr.org/2025/07/18/nx-s1-5469912/npr-congress-rescission-funding-trump?ref=werd.io">to roll back $9 billion in public media funding and foreign aid</a>. The impact on public media in the United States is profound: the cuts include $1.1 billion to the Corporation for Public Broadcasting, which completely cuts federal support for NPR, PBS, and their member stations. </p><p>The immediate question for NPR, PBS, their member stations, and the communities that depend on them is: what now?</p><p>I believe there are a few steps they could take.</p><p>Over the last few months, I’ve been running a series where I explore, as a thought experiment, how I might approach running services in the public interest: so far, this has included <a href="https://werd.io/if-i-ran-bluesky-product/">Bluesky</a>, <a href="https://werd.io/if-i-ran-mastodon/">Mastodon</a>, <a href="https://werd.io/if-i-started-fresh/">my own platform</a>, <a href="https://werd.io/lets-fund-the-open-social-web/">a fund for decentralized social media</a>, and <a href="https://werd.io/if-i-ran-x/">Twitter / X</a>. In the wake of these cuts to American public media, a number of readers asked me to extend my series to include how I might approach running both NPR and PBS.</p><p>I’m a technologist and entrepreneur who has spent over a decade working with, for, and in American media. As the first employee at Latakoo, I helped build a system that allowed TV journalists to more easily send footage back to newsrooms, with the content already formatted according to each newsroom’s standards. At Known, we worked with San Francisco PBS and NPR affiliate KQED to build an award-winning learning platform for teachers. At Matter, I helped run an accelerator for early-stage media companies in partnership with KQED, PRX, and the Knight Foundation, among others, and taught venture design to both startups and our partners. More recently, I’ve led technology inside non-profit newsrooms, first at The 19th and currently at ProPublica.</p><p>Through this work, I've developed relationships across the public interest media ecosystem, so I want to make one thing explicit. This post is, like the others, a thought experiment. None of my posts are a criticism of leadership (with the exception of the one about X, because, you know), and nor are they meant to imply that their leaders don’t already have smart solutions of their own. Instead, I want to surface different ideas and widen the conversation. I only have the deepest respect for the people who run public media, and I hope they will take this piece in that light. This is a stressful time — not least because the jobs of amazing people who have dedicated their careers to public service are on the line — and my intention is not to add to that stress.</p><p>It’s also worth noting that NPR and PBS are separate organizations. In this post, I’ll be considering them together: as you’ll see, I think convergence is part of the likely solution. But it’s important to remember that they each have their own budgets and funding.</p><p>First, I’ll explore what NPR and PBS are, how they are funded, and the implications of these funding cuts. Then how I’d go about mitigating them and building for the future.</p> How does American public media work? <p>Public media in America is very different to most countries, and receives significantly less government funding. Whereas the British Broadcasting Corporation receives around £100 ($134) in government funding per person per year, the US spent around $1.50 on public media before it was defunded completely. And rather than one large organization, the US public media landscape is a decentralized patchwork of individual stations. Like the country itself, it’s federated.</p><p>The public media brands you’re most likely to recognize are NPR and PBS. These are separate organizations. <a href="https://npr.org/?ref=werd.io">National Public Radio</a> is a radio network that distributes audio programming to member stations; the <a href="https://pbs.org/?ref=werd.io">Public Broadcasting Service</a> is a television network that distributes video programming to its member stations. In both cases, stations pay a fee to be a part of the network and receive programming in return, but they are independent organizations, and retain their own editorial control. There are also independent marketplaces for content like <a href="https://prx.org/?ref=werd.io">PRX</a> and <a href="https://www.americanpublicmedia.org/?ref=werd.io">American Public Media</a> that help distribute shows to public media stations.</p><p><a href="https://cpb.org/?ref=werd.io">The Corporation for Public Broadcasting</a> — the thing that was just defunded — was established in 1967, a few years before NPR and PBS, to support public media across the country. It received a small (around 0.01%) portion of the federal budget each year and spent most of its money on supporting local stations, with the bulk of the remainder spent on NPR and PBS themselves. It also awarded grants for innovation in the space and projects aimed at underserved communities.</p> Funding cuts disproportionately hurt rural America <p>Stations in bigger cities don’t tend to rely as much on federal funding: they have a bigger potential member-base to draw on, and are more likely to receive support from local foundations. It’s the stations in rural communities that are really hard-hit by funding cuts. Federal funding represents <a href="https://www.kqed.org/support/federal-funding?ref=werd.io">7% of the budget for KQED in San Francisco</a>; for <a href="https://www.alleghenymountainradio.org/?ref=werd.io">Allegheny Mountain Radio</a>, which serves rural counties on the border of Virginia and West Virginia, <a href="https://www.npr.org/2025/07/20/nx-s1-5469908/trump-npr-public-community-radio-corporation-broadcasting-federal-funding-cuts?ref=werd.io">it represents 65%</a>.</p><p><a href="https://www.npr.org/2025/07/20/nx-s1-5469908/trump-npr-public-community-radio-corporation-broadcasting-federal-funding-cuts?ref=werd.io">As NPR reported</a>, the station is an important conduit for local information that doesn’t have any real alternatives:</p><blockquote>Some people here say they really value the news and community information Allegheny Mountain provides. Jay Garber, mayor of the town of Monterey, Va., says the radio remains the fastest way to let citizens know about everything from water main breaks to road closures.</blockquote><p>Places like these are also the locations where broadband and mobile internet are scarcer, and where radio and television are a lifeline: the best way to understand what’s happening locally and receive safety alerts.</p><p>Not only will these communities lose that much-needed information, <a href="https://www.gmu.edu/news/2024-11/are-us-news-deserts-hothouses-corruption?ref=werd.io">when a community doesn’t have adequate local news, it often experiences increased political corruption</a>. There are real knock-on effects for citizens of these communities: a lack of scrutiny leads to more opportunities to be exploited.</p><p>In a world where these stations go away, there will be anti-democracy alternatives waiting in the wings. Networks like Truth Social are designed to be alternatives to America’s existing information infrastructure, providing media that is tailored to an autocratic worldview. Removing public media, along with <a href="https://www.nbcnews.com/business/media/david-ellison-fcc-chairman-paramount-deal-cbs-rcna219687?ref=werd.io">bringing commercial networks like CBS in line</a>, is part of changing the overarching media narrative in the United States.</p><p>So, if I was in charge of NPR and PBS, what would I do about it?</p> First: triage <p>The most important immediate problem is the shortfall in support for rural stations. Given the stakes, public media needs to move quickly to preserve the most vulnerable parts of the system.</p><p>Grassroots sites like <a href="https://adoptastation.org/?ref=werd.io">Adopt a Station</a> are already trying to build support for individual stations from the donating public. There’s energy here: huge portions of the public want to help. The public media networks can harness this energy to drive donations and help prevent news deserts from being created in rural areas.</p><p>I would start immediately; time is of the essence. The first step, within a month or two, would be to prioritize stations by a combination of vulnerability and likelihood to create a news desert in their absence. These would be tiered into those that can survive independently, those needing moderate support, and stations facing closure, with stations with no alternatives in their localities first in each category. I’d then launch an accelerated fundraising campaign targeting major donors, foundations, and corporate sponsors specifically aimed and preserving the stations that were most in need.</p><p>It would probably look a lot like <a href="https://www.niemanlab.org/2025/07/public-media-appeals-to-audiences-for-support-after-rescission-bill-passes/?ref=werd.io">the campaign that’s already underway</a>:</p><blockquote>NPR hasn’t minced words. “This decision is devastating for the millions of people who rely on public media every day,” a pop-up on its website reads. The organization has also inserted a two-minute statement from CEO Katherine Maher in every in-house podcast’s feed, and has been circulating a one-minute video of Maher on social media. The video has nearly half a million likes and 237,000 views on TikTok, for example.</blockquote><p>This strategy would start as soon as possible — as has, in fact, happened — and carry on for at least three months. Over time, in addition to messaging broadcast through stations, it would expand to include materials and technology libraries aimed at decentralizing the fundraising campaign. Sites like Adopt a Station could incorporate branding, images, and embedded fundraising code provided by the networks in order to more easily raise money for stations. Similarly, anyone could easily share customized fundraising links on social media, for example to hit their own fundraising goals. Fundraisers who hit a certain level would receive perks — from old standbys of tote bags and T-shirts to invitations to private events and acknowledgement on the air, depending on their levels.</p><p>Public media is rightly non-partisan, so the messaging for these campaigns wouldn’t be party political. But they would be pro-democracy, and emphasize the need for media to support communities across the country, as well as the dangers of government trying to undermine it.</p><p>Finally, in order to help offset costs for the most in-need stations over the first year, I would raise funding from major national foundations focused on democracy and media, regional foundations with stakes in rural communities, and corporate sponsors aligned with public service missions. Content and network membership costs for those stations would be reduced for the duration of the time they were under threat of closure. It wouldn’t replace the whole budget, but it would at least cut into costs and give stations a greater chance of survival.</p> Next: community <p>Public media should have community at its heart. Local media, of course, is all about community: rather than thinking of their listeners or viewers as an audience, stations should think of them as a community that they can interact and build relationships with directly. But the public service nature of public media can also extend to seeding its platforms and content as things it can co-create with its communities, too.</p><p>I would quickly make sure that every department at the networks is thinking like a product team: that is to say, prioritizing the needs of their stakeholders, ensuring that they understand what those needs are through rapid research and real-world prototype-driven testing rather than relying on internal assumptions and perspectives, and using an iterative process. Each team needs to test its assumptions and work with real-world stakeholders like listeners and donors as they make incremental improvements towards hitting defined <a href="https://rework.withgoogle.com/en/guides/set-goals-with-okrs?ref=werd.io">objectives and key results</a> (more on those in a moment). Design techniques like <a href="https://www.interaction-design.org/literature/topics/personas?ref=werd.io">using aggregated personas</a> would be banned; instead, every major development would need to be prototyped and tested with real people in the community.</p><p>Initiatives like <a href="https://knightlab.northwestern.edu/2025/07/21/jr3-workshop-recap/?ref=werd.io">the JR3 project</a>, which seeks to reconsider journalism’s role in the current era and given the increasing prevalence of <a href="https://reutersinstitute.politics.ox.ac.uk/digital-news-report/2025/dnr-executive-summary?ref=werd.io#avoidance">news avoidance</a>, are a step in the right direction, and would be built on, with wider participation, anchored in real-world qualitative research into real people. Reflection and ideation would be encouraged, but the people being served by journalism would always be the referee and the center of gravity for every decision. Ideally, new initiatives would be co-created with them.</p><p>Once key leaders at the networks had rapidly mastered this kind of human-centered, prototype-driven thinking, it would be offered to the stations for free. I would hire <a href="https://pointc.co/coreyford/?ref=werd.io">Corey Ford</a> to run week-long media venture design thinking bootcamps for leaders at at-risk media stations, and then retain support at the network level to provide ongoing mentorship for them.</p><p>The networks create safe community spaces as a way to help solve the stations’ most pressing challenges. Experts from across media, technology, and other relevant industries would be brought together to help figure out how to solve existential problems. This wouldn’t be slick consultancy arrangements; leaders across industries would give their time to support a bedrock of democracy, and have no-nonsense facilitated conversations that would move stations forward quickly. Those could evolve into direct, ongoing relationships between those leaders and the stations, or they could remain informal or temporal.</p><p>Finally, those network OKRs would be tethered to an aggressive three-year plan that would center on finding more individual members, building sustainable revenue, and removing dependence on proprietary platforms.</p> Removing dependence on proprietary platforms <p>Public media networks have offered their stations centrally managed content management systems and other software for years. But many of those systems are closed and one-size-fits-all: for example, NPR offers a CMS called Grove, <a href="https://greaterpublic.org/blog/how-grove-cms-features-address-the-growing-importance-of-digital-sponsorship/?ref=werd.io">which replaced its homegrown Core Publisher offering five years ago</a>. Grove has lots of features that stations might find useful, and its central management means that new features roll out to many stations at once, but <a href="https://current.org/2024/10/with-grove-adoption-nearly-complete-stations-look-to-enhancements-single-sign-on/?ref=werd.io">is based on Brightspot</a>, a commercial CMS. If there need to be changes or customizations, stations need to work with Brightspot, and they may find there’s a hefty price tag. Brightspot also doesn’t have great answers for posting to new networks like Bluesky or to use more experimental web features.</p><p>Just as the networks should be safe spaces where stations and experts can collaborate on experiments with business models and strategy, they should also foster collaboration on platform. There is a universe of mission-driven open source projects and engineers ready to help these stations.</p><p>This isn’t new for media: for example, <a href="https://www.tinynewsco.org/?ref=werd.io">the Tiny News Collective</a>, which supports newsroom startups, works with the open source <a href="https://ghost.org/?ref=werd.io">Ghost</a> team to provide a great CMS. Ghost is more suitable for smaller newsrooms, but because it’s open source, newsrooms don’t have to use the hosted service Tiny News Collective provides. They can graduate to their own as they grow, customizing its code, and eventually porting themselves to another CMS if they want — all without being locked into a particular provider. <a href="https://newspack.com/?ref=werd.io">Newspack</a>, a CMS for news based on WordPress, offers similar freedom. Open source projects would be paid to provide hosted infrastructure. Again, this isn’t new: Tiny News Collective includes Ghost Pro, while Newspack sits on top of Automattic’s WordPress hosting.</p><p>I would create space at the networks to bring together mission-driven developers and technology organizations in order to solve station problems — which, remember, were discovered through real-world research and testing with members of their communities. The stations get ready-made technology; the projects get exposure to validated, real-world problems and a set of users who need their work.</p><p>These wouldn’t just be solutions to external problems like content management system features and fundraising support. Projects would also include solutions for internal problems like easier analytics and secure management for tips from sources. The work would be funded by a combination of the networks themselves and the larger stations. It would always be released under an open source license, inviting collaborators, users, and donors from outside public media.</p><p>The result would be stronger, more innovative, more resilient platforms, produced at a lower cost.</p> Convergence <p>NPR and PBS were both authorized (alongside the CPB) by the Public Broadcasting Act of 1967. At the time, it made sense for radio and television to be treated separately: these were two very different mediums with different needs and different legacies.</p><p>In 2025, that distinction isn’t so clear. <a href="https://www.npr.org/series/718730324/video?ref=werd.io">NPR creates video content</a>, like the excellent <a href="https://www.youtube.com/playlist?list=PL1B627337ED6F55F0&amp;ref=werd.io">Tiny Desk Concerts</a>. <a href="https://www.pbs.org/articles/pbs-podcasts?ref=werd.io">PBS, in turn, creates a range of audio podcasts</a>, like the <a href="https://www.pbs.org/newshour/podcasts?ref=werd.io">News Hour</a>, which is also broadcast on some public radio stations.</p><p>Both publish on the web: a technology that supports audio, video, text, interactivity, immersive worlds, and more. Public radio and television both still have — and need to have — analogue content transmission, but they are now more alike than they are different. Not only should they share content production resources, but also fundraising and business resources, and even marketing plans.</p><p>Costs will be saved by avoiding duplication. I don’t mean laying people off: combining efforts on functions like fundraising, accounting, product, and technology infrastructure would create efficiencies and allow for more sophisticated strategies. Research conducted with listeners of a rural public radio station could be applied to public television — and removing the artificial separation of broadcast technologies would free both networks up to work on new media initiatives in collaboration with each other.</p><p>I’m not proposing a formal merger. But there are ways two independent organizations can collaborate more closely.</p> Community convergence <p>And then we go back to the communities.</p><p>Organizations like <a href="https://wearehearken.com/?ref=werd.io">Hearken</a> have made it more possible for the public to participate more fully in the public media that serves them. Hearken helps newsrooms crowdsource story ideas and involve communities in editorial processes; this should be at the core of the future of public media. Not only should the public be involved in reporting on their communities, but they should be trained in building media for their communities. This is how you get to more representative media that communities are more likely to invest in: you make sure it truly does represent them.</p><p>Just as initiatives and platforms should be co-created, so should the content itself. <a href="https://www.podcastgarage.org/?ref=werd.io">The PRX Podcast Garage</a> is an example of how public media organizations can work with communities in order not just elevate their voices but train them to elevate themselves. The networks should aim to provide more opportunities along these lines, and work towards an OKR that defines a percentage of total work that should be co-created — for example, 33% within three years.</p><p>This doesn’t need to be limited to streaming media like audio and video. I’d direct the networks to support grassroots local newsletters and blogs, embracing tools co-created with open source communities in order to create a decentralized commons for local news. These would sit on networks based on ActivityPub and the AT Protocol, allowing voices to more easily be shared throughout social media without losing direct ownership of their work and social relationships.</p><p>Finally, the networks should provide toolkits for in-person, local events, allowing stations to facilitate community in their areas safely.</p><p>Each of these acts of co-creation would flow from the network to stations to individuals. The network would create toolkits and provide platforms (in collaboration with outside experts, open source communities, and other volunteers); the stations would choose to use them as they saw fit; individuals could then take the opportunity to engage.</p> Sustainability and the commons <p>These resources — platforms, materials, techniques, infrastructure — wouldn’t just be useful for grassroots communities. There’s a world where they also become useful for corporations and other large profit-making enterprises. While grassroots organizations and communities should receive them for free, larger for-profit enterprises should pay for them. This could be through a sponsorship model, or it could be through direct licensing payments. But widening the tent would help to bring in new funds to support it all.</p><p>By turning public media into a commons that is co-created with communities at every level from the national to the hyper-local, we create cost savings, establish new opportunities for financial support, and better anchor local media to real-world needs. We also make it more resilient: each of these pieces is released openly, under an open license, allowing it to live on should the networks or stations eventually disappear.</p><p>But let me be clear: public media does not need to be profitable. There will always be a need for both institutional and individual investment, and it would certainly be far better if the federal government resumed its support. (Hopefully it will again in the future, under a more community-minded administration.)</p><p>Similarly, not every station can be sustainable, and this fact is independent of each station’s value to its community. Sometimes, particularly in rural areas, a station can be a vital lifeline, but also impossible to run sustainably in its own right. This is where the networks come into their own. It’s vitally important for larger, wealthier stations to help pay for smaller, more vulnerable ones.</p> Onwards <p>Public media is vital infrastructure. Without it, democracy is weakened. The act of defunding it isn’t just a crisis for the media companies themselves, nor even just for the communities that will become news deserts without this support. It’s a crisis for American democracy itself. There’s no silver lining here.</p><p>But there are ways networks, stations, and the communities they serve can re-orient themselves, becoming more open and collaborative to conserve resources while providing new services that deepen their democratic service. All is not lost: while the status quo might not be sustainable, there’s much to be gained by rapidly innovating without losing the public service soul that makes it so special.</p><p>As I said at the beginning, this has been a thought experiment. I don’t run NPR or PBS, and there are people far smarter than me who make these networks, stations, and communities work at every level, from the C-suite at the networks to the trenches at individual rural stations. These people are superheroes, and they’re already mobilizing with their own plans. I wish them only the best, and I hope, in some way, that this conversation has been useful.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://werd.io/content/images/2025/07/getty-images-F8YV03yzJkg-unsplash.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   12:28 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2745'>Updates to Audience Values for OAuth 2.0 Authorization Servers</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A new version of the Updates to Audience Values for OAuth 2.0 Authorization Servers specification has been published that incorporates feedback from the OAuth working group during IETF 122. I look forward to a vigorous and useful discussion of the specification at IETF 123 in Madrid. This specification updates a set of existing OAuth specifications […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>A <a href="https://www.ietf.org/archive/id/draft-ietf-oauth-rfc7523bis-02.html">new version</a> of the <a href="https://datatracker.ietf.org/doc/draft-ietf-oauth-rfc7523bis/">Updates to Audience Values for OAuth 2.0 Authorization Servers</a> specification has been published that incorporates feedback from the <a href="https://datatracker.ietf.org/wg/oauth/about/">OAuth working group</a> during IETF 122.  I look forward to a vigorous and useful discussion of the specification at <a href="https://www.ietf.org/meeting/123/">IETF 123</a> in Madrid.</p>
<p>This specification updates a set of existing OAuth specifications to address a <a href="https://openid.net/notice-of-a-security-vulnerability/">security vulnerability</a> identified during formal analysis of a previous version of the <a href="https://openid.net/specs/openid-federation-1_0.html">OpenID Federation</a> specification.  The vulnerability resulted from ambiguities in the treatment of the audience values of tokens intended for the authorization server.  The updates to these specifications close that vulnerability in the affected OAuth specifications – especially JWT client authentication in <a href="https://www.rfc-editor.org/rfc/rfc7523.html">RFC 7523</a>.  In parallel, the OpenID Foundation has also updated affected OpenID specifications, including <a href="https://openid.net/specs/openid-federation-1_0.html">OpenID Federation</a> and <a href="https://openid.net/specs/fapi-security-profile-2_0.html">FAPI 2.0</a>.</p>
<p>As summarized in the <a href="https://www.ietf.org/archive/id/draft-ietf-oauth-rfc7523bis-02.html#name-document-history">history entries</a>, the changes in this draft were:</p>
 
 Focused RFC 7523 updates on JWT client authentication case. 
 Described client responsibilities for the audience value of authorization grants. No longer mandate that the audience for authorization grants be the issuer identifier, so as to make a minimum of breaking changes. 
 Deprecated the use of SAML assertions for client authentication. 
 
<p>Finally, <a href="https://www.linkedin.com/in/filipskokan/">Filip Skokan</a> was added as an author, in recognition of his significant contributions to the work.  Thanks to Filip and <a href="https://www.linkedin.com/in/bcampbell/">Brian Campbell</a> for their work with me on this specification.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/oauth_logo_120x120.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   10:28 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/22/textual-v4/#atom-everything'>Textual v4.0.0: The Streaming Release</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Textual v4.0.0: The Streaming Release 
Will McGugan may no longer be running a commercial company around Textual, but that hasn't stopped his progress on the open source project. 
 He recently released v4 of his Python framework for building TUI command-line apps, and the signature feature is streaming Markdown support - super relevant in our current age of LLMs, most of which default to outputtin
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://static.simonwillison.net/static/2025/epic-table.gif'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://github.com/Textualize/textual/releases/tag/v4.0.0">Textual v4.0.0: The Streaming Release</a></p>
Will McGugan may <a href="https://textual.textualize.io/blog/2025/05/07/the-future-of-textualize/">no longer be running</a> a commercial company around Textual, but that hasn't stopped his progress on the open source project.<p></p>
<p>He recently released v4 of his Python framework for building TUI command-line apps, and the signature feature is <a href="https://github.com/Textualize/textual/pull/5950">streaming Markdown support</a> - super relevant in our current age of LLMs, most of which default to outputting a stream of Markdown via their APIs.</p>
<p>I took an example <a href="https://github.com/Textualize/textual/blob/03b94706399f110ff93fa396d4afbc79c3738638/tests/snapshot_tests/test_snapshots.py#L4378-L4400">from one of his tests</a>, spliced in my <a href="https://llm.datasette.io/en/stable/python-api.html#async-models">async LLM Python library</a> and <a href="https://chatgpt.com/share/687c3a6a-4e1c-8006-83a2-706b4bf04067">got some help from o3</a> to turn it into <a href="https://github.com/simonw/tools/blob/916b16cd7dfd3c23315d0a4ed02172878feafa45/python/streaming_textual_markdown.py">a streaming script</a> for talking to models, which can be run like this:</p>
 <code>uv run http://tools.simonwillison.net/python/streaming_textual_markdown.py \
'Markdown headers and tables comparing pelicans and wolves' \
-m gpt-4.1-mini
</code> 
<p>


    </p><p>Tags: <a href="https://simonwillison.net/tags/async">async</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/markdown">markdown</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/will-mcgugan">will-mcgugan</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/textual">textual</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/uv">uv</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://static.simonwillison.net/static/2025/epic-table.gif'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   00:32 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 21. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/21/pogocache/#atom-everything'>tidwall/pogocache</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   tidwall/pogocache 
New project from Josh Baker, author of the excellent tg C geospatial libarry (covered previously) and various other interesting projects: 
 
 Pogocache is fast caching software built from scratch with a focus on low latency and cpu efficency. 
 Faster: Pogocache is faster than Memcache, Valkey, Redis, Dragonfly, and Garnet. It has the lowest latency per request, providing the qu
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://github.com/tidwall/pogocache">tidwall/pogocache</a></p>
New project from Josh Baker, author of the excellent <code>tg</code> C geospatial libarry (<a href="https://simonwillison.net/2023/Sep/23/tg-polygon-indexing/">covered previously</a>) and various other <a href="https://github.com/tidwall">interesting projects</a>:<p></p>
<blockquote>
<p>Pogocache is fast caching software built from scratch with a focus on low latency and cpu efficency.</p>
<p>Faster: Pogocache is faster than Memcache, Valkey, Redis, Dragonfly, and Garnet. It has the lowest latency per request, providing the quickest response times. It's optimized to scale from one to many cores, giving you the best single-threaded and multithreaded performance.</p>
</blockquote>
<p>Faster than Memcache and Redis is a big claim! The README includes a <a href="https://github.com/tidwall/pogocache/blob/main/README.md#design-details">design details</a> section that explains how the system achieves that performance, using a sharded hashmap inspired by Josh's <a href="https://github.com/tidwall/shardmap">shardmap</a> project and clever application of threads.</p>
<p>Performance aside, the most interesting thing about Pogocache is the server interface it provides: it emulates the APIs for Redis and Memcached, provides a simple HTTP API and lets you talk to it over the PostgreSQL wire protocol as well!</p>
 <code>psql -h localhost -p 9401
=&gt; SET first Tom;
=&gt; SET last Anderson;
=&gt; SET age 37;

$ curl http://localhost:9401/last
Anderson
</code> 

    <p>Via <a href="https://news.ycombinator.com/item?id=44638076">Show HN</a></p>


    <p>Tags: <a href="https://simonwillison.net/tags/c">c</a>, <a href="https://simonwillison.net/tags/caching">caching</a>, <a href="https://simonwillison.net/tags/http">http</a>, <a href="https://simonwillison.net/tags/memcached">memcached</a>, <a href="https://simonwillison.net/tags/postgresql">postgresql</a>, <a href="https://simonwillison.net/tags/redis">redis</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   23:58 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/21/gemini-imo/#atom-everything'>Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad 
OpenAI beat them to the punch in terms of publicity by publishing their results on Saturday, but a team from Google Gemini achieved an equally impressive result on this year's International Mathematics Olympiad scoring a gold medal performance with their custom research mo
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/">Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad</a></p>
OpenAI beat them to the punch in terms of publicity by <a href="https://simonwillison.net/2025/Jul/19/openai-gold-medal-math-olympiad/">publishing their results on Saturday</a>, but a team from Google Gemini achieved an equally impressive result on this year's International Mathematics Olympiad scoring a gold medal performance with their custom research model.<p></p>
<p>(I saw an unconfirmed rumor that the Gemini team had to wait until Monday for approval from Google PR - this turns out to be inaccurate, see update below.)</p>
<p>It's interesting that Gemini achieved the exact same score as OpenAI, 35/42, and were able to solve the same set of questions - 1 through 5, failing only to answer 6, which is designed to be the hardest question.</p>
<p>Each question is worth seven points, so 35/42 cents corresponds to full marks on five out of the six problems.</p>
<p>Only 6 of <a href="https://www.imo-official.org/year_individual_r.aspx?year=2025">the 630 human contestants</a> this year scored all 7 points for question 6 this year, and just 55 more had greater than 0 points for that question.</p>
<p>OpenAI claimed their model had not been optimized for IMO questions. Gemini's model was different - emphasis mine:</p>
<blockquote>
<p>We achieved this year’s result using an advanced version of Gemini Deep Think – an enhanced reasoning mode for complex problems that incorporates some of our latest research techniques, including parallel thinking. This setup enables the model to simultaneously explore and combine multiple possible solutions before giving a final answer, rather than pursuing a single, linear chain of thought.</p>
<p>To make the most of the reasoning capabilities of Deep Think, we additionally trained this version of Gemini on novel reinforcement learning techniques that can leverage more multi-step reasoning, problem-solving and theorem-proving data. We also provided Gemini with access to a curated corpus of high-quality solutions to mathematics problems, and added some general hints and tips on how to approach IMO problems to its instructions.</p>
</blockquote>
<p>The Gemini team, like the OpenAI team, achieved this result with <a href="https://x.com/fredzhang0/status/1947364744412758305">no tool use or internet access</a> for the model.</p>
<p>Gemini's solutions are listed <a href="https://storage.googleapis.com/deepmind-media/gemini/IMO_2025.pdf">in this PDF</a>. If you are mathematically inclined you can compare them with OpenAI's solutions <a href="https://github.com/aw31/openai-imo-2025-proofs/">on GitHub</a>.</p>
<p>Last year Google DeepMind <a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/">achieved a silver medal in IMO</a>, solving four of the six problems using custom models called AlphaProof and AlphaGeometry 2:</p>
<blockquote>
<p>First, the problems were manually translated into formal mathematical language for our systems to understand. In the official competition, students submit answers in two sessions of 4.5 hours each. Our systems solved one problem within minutes and took up to three days to solve the others.</p>
</blockquote>
<p>This year's result, scoring gold with a single model, within the allotted time and with no manual step to translate the problems first, is much more impressive.</p>
<p>Update: Concerning the timing of the news, DeepMind CEO Demis Hassabis <a href="https://x.com/demishassabis/status/1947337618787615175">says</a>:</p>
<blockquote>
<p>Btw as an aside, we didn’t announce on Friday because we respected the IMO Board's original request that all AI labs share their results only after the official results had been verified by independent experts &amp; the students had rightly received the acclamation they deserved</p>
<p>We've now been given permission to share our results and are pleased to have been part of the inaugural cohort to have our model results officially graded and certified by IMO coordinators and experts, receiving the first official gold-level performance grading for an AI system!</p>
</blockquote>
<p>OpenAI's <a href="https://x.com/polynoamial/status/1947398538662437306">Noam Brown</a>:</p>
<blockquote>
<p>Before we shared our results, we spoke with an IMO board member, who asked us to wait until after the award ceremony to make it public, a request we happily honored.</p>
<p>We announced at ~1am PT (6pm AEST), after the award ceremony concluded. At no point did anyone request that we announce later than that.</p>
</blockquote>
<p>As far as I can tell the Gemini team was participating in an official capacity, while OpenAI were not. <a href="https://x.com/polynoamial/status/1947398532899738064">Noam again</a>:</p>
<blockquote>
<p>~2 months ago, the IMO emailed us about participating in a formal (Lean) version of the IMO. We’ve been focused on general reasoning in natural language without the constraints of Lean, so we declined. We were never approached about a natural language math option.</p>
</blockquote>
<p>Neither OpenAI nor Gemini used <a href="https://en.m.wikipedia.org/wiki/Lean_(proof_assistant)">Lean</a> in their attempts, which would have counted as tool use.

    </p><p>Via <a href="https://news.ycombinator.com/item?id=44637191">Hacker News</a></p>


    <p>Tags: <a href="https://simonwillison.net/tags/mathematics">mathematics</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/llm-reasoning">llm-reasoning</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:14 &bull;
   6 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/22/welcome-to-the-world-of.html'> Welcome to the world of The Tяump Аппаратчик Lisa 
 
 Sen. L</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Welcome to the world of The Tяump Аппаратчик Lisa 
 
 Sen. Lisa Murkowski said she feels “cheated” after she won a concession in the recently passed tax and spending law to protect wind and solar projects, only to see the president and his administration issue recent orders that she said seem designed to quickly quash such projects. “I feel cheated,” she said in an interview Friday. “I feel like w
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Welcome to the world of The Tяump Аппаратчик Lisa</p>
<blockquote>
<p>Sen. Lisa Murkowski said she feels “cheated” after she won a concession in the recently passed tax and spending law to protect wind and solar projects, only to see the president and his administration issue recent orders that she said seem designed to quickly quash such projects. “I feel cheated,” she said in an interview Friday. “I feel like we made a deal and then hours later, a deal was made to somebody else.”</p>
</blockquote>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:01 &bull;
   7 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/components-for-web-apps'>Components for Web Apps</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The web has come a long way since static HTML.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The web has come a long way since static HTML. Even so, building user interfaces is still often an exercise in complexity: frameworks layered on frameworks, intricate build tools, and brittle glue code tying everything together. But there's another way—native, composable building blocks, pieces of UI that can be easily reused, reasoned about, and combined without pulling in half the npm registry. That's the promise of web components, and it's why tools like <a href="https://xmlui.com/">XMLUI</a> are exciting. They let us focus on function and structure, not scaffolding and ceremony.</p><p>I'm going to skip the technical deep dive. You can get that on the <a href="https://xmlui.com/">XMLUI</a> site or in Jon Udell's excellent <a href="https://blog.jonudell.net/2025/07/18/introducing-xmlui/">XMLUI introduction</a>. But even just a simple example can show the power of components.</p><p>Imagine you need a table that displays updated information about the status of London tube stations.</p> <a href="https://substackcdn.com/image/fetch/$s_!drEr!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17cb1d1d-39dc-4df2-bee6-7e36538b68db_809x262.heic">          </a> <p>Normally, you'd link to an API, fetch the data, loop over the JSON, and build the DOM with JavaScript or a framework like React. Or...you could do it with XMLUI like this:</p> <code>&lt;App&gt;
    &lt;Table data="https://api.tfl.gov.uk/line/mode/tube/status"&gt;
        &lt;Column bindTo="name" /&gt;
        &lt;Column header="status" &gt;
            {$item.lineStatuses[0].statusSeverityDescription}
        &lt;/Column&gt;
    &lt;/Table&gt;
&lt;/App&gt;</code> <p>This is a web component in action: you name the data source, define the structure, and let XMLUI handle the heavy lifting. And this is just scratching the surface, there are multiple component types, styling options, even MCP (Multi-Component Pages) interfaces for multi-agent or AI-powered applications.</p><p>One reason I'm personally excited about XMLUI is that I've been looking for a way for <a href="https://picolabs.atlassian.net/wiki/spaces/docs/overview?homepageId=1189866">Picos</a> to create their own interfaces, rather than relying on an external React app, like we did with <a href="https://manifold.picolabs.io/#/login">Manifold</a>. Picos—distributed, autonomous agents with lightweight logic—used to have UI capabilities. XMLUI components might allow them to regain that ability, natively and declaratively. Bruce Conrad has already been experimenting with this, and I love the idea of using a tool we don't have to build ourselves. Lightweight, component-driven, and web-native, XMLUI seems like a natural fit for Pico-based architectures.</p><p>XMLUI isn't just another UI framework, it's a shift toward declarative, modular web development that feels especially well-suited to the world of Picos. By letting components define themselves, serve themselves, and run directly into the browser, we can finally build UIs that are as lightweight and autonomous as the agents they represent. There's still more to explore, but I'm optimistic that XMLUI can help bring back a native interface layer for Picos that's simple, composable, and entirely in their control for easier development and deployment.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!drEr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17cb1d1d-39dc-4df2-bee6-7e36538b68db_809x262.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:15 &bull;
   7 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://altmode.org'>Altmode</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://altmode.org/2025/07/21/adriatic-trip-day-7-florence-to-rome/'>Adriatic Trip, Day 7: Florence to Rome</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Monday, June 16, 2025 This morning we again boarded a train for our final destination of our Italian tour, Rome. The trains again proved to be very popular in Italy. A bus picked up our tour group at the train station and took us on a driving tour of Rome. At one point, we got […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Monday, June 16, 2025</p>



<p>This morning we again boarded a train for our final destination of our Italian tour, Rome. The trains again proved to be very popular in Italy.</p>



<p>A bus picked up our tour group at the train station and took us on a driving tour of Rome. At one point, we got off the bus and walked a couple of blocks to a restaurant for a delicious lasagne lunch. Then we got back on the bus and drove to the Coliseum, where we disembarked. Our local guide then led us on a walking tour around the Coliseum. She discussed the structure of the Coliseum, the types of events held there (not just the persecution of Christians), and its underground facilities for quick scene changes. It was a very technologically advanced area for its time.</p>


 
<a href="https://altmode.org/wp-content/uploads/2025/07/dsc03734.jpg"></a>Galleria Borghese statues 


<p>Following our walking tour, our rooms were ready, so we drove to our hotel, the Palazzo Ripetta. Like the other hotels on this trip, it is a luxury hotel located in a very convenient and safe part of the city. After checking into the hotel and getting settled, we boarded our bus for a private tour of the Galleria Borghese, an art museum renowned for its sculptures by Bernini. Both Kenna and I were amazed by the detail and realism of the sculptures, especially considering that they were carved out of marble.</p>



<p>After the tour, we went to a nearby pizza restaurant recommended by Alexa, met up with another couple from our tour, and had a delightful dinner to close our first day in Rome.</p>



 



<p>This article is part of a series about our recent trip to Italy, Greece, Albania, and Croatia. To see the introductory article in the series, click <a href="https://altmode.org/2025/07/15/adriatic-trip-day-1-to-venice/">here</a>.</p>



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://altmode.org/wp-content/uploads/2025/07/dsc03734.jpg?w=200'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:00 &bull;
   7 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/21/daniel-litt/#atom-everything'>Quoting Daniel Litt</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    An AI tool that gets gold on the IMO is obviously immensely impressive. Does it mean math is “solved”? Is an AI-generated proof of the Riemann hypothesis clearly on the horizon? Obviously not. 
 Worth keeping timescales in mind here: IMO competitors spend an average of 1.5 hrs on each problem. High-quality math research, by contrast, takes month or years. 
 What are the obstructions to AI perform
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <blockquote cite="https://x.com/littmath/status/1946987909439017108"><p>An AI tool that <a href="https://simonwillison.net/2025/Jul/19/openai-gold-medal-math-olympiad/">gets gold on the IMO</a> is obviously immensely impressive. Does it mean math is “solved”? Is an AI-generated proof of the Riemann hypothesis clearly on the horizon? Obviously not.</p>
<p>Worth keeping timescales in mind here: IMO competitors spend an average of 1.5 hrs on each problem. High-quality math research, by contrast, takes month or years.</p>
<p>What are the obstructions to AI performing high-quality autonomous math research? I don’t claim to know for sure, but I think they include many of the same obstructions that prevent it from doing many jobs: Long context, long-term planning, consistency, unclear rewards, lack of training data, etc.</p>
<p>It’s possible that some or all of these will be solved soon (or have been solved) but I think it’s worth being cautious about over-indexing on recent (amazing) progress.</p></blockquote>
<p>— <a href="https://x.com/littmath/status/1946987909439017108">Daniel Litt</a>, Assistant Professor of mathematics, University of Toronto</p>

    <p>Tags: <a href="https://simonwillison.net/tags/mathematics">mathematics</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/daniel-litt">daniel-litt</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   15:13 &bull;
   7 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/21/coding-with-llms/#atom-everything'>Coding with LLMs in the summer of 2025 (an update)</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Coding with LLMs in the summer of 2025 (an update) 
Salvatore Sanfilippo describes his current AI-assisted development workflow. He's all-in on LLMs for code review, exploratory prototyping, pair-design and writing "part of the code under your clear specifications", but warns against leaning too hard on pure vibe coding: 
 
 But while LLMs can write part of a code base with success (under your str
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://antirez.com/news/154">Coding with LLMs in the summer of 2025 (an update)</a></p>
Salvatore Sanfilippo describes his current AI-assisted development workflow. He's all-in on LLMs for code review, exploratory prototyping, pair-design and writing "part of the code under your clear specifications", but warns against leaning too hard on pure vibe coding:<p></p>
<blockquote>
<p>But while LLMs can write part of a code base with success (under your strict supervision, see later), and produce a very sensible speedup in development (or, the ability to develop more/better in the same time used in the past — which is what I do), when left alone with nontrivial goals they tend to produce fragile code bases that are larger than needed, complex, full of local minima choices, suboptimal in many ways. Moreover they just fail completely when the task at hand is more complex than a given level.</p>
</blockquote>
<p>There are plenty of useful tips in there, especially around carefully managing your context:</p>
<blockquote>
<p>When your goal is to reason with an LLM about implementing or fixing some code, you need to provide extensive information to the LLM: papers, big parts of the target code base (all the code base if possible, unless this is going to make the context window so large than the LLM performances will be impaired). And a brain dump of all your understanding of what should be done.</p>
</blockquote>
<p>Salvatore warns against relying too hard on tools which hide the context for you, like editors with integrated coding agents. He prefers pasting exactly what's needed into the LLM web interface - I share his preference there.</p>
<p>His conclusions here match <a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/">my experience</a>:</p>
<blockquote>
<p>You will be able to do things that are otherwise at the borders of your knowledge / expertise while learning much in the process (yes, you can learn from LLMs, as you can learn from books or colleagues: it is one of the forms of education possible, a new one). Yet, everything produced will follow your idea of code and product, and will be of high quality and will not random fail because of errors and shortcomings introduced by the LLM. You will also retain a strong understanding of all the code written and its design.</p>
</blockquote>

    <p>Via <a href="https://news.ycombinator.com/item?id=44623953">Hacker News</a></p>


    <p>Tags: <a href="https://simonwillison.net/tags/salvatore-sanfilippo">salvatore-sanfilippo</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   02:42 &bull;
   7 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 20. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2025/07/20/good-read/'>Good read</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I just got turned on to Paul Ford's What is Code, from 2015, but still current today. Shoulda been a book, like Neal Stephenson's In the Beginning Was the Command Line. You can still find the text online, such as here.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I just got turned on to <a href="https://en.wikipedia.org/wiki/Paul_Ford_(technologist)">Paul Ford</a>'s <a href="https://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/">What is Code</a>, from 2015, but still current today. Shoulda been a book, like Neal Stephenson's <a href="https://en.wikipedia.org/wiki/In_the_Beginning..._Was_the_Command_Line">In the Beginning Was the Command Line</a>. You can still find the text online, such as <a href="https://web.stanford.edu/class/cs81n/command.txt">here</a>.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:40 &bull;
   7 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2025/07/20/nice-i-hope/'>Nice, I hope</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  That "intention economy" appears (in a positive way) in this story from South Africa, in IOL.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>That "intention economy" appears (in a positive way) in <a href="https://iol.co.za/business-report/entrepreneurs/2025-07-20-the-sociable-app-is-making-in-person-connections-cool-again/">this story from South Africa</a>, in IOL.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:24 &bull;
   7 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://altmode.org'>Altmode</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://altmode.org/2025/07/20/adriatic-trip-day-6-chianti/'>Adriatic Trip, Day 6: Chianti</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Sunday, June 15, 2025 Today we took a bus ride to the Tuscan countryside to visit a Chianti winery. This was a lovely change of pace because of the crowds in Venice and Florence and the very warm weather. We began by driving south into the hills surrounding Florence, which provided a nice overview of […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Sunday, June 15, 2025</p>



<p>Today we took a bus ride to the Tuscan countryside to visit a Chianti winery. This was a lovely change of pace because of the crowds in Venice and Florence and the very warm weather. We began by driving south into the hills surrounding Florence, which provided a nice overview of the city.</p>


 
<a href="https://altmode.org/wp-content/uploads/2025/07/img_4194.jpg"></a>Florence American Cemetery 


<p>After driving for a bit, our tour director, Alexa, announced that we would be making an extra stop at the <a href="https://www.abmc.gov/cemeteries-memorials/about-florence-american-cemetery/">Florence American Cemetery</a>. While this wasn’t on our itinerary, she had cleared it with Tauck, and everyone on the bus was interested in stopping there. The cemetery is the resting place for 4,392 American soldiers in World War II whose families chose to have them buried in Italy rather than repatriated to the US. As with other military cemeteries we have seen, it was a very moving tribute, and immaculately maintained.</p>


 
<a href="https://altmode.org/wp-content/uploads/2025/07/img_4208.jpg"></a>Passagiano Winery cellar 


<p>We continued to a winery in the little village of Passignano. The winery is attached to an abbey called the Badia di Passignano. The tour itself was typical of winery tours, at least for those of us from California. The primary wine they produce is Chianti Classico, a specific local appellation.</p>



<p>Following the tour, we were treated to a wine tasting and gourmet lunch in the adjacent restaurant. The wine was quite different from the typical, inexpensive Chianti often found in Italian restaurants in the US. It has a very smooth taste, and like good wines, the alcohol wasn’t the primary flavor. A trademarked black rooster logo designates Chianti Classico. I will probably be looking for it at the wine store since it was too heavy for us to carry around.</p>


 
<a href="https://altmode.org/wp-content/uploads/2025/07/img_4220.jpg"></a>Cappelle Medici 


<p>We had a little free time following our return to Florence, and decided to go looking for a campanile to get some views of the city. We got a little lost and ended up at the Cappelle Medici, which houses the tombs of the Medici rulers. We were impressed by the building and artwork.</p>



<p>While walking back to our hotel, we heard the distant music of the calico storico parade. We followed the sound to the parade street and saw more of the teams’ supporters and festivities.</p>



<p>As part of the tour, we had the option to have a “fancy dinner” on one of our days in Florence. We opted for the third day because it seemed to have the most time available, although we were a little concerned about being too full after our gourmet lunch. The dinner was indeed very nice. We were at an outdoor seating area on a square facing the Palazzo Vecchio, and were told we could order anything on the menu. My pasta course included lobster, which was a real treat.</p>



 



<p>This article is part of a series about our recent trip to Italy, Greece, Albania, and Croatia. To see the introductory article in the series, click <a href="https://altmode.org/2025/07/15/adriatic-trip-day-1-to-venice/">here</a>.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://altmode.org/wp-content/uploads/2025/07/img_4194.jpg?w=300'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:00 &bull;
   8 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/trump-and-the-energy-industry-are-eager-to-power-ai-with-fossil-fuels/'>Trump and the Energy Industry Are Eager to Power AI With Fossil Fuels</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The Trump administration is using AI as a way to shill fossil fuels. But even for tech companies that don't care about climate change, renewables are a far better option.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>[<a href="https://www.wired.com/story/trump-energy-industry-ai-fossil-fuels-pittsburgh-summit/?ref=werd.io">Molly Taft in WIRED</a>]</p><p>AI is a useful technology for anyone who wants to push forward a basket of issues that include increased surveillance (it needs more and more data to be useful) and energy (AI services are hungry for it, as we’ll see).</p><p>Fossil fuels sit squarely in the middle of the mix. As WIRED reports:</p><blockquote>“Since taking office, Trump has used AI as a lever to open up opportunities for fossil fuels, including a well-publicized effort to resuscitate coal in the name of more computing power.”</blockquote><p>The piece talks about a summit that included Pennsylvanian natural gas producers and pipeline companies. But even on a purely economic level, this doesn’t make sense: companies like Google are turning to renewable energy sources like hydropower not because it’s good for the environment (although that would be a great reason), but because it’s cheaper and more sustainable for them.</p><blockquote>“Financial analyst Lazard said last month that installing utility-scale solar panels and batteries is still cheaper than building out natural gas plants, even without tax incentives. Gas infrastructure is also facing a global shortage that makes the timescales for setting up power generation vastly different.”</blockquote><p>If these technologies continue to grow in prominence — which for now seems like a safe bet — the demand for electricity is going to grow significantly. For folks with ties to last-century energy sources like fossil fuels, that’s an opportunity to spread propaganda for their cause. But renewables are the obvious way forward, economically, environmentally, and practically. There are still logistical problems to solve with these technologies, but this is something added investment and attention can directly help with.</p><p>Of course, the actual growth of AI won’t necessarily match what’s currently being hyped. So there’s a potential pitfall here too: if companies invest in infrastructure based on the hype cycle, they might find themselves in trouble later on when the bubble bursts. On the other hand, if that investment went into renewable infrastructure, we might find that it’s useful nonetheless.</p><p>[<a href="https://www.wired.com/story/trump-energy-industry-ai-fossil-fuels-pittsburgh-summit/?ref=werd.io">Link</a>]</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:43 &bull;
   8 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/openais-gold-medal-performance-on-the-international-math-olympiad/'>OpenAI’s gold medal performance on the International Math Olympiad</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  OpenAI claims a significant result: gold-level performance International Mathematical Olympiad. But they're scant on details and it needs to be independently verified.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>[<a href="https://simonwillison.net/2025/Jul/19/openai-gold-medal-math-olympiad/?ref=werd.io">Simon Willison</a>]</p><p>This is a genuinely impressive-sounding result from OpenAI, <a href="https://simonwillison.net/2025/Jul/19/openai-gold-medal-math-olympiad/?ref=werd.io">as reshared by Simon Willison</a>:</p><blockquote>“I’m excited to share that our latest OpenAI experimental reasoning LLM has achieved a longstanding grand challenge in AI: gold medal-level performance on the world’s most prestigious math competition — the International Math Olympiad (IMO).”</blockquote><p>It uses an unreleased model — nothing on their current site, and not the upcoming GPT-5. It’s not as consumer-friendly, given its propensity to take hours to solve problems, but on the face of it, this is an interesting outcome.</p><p>As OpenAI research scientist Alexander Wei says:</p><blockquote>“Just to spell it out as clearly as possible: a next-word prediction machine (because that's really what it is here, no tools no nothing) just produced genuinely creative proofs for hard, novel math problems at a level reached only by an elite handful of pre‑college prodigies.”</blockquote><p>So, how can we independently evaluate and consider this outcome?</p><p>It’s worth noting that it was graded by “three former Olympiads”, not the formal scorers, with no indication of how they were paid or what the arrangement was. It’s also obviously a black box, like almost all AI models; it’s not clear how this was achieved. In particular, while they claim no tools, it’s not clear what the training data was, or which techniques were used to build the model.</p><p>That’s not to dismiss these results outright! The IMO challenges are unique each year, and this has the potential to be a genuine breakthrough in computing. The next step would hopefully be a research paper that lays these things out. If this really is what they claim it is, it’s undeniably impressive. But it’s not enough to say it; it needs to be independently verified and repeated.</p><p>[<a href="https://simonwillison.net/2025/Jul/19/openai-gold-medal-math-olympiad/?ref=werd.io">Link</a>]</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:22 &bull;
   8 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/20/armin-ronacher/#atom-everything'>Quoting Armin Ronacher</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    Every day someone becomes a programmer because they figured out how to make ChatGPT build something. Lucky for us: in many of those cases the AI picks Python. We should treat this as an opportunity and anticipate an expansion in the kinds of people who might want to attend a Python conference. Yet many of these new programmers are not even aware that programming communities and conferences exist.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <blockquote cite="https://lucumr.pocoo.org/2025/7/20/the-next-generation/"><p>Every day someone becomes a programmer because they figured out how to make ChatGPT build something. Lucky for us: in many of those cases the AI picks Python. We should treat this as an opportunity and anticipate an expansion in the kinds of people who might want to attend a Python conference. Yet many of these new programmers are not even aware that programming communities and conferences exist. It’s in the Python community’s interest to find ways to pull them in.</p></blockquote>
<p>— <a href="https://lucumr.pocoo.org/2025/7/20/the-next-generation/">Armin Ronacher</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/pycon">pycon</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/armin-ronacher">armin-ronacher</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   10:54 &bull;
   8 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/20/today-i-updated-a-friends.html'> Today I updated a friends iphone apps .. triple digits .. ap</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Today I updated a friends iphone apps .. triple digits .. apparently Clubhouse is still going .. no - don’t delete it she said - I might need it. 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Today I updated a friends iphone apps .. triple digits .. apparently Clubhouse is still going .. no - don’t delete it she said - I might need it.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   07:38 &bull;
   8 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/20/tim-sweeney/#atom-everything'>Quoting Tim Sweeney</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    There’s a bigger opportunity in computer science and programming (academically conveyed or self-taught) now than ever before, by far, in my opinion. The move to AI is like replacing shovels with bulldozers. Every business will benefit from this and they’ll need people to do it.  
 — Tim Sweeney, Epic Games 

     Tags: ai-assisted-programming, careers, ai 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <blockquote cite="https://x.com/timsweeneyepic/status/1946721961746608267"><p>There’s a bigger opportunity in computer science and programming (academically conveyed or self-taught) now than ever before, by far, in my opinion. The move to AI is like replacing shovels with bulldozers. Every business will benefit from this and they’ll need people to do it.</p></blockquote>
<p>— <a href="https://x.com/timsweeneyepic/status/1946721961746608267">Tim Sweeney</a>, Epic Games</p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/ai">ai</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   03:22 &bull;
   8 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/20/142705.html'> 💬 
</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   💬 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/f6f6d44cd9.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>💬</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/f6f6d44cd9.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   02:27 &bull;
   8 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 19. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/20/on-the-road-was-published.html'> 💬 On the Road was published nearly seventy years ago .. just</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   💬 On the Road was published nearly seventy years ago .. just to remind us all that history rhymes .. and arguably never changes. 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/07a6184407.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>💬 On the Road was published nearly seventy years ago .. just to remind us all that history rhymes .. and arguably never changes.</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/07a6184407.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:55 &bull;
   8 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/19/openai-gold-medal-math-olympiad/#atom-everything'>OpenAI's gold medal performance on the International Math Olympiad</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   OpenAI's gold medal performance on the International Math Olympiad 
This feels notable to me. OpenAI research scientist Alexander Wei: 
 
 I’m excited to share that our latest @OpenAI experimental reasoning LLM has achieved a longstanding grand challenge in AI: gold medal-level performance on the world’s most prestigious math competition—the International Math Olympiad (IMO). 
 We evaluated our mo
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://static.simonwillison.net/static/2025/iom-question-1.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p><a href="https://x.com/alexwei_/status/1946477742855532918">OpenAI's gold medal performance on the International Math Olympiad</a></p>
This feels notable to me. OpenAI research scientist <a href="https://www.alexwei.org/">Alexander Wei</a>:<p></p>
<blockquote>
<p>I’m excited to share that our latest @OpenAI experimental reasoning LLM has achieved a longstanding grand challenge in AI: gold medal-level performance on the world’s most prestigious math competition—the International Math Olympiad (IMO).</p>
<p>We evaluated our models on the 2025 IMO problems under the same rules as human contestants: two 4.5 hour exam sessions, no tools or internet, reading the official problem statements, and writing natural language proofs. [...]</p>
<p>Besides the result itself, I am excited about our approach: We reach this capability level not via narrow, task-specific methodology, but by breaking new ground in general-purpose reinforcement learning and test-time compute scaling.</p>
<p>In our evaluation, the model solved 5 of the 6 problems on the 2025 IMO. For each problem, three former IMO medalists independently graded the model’s submitted proof, with scores finalized after unanimous consensus. The model earned 35/42 points in total, enough for gold!</p>
<p>HUGE congratulations to the team—<a href="https://x.com/SherylHsu02">Sheryl Hsu</a>, <a href="https://x.com/polynoamial">Noam Brown</a>, and the many giants whose shoulders we stood on—for turning this crazy dream into reality! I am lucky I get to spend late nights and early mornings working alongside the very best.</p>
<p>Btw, we are releasing GPT-5 soon, and we’re excited for you to try it. But just to be clear: the IMO gold LLM is an experimental research model. We don’t plan to release anything with this level of math capability for several months.</p>
</blockquote>
<p>(Normally I  would just link to the tweet, but in this case Alexander built a thread... and Twitter threads no longer work for linking as they're only visible to users with an active Twitter account.)</p>
<p>Here's Wikipedia on <a href="https://en.wikipedia.org/wiki/International_Mathematical_Olympiad">the International Mathematical Olympiad</a>:</p>
<blockquote>
<p>It is widely regarded as the most prestigious mathematical competition in the world. The first IMO was held in Romania in 1959. It has since been held annually, except in 1980. More than 100 countries participate. Each country sends a team of up to six students, plus one team leader, one deputy leader, and observers.</p>
</blockquote>
<p>This year's event is in Sunshine Coast, Australia. Here's <a href="https://www.imo-official.org/year_info.aspx?year=2025">the web page for the event</a>, which includes a button you can click to access a PDF of the six questions - maybe they don't link to that document directly to discourage it from being indexed.</p>
<p>The first of the six questions looks like this:</p>
<p></p>
<p>Alexander shared <a href="https://github.com/aw31/openai-imo-2025-proofs/">the proofs produced by the model</a> on GitHub. They're in a slightly strange format - not quite MathML embedded in Markdown - which Alexander <a href="https://x.com/alexwei_/status/1946477760614166943">excuses</a> since "it is very much an experimental model".</p>
<p>The most notable thing about this is that the unnamed model achieved this score without using any tools. OpenAI's Sebastien Bubeck <a href="https://x.com/SebastienBubeck/status/1946577650405056722">emphasizes that here</a>:</p>
<blockquote>
<p>Just to spell it out as clearly as possible: a next-word prediction machine (because that's really what it is here, no tools no nothing) just produced genuinely creative proofs for hard, novel math problems at a level reached only by an elite handful of pre‑college prodigies.</p>
</blockquote>
<p>There's a bunch more useful context in <a href="https://x.com/polynoamial/status/1946478249187377206">this thread</a> by Noam Brown, including a note that this model wasn't trained specifically for IMO problems:</p>
<blockquote>
<p>Typically for these AI results, like in Go/Dota/Poker/Diplomacy, researchers spend years making an AI that masters one narrow domain and does little else. But this isn’t an IMO-specific model. It’s a reasoning LLM that incorporates new experimental general-purpose techniques.</p>
<p>So what’s different? We developed new techniques that make LLMs a lot better at hard-to-verify tasks. IMO problems were the perfect challenge for this: proofs are pages long and take experts hours to grade. Compare that to AIME, where answers are simply an integer from 0 to 999.</p>
<p>Also this model thinks for a long time. o1 thought for seconds. Deep Research for minutes. This one thinks for hours. Importantly, it’s also more efficient with its thinking. And there’s a lot of room to push the test-time compute and efficiency further.</p>
<p>It’s worth reflecting on just how fast AI progress has been, especially in math. In 2024, AI labs were using grade school math (GSM8K) as an eval in their model releases. Since then, we’ve saturated the (high school) MATH benchmark, then AIME, and now are at IMO gold. [...]</p>
<p>When you work at a frontier lab, you usually know where frontier capabilities are months before anyone else. But this result is brand new, using recently developed techniques. It was a surprise even to many researchers at OpenAI. Today, everyone gets to see where the frontier is.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/mathematics">mathematics</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm-reasoning">llm-reasoning</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://static.simonwillison.net/static/2025/iom-question-1.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:27 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://altmode.org'>Altmode</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://altmode.org/2025/07/19/adriatic-trip-day-5-florence/'>Adriatic Trip, Day 5: Florence</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Saturday, June 14, 2025 Following breakfast at our hotel, we met up with our local guide and were taken on a tour of the Uffizi Gallery, a large museum with an enormous collection of art, including some well-known classics such as Botticelli’s The Birth of Venus. Following our tour, we walked the Ponte Vecchio bridge […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Saturday, June 14, 2025</p>



<p>Following breakfast at our hotel, we met up with our local guide and were taken on a tour of the <a href="https://en.wikipedia.org/wiki/Uffizi">Uffizi Gallery</a>, a large museum with an enormous collection of art, including some well-known classics such as Botticelli’s The Birth of Venus.</p>



<p>Following our tour, we walked the Ponte Vecchio bridge across the Arno River. A walking bridge, Ponte Vecchio also supports the enclosed walkway used by the Medici rulers to travel between their palaces safely. Today, it is a bustling and touristy commercial district, lined with stores selling jewelry and souvenirs.</p>


 
<a href="https://altmode.org/wp-content/uploads/2025/07/img_4126.jpg"></a>Ponte Vecchio 


<p>Once across the bridge, we made our way to Palazzo Pitti, the newer of the Medici palaces. The palazzo is now a museum, and we were hoping to see the extensive formal gardens behind it. But unfortunately, the gardens were accessible only with admission to the museum, which we weren’t inclined to do.</p>



<p>After a bit of wandering around on that side of the Arno, we crossed back on the Ponte Vecchio and had a late lunch near the Uffizi. We then went to the Galileo Museum, which highlighted the many Italian contributions to a wide range of scientific fields.</p>



<p>We returned to our hotel to relax for a bit, and started to hear thumping noises suggesting perhaps a band playing outside. So we went out and followed the sound, only to encounter a parade of musicians and others in medieval dress marching in the direction of the <a href="https://en.wikipedia.org/wiki/Calcio_storico_fiorentino">calcio storico</a> field. Apparently, the games are accompanied by a good deal of pageantry as well, with each of the teams and their supporters identified by their team colors. The prize for winning the tournament is a cow, and we also got to see the cow as it walked past us down the street. The parade was a lot of fun, and the costumes were gorgeous.</p>




<a href="https://altmode.org/wp-content/uploads/2025/07/img_6469.jpg"></a>Calcio storico parade



<a href="https://altmode.org/wp-content/uploads/2025/07/img_6468.jpg"></a>The prize cow




<p>Our evening activity was a private tour of the Duomo museum led by our local guide. The Duomo is the cathedral in the center of Florence, and the museum displays and protects significant artifacts such as the doors to the baptistry (the ones on the actual baptistry are replicas). One thing that struck me was the intentional distortion of the proportions of some statues that are intended to be viewed from a much lower perspective. Their features are exaggerated vertically so that they look normal when viewed as intended. Michelangelo also exaggerated the features of David, which was originally meant to be placed atop the Duomo.</p>



<p>After the museum tour, we grabbed a very informal late dinner by stopping by one of the many <a href="http://allanticovinaio.com">All’Antico Vinaio</a> sandwich shops. They feature a large variety of enormous sandwiches on focaccia bread. Kenna and I ordered one cut in half to share. It was delicious and about the right amount of food.</p>



 



<p>This article is part of a series about our recent trip to Italy, Greece, Albania, and Croatia. To see the introductory article in the series, click <a href="https://altmode.org/2025/07/15/adriatic-trip-day-1-to-venice/">here</a>.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://altmode.org/wp-content/uploads/2025/07/img_4126.jpg?w=300'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:00 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wrenchinthegears.com/2025/07/19/caddo-pottery-patterns-echoes-of-other-worlds/'>Caddo Pottery Patterns – Echoes Of Other Worlds</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I’m making this brief post primarily as a placeholder for the photographs I took yesterday of the collection of Caddo pottery at Henderson State College in Arkadelphia, AR. The collection of artifacts, donated by the Hodges family to the school, is impressive and well worth a day trip. It contains powerful energies. Look to the [...]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I’m making this brief post primarily as a placeholder for the photographs I took yesterday of the collection of Caddo pottery at Henderson State College in Arkadelphia, AR. The collection of artifacts, donated by the Hodges family to the school, is impressive and well worth a day trip. It contains powerful energies. Look to the left as you enter the campus and you will see the Caddo Center, now the admissions and financial aid offices. Parking is on the right. A bonus in 97 degree heat and thick humidity was excellent air conditioning. I felt called to come see the pottery, because the incised patterns for me are echoes of phase shifts, topology, knots, labyrinths, and physics. Many of the ornamental pieces were used as burial objects. The Caddo people, who also lived in Oklahoma, east Texas, and Louisiana were a mound building people. Settlements of affiliated tribes included 250,000 people before the arrival of the conquistadors. Read more about them <a href="https://www.caddopottery.com/culture">here</a> and <a href="https://www.redriverhistorian.com/post/mound-city-caddo-southwest-arkansas">here</a>.</p>
<p>The feature photo is a double rainbow over my house. After seeing the pottery, friends and I took a small hike out to the Hernando DeSoto Bluff overlooking the Ouchaita River (the one that was dammed to create multiple lakes), and had a lovely picnic in the log pavilion there. Highly recommended. After, we swum at the Caddo Bend Beach on DeGray Lake. A bonus is that state parks in Arkansas are free for day use. We saw migratory sandpipers, button bush, mallow, and ate popcorn in the shade of the pine trees. Several turtles eyed us up from the distance, but no nibbles. Yay. Dropped in at a rural health food store on the way home, just in time for drenching evening rains, which my garden loved.&nbsp;</p>
<p>I am so grateful. I am so grateful. I am so grateful for landing here in this special place with so many fascinating stories and so many deep, caring people.&nbsp;</p>
<p></p>
<p></p>
<p>If you missed my July 4 update on youtube. You can watch it below – a manageable 40 minutes.</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p>&nbsp;</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2025/07/imagejpeg_0-1.jpg?resize=1020%2C483&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:34 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/microsoft-cant-protect-french-data-from-us-government-access/'>Microsoft can't protect French data from US government access</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  When vendor promises meet government warrants, the warrants win every time. Microsoft's Senate testimony shows why "trust us" isn't a data protection strategy.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>[<a href="https://ppc.land/microsoft-cant-protect-french-data-from-us-government-access/?ref=werd.io">Luis Rijo at PPC Land</a>]</p><p>Microsoft’s disclosure that it can’t protect French data from being silently accessed by its US business is in apparent tension with the GDPR but in compliance with the <a href="https://en.wikipedia.org/wiki/CLOUD_Act?ref=werd.io">US CLOUD Act</a>:</p><blockquote>“The CLOUD Act asserts that U.S. data and communication companies must provide stored data for a customer or subscriber on any server they own and operate when requested by warrant.”</blockquote><p>There are provisions for a provider to reject or contest these warrants and subpoenas, but they’re limited in scope. The EU previously found that <a href="https://www.edps.europa.eu/sites/default/files/publication/19-07-10_edpb_edps_cloudact_annex_en.pdf?ref=werd.io">the Act was in conflict with privacy protections guaranteed by the GDPR</a>. Microsoft has claimed that there have been no such accesses “in recent years”, but this excludes classified requests and national security letters.</p><p>Microsoft may have found that its hands were tied because of the CLOUD Act, but it’s nonetheless made assurances that European data couldn’t be accessed from the US. <a href="https://ppc.land/microsoft-cant-protect-french-data-from-us-government-access/?ref=werd.io">From the linked post</a>:</p><blockquote>“The testimony contradicts years of Microsoft's security assurances regarding European data hosting. Despite implementing encryption and technical safeguards, the company acknowledged that US legislation ultimately supersedes protective measures when federal agencies issue valid data requests.”</blockquote><p>For organizations storing sensitive data, this suggests some important principles to consider.</p>  It’s not enough for a vendor to assure that data cannot be accessed via warrant or subpoena. Because US law supersedes those assurances, for your data to be truly protected it must be physically (not just legally) impossible for anyone to access it, in a way that’s fully auditable and under your control. That implies client-side encryption and self-hosting strategies.  Because the subjects of criminal subpoenas are unlikely to be notified, they present a useful route for government and law enforcement to silently access data. If you deal with sensitive or personally identifying data, this needs to be a part of your threat model.  Promises aren’t worth the paper they’re printed on.  Governments and organizations outside the US should invest in building their own software and platforms that adhere to their values and responsibilities.  Governments and organizations inside the US also can’t trust the safety of their data, and should consider doing the same. In particular, using strong, sovereign encryption ensures that an organization will know when their data is accessed — because it’s impossible to do so without their involvement.  <p>This issue is not limited to Microsoft. As the article points out:</p><blockquote>“Amazon Web Services, Google Cloud, and other hyperscale providers operate under identical legal frameworks, potentially exposing European data to extraterritorial access. The testimony suggests widespread vulnerability in European digital infrastructure built on American technological foundations.”</blockquote><p>Reliance on US services has become a point of vulnerability for everyone. This should be a concern regardless of American leadership; under the current administration, it’s become a frequent topic of conversation for security leaders both inside and outside of the country.</p><p>France has mandated that sensitive data is migrated to services certified by SecNumCloud, a French security qualification to ensure the robustness of cloud solutions, particularly for sensitive and critical data. But this concern goes far beyond France. Any organization that needs to keep its information private, particularly for the safety and privacy of vulnerable individuals, must make tough choices about how to protect the sanctity of its data. For many of them, the prevailing cloud strategy of the last decade may have outlived its usefulness.</p><p>[<a href="https://ppc.land/microsoft-cant-protect-french-data-from-us-government-access/?ref=werd.io">Link</a>]</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:44 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2025/07/7374/'>バッハ「トッカータとフーガ ニ短調 BWV 565」はもとはバイオリン曲？</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  バッハ「トッカータとフーガ ニ短調 BWV 565」は別人の作品？ J.S.バッハの代表曲の一つに、「バッハ「トッカータとフーガ ニ短調 BWV 565」があります。あの、「ちゃららー、ちゃららららーらー」という出だしのパッセージが有名な曲です。 この「代表曲」である「トッカータ…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 バッハ「トッカータとフーガ ニ短調 BWV 565」は別人の作品？ 



<p>J.S.バッハの代表曲の一つに、「バッハ「トッカータとフーガ ニ短調 BWV 565」があります。あの、「ちゃららー、ちゃららららーらー」という出だしのパッセージが有名な曲です。</p>



 

 



<p>この「代表曲」である「トッカータ」、実は「本当にバッハなのか？」という疑問が1833年初出版の時から呈されている曲でもあります。理由はいくつかあります。</p>



 
 この作品はバッハの自筆譜が現存せず、もっとも古い写譜も18世紀後半のもので、そのため成立事情が不明瞭 



 フーガの書法が異例であること。特に主題が単独で提示されるオルガンフーガ、および短調の変終止で終わるオルガンフーガはバッハの全生涯を通じて他に例がない  



 フーガの対位法が薄い 



 動機主題作業を行っている（←これはJ.S.バッハの次の世代の音楽であることを示唆） 



 J.S.Bachの時代のオルガンには珍しいC#が必要になって当時の一般的なオルガンでは演奏不能。 



 etc.  
 



<p>そんなわけで、これはバッハの真作ではないのではないかと言われているのです。代表的な説には</p>



 
 ペーター・ケルナー（1705年 – 1772年）作説(Claus, 1998)<a href="https://www.sakimura.org/2025/07/7374/#easy-footnote-bottom-2-7374" title="Claus, Rolf-Dietrich. 1998. Zur Echtheit von Toccata und Fuge d-moll BWV 565. Verlag Dohr, 2nd ed. Cologne.&amp;nbsp;<a href=&quot;https://ja.wikipedia.org/wiki/%E7%89%B9%E5%88%A5:%E6%96%87%E7%8C%AE%E8%B3%87%E6%96%99/3925366377&quot;>ISBN 3-925366-37-7</a>.&amp;nbsp;">2</a>、 



 バッハの弟子であるコルネリウス・ハインリヒ・ドレッツェル (1697-1775)作説(Hall, 2013) <a href="https://www.sakimura.org/2025/07/7374/#easy-footnote-bottom-3-7374" title="Hall, Jonathan B. 2013. BWV 565: Composer Found?. The Diapason. <a href=&quot;https://www.thediapason.com/content/bwv-565-composer-found&quot;>https://www.thediapason.com/content/bwv-565-composer-found</a>. (2025-07-19取得) ">3</a>、あるいは 



 バッハの次男であるC.P.E.バッハ（1714年 – 1788年）作説 (Gaiilit, 2022) <a href="https://www.sakimura.org/2025/07/7374/#easy-footnote-bottom-4-7374" title="Gailit, Michael. 2022. Exploring the unknown of BWV 565, Part 6. The Diapason. <a href=&quot;https://www.thediapason.com/content/exploring-unknown-bwv-565-part-6&quot;>https://www.thediapason.com/content/exploring-unknown-bwv-565-part-6</a> (2025-07-19取得)">4</a> 
 



<p>などがあります。</p>



 ケルナー(Johann Peter Kellner)説 



<p>ヨハン・ペーター・ケルナーは北ドイツで活躍したオルガニスト・作曲家で、J.S.バッハをよく知る人物でした。弟子であった可能性もありますが定かではありません。確かなのは、J.S.バッハの作品を広げるのに非常に貢献した人物であったということです。特に、現在私達がバッハの６曲のチェロ・ソナタに接することができるのは彼の貢献が大きいとされます。</p>



<p>ケルナー説は、この曲の最初の写本が彼の弟子Johannes Ringk によるものというところから来ているようです。しかし、作曲様式比較の面からも、写本の筆跡鑑定の観点からJohannes Ringkによるものではないという点<a href="https://www.sakimura.org/2025/07/7374/#easy-footnote-bottom-5-7374" title="Gailit, Michael. 2022. Exploring the unknown of BWV 565, Part 5. The Diapazon. <em><a href=&quot;https://www.thediapason.com/content/exploring-unknown-bwv-565-part-5&quot;>September 2022 issue, pp 19–21</a></em> <a href=&quot;https://www.thediapason.com/content/exploring-unknown-bwv-565-part-5&quot;>https://www.thediapason.com/content/exploring-unknown-bwv-565-part-5</a> (2025-007-19取得) ">5</a>からも、現在は否定的意見が多いです。でも、一応、ケルナーの曲も聞いておきましょうか。</p>



 

 



<p></p>



 ドレッツェル(Cornelius Heinrich Dretzel)説 



<p>コルネリウス・ハインリヒ・ドレッツェルは、南ドイツ地方で活躍したオルガニスト・作曲家で、J.S.バッハの弟子です。活躍した地方の特性上、作曲様式はイタリアの影響が顕著とされます。</p>



<p>やはりバッハの作とされていたBWV 897の前奏曲の真作者とわかっていて、バッハ作とされているもので、作曲技法的に違和感があるものの場合、真っ先に上がる作曲家です。音楽学者ジョナサン・ホールは、2013年のThe Diapason<a href="https://www.sakimura.org/2025/07/7374/#easy-footnote-bottom-6-7374" title="オルガンを含む鍵盤楽器音楽に特化した、100年以上の歴史を持つ国際的学術誌">6</a> での論文(Hall, 2013)で、ドレッツェルの鍵盤作品（特に《ディヴェルティメント・アルモニコ》）とBWV 565の間に「非常に高い様式的一致」を指摘しています。まぁ、聞いてみてください。確かに似てますね。</p>



 

 



<p>どころで、ドレッツェルってほとんど曲が伝わってなくて、IMSLP(国際楽譜図書館事業)でもこれ１曲しか出てこないし、Naxos Music Library でも出てこないんですよね。惜しいことです。</p>



<p></p>



 C.P.E.バッハ(Carl Philipp Emanuel Bach) 説 



<p>さらに、バッハの人気のある作品（<a href="https://www.sakimura.org/2014/02/2206/">シチリアーノ BWV 1031</a>、とか<a href="https://youtu.be/tQgC9OZQj7Q?si=KlN14RjMm7UbvDH-&amp;t=19">BWV 1020</a> など）の真作者とされるJ.S.バッハの次男、<a href="https://www.sakimura.org/2014/08/2498/">カール・フィリップ・エマニュエル・バッハ (C.P.E. Bach) </a>も同様に候補に上がっています。</p>



<p>C.P.E.バッハ説は音楽誌 The DiapazonでMichael Gailitによって2022年に発表された比較的新しい説で、以下のようなことをその理由としてあげています。</p>



 
 「J.S.バッハの他の鍵盤曲とBWV565の間の楽譜の見た目にはほとんど共通点がないが、C.P.E.バッハの鍵盤曲との間にはいくらでも見つけられる」（筆者注：実際そう思う）、 



 「BWV565は動機主題作業<a href="https://www.sakimura.org/2025/07/7374/#easy-footnote-bottom-7-7374" title="動機主題作業=主題を動機分解して展開して音楽を構築する方法。C.P.E.バッハがおそらく最初に行っている">7</a>を行っている」（これは、Gailit(2022)での新発見だそうです）、 



 「C.P.E.バッハは自分の楽譜を大量に廃棄しているので、彼のカタログに乗っていない作品があっても自然」、 



 1755年と1776年にC.P.E.バッハが仕えていたフリードリヒ2世の末妹アンナ・アマリア王女が依頼した2台のオルガンは、全手鍵盤とペダルにフルコンパスを備えていて<a href="https://www.sakimura.org/2025/07/7374/#easy-footnote-bottom-8-7374" title="当時の音域内の全半音を演奏できるということ">8</a>、この曲をそのまま演奏可能、 
 



<p>といったような点が挙げられています。ちょっと聞いてみましょう。彼のオルガンのためのファンタジーとフーガハ短調 Wq 119/7 です。</p>



 

 



<p>しかし、いずれも決定打になるような証拠はありません。ただ言えそうなのは、オルガン曲だとすれば、おそらくJ.S.バッハの次の世代の曲ではないかということでしょうか。ちなみに、個人的には Dretzel っぽいなぁと思います。</p>



 BWV 565はバイオリン曲?! 



<p>一方、「いや、J.S.Bachだ」という声ももちろんあって、その中には「対位法的書法が薄いのは、もとはバイオリンなど弦楽器用の作品だったからだ」という声もあります。わたしがこの曲がもとは弦楽器曲ではないかという論に初めて接したのは2022年のことでした。</p>



 
 <blockquote><p>バッハのトッカータとフーガニ短調  BWV 565、本来はオルガン曲じゃなくてバイオリンなどの弦楽器ソロ曲の編曲じゃないのかという指摘、確かになぁ。オルガンみたいに多声部書きやすい楽器のためだったらバッハはもっと稠密に書くだろうなぁ。</p>— Nat Sakimura/崎村夏彦 (@_nat) <a href="https://twitter.com/_nat/status/1484163444009963520?ref_src=twsrc%5Etfw">January 20, 2022</a></blockquote> 
 



<p>この説は、<a href="https://en.wikipedia.org/wiki/Peter_Williams_(musicologist)">音楽学者ピーター・ウィリアムズ</a>の1981年の論文(Williams, 1981)<a href="https://www.sakimura.org/2025/07/7374/#easy-footnote-bottom-9-7374" title="Williams, Peter. 1981. BWV 565: A Toccata in D Minor for Organ by J. S. Bach?. Early Music (July 1981), pp330–337.">9</a>（ただし、ウィリアムズ自身はJ.S.バッハ作ではないという立場）やブルース・フォックス-レフリッシュの論文(Fox-Lefrish, 2004)<a href="https://www.sakimura.org/2025/07/7374/#easy-footnote-bottom-10-7374" title="Fox-Lefriche, Bruce. 2004. The Greatest Violin Sonata that J. S. Bach Never Wrote,”&amp;nbsp;<em>Strings</em>&amp;nbsp;(October 2004), pp44–55.">10</a>などによって「楽曲構成や音型・パッセージワークに、オルガン曲としては異例で、ヴァイオリン曲としてより自然である部分が多い」として唱えられる説です。フーガの書法が薄いのもバイオリンソロのためであれば納得できます。主なヴァイオリン的特徴としては以下のような点が挙げられます。</p>



 主なヴァイオリン的特徴 



 
 アルペジオや単一音型の多用<br>　曲冒頭や各節でみられる分散和音（アルペジオ的パッセージ）は、バロック期のヴァイオリン独奏曲でよく使われた技法です。オルガンで演奏すると音響が重厚に響きますが、ヴァイオリン用だと和音を素早く分散して響かせる技巧的要求として自然です<a href="https://en.wikipedia.org/wiki/Toccata_and_Fugue_in_D_minor,_BWV_565"></a>。 



 広い音域移動と単音の流れ<br>　ヴァイオリン奏者のポジション移動による音域の広がりや、連続的な単音（レガートなラインや速いパッセージ）は、オルガンよりも弦楽器独奏曲に適しています<a href="https://en.wikipedia.org/wiki/Toccata_and_Fugue_in_D_minor,_BWV_565"></a>。 



 重音の活用と和声処理の薄さ<br>　三度・六度などの重音進行や、部分的に薄い和声が目立つ点も、ヴァイオリン作品に特徴的です。オルガンでは和声を厚く補うのが一般的ですが、BWV 565には意図的に「間」を感じさせる部分が多いとされています<a href="https://en.wikipedia.org/wiki/Toccata_and_Fugue_in_D_minor,_BWV_565"></a><a href="https://www.good-music-guide.com/community/index.php?topic=638.1320"></a>。 



 バリオラージュ奏法的モチーフ（開放弦と指弦の交錯）<br>　フーガ主題をはじめ、バリオラージュ（ヴァイオリンで開放弦と他の弦を交互に素早く演奏する手法）的パターンが随所に現れます。これは鍵盤よりむしろ弦楽器で技巧的・効果的な処理<a href="https://www.thediapason.com/content/bwv-565-composer-found"></a>。 
 



<p>実際にバイオリンで弾いたバージョンもあります。なるほどなー、と思わせるところはあります。というわけで、お聞きください。バイオリンソロ版トッカータとフーガ。</p>



 バイオリンソロ版トッカータとフーガ 



 

 



 謝辞 



<p>なお、この記事は、うちでやっている「<a href="https://www.youtube.com/@tonescape6939">十三音会</a>」の常連でもある、yamauchi_toru さんの note の記事「<a href="https://note.com/toruy2248/n/n521f2ca4c505">音楽家と歴史・社会 -42: J.S. バッハの真作・贋作論争を巡って</a>」に触発されて書きました。このような機会を作っていただいたことに篤く御礼申し上げます。</p>



 音楽カテゴリーの他の記事 


  <a href="https://www.sakimura.org/2025/07/7374/">バッハ「トッカータとフーガ ニ短調 BWV 565」はもとはバイオリン曲？</a>  <a href="https://www.sakimura.org/2023/12/5465/">季節のご挨拶〜バッハ：シチリアーノ（フルート、スピネット）</a>  <a href="https://www.sakimura.org/2022/11/5063/">ヴァイオレット・エヴァーガーデンの舞台で「みちしるべ」</a>  <a href="https://www.sakimura.org/2021/03/4769/">おすすめの隠れた名曲〜ダマーズ:フルート、オーボエ、クラリネットとピアノのための四重奏曲</a>  <a href="https://www.sakimura.org/2020/12/4766/">季節のご挨拶〜J. S. バッハ: 羊は安らかに草を食み（フルート、ピアノ、スピネット、マリンバ）</a>  <a href="https://www.sakimura.org/2019/12/4591/">今年もお世話になった方々へのご挨拶〜Fauré: Pie Jesu</a>  <a href="https://www.sakimura.org/2018/07/4231/">『２台のマリンバのための嬉遊曲』〜横山菁児１周忌によせて</a>  <a href="https://www.sakimura.org/2018/05/4156/">ベートーヴェンが求めた「月光」の響き</a>  <a href="https://www.sakimura.org/2017/12/4069/">季節のご挨拶：『Cantique de Noël (O Holy Night) 賛美歌：第二編219番』に寄せて</a>  <a href="https://www.sakimura.org/2017/11/4016/">のだめデビューコンサートの場所は？異色だけど良い演奏？</a>  



<p></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   09:22 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/19/whilst-talking-about-broken-escalators.html'> … whilst 🔗 talking about broken escalators - I wonder when t</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   … whilst 🔗 talking about broken escalators - I wonder when this will be fixed … 10 days and counting so far. 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/img-6481.jpeg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>… whilst <a href="https://john.philpin.com/2025/04/23/no-pictures-so-no-embarrassment.html">🔗 talking about broken escalators</a> - I wonder when this will be fixed … 10 days and counting so far.</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/img-6481.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   04:12 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.jonudell.net'>Jon Udell</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.jonudell.net/2025/07/18/introducing-xmlui/'>Introducing XMLUI</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  In the mid-1990s you could create useful software without being an ace coder. You had Visual Basic, you had a rich ecosystem of components, you could wire them together to create apps, standing on the shoulders of the coders who built those components. If you’re younger than 45 you may not know what that was … Continue reading Introducing XMLUI
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>
In the mid-1990s you could create useful software without being an ace coder. You had Visual Basic, you had a rich ecosystem of components, you could wire them together to create apps, standing on the shoulders of the coders who built those components. If you’re younger than 45 you may not know what that was like, nor realize web components have never worked the same way. The project we’re announcing today, <a href="https://xmlui.org">XMLUI</a>, brings the VB model to the modern web and its React-based component ecosystem. XMLUI wraps React and CSS and provides a suite of components that you compose with XML markup. Here’s a little app to check the status of London tube lines.
</p>
 &lt;App&gt;
  &lt;Select id="lines" initialValue="bakerloo"&gt;
    &lt;Items data="https://api.tfl.gov.uk/line/mode/tube/status"&gt;
    &lt;/Items&gt;
  &lt;/Select&gt;
  &lt;DataSource
    id="tubeStations"
    url="https://api.tfl.gov.uk/Line/{lines.value}/Route/Sequence/inbound"
    resultSelector="stations"/&gt;
  &lt;Table data="{tubeStations}" height="280px"&gt;
    &lt;Column bindTo="name" /&gt;
    &lt;Column bindTo="modes" /&gt;
  &lt;/Table&gt;
&lt;/App&gt;
 
<p>

</p>
<p>
A dozen lines of XML is enough to:
</p>
 
 
Define a <a href="https://docs.xmlui.org/components/Select">Select</a> and fill its <a href="https://docs.xmlui.org/components/Items">Items</a> with data from an API call.
 
 
Define a <a href="https://docs.xmlui.org/components/DataSource">DataSource</a> to fetch data from another API call.
 
 
Use the value of the Select to dynamically form the URL of the DataSource.
 
 
Use a <a href="https://docs.xmlui.org/components/DataSource#resultselector">resultSelector</a> to drill into the result of the second API call.
 
 
Bind that result to a <a href="https://docs.xmlui.org/components/Table">Table</a>.
 
 
Bind fields in the result to <a href="https://docs.xmlui.org/components/Column">Columns</a>.
 
 
<p>
This is a clean, modern, component-based app that’s <a href="https://docs.xmlui.org/reactive-intro">reactive</a> and <a href="https://docs.xmlui.org/themes-intro">themed</a> without requiring any knowledge of React or CSS. That’s powerful leverage. And it’s code you can read and maintain, no matter if it was you or an LLM assistant who wrote it. I’m consulting for the project so you should judge for yourself, but to me this feels like an alternative to the JavaScript industrial complex that ticks all the right boxes.
</p>
 Components 
<p>My most-cited BYTE article was a 1994 cover story called <a href="https://web.archive.org/web/19961220155530/http://www.byte.com/art/9405/sec5/art1.htm">Componentware</a>. Many of us had assumed that the engine of widespread software reuse would be libraries of low-level objects linked into programs written by skilled coders. What actually gained traction were components built by professional developers and used by business developers.
</p>
<p>
There were Visual Basic components for charting, network communication, data access, audio/video playback, and image scanning/editing. UI controls included buttons, dialog boxes, sliders, grids for displaying and editing tabular data, text editors, tree and list and tab views. People used these controls to build point-of-sale systems, scheduling and project management tools, systems for medical and legal practice management, sales and inventory reporting, and much more.
</p>
<p>
That ecosystem of component producers and consumers didn’t carry forward to the web. I’m a fan of web components but it’s the React flavor that dominate and they are not accessible to the kind of developer who could productively use Visual Basic components back in the day. You have to be a skilled coder not only to create a React component but also to use one. XMLUI wraps React components so solution builders can use them.
</p>
 User-defined components 
<p>
XMLUI provides a deep <a href="https://docs.xmlui.org/components/_overview">catalog of components</a> including all the interactive ones you’d expect as well as behind-the-scenes ones like DataSource, <a href="https://docs.xmlui.org/components/APICall">APICall</a>, and <a href="https://docs.xmlui.org/components/Queue">Queue</a>. You can easily define your own components that interop with the native set and with one another. Here’s the markup for a TubeStops component.
</p>
 &lt;Component name="TubeStops"&gt;
  &lt;DataSource
    id="stops"
    url="https://api.tfl.gov.uk/Line/{$props.line}/StopPoints"
    transformResult="{window.transformStops}"
  /&gt;
  &lt;Text variant="strong"&gt;{$props.line}&lt;/Text&gt;
  &lt;Table data="{stops}"&gt;
    &lt;Column width="3*" bindTo="name" /&gt;
    &lt;Column bindTo="zone" /&gt;
    &lt;Column bindTo="wifi" &gt;
      &lt;Fragment when="{$item.wifi === 'yes'}"&gt;
        &lt;Icon name="checkmark"/&gt;
      &lt;/Fragment&gt;
    &lt;/Column&gt;
    &lt;Column bindTo="toilets" &gt;
      &lt;Fragment when="{$item.toilets === 'yes'}"&gt;
        &lt;Icon name="checkmark"/&gt;
      &lt;/Fragment&gt;
    &lt;/Column&gt;
  &lt;/Table&gt;
&lt;/Component&gt;
 
<p>
Here’s markup that uses the component twice in a side-by-side layout.
</p>
   &lt;HStack&gt;
    &lt;Stack width="50%"&gt;
      &lt;TubeStops line="victoria" /&gt;
    &lt;/Stack&gt;
    &lt;Stack width="50%"&gt;
      &lt;TubeStops line="waterloo-city" /&gt;
    &lt;/Stack&gt;
  &lt;/HStack&gt;
 
<p>
It’s easy to read and maintain short snippets of XMLUI markup. When the markup grows to a hundred lines or more, not so much. But I never need  to look at that much code;  when components grow too large I refactor them. In any programming environment that maneuver entails overhead: you have to create and name files, identify which things to pass as properties from one place, and unpack them in another. But the rising LLM tide lifts all boats. Because I can delegate the refactoring to my team of AI assistants I’m able to do it fluidly and continuously. LLMs don’t “know” about XMLUI out of the box but they do know about XML, and with the help of MCP (see below) they can “know” a lot about XMLUI specifically.
</p>
 Reactivity 
<p>
If you’ve never been a React programmer, as I have not, the biggest challenge with XMLUI-style reactivity isn’t what you need to learn but rather what you need to unlearn. Let’s take another look at the code for the app shown at the top of this post.
</p>
 &lt;App&gt;
  &lt;Select id="lines" initialValue="bakerloo"&gt;
    &lt;Items data="https://api.tfl.gov.uk/line/mode/tube/status"&gt;
        &lt;Option value="{$item.id}" label="{$item.name}" /&gt;
    &lt;/Items&gt;
  &lt;/Select&gt;
  &lt;DataSource
    id="tubeStations"
    url="https://api.tfl.gov.uk/Line/{lines.value}/Route/Sequence/inbound"
    resultSelector="stations"/&gt;
  &lt;Table data="{tubeStations}" height="280px"&gt;
    &lt;Column bindTo="name" /&gt;
    &lt;Column bindTo="modes" /&gt;
  &lt;/Table&gt;
&lt;/App&gt;
 
<p>
Note how the Select declares the property id="lines". That makes lines a reactive variable.
</p>
<p>
Now look at the url property of the DataSource. It embeds a reference to lines.value. Changing the selection changes lines.value. The DataSource reacts by fetching a new batch of details. Likewise the Table‘s data property refers to tubeStations (the DataSource) so it automatically displays the new data.
</p>
<p>
There’s a name for this pattern: reactive data binding. It’s what spreadsheets do when a change in one cell propagates to others that refer to it. And it’s what React enables for web apps. React is a complex beast that only expert programmers can tame. Fortunately the expert programmers who build XMLUI have done that for you. As an XMLUI developer you may need to unlearn imperative habits in order to go with the declarative flow. It’s a different mindset but if you keep the spreadsheet analogy in mind you’ll soon get the hang of it. Along the way you’ll likely discover happy surprises. For example, here’s the search feature in our demo app, <a href="https://github.com/xmlui-org/xmlui-invoice/">XMLUI Invoice</a>.
</p>
<p></p>
<p>
Initially I wrote it in a conventional way, with a search button. Then I realized there was no need for a button. The DataSource URL that drives the query can react to keystrokes in the TextBox, and the Table can in turn react when the DataSource refreshes.
</p>
 &lt;Component name="SearchEverything"&gt;
    &lt;VStack paddingTop="$space-4"&gt;
        &lt;TextBox
            placeholder="Enter search term..."
            width="25rem"
            id="searchTerm"
        /&gt;
        &lt;Card when="{searchTerm.value}"&gt;
            &lt;DataSource
              id="search"
              url="/api/search/{searchTerm.value}"
            /&gt;
            &lt;Text&gt;Found {search.value ? search.value.length : 0} results for
                "{searchTerm.value}":&lt;/Text&gt;
            &lt;Table data="{search}"&gt;
                &lt;Column  bindTo="table_name" header="Type" width="100px" /&gt;
                &lt;Column  bindTo="title" header="Title" width="*" /&gt;
                &lt;Column  bindTo="snippet" header="Match Details" width="3*" /&gt;
            &lt;/Table&gt;
        &lt;/Card&gt;
    &lt;/VStack&gt;
&lt;/Component&gt;
 
 Themes 
<p>
When the team first showed me the XMLUI <a href="https://docs.xmlui.org/themes-intro">theme system</a> I wasn’t too excited. I am not a designer so I appreciate a nice default theme that doesn’t require me to make color choices I’m not qualified to make. The ability to switch themes has never felt that important to me, and I’ve never quite understood why developer are so obsessed with dark mode. I have wrestled with CSS, though, to achieve both style and layout effects, and the results have not been impressive. XMLUI aims to make everything you build look good, and behave gracefully, without requiring you to write any CSS or CSS-like style and layout directives.
</p>
<p>
You can apply inline styles but for the most part you won’t need them and shouldn’t use them. For me this was another unlearning exercise. I know enough CSS to be dangerous and in the early going I abused inline styles. That was partly my fault and partly because LLMs think inline styles are catnip and will abuse them on your behalf. If you look at the code snippets here, though, you’ll see almost no explicit style or layout directives. Each component provides an extensive sets of theme variables that influence its text color and font, background color, margins, borders, paddings, and more. They follow a naming convention that enables a setting to control appearance globally or in progressively more granular ways. For example, here are the variables that can control the border color of a solid button using the primary color when the mouse hovers over it.</p>
 color-primary
backgroundColor-Button
backgroundColor-Button-solid
backgroundColor-Button-primary
backgroundColor-Button-primary-solid
backgroundColor-Button-primary-solid--hover
 
<p>
When it renders a button, XMLUI works up the chain from the most specific setting to the most general. This arrangement gives designers many degrees of freedom to craft exquisitely detailed themes. But almost all the settings are optional, and those that are defined by default use logical names instead of hardcoded values. So, for example, the default setting for backgroundColor-Button-primary is $color-primary-500. That’s the midpoint in a range of colors that play a primary role in the UI. There’s a set of such semantic roles, each associated with a color palette. The key roles are:
</p>
<p>
Surface: creates neutral backgrounds and containers.</p>
<p>
Primary: draws attention to important elements and actions.</p>
<p>
Secondary: provides visual support without competing with primary elements.</p>
<p>
What’s more, you can generate complete palettes from single midpoint value for each.
</p>
 name: Earthtone
id: earthtone
themeVars:
  color-primary: "hsl(30, 50%, 30%)"
  color-secondary: "hsl(120, 40%, 25%)"
  color-surface: "hsl(39, 43%, 97%)"
 
<p></p>
<p>
Themes aren’t just about colors, though. XMLUI components work hard to provide default layout settings that yield good spacing, padding, and margins both within individual components and across a canvas that composes sets of them. I am, again, not a designer, so not really qualified to make a professional judgement about how it all works. But the effects I can achieve look pretty good to me.
</p>
 Scripting 
<p>
As a Visual Basic developer you weren’t expected to be an ace coder but were expected to be able to handle a bit of scripting. It’s the same with XMLUI. The language is JavaScript and you can go a long way with tiny snippets like this one in TubeStops.
</p>
 &lt;Fragment when="{$item.wifi === 'yes'}"&gt;&lt;/Fragment&gt;
 
<p>
TubeStops does also use the transformResult property of its DataSource to invoke a more ambitious chunk of code.
</p>
 function transformStops(stops) {
  return stops.map(stop =&gt; {
    // Helper to extract a value from additionalProperties by key
    const getProp = (key) =&gt; {
      const prop = stop.additionalProperties &amp;&amp; stop.additionalProperties.find(p =&gt; p.key === key);
      return prop ? prop.value : '';
    };
    return {
      name: stop.commonName,
      zone: getProp('Zone'),
      wifi: getProp('WiFi'),
      toilets: getProp('Toilets'),
      // A comma-separated list of line names that serve this stop
      lines: stop.lines ? stop.lines.map(line =&gt; line.name).join(', ') : ''
    };
  });
}
 
<p>
This is not trivial, but it’s not rocket science either. And of course you don’t need to write stuff like this nowadays, you can have an LLM assistant do it for you. So we can’t claim that XMLUI is 100% declarative. But I think it’s fair to say that the imperative parts are well-scoped and accessible to a solution builder who doesn’t know, or want to know, anything about the JavaScript industrial complex.
</p>
 Model Context Protocol 
<p>
In the age of AI, who needs XMLUI when you can just have LLMs write React apps for you? It’s a valid question and I think I have a pretty good answer. The first version of XMLUI Invoice was a React app that Claude wrote in 30 seconds. It was shockingly complete and functional. But I wasn’t an equal partner in the process. I’m aware that React has things like useEffect and useContext but I don’t really know what they are or how to use them properly, and am not competent to review or maintain JavaScript code that uses these patterns. The same disadvantage applies to the CSS that Claude wrote. If you’re a happy vibe coder who never expects to look at or work with the code that LLMs generate, then maybe XMLUI isn’t for you.
</p>
<p>
If you need to be able review and maintain your app, though, XMLUI levels the playing field. I can read, evaluate, and competently adjust the XMLUI code that LLMs write. In <a href="https://www.youtube.com/watch?v=LCEmiRjPEtQ">a recent talk</a> Andrej Karpathy argues that the sweet spot for LLMS is a collaborative partnership in which we can dynamically adjust how much control we give them. The “autonomy slider” he envisions requires that we and our assistants operate in the same conceptual/semantic space. That isn’t true for me, nor for the developers XMLUI aims to empower, if the space is React+CSS. It can be true if the space is XMLUI.
</p>
<p>
To enhance the collaboration we provide <a href="https://github.com/xmlui-org/xmlui-mcp">an MCP server</a> that helps you direct agents’ attention as you work with them on XMLUI apps. In <a href="https://thenewstack.io/mcp-is-rss-for-ai-more-use-cases-for-model-context-protocol/">MCP is RSS for AI</a> I described the kinds of questions that agents like Claude and Cursor can use xmlui-mcp to ask and answer:
</p>
<p>
Is there a component that does [X]?
</p>
<p>
What do the docs for [X] say about topic [Y]?
</p>
<p>
How does the source code implement [X]?
</p>
<p>
How is [X] is used in other apps?
</p>
<p>
You place the xmlui-mcp server alongside the xmlui repo which includes docs and source code. And the repo in which you are developing an XMLUI app. And, ideally, other repos that contain reference apps like XMLUI Invoice.
</p>
  Working with LLMs 
<p>
This arrangement has mostly exceeded my expectations. As I build out a suite of apps that exemplify best practices and patterns, the agentic collaboration improves. This flywheel effect is, of course, still subject to the peculiar habits of LLM assistants who constantly need to be reminded of the rules.
</p>
 
<p>
1 don’t write any code without my permission, always preview proposed changes, discuss, and only proceed with approval.</p>
<p>
2 don’t add any xmlui styling, let the theme and layout engine do its job</p>
<p>
3 proceed in small increments, write the absolute minimum amount of xmlui markup necessary and no script if possible</p>
<p>
4 do not invent any xmlui syntax. only use constructs for which you can find examples in the docs and sample apps. cite your sources.</p>
<p>
5 never touch the dom. we only use xmlui abstractions inside the App realm, with help from vars and functions defined on the window variable in index.html</p>
<p>
6 keep complex functions and expressions out of xmlui, they can live in index.html or (if scoping requires) in code-behind</p>
<p>
7 use the xmlui mcp server to list and show component docs but also search xmlui source, docs, and examples</p>
<p>
8 always do the simplest thing possible
</p> 
<p>
It’s like working with 2-year-old savants. Crazy, but it can be effective!
</p>
<p>
To increase the odds that you’ll collaborate effectively, we added a <a href="https://docs.xmlui.org/howto">How To</a> section to the docs site. The MCP server makes these articles visible to agents by providing tools that list and search them. This was inspired by a friend who asked: “For a Select, suppose you don’t have a static default first item but you want to fetch data and choose the first item from data as the default selected, how’d you do that in xmlui?” It took me a few minutes to put together an example. Then I realized that’s the kind of question LLMs should be able to ask and answer autonomously. When an agent uses one of these tools it is anchored to ground truth: an article found this way has a citable URL that points to a working example.
</p>
<p>
It’s way easier for me to do things with XMLUI than with React and CSS, but I’ve also climbed a learning curve and absorbed a lot of tacit knowledge. Will the LLM-friendly documentation flatten the learning curve for newcomers and their AI assistants? I’m eager to find out.
</p>
 Content management 
<p>
We say XMLUI is for building apps, but what are apps really? Nowadays websites are often apps too, built on frameworks like Vercel’s <a href="https://en.wikipedia.org/wiki/Next.js">Next.js</a>. I’ve used publishing systems built that way and I am not a fan. You shouldn’t need a React-savvy front-end developer to help you make routine changes to your site. And with XMLUI you don’t. Our <a href="https://demo.xmlui.org">demo site</a>, <a href="https://docs.xmlui.org">docs site</a>, and <a href="https://xmlui.org">landing page</a> are all XMLUI apps that are much easier for me to write and maintain than the Next.js sites I’ve worked on.
</p>
<p>
“Eating the dogfood” is an ugly name for a beautiful idea: Builders should use and depend on the things they build. We do, but there’s more to the story of XMLUI as a CMS. When you build an app with XMLUI you are going to want to document it. There’s a nice synergy available: the app and its documentation can be made of <a href="https://demo.coressh.com/ui/help">the same stuff</a>. You can even showcase live demos of your app in your docs as we do in <a href="https://docs.xmlui.org/components/_overview">component documentation</a>, <a href="https://docs.xmlui.org/tutorial-01">tutorials</a>, and <a href="https://docs.xmlui.org/howto">How To</a> articles.
</p>
<p>
I was an early proponent of screencasts for software demos, and it can certainly be better to show than tell, but it’s infuriating to search for the way to do something and find only a video. Ideally you show and tell. Documenting software with a mix of code, narrative, and live interaction brings all the modalities together.
</p>
 Extensibility 
<p>
Out of the box, XMLUI wraps a bunch of React components. What happens when the one you need isn’t included? This isn’t my first rodeo. In a <a href="https://blog.jonudell.net/2023/06/14/radical-just-in-time-learning/">previous effort</a> I leaned heavily on LLMs to dig through layers of React code but was still unable to achieve the wrapping I was aiming for.
</p>
<p>
For XMLUI the component I most wanted to include was the <a href="https://tiptap.dev/">Tiptap</a> editor which is itself a wrapper around the foundational <a href="https://prosemirror.net/">ProseMirror</a> toolkit. Accomplishing that was a stretch goal that I honestly didn’t expect to achieve before release. But I was pleasantly surprised, and here is the proof.
</p>
<p></p>
<p>
This XMLUI TableEditor is the subject of our <a href="https://docs.xmlui.org/build-editor-component">guide</a> for developers who want to understand how to create an XMLUI component that wraps a React component. And isn’t just a toy example. When you use XMLUI for publishing, the foundation is <a href="http://docs.xmlui.org/working-with-markdown">Markdown</a> which is wonderful for writing and editing headings, paragraphs, lists, and code blocks, but awful for writing and editing tables. In that situation I always resort to a visual editor to produce Markdown table syntax. Now I have that visual editor as an XMLUI component that I can embed anywhere.
</p>
<p>
The React idioms that appear in that guide were produced by LLMs, not by me, and I can’t fully explain how they work, but I am now confident it will be straightforward for React-savvy developers to extend XMLUI. What’s more, I can now see the boundary between component builders and solution builders begin to blur. I am mainly a solution builder who has always depended on component builders to accomplish anything useful at that level. The fact that I was able to accomplish this useful thing myself feels significant.
</p>
 Deployment 
<p>
Here’s the minimal XMLUI deployment footprint for the TableEditor.
</p>
 TableEditor
├── Main.xmlui
├── index.html
└── xmlui
    └── 0.9.67.js
 
<p>
The index.html just sources the latest <a href="https://docs.xmlui.org/change-log"></a>standalone build of XMLUI.
</p>
 &lt;script src="xmlui/0.9.67.js"&gt;&lt;/script&gt;
 
<p>
Here’s Main.xmlui.
</p>
 &lt;App var.markdown=""&gt;
  &lt;Card&gt;
    &lt;TableEditor
      id="tableEditor"
      size="xs"
      onDidChange="{(e) =&gt; { markdown = e.markdown }}"
    /&gt;
  &lt;/Card&gt;
&lt;Card&gt;
  &lt;HStack&gt;
    &lt;Text variant="codefence" preserveLinebreaks="{true}"&gt;
      { markdown }
    &lt;/Text&gt;
    &lt;SpaceFiller /&gt;
    &lt;Button
      icon="copy"
      variant="ghost"
      size="xs"
      onClick="navigator.clipboard.writeText(markdown)"
    /&gt;
  &lt;/HStack&gt;
&lt;/Card&gt;
&lt;/App&gt;
 
<p>
You can use any static webserver to host the app. You can even run it <a href="http://xmlui-table-editor.s3-website-us-east-1.amazonaws.com/">from an AWS bucket</a>.
</p>
<p>
For XMLUI Invoice we provide a <a href="https://github.com/xmlui-org/xmlui-invoice">test server</a> that includes a localhost-only static server, embeds sqlite, and adds a CORS proxy for apps that need that support when talking to APIs (like Hubspot’s) that require CORS. You may need to wrap similar capabilities around your XMLUI apps but the minimal deployment is dead simple.
</p>
 Web development for the rest of us 
<p>
XMLUI was conceived by Gent Hito who founded <a href="https://www.nsoftware.com/">/n software</a> and <a href="https://cdata.com">CData</a>. The mission of /n software: make network communication easy for developers. For CData: make data access easy for developers. And now for XMLUI: make UI easy for developers.
</p>
<p>
“We are backend people,” Gent says. “All our components are invisible, and when we tried to build simple business UIs we were surprised to find how hard and frustrating that was.”</p>
<p>
Those of us who remember the Visual Basic era know it wasn’t always that way. But the web platform has never been friendly to solution builders who need to create user interfaces. That’s become a game for specialists who can wrap their heads around an ongoing explosion of complexity.
</p>
<p>
It shouldn’t be that way. Some apps do require special expertise. But many shouldn’t. If you are /n software, and you need to give your customers an interface to monitor and control the <a href="https://www.nsoftware.com/coresshserver">CoreSSH Server</a>, you shouldn’t need to hire React and CSS pros to make that happen. Your team should be able to do it for themselves and <a href="https://demo.coressh.com/">now they can</a>.
</p>
<p>
I’m having a blast creating interfaces that would otherwise be out of my reach. Will you have the same experience? Give it a try and <a href="mailto:team@xmlui.org">let us know</a> how it goes!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/jonudell.net/xmlui/xmlui2.gif?w=656&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   04:06 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://simonwillison.net'>Simon Willison</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/19/new-tags/#atom-everything'>New tags</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   A few months ago I added a tool to my blog for bulk-applying tags to old content. It works as an extension to my existing search interface, letting me run searches and then quickly apply a tag to relevant results. 
 Since adding this I've been much more aggressive in categorizing my older content, including adding new tags when I spot an interesting trend that warrants its own page. 
 Today I adde
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>A few months ago I <a href="https://github.com/simonw/simonwillisonblog/commit/12da4167396c2d54526bf690add14aebbb244148">added a tool</a> to my blog for bulk-applying tags to old content. It works as an extension to my existing search interface, letting me run searches and then quickly apply a tag to relevant results.</p>
<p>Since adding this I've been much more aggressive in categorizing my older content, including adding new tags when I spot an interesting trend that warrants its own page.</p>
<p>Today I added <a href="https://simonwillison.net/tags/system-prompts/">system-prompts</a> and applied it to 41 existing posts that talk about system prompts for LLM systems, including a bunch that directly quote system prompts that have been deliberately published or leaked.</p>
<p>Other tags I've added recently include <a href="https://simonwillison.net/tags/press-quotes/">press-quotes</a> for times I've been quoted in the press, <a href="https://simonwillison.net/tags/agent-definitions/">agent-definitions</a> for my ongoing collection of different ways people define "agents" and 
<a href="https://simonwillison.net/tags/paper-review/">paper-review</a> for posts where I review an academic paper.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/blogging">blogging</a>, <a href="https://simonwillison.net/tags/tagging">tagging</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   02:02 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://simonwillison.net/2025/Jul/19/steve-yegge/#atom-everything'>Quoting Steve Yegge</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    So one of my favorite things to do is give my coding agents more and more permissions and freedom, just to see how far I can push their productivity without going too far off the rails. It's a delicate balance. I haven't given them direct access to my bank account yet. But I did give one access to my Google Cloud production instances and systems. And it promptly wiped a production database passwo
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <blockquote cite="https://x.com/steve_yegge/status/1946360175339974807"><p>So one of my favorite things to do is give my coding agents more and more permissions and freedom, just to see how far I can push their productivity without going too far off the rails. It's a delicate balance. I haven't given them direct access to my bank account yet. But I did give one access to my Google Cloud production instances and systems. And it promptly wiped a production database password and locked my network. [...]</p>
<p>The thing is, autonomous coding agents are extremely powerful tools that can easily go down very wrong paths. Running them with permission checks disabled is dangerous and stupid, and you should only do it if you are willing to take dangerous and stupid risks with your code and/or production systems.</p></blockquote>
<p>— <a href="https://x.com/steve_yegge/status/1946360175339974807">Steve Yegge</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/steve-yegge">steve-yegge</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   00:54 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 18. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/19/i-have-just-extended-my.html'> I have just extended my ‘text-expander shortcuts’ to account</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   I have just extended my ‘text-expander shortcuts’ to account for the entire disaster that supports the Orange clown … 
 The Tяump Аппаратчик 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I have just extended my ‘text-expander shortcuts’ to account for the entire disaster that supports the Orange clown …</p>
<p>The Tяump Аппаратчик</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   23:11 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/19/is-everybody-in-new-zealand.html'> Is everybody in New Zealand happy to accept Luxon’s decision</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Is everybody in New Zealand happy to accept Luxon’s decision to lift the terrorist designation of The Proud Boys in New Zealand? Against the advice of the Kiwi police and his security advisors .. and out of step with Canada .. while fully in step with the Tяump apparatchik. 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Is everybody in New Zealand happy to accept Luxon’s decision to lift the terrorist designation of The Proud Boys in New Zealand? Against the advice of the Kiwi police and his security advisors .. and out of step with Canada .. while fully in step with the Tяump apparatchik.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   22:24 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2025/07/18/one-reason-i-love-indiana/'>One reason I love Indiana</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  My car’s dashboard has been telling me we have a slow leak in the right front tire. So I drove up to Tieman Tire here in Bloomington. It was busy, but they took me as a walk/drive-in, and then took an hour to remove the tire, find the leak in a tub of water (which […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p></p>
<p>My car’s dashboard has been telling me we have a slow leak in the right front tire. So I drove up to <a href="https://www.tiemantire.com">Tieman Tire</a> here in Bloomington. It was busy, but they took me as a walk/drive-in, and then took an hour to remove the tire, find the leak in a tub of water (which wasn’t easy, because the leak was too sphinctered to make bubbles: they had to feel in and on the tread all around the tire to locate the leak, which was from a tiny nail), remove and patch the tire, balance it, and torque it back onto the car… and then to make sure all four tires and the spare were all properly inflated. It was also fun to talk shop about cars and tires. One guy thought my car made more sense than one of those big ole SUVs.</p>
<p>Price: $20.</p>
<p>They now have me as a loyal customer.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/07/07AAB2FC-A681-4D90-8C36-781E3E806877_1_105_c.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:07 &bull;
   9 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2025/07/18/dame-time/'>Dame Time!</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I love that Damien Lillard is returning to the Portland Trailblazers. He and the town love each other, and the team is already on the ascent. It's a great move.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I love that <a href="https://www.nba.com/news/damian-lillard-to-return-to-trail-blazers">Damien Lillard is returning to the Portland Trailblazers</a>. He and the town love each other, and the team is already on the ascent. It's a great move.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:51 &bull;
   10 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2025/07/18/the-eagle-in-the-coal-mine/'>The Eagle in the Coal Mine</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Public broadcasting is the strongest form of broadcasting that’s still left. One reason is that it’s the only form of broadcasting for which its consumers are also its customers. Yes, not all those customers pay, but the market is there. If you donate to public radio or television stations, or to public radio programs and […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Public broadcasting is the strongest form of broadcasting that’s still left.</p>
<p>One reason is that it’s the only form of broadcasting for which its consumers are also its customers. Yes, not all those customers pay, but the market is there. If you donate to public radio or television stations, or to public radio programs and podcasts supported by subscriptions, you are paying for goods and services. You are customers in an open marketplace.</p>
<p>But broadcasting itself is an anachronism. For radio, listening is moving from radios to phones* pads, and smart speakers. For television, viewing is moving from antennas and cable to Internet streams. Even the PBS app on your streaming box requires that you first pay your public TV station. Here’s why: PBS wholesales its programs to stations, which in turn retail their programming to you. With apps, they can force you to pay. With free over-the-air broadcast, they can’t. Financially, free over-the-air broadcast isn’t a feature. It’s a bug: a giant financial loophole.)</p>
<p>It’s a matter of time before AM and even FM radio are gone from cars, because every station has a worldwide coverage footprint over the cellular data system (and Starlink: see the comment from Steve Stroh below), making stations’ over-the-air coverage obsolete.</p>
<p>Also, most stations are now just ways to route programs. Few medium and small market stations are still programmed locally, or still employ local talent other than in clerical and ad sales positions. Local and regional public radio stations still tend to be staffed, because they are in the business of programming more than distribution, and belong to their communities. But the primary listening devices today are apps on phones, not radios. Even in cars, the radio isn’t even in the back seat. It’s in the virtual trunk, with the spare tire. For example, try to find your way into the radio on the dashboard of the 2025 Audi Q5:</p>
No radio, no knobs, nothing to “tune” with.
<p>There are going to be some big victims. Rural public radio in Alaska for example, just got clobbered by the end of CPB funding, which was its major source of income. In many locales, local public radio is the only source of information, entertainment, and emergency help.</p>
<p>But listeners can still pay to keep the stations going. That’s why I wrote <a href="https://doc.searls.com/2025/07/16/if-you-like-public-broadcasting-be-customers-not-just-consumers/">If you like public broadcasting, be customers, not just consumers</a>. Read it again, if you haven’t already.</p>
<p>And then, if you really care, help develop <a href="https://cyber.harvard.edu/projectvrm/EmanciPay">EmaniPay</a>, which will make it much easier for consumers to become customers.</p>
<p>_____</p>
<p>*Even in cars, through CarPlay and Android Auto. In Teslas, the equivalent happens without a phone.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/07/eagle-in-coal-mine-683x1024.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:02 &bull;
   10 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://altmode.org'>Altmode</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://altmode.org/2025/07/18/adriatic-trip-day-4-venice-to-florence/'>Adriatic Trip, Day 4: Venice to Florence</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Friday, June 13, 2025 Today was the first of the intercity train journeys of the Italy tour. Our Tauck tour arranged for us to ride one of Italy’s high-speed trains from Venice to Florence. But getting to the train was much of the fun: first we boarded a water taxi to take us to our […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Friday, June 13, 2025</p>



<p>Today was the first of the intercity train journeys of the Italy tour. Our Tauck tour arranged for us to ride one of Italy’s high-speed trains from Venice to Florence. But getting to the train was much of the fun: first we boarded a water taxi to take us to our bus, then the bus to the train station.</p>



<p>The train station was quite busy. Alexa explained that train travel is very popular in Italy, partly due to the country’s size and shape. The train was clean, modern, and indeed fast, traveling at about 150 miles per hour. Our route featured a diverse range of scenery, from the coastal plain through farmland, followed by a mountainous stretch that included several tunnels.</p>



<p>On arrival in Florence, we boarded another small bus and rode to our hotel, the <a href="https://hotelbernini.duetorrihotels.com/">Hotel Bernini Palace</a>. We arrived in the late morning, so we began with a short introduction to art history in Florence from a local guide, followed by lunch. We then set out on a walking tour of Florence, which is a very compact and walkable city. Our tour took us to one of the highlights of Florence, the Galleria dell’Accademia, where we had the opportunity to see Michelangelo’s David, a true highlight of our tour. Even from the far end of the room in which it is displayed, David has an amazing presence and detail that shows him sizing up Goliath before taking him down.</p>



<p>In the evening, we walked around a bit more, hoping to visit the Basilica of Santa Croce, where Michelangelo and Galileo are buried. However, the area was closed off for the annual <a href="https://en.wikipedia.org/wiki/Calcio_storico_fiorentino">Calcio Storico Fiorentino</a> tournament. Our guide explained that this is a soccer- or rugby-like game with very few rules other than “no weapons”. The tournament is a competition between different Florence neighborhoods.</p>



 



<p>This article is part of a series about our recent trip to Italy, Greece, Albania, and Croatia. To see the introductory article in the series, click <a href="https://altmode.org/2025/07/15/adriatic-trip-day-1-to-venice/">here</a>.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:00 &bull;
   10 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2025/07/18/from-dates-to-tweets/'>From  Dates to Tweets</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  For the past few weeks I've been writing the blog mostly in Wordland, which is awesome. I'll still keep doing that (such as right now). But I'm hitting the pause button on combining a day's postings under title that's a date. I went with dates-as-headlines because it most closely resembled the way I wrote on […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>For the past few weeks I've been writing the blog mostly in <a href="https://wordland.social/">Wordland</a>, which is awesome. I'll still keep doing that (such as right now). But I'm hitting the pause button on combining a day's postings under title that's a date.</p>
<p>I went with dates-as-headlines because it most closely resembled the way I wrote on my original blog, which is <a href="http://weblog.searls.com/">archived here</a>. Note that most of the posts under each date were short, kind of like a tweet. Each also had a short snarky headline that worked as the punchline for the post. (A trick I learned form Esquire's Dubious Achievement Awards, which, alas, are all paywalled).&nbsp; For example, <a href="http://weblog.searls.com/2007/07/28#furtherProofOfLifeAfterBirth">Further proof of life after birth</a> was the headline of my last post before I turned 60.</p>
<p>I sorta replicated that approach here by putting a boldfaced one-liner at the start of each post under a date headline. That worked for readers (meaning it looked good), but a problem showed up when I looked back through posts in my WordPress dashboard: All I could see were dates. I couldn't see the leading lines (sub-headlines of a sort) of each post under the date headline, because those sub-headlines were buried in text. I needed clues in the form of posts' headlines.</p>
<p>So now I'm leaving the dateline up to WordPress and writing headlines anew for each short post.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   15:28 &bull;
   10 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://kyledenhartog.com/posts/'>Kyle Den Hartog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://kyledenhartog.com/decentralized-age-verification/'>Decentralizing Age Verification with SSI: Separating Content Moderation from Guardianship</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Age verification laws are coupling content moderation with guardianship problems, reducing user agency because of our centralized trust architectures. By separating these concerns and leveraging client-side filtering technologies, we can create more private and decentralized solutions for protecting children online without compromising everyone's Web experience.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Today, we see that age verification laws are being passed, which enshrine the principle that we can perform age checks and protect people under a certain age through content moderation. However, we ultimately adopted this centralized content moderation solution due to the inherent architecture of the centralized trust model established by the digital credentials we rely on. That centralization led us down a path to coupling a generic content moderation problem and a guardianship problem as a means to an end of protecting children. The byproduct is that we are reducing the agency of everyone on the Web because servers need to, by default, assume users are not old enough unless they can prove they are with a third-party attested claim. There is a more private and decentralized way to solve these problems if we separate the content moderation problem and guardianship problem with separate answers to each.</p>

<p>To solve the content moderation problem, we rely on the assumption that content can be classified into buckets of safe and unsafe content on a granular level. This assumption has to hold whether it is done in a centralized way with age verification credentials (so the server can filter out the content before sending it) or an alternative way, such as with client-side filtering relying on lists like what we have with SafeBrowsing and Adblock lists, which are more private and decentralized alternatives to preform content moderation.</p>

<p>Today, these lists work by classifying content based on the origin and filtering the request in the browser. This heuristic has been good enough for the most part. However, with SafeBrowsing V5, Google Chrome is introducing the use of on-device real-time classification to detect sites that impact users’ security in real-time. The same model could theoretically be repeated with any content or served in an HTML page by adding classification tags to the HTML. Alternatively, the server could tag it directly in HTML, and then lists or configurations inform the browser how it should filter the page locally before rendering the content. So that is what makes content moderation more private because it happens only on the device. Furthermore, it becomes a more generally applicable approach to content moderation, which may be useful for blocking any form of content on the Web. For example, I configure my Twitter account to block all tweets that mention Elon Musk, but theoretically, with this system, I could apply it across the Web. I could also subscribe to a list maintained by a third party I trust who blocks all content related to topics I wish to self-moderate from. However, this system has to be opted out of at least in order to provide the agency principle.</p>

<p>Now, I’m sure many of you are already thinking that the children will just opt out, but that is where the guardianship problem comes into play. Today, the most effective forms of enforcement of these content filtering systems occur within schools via IT administrators applying device management policies or network-level blocking. So, if we extended these capabilities to configure the generic content filtering at the operating system level, then the browser or other applications on the device rely upon those features to make sure the content filtering happens on the children’s devices and doesn’t get bypassed. Additionally, if the browser is not able to determine that the content is safe, it could be configured by default to block the site and allow bypass approvals from an authorized guardian, such as the school IT administrator, a teacher, or a parent. Alternatively, it could be configured to allow access to the content but log it so the school lists could be updated.</p>

<p>Furthermore, since parents could utilize these same theoretical operating system guardianship features (or provide consent for the IT admins at their school to configure it through BYOD policies) such that these devices can’t bypass the system it becomes a more technologically appropriate solution that allows parents, teachers, and IT admins to fluidly enforce as it aligns with their morals when raising their child. For example, guardians may choose to allow access to sensitive books or block sites relevant to topics they deem unsuitable for their children, but other guardians may be all right with it. In this way, we don’t end up with centralized institutions for content platforms, the governments issuing the credentials, and the regulators determining which content needs to be filtered.</p>

<p>In this way, we decentralize the enforcement out to millions of school districts or well-informed parents acting as guardians who should understand these problems and are well-versed in the cat-and-mouse game of content filtering bypasses. Furthermore, digital credentials still come into use here, too, but we subtly shift the trust triangle to make it work.</p>

<p>For example, let’s say that a teacher has a managed device and wants to reference a specific blocked page, or a child wants access to a specific chat feature in a game at home for a limited period of time. Then the operating system would be configured to recognize and trust the teacher’s or parent’s DID, which could issue a digital credential authorizing permission to access the content. In this way, the browser (or other applications) and operating system work in tandem to act as the verifier, not the centralized site server. Furthermore, because there’s only a limited number of guardians who could issue these credentials, the system doesn’t need to fall back to a small number of centralized, known issuers or content classifications that enforce their moral discretion onto large populations of people. Instead, people would be able to selectively either self-moderate or defer moderation rights to a guardian, which, as Ada Palmer points out in the blog post I linked previously, is the most effective method of moderation today.</p>

<p>To understand how this might appear, from a user experience perspective, the child would attempt to access a piece of content like normal, and it would be blocked. The child’s browser (the verifier) would then request that the operating system (the holder) provide a valid credential. If it has one, it presents it; otherwise, the operating system reaches out to the parent’s or teacher’s device to get issued a new credential. A notification would pop up on the guardian’s device, where a clear prompt would identify what the credential is for, how long it’s used for, and maybe even whether it should be logged by the operating system so it can be sent back to the guardian for later review. Side note, this might be a circumstance where phone home is a feature rather than a bug to help parents monitor the content their children are accessing.</p>

<p>In this way, by subtly shifting who plays what role, we’ve reused the technology for the same purposes, but in a more decentralized way because the issuance is not bound to only a small select number of institutions, but it is still scalable. Furthermore, the solution is more private for everyone on the Web because sites are not required to collect personal data. However, they do still have a responsibility to tag content using the content tags that are required by regulators. Additionally, the user can configure their content moderation themselves or defer it to third parties of their choosing, like we do with Adblock lists, depending on how granular the classification problem becomes. In this way, we achieve a more private and secure solution that remains scalable, allows individuals or guardians to self-moderate as is best aligned with their moral discretion, and this is achieved because we opt for a decentralized architecture both in terms of credential issuance and in the sense of content moderation lists, where users opt in.</p>

<p>In summary, this is just one example of how the choices we make for the trust architecture have a profound impact on the solutions we end up with. It acts as a blueprint too for how we can think about different approaches for other use cases that balance tradeoffs by using decentralized trust as a means to an end, not a liability to be avoided. I hope this helps exemplify more meaningfully, too, for how we can leverage these technologies in an alternative way that leads to more equitable outcomes for all and remain aligned with our principles.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   00:00 &bull;
   10 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://kyledenhartog.com/centralized-ssi/'>How SSI Becomes a Centralized Tool For Gatekeeping</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Self Sovereign Identity subtly introduces centralization through its chosen trust architecture. We've re-created a structure where institutional claims about subjects are more trusted which recreates the same gatekeeping mechanisms SSI was meant to eliminate.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Following up on my last blog post, <a href="https://kyledenhartog.com/a-pattern-of-moral-crisis/">From Printing Press to Digital Identity: A Pattern of Moral Crisis</a>, I allude to the idea that Self Sovereign Identity is centralized, but don’t go into great detail how. In this post, I’m going to follow up on this idea to show how we’ve subtly introduced the centralization through the chosen trust architecture. Then I’ll point out some of the ways in which this centralization could be weaponized against the people that we intended to provide more agency too.</p>

<p>To start, we first have to acknowledge that in a claims based model like with verifiable credentials or any other digital credential data model the technology itself is ambivalent to how it’s used. Put simply, all of the various data models are designed to encode “who says what about whom” into a digital form. So, the subtly in this is that the credentials themselves can’t really enforce centralization or decentralization, but rather how we choose to encode information into them are what provides this enforcement.</p>

<p>Let’s take for example the 3 most commonly suggested or deployed use cases of these digital credentials as of yet:</p>

 
   COVID Passes 
   Age Verification 
   “Know Your Customer” 
 

<p>In all 3 of these use cases we’ve defined a trust architecture where the structure of the data is: An institution with a well known identifier makes claims about a subject with an untrusted identifier such that an arbitrary verifier can rely upon said claims for any purpose they choose. This is most commonly referred to as a “High Assurance” credential. The alternative approach that’s been described is a self attested model, which is the most common approach on the Web to date.</p>

<p>In a self attested claims model (or could also be referred to as a “low assurance” credential) it works roughly like this. A subject uses an untrusted identifier as an issuer to make claims about themselves as the subject with the same untrusted identifier such that an arbitrary verifier can rely upon said claims. While the technology under the hood isn’t quite the same, the closest example we have to this today is the social login systems like Sign in With Facebook or Google. In each of these systems, at the very beginning when I register my account I self declare my information to Facebook or Google and that information could be correct or not. It doesn’t much matter what my name is or what my date of birth is because Facebook and Google aren’t making any assurances about the claims themselves. They’re simply collect it and relaying it onto third parties using Open ID Connect. In this trust architecture, the ability to self attest this information at the start allows me to falsify claims on registration. I may choose to do that to enhance my privacy while still being able to easily authenticate on the Web or it could be to impersonate another person.</p>

<p>So, what makes something high assurance or low assurance? It essentially boils down to what enforcement mechanisms exist for the verifier to have assurance that the claims are valid and how we’re opting to do this in “high assurance” credentials is to remove agency from the subject and bestow that into well known identifiers that represent institutions that are “trusted”. We assume this trust is valid because we assume the likelihood of falsified records is lower (not zero, it’s actually not <a href="https://www.bitsaboutmoney.com/archive/optimal-amount-of-fraud/">economically feasible</a> to achieve this) than that of the self-attested model. But at what cost? The cost comes in the form of the subject’s agency.</p>

<p>Under the new “high assurance” trust models we bestow the subject the ability to share claims made about them which is new, but in exchange for that capability we remove the ability to make claims about themselves. That power is now only granted to well known trusted entities. Does that remind you of any other PKI systems on the Web because it does to me?</p>

<p>If we think about the x509 system for TLS certificates they essentially work in the same way. Our browsers don’t trust a self-signed certificate by default but it does trust an intermediary certificate that’s been signed by a well known trusted root certificate. Now tell me, how many of you regularly allow self-signed certificates when visiting a website today? It probably happens once in a while, but its certainly not the prevalent trust model on the Web anymore. The issue here is that, as soon as we introduced the alternative mechanism for a hierarchical trust infrastructure rooted in a select number of institutions (the root certificate authorities) providing some assurance about the intermediary certificates we stopped using self signed certificates because they were deemed less safe even though they were more decentralized. Furthermore, that same x509 infrastructure which operates on a decentralized trust model has been shown that it can be scaled with PGP. Sure, it’s by no means an enjoyable tool to use, but that’s more a factor of the tooling being built-in 1991 where we hadn’t done much research on human-computer interaction patterns. It therefore, shouldn’t be used as an invalidation of the safety of the trust model itself.</p>

<p>So in my original blog post, I suggest that’s exactly what will happen with digital credentials too and the evidence with the most prevalent use cases suggests that’s already what is happening. In each of these systems, the issuer maps to the root certificate authority, the subject maps to the intermediary certificate, and the verifier is the one setting the rules for the game which effectively means that we’ve recreated a hierarchical trust model. This isn’t a damnation of the technology itself, just as Web certificates isn’t a damnation of x509 as shown by PGP.</p>

<p>So now that we understand how the issuance side has centralized let’s take a look at what this will mean in practice on the Web and in real life. Rather than re-iterate the points we made about the digital credentials API at Brave, I’ll just point back to our <a href="https://lists.w3.org/Archives/Public/public-review-comments/2024Sep/0017.html">formal objection</a> of it because they’re exactly the same as what I’d say here.</p>

<p>To exemplify on this a bit further though let’s take a look at the age verification use case. In this use case, what we’re seeing is that users on the Web are having their agency removed as a byproduct of these problems being solved with a centralized, hierarchical trust model. In advertantly we achieve greater “compliance” from the “higher” assurance (most everyone can think of a few ways that this will be bypassed) and in exchange we lose some agency and privacy because we want better content moderation capabilities for children, a protected class of people. The issue within this specific use case is that that chosen trust architecture then becomes a weapon against speech inadvertantly. Either due to people choosing to self-censor because they don’t want to provide age assurance credentials to websites or because they don’t have a credential such as children not having one or because the issuer revoked it from them. That revocation may occur simply because of the speech they make or it may occur because of a more benign reason such as the person forgot to pay for insurance and had their driver’s license revoked, so the site errors on the side of caution and doesn’t accept it due to strict liability falling on the site.</p>

<p>It’s easy to argue that I’m contriving these sorts of examples in a game of what-ifs, but let’s look at what’s most recently happened in the case of “financial compliance” with KYC and debanking. Within the past decade or so, there’s been a growing trend of debanking people based on how the funds are generated. In the further out example, we saw this happen with <a href="https://en.wikipedia.org/wiki/Operation_Choke_Point">operation chokepoint</a> where the US government leveraged a capability they already had (financial compliance afforded through KYC) and repurposed it to limit the capaibilities of people they deemed to be participating in “high risk” activities.</p>

<p>In this previous financial compliance system, the new age assurance mechanisms, and in any other use case that relies upon deferred instituitional trust through high assurance credentials we should expect to see that the technology will also be repurposed for alternative means than what they were designed for. In some cases, people will see this as a feature rather than a bug to protect others, but it remains an unintended consequence by design of the system. This happens because the verifier bestows a new set of hard power in the issuer (trust) by removing hard power from the subject (agency and in some cases privacy) which will eventually be repurposed when the next moral crisis occurs as <a href="https://www.exurbe.com/tools-for-thinking-about-censorship/">history suggests</a>. None of this is because of the technical design of the technologies though, it’s simply because this is how we’ve chosen to use them and that’s what makes this such a subtle inversion of power that goes against the original goals many of us have been working towards.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   00:00 &bull;
   10 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 17. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-140'>The Pulse #140: Does Cursor make developers less effective?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Also: the complicated story of the Windsurf sale, beginning of the end for VC-subsidized tokens, and more.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering events, insights, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Today, we cover:</p>  <p>Cursor makes developers less effective? A study into the workflows of experienced developers found that devs who use Cursor for bugfixes are around 19% slower than devs who use no AI tools at a…</p>  
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-140">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:29 &bull;
   11 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/senate-votes-to-strip-1-1-billion-from-public-broadcasting-in-blow-to-pbs-and-npr/'>Senate Votes to Strip $1.1 Billion From Public Broadcasting in Blow to PBS and NPR</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Without public media funding, local stations will close, creating news deserts and allowing political corruption to thrive.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>[<a href="https://variety.com/2025/politics/news/senate-strips-billion-public-broadcasting-1236462722/?ref=werd.io">Variety</a>]</p><p>Removing over a billion dollars in funding for the Corporation for Public Broadcasting, as the Senate has voted to do, will have the following effects:</p>  Local public news stations, which disproportionately receive money from the CPB, will shutter.  More news deserts will be created.  PBS will be very hard-hit.  Political corruption will thrive.  <p>The idea that public media is a mouthpiece for “partisan left-wing activism” is ridiculous. It is an independent voice, which may be out of step with the politics of the current administration. But it broadcasts conservative voices and aims to highlight all major sides of the issues it covers.</p><p>In addition to news, one of the important things public media does is provide programming that might not be viable on commercial stations. Sesame Street is the canonical example, but every state has programs that highlight local businesses and attractions, covers local culture, and reflects the pulse of regions around the country.</p><p>It’s a huge loss. Hopefully there are other ways to make up the shortfall, but patronage from private individuals and foundations isn’t the same thing. These services deserve sustained public funding. This is yet another wound inflicted upon the country that will be felt for years to come.</p><p>[<a href="https://variety.com/2025/politics/news/senate-strips-billion-public-broadcasting-1236462722/?ref=werd.io">Link</a>]</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   13:13 &bull;
   11 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blogs.harvard.edu/doc/'>Doc Searls Weblog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://doc.searls.com/2025/07/16/thursday-17-july-2025/'>Thursday, 17 July 2025</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  An incomplete waste of time.&nbsp;New colors without shooting lasers into your eyes. No shit. Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models is a scientific paper by four authors from Princeton and two from UC Berkeley. A pull-quote: "While previous work has explored large language model (LLM) hallucination and sycophancy, we […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>An incomplete waste of time.&nbsp;<a href="https://dynomight.substack.com/p/colors">New colors without shooting lasers into your eyes</a>.</p>
<p>No shit. <a href="https://arxiv.org/pdf/2507.07484">Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models</a> is a scientific paper by four authors from Princeton and two from UC Berkeley. A pull-quote: "While previous work has explored large language model (LLM) hallucination and sycophancy, we propose machine bullshit as an overarching conceptual framework that can allow researchers to characterize the broader phenomenon of emergent loss of truthfulness in LLMs and shed light on its underlying mechanisms. We introduce the Bullshit Index, a novel metric quantifying LLMs’ indifference to truth, and propose a complementary taxonomy analyzing four qualitative forms of bullshit: empty rhetoric, paltering, weasel words, and unverified claims."</p>
<p>We're sorry now. One of my most vivid memories is walking from the <a href="https://www.mantoloking.org/">Mantoloking Beach</a> snack stand to our family's red umbrella with a white stripe around it. I wove my way through a forest of other umbrellas and blankets (it was a crowded beach), while a Nutty Buddy and an ice cream sandwich melted in my hands and "<a href="https://en.wikipedia.org/wiki/Lipstick_on_Your_Collar_(song)">Lipstick on your collar</a>" played from more than a dozen transistor radios along my path. The singer was <a href="https://en.wikipedia.org/wiki/Connie_Francis">Connie Francis</a>, and the radio station was my fave* at the time:&nbsp;<a href="https://www.musicradio77.com/wmca/earlyhistory.html">WMCA/570</a>&nbsp;from New York. <a href="https://lavocedinewyork.com/en/people/2025/07/17/legendary-italian-american-singer-connie-francis-dead-at-87/">Connie died yesterday</a>&nbsp;at 87. She was a powerful singer with a brilliant mind who led a long, interesting, and often troubled life—one totally worth reading about. Glad she lived a long one. *Top 40 competitors in that age were WINS/1010 and WMGM/1050. Though both were 10x WMCA's power, their signals were aimed away from the Jersey Shore while WMCA's wasn't. Later, WABC/770 would blow all three away.</p>
<p>It's Pop's birthday. Were he alive, he'd be 117 years old. <a href="https://flickr.com/photos/docsearls/albums/72157606112277250">Here is the collection of photos from his life (starting with ancestors) that I posted on his 100th birthday</a>.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   05:12 &bull;
   11 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2025/07/7320/'>MyDataJapan2025開会宣言</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  皆さん、おはようございます。 MyData Japan 2025にご参加いただき、心から感謝申し上げます。 今日ここに集まってくださったのは、企業の方、行政関係者、技術者、研究者、そして市民の皆さま──立場も専門性も異なる方々ばかりです。しかし私たちは今、「データとのよりよい関係…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<a>MyData-Japan-2025<br></a>
<p></p>



<p>皆さん、おはようございます。</p>



<p>MyData Japan 2025にご参加いただき、心から感謝申し上げます。</p>



<p>今日ここに集まってくださったのは、企業の方、行政関係者、技術者、研究者、そして市民の皆さま──立場も専門性も異なる方々ばかりです。しかし私たちは今、「データとのよりよい関係を、社会全体で築く」という、共通の目的のもとに集まりました。</p>



<p>MyDataの取り組みは、「理想」から「制度」へ、そして今、「実践」へと進もうとしています。本日は、その節目となる特別な日です。</p>



<p>MyDataは、一言で言えば──<br>「わたしの情報について、わたしが決められるようにする」こと。</p>



<p>これは、単なる個人情報保護の話ではありません。医療、教育、行政、日常生活のあらゆる場面に関わる、根源的な人間の尊厳と選択の問題です。</p>



<p>私たちは、次のような社会をめざしています：</p>



 
 情報の主導権が、組織から個人へと戻る社会。 



 「使われる」から「活かす」への転換。 



 同意・訂正・削除・再利用といった「ワンクリック権利」が日常的に行使できる社会。 
 



<p>MyDataの原則は、このビジョンを実現するための羅針盤です。そして今日ここに集まった皆さんは、この変革の担い手です</p>



<p>私たちは、自分に関する情報が、どのように使われているか、どこまで気づいているでしょうか？</p>



<p>スマホを開いた瞬間、交通系ICカードを使った瞬間、SNSで「いいね」を押した瞬間──<br>そのたびに、“わたしのデータ”は、どこかに蓄積され、分析され、誰かの意思決定に使われているかもしれません。</p>



<p>「“わたしらしさ”は、誰が決めるのか？」<br>「“選んだつもり”の同意は、本当に納得だったのか？」</p>



<p>この問いは、私たちに「自分の情報との向き合い方を見直そう」と促します。</p>



<p>MyDataは、「情報をどう守るか」だけでなく、「情報をどう活かすか」そして、「情報時代にどう生きるか」を問う、新しい社会の設計思想です。</p>



<p>本年のテーマは「MyData in Practice」──<br>「理念」を「設計」に、「設計」を「実装」に進めていく挑戦です。</p>



<p>2024年のテーマ「MyData by Design」では、BLTS──ビジネス、法律、技術、社会──それぞれの観点からの設計を深掘りしました。今年はそれを一歩前に進め、「現実世界でどう機能させるか」に挑戦します</p>



<p>本カンファレンスでは、</p>



 
 DID/VCによる分散型IDの実装 



 カメラ画像の利活用におけるガバナンスの設計 



 子どもを取り巻く情報環境とその保護 
 



<p>など、実際のプロジェクトを通じて「実践知」が共有されます。</p>



<p>多様なプレイヤーによる対話と連携によって、現場からの知見が集まり、新たな共通基盤となっていく。それこそが、MyData Japanの真価です。</p>



<p>最後に皆さんに問いかけたいと思います。</p>



 
 あなたの組織では、個人が情報を活かせる仕組みが整っていますか？ 



 あなたの手がける技術は、人の尊厳と自由を支えていますか？ 



 あなた自身は、自分の情報について語れていますか？ 
 



<p>MyDataは、誰かがやってくれるものではありません。<br>MyDataは、あなたの実践が形づくる社会そのものです。</p>



<p>今日この場での出会いや学びが、ひとつでも明日からの行動に繋がっていくことを願っています。そうした行動の積み重ねが、5年後、10年後の「わたしらしい社会」の礎になります。</p>



<p>それでは──<br>MyData Japan 2025、開会いたします。<br>ありがとうございました。</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   01:43 &bull;
   11 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://kyledenhartog.com/posts/'>Kyle Den Hartog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://kyledenhartog.com/a-pattern-of-moral-crisis/'>From Printing Press to Digital Identity: A Pattern of Moral Crisis</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This isn't a new problem in decentralized identity - it's been ongoing for centuries, dating back to the printing press. We keep inverting solutions and repeating historical patterns.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I was recently reading the mailing list of the W3C Credentials Community Group and decided to write up a response as a blog post to <a href="https://lists.w3.org/Archives/Public/public-credentials/2025Jul/0086.html">Manu’s latest email</a>. The conversation is still evolving, but the piece I wanted to add to this discussion is that this isn’t a new problem. I largely agree with Christopher Allen’s framing in <a href="https://www.blockchaincommons.com/musings/gdc25/">Musings of a Trust Architect: When Technical Standards Meet Geopolitical Reality</a>. However, from my perspective we need to look back to history to understand our uneasiness. Even if we remain hopeful that the right thing™ will eventually come.</p>

<p>I don’t know that I necessarily agree with Manu that this is a temporary solution and a long term solution will emerge that’s better. This isn’t a new problem, it’s been ongoing for centuries dating back even to the creation of the printing press.</p>

<p>To tie this back to recent history we can look at <a href="https://openid.net/specs/openid-connect-core-1_0.html#SelfIssued">SIOP</a> (and the attempt to revive it with <a href="https://openid.net/specs/openid-connect-self-issued-v2-1_0.html">SIOPv2</a>) and the lack of adoption that came with it. If I remember the historical lore correctly too, that’s what kicked off OIDC in the first place. So it seems history is rhyming once again, but somehow we keep inverting things.</p>

<p>I guess our one bastion of hope remains in users rejecting these systems outright and migrating with their feet away. There’s already signals via a <a href="https://petition.parliament.uk/petitions/722903">petition from citizens</a> in the UK who are the first to encounter these technologies. There’s evidence of similar resistance when COVID passes were created too which was the first large scale use of this technology.</p>

<p>Even if that petition is successful, (which I doubt it does with less than 1% of the population signing the petition) we’ll be back to debating the same root moral dilemmas of the next moral crisis with a new technology eventually. I’d like to hope that I’m missing something, but this appears to be just the latest moral crisis where we in the tech industry need to ask ourselves: <a href="https://www.youtube.com/watch?v=h242eDB84zY">are we the baddies?</a>. It seems we’re effectively representing the private partners of the state to develop the next generation of censorship tools like we’ve been doing for <a href="https://www.exurbe.com/tools-for-thinking-about-censorship/">centuries</a> again.</p>

<p>In saying all that, I know everyone that’s worked on these technologies over the years mean well and genuinely wants to improve things. That improvement is inherently subjective though and reasonable minds will interpret this differently. So as much as that last statement could be construed as a personal offense to those who’ve helped build these technologies (including myself) I don’t think of this technology as a violation of the principles. Rather, it’s how we choose to use them that reflects our principles.</p>

<p>In fact, I know nearly all of us still do believe in the principles of agency, privacy, and the various other 10 SSI principles. Rather, I think it’s just a case of human interpretation and the struggle of getting a large group of humans to agree when we’re all working on related but different problems. I am left with strong hope though. The world didn’t fall over and end on any of the previous iterations of tools to censor. It however has led to a little less expression of the humanity along the way. I suppose it depends on the problem each of us are trying to solve (such as content moderation, convenience in the digital world, enhancing digital trust, reducing surveillance capitalism, building a business and finding product-market fit, etc) and how we choose to interpret the principles.</p>

<p>I know none of us want these systems to be abused for the purposes of identifying and harming human rights globally. That was the whole point of making them decentralized in the first place was to prevent the efficiency of abuse when the failure cases inevitably occur. So, I just hope that we’re able to have the collective foresight to prevent this technology from further derailing now that it is centralized when the next political factions gain control of the identity systems and use them in ways we didn’t intend.</p>

<p>So to summarize I like to think we’ll notice it and balance these tradeoffs appropriately with this new system, but I’m not convinced with our track record over the past 9 or so years. Furthermore, I’d like to think we’ll balance these tradeoffs better the next time, but history suggests that’s the exception not the norm so we’ll just have to keep iterating. I’m even of the belief the only reason we keep on reaching these same outcomes is because we keep framing the moral crisis wrong each time too, but in each of those problems I have no real solution to offer. I’m just left pondering on why it keeps happening and I think that’s what keeps causing our collective unease.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   00:00 &bull;
   11 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 16. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/amazon-google-and-vibe-coding-with'>Amazon, Google and Vibe Coding with Steve Yegge</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Steve Yegge shares why Google struggles with platforms, how AI coding is deceptively hard, and why the "AI Fixer" role could be one spreading inside tech companies, in the future.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Stream the Latest Episode <p>Listen and watch now on <a href="https://youtu.be/TZE33qMYwsc">YouTube</a>,  <a href="https://open.spotify.com/episode/58fYN0snOQS5otyG1fjl27">Spotify</a> and <a href="https://podcasts.apple.com/us/podcast/amazon-google-and-vibe-coding-with-steve-yegge/id1769051199?i=1000717556804">Apple</a>. See the episode transcript at the top of this page, and timestamps for the episode at the bottom.</p> Brought to You By   <p><a href="https://workos.com/">WorkOS</a> — The modern identity platform for B2B SaaS.</p>  <p><a href="http://statsig.com/pragmatic">Statsig</a> <a href="https://modal.com/pragmatic">⁠</a> — <a href="https://modal.com/pragmatic">⁠</a> The unified platform for flags, analytics, experiments, and more.</p>  <p><a href="http://sonarsource.com/pragmaticsecurity">Sonar</a> — Code quality and code security for ALL code.</p>  <p>—</p> In This Episode <p><a href="https://www.linkedin.com/in/steveyegge/">Steve Yegge</a> is known for his writing and “rants”, including the famous “Google Platforms Rant” and the evergreen “Get that job at Google” post. He spent 7 years at Amazon and 13 at Google, as well as some time at Grab before briefly retiring from tech. Now out of retirement, he’s building AI developer tools at Sourcegraph—drawn back by the excitement of working with LLMs. He’s currently writing the book Vibe Coding: Building Production-Grade Software With GenAI, Chat, Agents, and Beyond.</p><p>In this episode of The Pragmatic Engineer, I sat down with Steve in Seattle to talk about why Google consistently failed at building platforms, why AI coding feels easy but is hard to master, and why a new role, the AI Fixer, is emerging. We also dig into why he’s so energized by today’s AI tools, and how they’re changing the way software gets built.</p><p>We also discuss:</p>  <p>The “interview anti-loop” at Google and the problems with interviews</p>  <p>An inside look at how Amazon operated in the early days before microservices</p>  <p>What Steve liked about working at Grab</p>  <p>Reflecting on the Google platforms rant and why Steve thinks Google is still terrible at building platforms</p>  <p>Why Steve came out of retirement</p>  <p>The emerging role of the “AI Fixer” in engineering teams</p>  <p>How AI-assisted coding is deceptively simple, but extremely difficult to steer</p>  <p>Steve’s advice for using AI coding tools and overcoming common challenges</p>  <p>Predictions about the future of developer productivity</p>  <p>A case for AI creating a real meritocracy</p>  <p>And much more!</p>       Interesting quotes from the episode <p>Two interesting thoughts from this conversation with Steve:</p> Google would not have hired 60% of its own people <blockquote><p>Steve (<a href="https://youtu.be/TZE33qMYwsc?si=3aLmqMOuzWRTPjiL&amp;t=674">11:13</a>): “When I was at Google, I was on their hiring committee, which is a “double blind”. They don't see the candidates, they don't know the interviewers. They're just reading feedback packets, and the interviewers don't bias each other. </p><p>One day they did an experiment with us, because we were the ones that ultimately decided to make that [hiring] decision about, the thumbs up, thumbs down type thing.</p><p>Google has a separate committee that looks at all the feedback. The recruiters did an exercise with us where they presented a bunch of packets. They just said, these were just a bunch of candidates, we're going to go and do the process on them. </p><p>Went through and we evaluated 'em all and decided we were going to not hire 60% of them. You figured this one out yet?”</p><p>Gergely: “No, not yet.”</p><p>Steve: “We were reviewing our own packets, so we voted not to hire 60% of ourselves. And it was a very sobering realization! The next week or two was the best time to apply to Google. We were just like ‘come on through’. I mean, it was nuts!”</p></blockquote> Working with AI tools is hard, and great developers will thrive  <blockquote><p>Gergely (<a href="https://youtu.be/TZE33qMYwsc?si=DWNOdvcVsEu2g7XR&amp;t=4028">01:07:10</a>): “These AI agents can write a lot of code. And I'm wondering — is it good code? Is it the code that you actually want?</p><p>Steve: Nobody's born knowing how to do it. It's completely new to humanity to have this sort of human but non-human, distinctly different helpers. And the best advice that I can possibly give you is to give them the tiniest task, the most molecularly tiny segmented task you can give them. And if you can find a way to make it smaller, do that at a time. Keep it real careful, track with them on what they're working on at all times, and then own every line of code that they'll ultimately commit.</p><p>You cannot trust anything. And that means multiple safeguards and guardrails and sentries and security and practices. And you have to train yourself to say the right things, do the right things, and look for the right things. And it is not easy.</p><p>It has reinforced my belief that people who are really good developers are going to thrive in this new world because it takes all of your skills to keep these things on the rails.</p></blockquote> The Pragmatic Engineer deepdives relevant for this episode   <p><a href="https://newsletter.pragmaticengineer.com/p/steve-yegge">The full circle of developer productivity with Steve Yegge</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/amazon">Inside Amazon’s engineering culture</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/vibe-coding-as-a-software-engineer">Vibe coding as a software engineer</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/ai-engineering-in-the-real-world">AI engineering in the real world</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/the-ai-engineering-stack">The AI Engineering stack</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/inside-sourcegraphs-engineering-culture">Inside Sourcegraph’s engineering culture</a></p>   Timestamps <p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc">00:00</a>) Intro</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=295s">04:55</a>) An explanation of the interview anti-loop at Google and the shortcomings of interviews</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=464s">07:44</a>) Work trials and why entry-level jobs aren’t posted for big tech companies</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=590s">09:50</a>) An overview of the difficult process of landing a job as a software engineer</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=948s">15:48</a>) Steve’s thoughts on Grab and why he loved it</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=1222s">20:22</a>) Insights from the Google platforms rant that was picked up by TechCrunch</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=1664s">27:44</a>) The impact of the Google platforms rant</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=1780s">29:40</a>) What Steve discovered about print ads not working for Google</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=1908s">31:48</a>) What went wrong with Google+ and Wave</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=2104s">35:04</a>) How Amazon has changed and what Google is doing wrong</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=2570s">42:50</a>) Why Steve came out of retirement</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=2716s">45:16</a>) Insights from “the death of the junior developer” and the impact of AI</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=3200s">53:20</a>) The new role Steve predicts will emerge</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=3292s">54:52</a>) Changing business cycles</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=3368s">56:08</a>) Steve’s new book about vibe coding and Gergely’s experience</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=3564s">59:24</a>) Reasons people struggle with AI tools</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=3756s">1:02:36</a>) What will developer productivity look like in the future</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=3910s">1:05:10</a>) The cost of using coding agents</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=4028s">1:07:08</a>) Steve’s advice for vibe coding</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=4182s">1:09:42</a>) How Steve used AI tools to work on his game Wyvern</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=4500s">1:15:00</a>) Why Steve thinks there will actually be more jobs for developers</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=4709s">1:18:29</a>) A comparison between game engines and AI tools</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=4873s">1:21:13</a>) Why you need to learn AI now</p><p>(<a href="https://www.youtube.com/watch?v=TZE33qMYwsc&amp;t=5408s">1:30:08</a>) Rapid fire round</p> References <p>Where to find Steve Yegge:</p><p>• X: <a href="https://x.com/steve_yegge">https://x.com/steve_yegge</a></p><p>• LinkedIn: <a href="https://www.linkedin.com/in/steveyegge/">https://www.linkedin.com/in/steveyegge/</a></p><p>• Website: <a href="https://steve-yegge.blogspot.com/">https://steve-yegge.blogspot.com/</a></p><p>• His upcoming book, Vibe Coding <a href="https://itrevolution.com/product/vibe-coding-book/">https://itrevolution.com/product/vibe-coding-book/</a></p><p>Mentions during the episode:</p><p>• Get that job at Google: <a href="https://steve-yegge.blogspot.com/2008/03/get-that-job-at-google.html">https://steve-yegge.blogspot.com/2008/03/get-that-job-at-google.html</a></p><p>• Get that job at Grab: <a href="https://steve-yegge.medium.com/get-that-job-at-grab-eea6de1d8421">https://steve-yegge.medium.com/get-that-job-at-grab-eea6de1d8421</a></p><p>• <a href="https://gist.github.com/chitchcock/1281611">Stevey’s Google Platforms Rant</a> </p><p>• Anthropic: <a href="https://www.anthropic.com/">https://www.anthropic.com/</a></p><p>• Coda: <a href="https://coda.io/">https://coda.io/</a></p><p>• OpenAI: <a href="https://openai.com/">https://openai.com/</a></p><p>• Linear: <a href="https://linear.app/">https://linear.app/</a></p><p>• Groupon: <a href="https://www.groupon.com/">https://www.groupon.com/</a></p><p>• Google Wave: <a href="https://en.wikipedia.org/wiki/Google_Wave">https://en.wikipedia.org/wiki/Google_Wave</a></p><p>• Slack: <a href="https://slack.com/">https://slack.com/</a></p><p>• Digg: <a href="https://en.wikipedia.org/wiki/Digg">https://en.wikipedia.org/wiki/Digg</a></p><p>• React Native: <a href="https://reactnative.dev/">https://reactnative.dev/</a></p><p>• Skype: <a href="https://en.wikipedia.org/wiki/Skype">https://en.wikipedia.org/wiki/Skype</a></p><p>• Azure: <a href="https://azure.microsoft.com">https://azure.microsoft.com</a></p><p>• Borg: <a href="https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/">https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/</a></p><p>• The death of the junior developer: <a href="https://sourcegraph.com/blog/the-death-of-the-junior-developer">https://sourcegraph.com/blog/the-death-of-the-junior-developer</a></p><p>• Collapsing the Talent Stack, Persona-Led Growth &amp; Designing Organizations for the Future: </p> <a href="https://www.implications.com/p/insights-on-collapsing-the-talent?utm_source=substack&amp;utm_campaign=post_embed&amp;utm_medium=web"> Implications, by Scott Belsky   Collapsing the Talent Stack, Persona-Led Growth &amp; Designing Organizations for the Future   Edition #8 of Implications…  Read more  2 years ago · 50 likes · 2 comments · Scott Belsky </a> <p>• Sourcegraph: <a href="https://sourcegraph.com/">https://sourcegraph.com/</a></p><p>• Building Windsurf with Varun Mohan: <a href="https://newsletter.pragmaticengineer.com/p/building-windsurf-with-varun-mohan">https://newsletter.pragmaticengineer.com/p/building-windsurf-with-varun-mohan</a></p><p>• Vibe Coding: Building Production-Grade Software With GenAI, Chat, Agents, and Beyond: <a href="https://itrevolution.com/product/vibe-coding-book/">https://itrevolution.com/product/vibe-coding-book/</a></p><p>• Claude Code: <a href="https://www.anthropic.com/claude-code">https://www.anthropic.com/claude-code</a></p><p>• Andrej Karpathy’s post about vibe coding: <a href="https://x.com/karpathy/status/1886192184808149383?lang=en">https://x.com/karpathy/status/1886192184808149383</a></p><p>• TDD, AI agents and coding with Kent Beck: <a href="https://newsletter.pragmaticengineer.com/p/tdd-ai-agents-and-coding-with-kent">https://newsletter.pragmaticengineer.com/p/tdd-ai-agents-and-coding-with-kent</a></p><p>• Perplexity: <a href="https://www.perplexity.ai/">https://www.perplexity.ai/</a></p><p>• CodeX: <a href="https://openai.com/index/introducing-codex/">https://openai.com/index/introducing-codex/</a></p><p>• Cline: <a href="https://cline.bot/">https://cline.bot/</a></p><p>• Cursor: <a href="https://cursor.com/">https://cursor.com/</a></p><p>• AI tools for software engineers, but without the hype – with Simon Willison (co-creator of Django): <a href="https://newsletter.pragmaticengineer.com/p/ai-tools-for-software-engineers-simon-willison">https://newsletter.pragmaticengineer.com/p/ai-tools-for-software-engineers-simon-willison</a></p><p>• Jason Clinton on LinkedIn: <a href="https://www.linkedin.com/in/jason-d-clinton/">https://www.linkedin.com/in/jason-d-clinton/</a></p><p>• Wyvern on Steam: <a href="https://store.steampowered.com/app/1541710/Wyvern/">https://store.steampowered.com/app/1541710/Wyvern/</a></p><p>• Brendan Hopper on LinkedIn: <a href="https://www.linkedin.com/in/bhop/">https://www.linkedin.com/in/bhop/</a></p><p>• Doom on Steam: <a href="https://store.steampowered.com/app/379720/DOOM/">https://store.steampowered.com/app/379720/DOOM/</a></p><p>• Wolfenstein on Steam: <a href="https://store.steampowered.com/franchise/Wolfenstein">https://store.steampowered.com/franchise/Wolfenstein</a></p><p>• Quake on Steam: <a href="https://store.steampowered.com/app/2310/Quake/">https://store.steampowered.com/app/2310/Quake/</a></p><p>• Unity: <a href="https://unity.com/">https://unity.com/</a></p><p>• Unreal: <a href="https://www.unrealengine.com">https://www.unrealengine.com</a></p><p>• Tommy Wiseau’s advice on X: <a href="https://x.com/tommywiseau/status/940455567830556672">https://x.com/tommywiseau/status/940455567830556672</a></p><p>• Dario Amodei on X: <a href="https://x.com/darioamodei">https://x.com/darioamodei</a></p><p>• Moore’s law: <a href="https://en.wikipedia.org/wiki/Moore%27s_law">https://en.wikipedia.org/wiki/Moore%27s_law</a></p><p>• From Software Engineer to AI Engineer – with Janvi Kalra: <a href="https://newsletter.pragmaticengineer.com/p/from-software-engineer-to-ai-engineer">https://newsletter.pragmaticengineer.com/p/from-software-engineer-to-ai-engineer</a></p><p>• Gene Kim’s website: <a href="http://www.realgenekim.me/">http://www.realgenekim.me/</a></p><p>• Craft Docs: <a href="https://www.craft.do/">https://www.craft.do/</a></p><p>• TypeScript: <a href="https://www.typescriptlang.org/">https://www.typescriptlang.org/</a></p><p>• Sourcegraph Amp: <a href="https://sourcegraph.com/amp">https://sourcegraph.com/amp</a></p><p>• Operator: <a href="https://openai.com/index/introducing-operator/">https://openai.com/index/introducing-operator/</a></p><p>• Sapiens: A Brief History of Humankind: <a href="https://www.amazon.com/Sapiens-Humankind-Yuval-Noah-Harari/dp/0062316095">https://www.amazon.com/Sapiens-Humankind-Yuval-Noah-Harari/dp/0062316095</a></p><p>• The Full Circle of Developer Productivity with Steve Yegge: <a href="https://steve-yegge.medium.com/get-that-job-at-grab-eea6de1d8421">https://steve-yegge.medium.com/get-that-job-at-grab-eea6de1d8421</a></p><p>• Inside Amazon's Engineering Culture: <a href="https://newsletter.pragmaticengineer.com/p/amazon">https://newsletter.pragmaticengineer.com/p/amazon</a></p><p>• Deepdives: <a href="https://newsletter.pragmaticengineer.com/s/deepdives">https://newsletter.pragmaticengineer.com/s/deepdives</a></p><p>—</p><p>Production and marketing by <a href="https://penname.co/">Pen Name</a>. </p><p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!FMXM!,w_56,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5b4c1db8-305e-429e-8b81-deadcb1cb489_1280x1280.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:34 &bull;
   12 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2025/07/7271/'>金融庁、監督指針改正。フィッシング耐性のある認証手段の必須化へ。一部記事みたいに生体認証ではないぞ！パブコメは8/18まで</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  金融庁は15日、「金融商品取引業者等向けの総合的な監督指針」等の一部改正（案）の公表について とのタイトルでパブコメの募集を開始しました。締切は８月18日（月曜）17時00分（必着）です。 本件は、証券会社のウェブサイトを装ったフィッシングサイト等で窃取した顧客情報（ログインID…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>金融庁は15日、<a href="https://www.fsa.go.jp/news/r7/shouken/20250715/20250715.html">「金融商品取引業者等向けの総合的な監督指針」等の一部改正（案）の公表について</a> とのタイトルでパブコメの募集を開始しました。締切は８月18日（月曜）17時00分（必着）です。</p>



<p>本件は、証券会社のウェブサイトを装ったフィッシングサイト等で窃取した顧客情報（ログインIDやパスワード等）によるインターネット取引サービスでの不正アクセス・不正取引（第三者による取引）の被害が多発したことを踏まえ、インターネット取引における認証方法や不正防止策を強化するために、所要の改正を行うものです。</p>



<p>このブログの読者にとって興味深いのは、（別紙1）の以下の部分でしょうか？</p>



<blockquote>
<p><a href="https://www.fsa.go.jp/news/r6/sonota/20241004/18.pdf">「金融分野におけるサイバーセキュリティに関するガイドライン」</a>や<a href="https://www.jsda.or.jp/about/public/bosyu/files/20250715_guideline_public.pdf">日本証券業協会の「インターネット取引における不正アクセス等防止に向けたガイドライン」</a>等も踏まえ、提供するサービスの内容に応じた適切なセキュリティ対策を講じているか。その際、犯罪手口の高度化・巧妙化等（「中間者攻撃」や「マン・イン・ザ・ブラウザ攻撃」など）を考慮しているか。<br>（中略）<br>ログイン、出金、出金先銀行口座の変更など、重要な操作時におけるフィッシングに耐性のある多要素認証（例：パスキーによる認証、PKI（公開鍵基盤）をベースとした認証）の実装及び必須化（デフォルトとして設定）<br>（出所）金融庁<a href="https://www.fsa.go.jp/news/r7/shouken/20250715/01.pdf">「金融商品取引業者等向けの総合的な監督指針」の一部改正（案）【新旧対照表】</a></p>
</blockquote>



<p>フィッシングに耐性のある認証の必須化、来ましたねぇ。なんでこれが重要かは<a href="https://www.sakimura.org/2025/06/7160/">『ワンタイムパスワードでは防げない、リアルタイムフィッシングの脅威～パスキーによるフィッシング耐性の本質とは～』</a>などもご参照ください。一方では、「多要素」である必要があるかは議論のあるところだと思います。従前から申し上げているように、そろそろ「多要素認証の呪縛」から離れて、「どの脅威に対応しているのか」ということにより着目すべきだと思うからです。</p>



<p>他の資料は以下のとおりです。</p>



<p>（別紙１）<a href="https://www.fsa.go.jp/news/r7/shouken/20250715/01.pdf">「金融商品取引業者等向けの総合的な監督指針」の一部改正（案）【新旧対照表】</a><br>（別紙２）<a href="https://www.fsa.go.jp/news/r7/shouken/20250715/02.pdf">「信用格付業者向けの監督指針」の一部改正（案）【新旧対照表】</a><br>（別紙３）<a href="https://www.fsa.go.jp/news/r7/shouken/20250715/03.pdf">「高速取引行為者向けの監督指針」の一部改正（案）【新旧対照表】</a><br>（別紙４）<a href="https://www.fsa.go.jp/news/r7/shouken/20250715/04.pdf">「投資運用関係業務受託業者向けの監督指針」の一部改正（案）【新旧対照表】</a></p>



 一方では、解せぬ報道も〜生体認証の必須化ではないぞ！ 



<p>一方では解せぬ報道もあります。代表例が日経の「<a href="https://www.nikkei.com/article/DGXZQOUB142C40U5A710C2000000/">証券口座乗っ取り対策、生体認証を必須に　金融庁・日証協が新指針</a>」。ニッキンも<a href="https://www.nikkinonline.com/article/297465">「金融庁、生体など使う多要素認証必須に　証券口座乗っ取り対策強化案」</a>と記事にしていますね。</p>



<p>しかし、上記監督指針の改正案には「生体認証」なんて書いていません。例としてパスキーも出ていますが、パスキーは生体認証ではありません。こういうところは「わかりやすいから」で逃げないでちゃんと書いてほしいところです。今回重要なのは、フィッシングに耐性のある多要素認証の必須化なんです。生体認証ではありません。当然、ローカルで「生体認証」してパスワードをサイトに送信するようなのはだめなんです。</p>



<p>というわけで、マスコミの方々にはぜひとも気をつけていただきたいところです。</p>



<p>なお、生体認証にはローカルとリモートがあります。モバイルデバイスを使うものに関しては、ISOで以下のように標準化されています。いずれも現在わたしが国内の対応する専門委員会の委員長をしているSC27 Information security, cybersecurity and privacy protection の出版物です。</p>



<p>ISO/IEC 27553-1:2022 Information security, cybersecurity and privacy protection — Security and privacy requirements for authentication using biometrics on mobile devices — Part 1: Local modes<br>ISO/IEC 27553-2:2025 Information security, cybersecurity and privacy protection — Security and privacy requirements for authentication using biometrics on mobile devices — Part 2: Remote modes</p>



<p>ご参考になれば。</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   10:37 &bull;
   12 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 15. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pragmatic-engineer-2025-survey'>The Pragmatic Engineer 2025 Survey: What’s in your tech stack?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Which tools do software engineers use for backend development, frontend, infrastructure, AI tooling, and more, today? Reader survey, with feedback and analysis, based on 3,000+ responses
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Hi – this is Gergely with the monthly, free issue of the Pragmatic Engineer Newsletter. In every issue, I cover challenges at Big Tech and startups through the lens of senior engineers and engineering leaders. If you’ve been forwarded this email, you can<a href="https://newsletter.pragmaticengineer.com/about"> subscribe here</a>.</p><p>Many subscribers expense this newsletter to their learning and development budget. If you have such a budget, here’s<a href="https://blog.pragmaticengineer.com/request-to-expense-the-pragmatic-engineer-newsletter/">an email you could send to your manager</a>.</p>   <p>During April and May, we asked readers of this newsletter about the tools you use in your stack, and your opinions of them. In today’s article, the results are in.</p><p>We received more than 3,000 responses to the survey, and after summarily purging the small number of duplicated, automated, and spam ones, there were a total of 2,997 from tech professionals and others. It is this first-hand, primary source information from across the tech industry that forms the basis of the findings in this article.</p><p>We’d like to say thanks to everyone who’s provided data; you’ve contributed to what is an exclusive – and hopefully also an enhanced – understanding of the state of tooling, today, based on the highest-ever number of responses to a Pragmatic Engineer survey.</p><p>This issue covers:</p>  <p>Demographics</p>  <p>AI tools</p>  <p>Most used, most-loved programming languages</p>  <p>Most hated (and loved) tools</p>  <p>IDEs and terminals</p>  <p>Version control and CI/CD</p>  <p>Cloud providers, IaaS and PaaS</p>  <p>Before we begin: last year, we ran a survey that focused on just AI tooling. Check out the results and analysis in <a href="https://newsletter.pragmaticengineer.com/p/ai-tooling-2024">AI tooling for software engineers in 2024: reality check</a>.</p> 1. Demographics <p>Let’s begin this year’s survey analysis by stating what might be the resoundingly obvious: most people who filled in this tech stack survey are software engineers.</p> <a href="https://substackcdn.com/image/fetch/$s_!b6ox!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46e94069-6e5c-4de3-814a-df6bbf992cd1_1402x970.png">          </a>Split by role <p>Most respondents have been professionals for between 5 and 20 years, and there’s also plenty of seasoned practitioners who have been doing it longer than that, and a similar number of junior folks at the opposite end of the experience spectrum.</p> <a href="https://substackcdn.com/image/fetch/$s_!--A4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe60e46be-c7af-46b1-af15-90f97d06e050_944x922.png">          </a>Split by years of experience <p>There’s a roughly even split of people working at tiny, small, mid-sized, large, and huge companies:</p> <a href="https://substackcdn.com/image/fetch/$s_!oW4O!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d25800c-ed6e-4ef1-b192-30b19aad93d6_1116x716.png">          </a>Split by size of employer <p>We asked you what the primary focus of your day-to-day work is, and unsurprisingly, it’s the backend more than anything else:</p> <a href="https://substackcdn.com/image/fetch/$s_!enFm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff884df34-fe51-47e1-aa09-29e986b17216_1248x768.png">          </a>Split by primary work focus <p>Based on the data, the “median respondent” in these results is a senior software engineer with 6-10 years of experience, who’s working on the backend stack at a company of any size – from tiny startup to tech giant.</p> 2. AI tools <p>AI tools are surging in popularity, which is one reason why we’re covering more <a href="https://newsletter.pragmaticengineer.com/t/ai-engineering">AI engineering-related topics</a> than ever in The Pragmatic Engineer.</p><p>Most respondents mention AI tools. 85% (2,555) mention at least one in the survey. A total of 2,555 people cite at least one AI tool, while 130 say they don’t use them, and 312 don’t mention AI tooling at all.</p> <a href="https://substackcdn.com/image/fetch/$s_!YCaQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2c45b5d-0791-4a82-899b-7cf2e6edbfd1_1406x1116.png">          </a>Split by mentions of AI <p>Interestingly, four percent (130 people) explicitly do not use any AI tools, either because their workplace forbids it, or they don’t find them helpful, or because of ethical concerns.</p> Most popular AI tools <p>AI tools with at least 8 mentions (meaning 0.3% of respondents or more mentioned them) were these:</p> <a href="https://substackcdn.com/image/fetch/$s_!hQbE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eaae6db-c0fe-4157-be1c-1b22ed4fc66a_926x896.png">          </a>Answers to the question: “which AI-powered tooling or ‘copilots’ do you use?” (2,997 respondents) <p>The 9 most-mentioned tools:</p> <a href="https://substackcdn.com/image/fetch/$s_!csa9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e14edab-0aee-4de8-9e99-ac6505f8c3a5_1004x896.png">          </a>The 9 most mentioned AI tools <p>These results are a little surprising to me for two reasons:</p>  <p>GitHub Copilot (still) wildly popular. In <a href="https://newsletter.pragmaticengineer.com/i/146678491/popular-software-engineering-ai-tools">last year’s survey</a>, more developers mentioned using ChatGPT than GitHub Copilot – but now Copilot is the most-used AI tool. Indeed, in this year’s survey, every second respondent says they use this tool. This is a huge number – especially when we consider GitHub Copilot launched only four years ago.</p>  <p>Cursor surging in popularity. Cursor is an AI-powered IDE based on a fork of VS Code. This tool is the second most-mentioned IDE, despite Cursor only launching in 2023. Also, the company <a href="https://www.bloomberg.com/news/articles/2025-04-07/cursor-an-ai-coding-assistant-draws-a-million-users-without-even-trying">has not spent</a> a single dollar on marketing, to date.</p>  <p>Claude gaining a lot of ground compared to ChatGPT. It’s surprising ChatGPT (803 mentions) is not further ahead of Claude (533 mentions). A year ago, <a href="https://newsletter.pragmaticengineer.com/p/ai-tooling-2024">in our previous survey</a>, ChatGPT had 8x as many mentions as Claude, but that’s changed. Anthropic’s release of Sonnet 3.5, 3.7, and 4.0 – which excel at coding – are one reason for its popularity among techies.</p>  <p>Claude Code had momentum before it was even released. This survey closed at the end of May, and Claude Code was released on 22 May. So most respondents had likely only used Claude Code via the waitlist, on the private beta. To rack up so many mentions before the official release suggests good early traction for Claude Code, which has boomed in popularity in the weeks since the survey closed.</p>  <p>Are newer AI IDEs “overrepresented” in social media vs usage across the industry? In November of last year we ran a survey on <a href="https://newsletter.pragmaticengineer.com/p/ide-that-software-engineers-love">IDEs with AI features that developers love </a>- collecting answers only over social media. Back then, Cursor came out as the leading answer, slightly behind of GitHub Copilot:</p> <a href="https://substackcdn.com/image/fetch/$s_!g0d_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cbeaa73-0c08-4d72-a854-8fac151a47ef_1152x1046.png">          </a>Cursor, Zed, Windsurf and other new IDEs getting a larger share of social media mentions than their usage across the industry? <p>In the article, I <a href="https://newsletter.pragmaticengineer.com/i/152199792/data-source">added the disclaimer</a> that we’re likely seeing leading indicators at most, and some of the tools mentioned might not go mainstream:</p><blockquote><p>“Data from early adopters tends to indicate where innovation is within tooling. However, many tools which early adopters use never go mainstream, often because status-quo vendors adapt their tooling for customers before new competitors can take too many customers. In this case, “mainstream” IDEs are Visual Studio, Visual Studio Code, and JetBrains IDEs.”</p></blockquote><p>GitHub Copilot and ChatGPT usage seem to be dropping, and competitors growing. Here is how mentions compare between last year’s AI tooling survey (concluded in April 2024) and this year’s reader survey:</p> <a href="https://substackcdn.com/image/fetch/$s_!jgHc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25bc049f-be31-4ee0-b1e1-6dcf00fe5b0c_1242x1050.png">          </a>Change in survey responses, a year apart. Copilot and ChatGPT drop, while Cursor and Claude surge <p>AI search engine Perplexity is mentioned about as often as it was last year, with around 4% of respondents using it. As a note, annual subscribers of The Pragmatic Engineer get <a href="https://newsletter.pragmaticengineer.com/p/free-kagi-and-perplexity-access">a year-long free trial of Perplexity</a>. They don’t pay me for this endorsement: I set up this collaboration with Perplexity because I’ve been impressed with it, and am happy to help others discover this tool. We previously covered <a href="https://newsletter.pragmaticengineer.com/p/perplexity-and-kagi">how Perplexity and Kagi are taking on Google</a>.</p> AI tools by company size <p>How are the tools mentioned above actually used in workplaces of different sizes? Perplexity aside, the results are very interesting:</p> <a href="https://substackcdn.com/image/fetch/$s_!2sMs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed2f7963-e953-4dd4-8739-57deee63dce9_1236x972.png">          </a>Split of tool mentions, by company size. Sample size: 430-760 respondents per category. Respondents could mention multiple tools <p>Interesting findings from mentions of AI tools by company size:</p><p>The larger the company, the more likely it is that devs use GitHub Copilot. With the exception of “huge” (10,000+ people) places, the larger the company, the more that GitHub Copilot adoption is reported. If you want to use GitHub Copilot day-to-day, your best bet might be to join a large company!</p><p>The smaller the company, it’s more likely that people use Cursor, Zed, or another AI IDE. More devs in smaller workplaces mention Cursor, ChatGPT, Claude, and Zed – likely instead of Copilot. This trend is also present with the likes of Windsurf, Claude Code, and other alternative IDEs. I wonder if this might be because smaller startups have laxer policies for AI tool usage (i.e: “use whatever you want”), while larger companies use tools supplied by enterprise vendors. Nowhere is better at selling enterprise contracts than Microsoft is (GitHub).</p><p>The smaller the company, the more likely that ChatGPT and Claude are used. This was a bit of a surprising finding, too. As above, one possibility is that larger companies are stricter about access to AI tools, and so maybe devs have just the in-house AI tool to use. For example, at Amazon a developer told me that ChatGPT and other third-party LLMs are banned, and only LLMs like Amazon Q, and the internally hosted Claude, are allowed.</p><p>Gemini is the only tool for which company size seems irrelevant. Gemini is mentioned by only around 8% of respondents from the largest of companies (10,000+ people) to the smallest (50 or less). This is very curious: why are some models popular with small companies, but not Google’s?</p><p>My two guesses are:</p>  <p>Some Gemini usage is reported by respondents whose employer has access to Google Workspace, with which Gemini comes bundled. Around 12% of respondents mention having access to Google Workspace – and if a subset of them use Gemini, then 8% checks out.</p>  <p>Some more Gemini usage could be coming from Android developers. As a software engineer shared in the survey: "Android folks make use of Gemini, but are starting to look into <a href="https://firebender.com/">Firebender</a> (‘the Cursor for Android Studio’)."</p>  <p>If you have another theory, please leave a comment!</p>  <p>The bigger the company, the more likely it is that devs only use one AI tool. Among all companies, the “average” respondent mentions more than one tool, except at huge places with workforces of 10,000+. This suggests the biggest companies might have the strictest policies on use of tools and allow only one. In the <a href="https://newsletter.pragmaticengineer.com/p/how-ai-is-changing-software-engineering">podcast episode</a> with Shopify’s Head of Engineering, Farhan Thawar, he shared how Shopify used to have this policy, and recently changed it in order to experiment with more AI tools.</p> Other AI tools <p>Some lower-profile AI tools are also mentioned by respondents in the full results. In descending order by number of mentions:</p> <a href="https://substackcdn.com/image/fetch/$s_!eKQx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44270fce-6cc4-47dc-9e0b-b443180ab6e4_1326x1136.png">          </a>More AI tools, by number of mentions <p>The tools listed:</p>  <p><a href="https://copilot.microsoft.com/">Microsoft Copilot</a>: Microsoft’s chatbot. Not to be confused with GitHub Copilot, although Microsoft sure does help confuse things by using such a similar name to the more popular coding copilot.</p>  <p><a href="https://www.deepseek.com/">DeepSeek</a>: the open LLM created by a small team in China that lapped even the best LLMs back in January when it was released. We cover more in <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-122-deepseek-rocks-the">DeepSeek rocks the tech industry.</a></p>  <p><a href="https://www.warp.dev/">Warp</a>: an AI-powered terminal. The startup was founded in 2020 and has raised $73M in funding since.</p>  <p><a href="https://graphite.dev/">Graphite</a>: an “AI developer productivity platform.” The company started as a solution for stacked diffs and has expanded into pull request (PR) review, and helping teams ship faster. Founded in 2020, the startup raised $81M in funding. We cover more in <a href="https://newsletter.pragmaticengineer.com/p/stacked-diffs">Stacked Diffs (and why you should know about them)</a>, and also in <a href="https://newsletter.pragmaticengineer.com/p/stacked-diffs-and-tooling-at-meta">Stacked diffs and tooling at Meta</a> (note: I’m <a href="https://blog.pragmaticengineer.com/investing/">an investor</a> in Graphite).</p>  <p><a href="https://www.jetbrains.com/ai/">JetBrains AI</a>: AI assistants built into JetBrains IDEs. Interestingly, across all responses, most developers using JetBrains IDEs seem to use other AI tools that integrate into JetBrains IDEs – most notably, GitHub Copilot.</p>  <p><a href="https://sourcegraph.com/cody">Cody by Sourcegraph</a>: an AI assistant that integrates into the most popular IDEs.</p>  <p><a href="https://aws.amazon.com/q/developer/">Amazon Q Developer</a>: Amazon’s AI-powered assistants. Within the online retail giant, usage is strongly encouraged, and most devs do use it. The AI is particularly good with AWS APIs, and working with the AWS command line interface (AWS CLI).</p>  <p><a href="https://ollama.com/">Ollama</a>: run LLMs locally, such as DeepSeek and Llama.</p>  <p><a href="https://aider.chat/">Aider</a>: AI pair programming inside a terminal.</p>  <p><a href="https://cline.bot/">Cline</a>: an open source “collaborative AI” assistant, with access to frontier models. You select the models to use, and pay via a Cline account.</p>  <p><a href="https://grok.com/">Grok</a>: the large language model built by X.ai.</p>  <p><a href="https://v0.dev/">Vercel v0</a>: a “vibe coding tool” built by Vercel. Most usage is for prototyping and throwaway mockups. One founding engineer at a seed-stage startup shares: “we’ve almost replaced all our Figma usage with one-off v0 mockups.”</p>  <p><a href="https://www.raycast.com/">Raycast</a>: an app launcher for MacOS (a “command center”) that comes with <a href="https://www.raycast.com/core-features/ai">additional AI features</a> for Pro users (use natural language and LLMs to get work done with apps already on your machine). In the responses, Raycast has 14 mentions, while Raycast AI 4 mentions. Many devs seem to use Raycast to launch AI tools or improve their workflow that includes AI tools, so we grouped all mentions for this one tool.</p>  <p><a href="https://www.tabnine.com/">Tabnine</a>: the AI assistant that entered the market years before any other tool. It launched <a href="https://blog.pragmaticengineer.com/github-copilot-alternatives/">in 2019</a>, two years before GitHub Copilot. With much more competition six years later, it’s tough for this first entrant to stand out.</p>  <p><a href="https://mistral.ai/">Mistral</a>: an open, free to use, LLM built by the best-known European foundational models company.</p>  <p><a href="https://www.augmentcode.com/">Augment Code</a>: an AI software development platform optimized for larger codebases.</p>  <p><a href="https://www.coderabbit.ai/">Coderabbit</a>: AI-powered code reviews.</p>  <p><a href="https://bolt.new/">Bolt.new</a>: a “vibe coding” tool with impressive growth rate: the tool went from $0 to $40M in annual recurring revenue (ARR) <a href="https://www.lennysnewsletter.com/p/inside-bolt-eric-simons">in 5 months time</a> (!) and crossed 6 million users (!!), CEO <a href="https://x.com/EricSimons">Eric Simons</a> told me. Eric told me that the tool is primarily used by non-devs (PMs, designers and others) — so it’s no wonder that uptake is lower in this group.</p>  <p><a href="https://lovable.dev/">Lovable</a>: one of the most popular “vibe coding tools” across non-developers. The startup <a href="https://www.linkedin.com/posts/antonosika_lovable-just-reached-40m-arr-in-5-months-activity-7318691301950943232-7a_D/">went from zero to</a> $40M annual recurring revenue (ARR) also in 5 months’ time, by April 2025, and <a href="https://www.linkedin.com/posts/antonosika_lovable-just-reached-40m-arr-in-5-months-activity-7318691301950943232-7a_D/">claims</a> to have more than 1M users. As with Bolt, this surge in popularity is likely coming from non-developers.</p>  <p><a href="https://devin.ai/">Devin</a>: an AI agent called “Devin, the AI software engineer” which markets itself as “crush your backlog with your personal AI engineering team.” The positioning of this tool is that it can replace software engineers; understandably, this doesn’t make it too popular with devs.</p>  <p><a href="https://www.jetbrains.com/junie/">Junie</a> by JetBrains: an AI coding agent</p>  <p><a href="https://replit.com/">Replit</a>: a vibe coding tool surging in popularity: its annual recurring revenue (ARR) <a href="https://www.saastr.com/100mreplit/">increased</a> from $10M to $100M from January to June 2025. As with Bolt and Lovable, the surge is most definitely coming from non-software engineers using it.</p>  <p>A few interesting observations from this list and the responses:</p><p>Most respondents who mention vibe coding tools aren’t engineers. Around two thirds of those who mention Vercel v0, Bolt.new, and Lovable, are founders, director+ folks, or engineering leads. Of the remaining third, half are staff+ engineers, and the rest software engineers. This suggests that vibe coding tools might be more helpful for less hands-on folks who want to prototype something, perhaps to show to their engineering team.</p><p>Devin is not popular with developers – but Windsurf is, so is this why Cognition is buying Windsurf? It was <a href="https://cognition.ai/blog/windsurf">reported</a> yesterday (14 July), that Cognition, the company behind Devin, is acquiring Windsurf. The story so far is a bit messy:</p>  <p>OpenAI made an offer to acquire Windsurf for $3B in May</p>  <p>The deal fell through – likely because Microsoft (an investor in OpenAI, owning 49% of the company) and OpenAI were unable to agree on the IP rights about Windsurf post-acquisition (as per terms, Microsoft owns all IP in OpenAI)</p>  <p>Google then announced it was “buying out” Windsurf’s core team for $2.4B, and all founders and core Windsurf employees quit for Google with payouts, while Windsurf investors were made whole. The staff from Windsurf works on Gemini</p>  <p>Windsurf is left with $100M of cash in hand, and the employees who didn’t quit for Google.</p>  <p>The deal is unusual but does fit a trend of Big Tech shunning normal-style acquisitions in order to avoid disruptive governmental antitrust investigations. Google gets the people it wants, without a product it doesn’t want (Windsurf), and leaves the company technically operational. But Windsurf is now effectively a “zombie” company, with some staff and funding left, but potentially a declining userbase, after the founding team all jumped ship.</p><p>Following that, Cognition stepped in to acquire Windsurf. Our survey shows that Devin has about 10% of the usage as Windsurf has respondents (and 1% of usage than Cursor has with devs), so could the creator of Devin buying Windsurf be Cognition’s attempt to gain more developer traction? Cognition was founded in 2023, raised $200M to date, and was valued close to $4B in their last funding round.</p><p>Devin has been positioned for founders as a way to replace developers with an “AI Engineer” that costs approximately $500/month. We previously analyzed why Devin likely <a href="https://blog.pragmaticengineer.com/ai-developer-marketing-stunt/">needed to position itself like this</a> to gain market share. At the same time, it was revealed Devin <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-90">faked its launch demo</a> by pretending to “complete real jobs on Upwork” which it didn’t do. The company has not yet corrected its inaccurate statements.</p><p>My two cents is that a company like Cognition cannot have it both ways: it cannot market itself as an “AI software engineer” that replaces devs, but also wants developers to use this tool. It should choose one direction, though my sense is that Cognition will keep marketing itself as a replacement for human developers.</p> 3. Most used, most-loved programming languages  Most-used languages <p>The most frequently mentioned programming languages:</p> <a href="https://substackcdn.com/image/fetch/$s_!CGhI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92dbe407-348a-4f37-82da-23d8b86a7807_922x868.png">          </a>The 18 most-mentioned programming languages <p>TypeScript being the most-used is less of a surprise these days: it’s a type-safe language that can be deployed on both the front and backend. Engineering teams at <a href="https://newsletter.pragmaticengineer.com/i/86186964/technology-stack">Linear</a> and <a href="https://newsletter.pragmaticengineer.com/i/145059916/tech-stack">Bluesky</a> share that this is why they use it.</p><p>Python seems to me like it’s having something of a renaissance. Not only is it an expressive, powerful language, but it’s the “shared language of choice” among software engineers, data scientists, and ML/AI engineers.</p><p>Swift has clearly become the default language choice for those building native iOS applications, with 6x as many mentions than Objective C. It’s been 11 years since the language was publicly announced in 2014, and my guess is that most iOS engineers working with Objective C are likely doing so on legacy codebases, where a rewrite is not worth the effort.</p> Most-loved languages <p>Gathering this data was a little tricky because answers are shared with us in “freeform,” meaning we have to interpret each response, such as “I don’t really like JavaScript: I could use Python instead.” In this example sentence, “JavaScript” is disliked, and “Python” is loved. We did our best to count all positive and negative mentions. Here’s the top 10:</p> <a href="https://substackcdn.com/image/fetch/$s_!e0Rm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc97260b9-9270-41f7-8af7-ad356f338b9c_1080x1108.png">          </a> <p>This table is pretty predictable, except perhaps for how much love there is for Ruby on Rails. The language is the 5th most popular in usage, and the 3rd most loved one. Elixir being the 10th most loved language, despite being the 16th most used one, suggests that devs using it really appreciate how well it works.</p><p>One interesting finding is that no language has a “highly negative” rating, meaning that none have received significantly more “negative” mentions than positive ones. To me, this suggests that popular languages these days are all pretty good: after all, a poorly designed language won’t win much adoption.</p> 4. Most-loved (and hated) tools <p>We gathered details on the most-loved and most disliked tools, similarly to what we did for programming languages. We counted all positive and negative mentions, and then deducted the negatives from the positives:</p> Most-loved tools  <a href="https://substackcdn.com/image/fetch/$s_!1iST!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff618e558-771e-4618-b169-737013551ae5_1084x1238.png">          </a> <p>The presence of so many IDes and text editors on this list, such as VS Code, JetBrains, Cursor, Neovim, Vim, Zed, and Emacs, indicates that engineers spend a good part of their days inside the editor.</p><p>JetBrains products get plenty of love by garnering nearly as many mentions as VS Code, which shows JetBrains know what they’re doing.</p><p>Cursor being the third-most loved product is a standout achievement, especially because the product is barely two years old. We recently did a deepdive into <a href="https://newsletter.pragmaticengineer.com/p/cursor">how Cursor is built, with cofounder Sualeh Asif.</a></p><p>Linear is mentioned positively mostly in comparison to JIRA (see below), for being a snappier experience, and more pleasant to use than Atlassian’s flagship product. We also did a deepdive <a href="https://newsletter.pragmaticengineer.com/p/linear">Inside Linear’s engineering culture</a>, and a podcast episode on <a href="https://newsletter.pragmaticengineer.com/p/linear-move-fast-with-little-process">How Linear moves so fast</a>.</p><p>Expo is the only framework to make it to this “most-loved” list. It is a cross-platform mobile development library and we previously did <a href="https://newsletter.pragmaticengineer.com/p/expo">a deepdive on Expo</a>, covering plenty of details.</p> Most-disliked tools  <a href="https://substackcdn.com/image/fetch/$s_!oTmH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2054b0a0-674a-4b44-8a3f-65552227d633_1080x1106.png">          </a> <p>JIRA is by far the most-disliked tool among respondents, with more negative mentions than the next four tools combined! For example, many mentions of Linear were in the context of devs wishing they could use this more lightweight tool instead of JIRA. Chief complaints about JIRA are that it’s slow, complex, and cumbersome.</p><p>Linear is the most frequently-mentioned replacement for JIRA. Slack is often mentioned as the tool devs currently using Microsoft Teams say they wish they could use, instead. Coda (a document collaboration platform) is often mentioned as what devs would like to replace Confluence (Atlassian’s team collaboration and knowledge management platform) with.</p> Reasons for disliking tools <p>Factors which developers mention for disliking something:</p> <a href="https://substackcdn.com/image/fetch/$s_!EjJF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b5b8688-2027-48a2-a607-bb0cf40ada4b_1302x1120.png">          </a> <p>There are some obvious themes in why devs dislike tools:</p>  <p>Slowness! The single biggest complaint about JIRA, Confluence, and other tools that devs criticize.</p>  <p>Bugs and crashing. A telling description by a dev of why they dislike MS Teams: “Lack of custom emojis, buggy, and crashes frequently, split chat and teams, the list goes on and on.”</p>  <p>Redesigning the UX. One respondent on a feature flagging vendor: “it used to be good but I despise the redesigned interface. Beyond basic usability issues, the redesign makes it much easier to apply changes to the wrong environment by mistake.”</p>  <p>Complicated, or too “heavy.” This is a common criticism of frontend frameworks. As one engineer summarizes: “Most frontend technologies are too complex [and] have too many dependencies. Usually just too complicated to deal with for what the client really needs.”</p>   5. IDEs and Terminals  IDEs <p>The most popular by number of mentions:</p> <a href="https://substackcdn.com/image/fetch/$s_!Gf82!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F702ef20f-fb4d-4143-9f8e-a52cd05e8c23_1196x1042.png">          </a>Commonly-mentioned IDEs by respondents <p>Cursor being the second most-used IDE is very surprising, given it’s been around for only two years!</p><p>JetBrains has seven IDEs in the above list: IntelliJ (tailored for Kotlin and Java), Rider (.NET), Webstorm (web), PHPStorm (PHP), Goland (Go), Datagrip (data), and RubyMine (Ruby). Additionally, they also have CLion (C/C++) and RustRover (Rust), which were mentioned 10 and 8 times, respectively. If we combine the two Microsoft IDE mentions (VS Code and Visual Studio) and these seven IntelliJ IDEs, this is the split:</p> <a href="https://substackcdn.com/image/fetch/$s_!pMS4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7afea9bc-bba0-4f1c-b508-5fc8d73810c6_1290x1044.png">          </a>Split between Microsoft and JetBrains IDEs (3,131 mentions. Respondents could name more than one tool per response.)  Terminals  <a href="https://substackcdn.com/image/fetch/$s_!3C4o!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ec055bd-0772-4079-8db2-267f9f3288e0_1014x1022.png">          </a> <p>Terminals mentioned:</p>  <p><a href="https://neovim.io/">Neovim</a>: an extensible, Vim-based editor</p>  <p><a href="https://www.gnu.org/software/bash/">Bash</a> (the Unix / Linux shell)</p>  <p><a href="https://en.wikipedia.org/wiki/Vim_%28text_editor%29">Vim</a>: an editor part of most Unix / Linux systems</p>  <p><a href="https://www.gnu.org/software/emacs/">Emacs</a>: an extensible and customizable editor</p>  <p><a href="https://www.warp.dev/">Warp</a>: a terminal enhanced with AI</p>  <p><a href="https://zsh.sourceforge.io/Doc/Release/Zsh-Line-Editor.html">Zsh</a>: a Unix shell and command-line interpreter</p>  <p><a href="https://github.com/tmux/tmux">Tmux</a>: a terminal multiplexer for Unix</p>  <p><a href="https://ghostty.org/">Ghostty</a>: a fast and feature-rich terminal</p>  <p><a href="https://iterm2.com/">iTerm2</a>: a “replacement for Terminal” on Mac</p>   6. Version control, CI/CD <p>Which tools do devs use for version control, continuous integration (CI), and continuous deployment (CD)? Here’s what the survey says:</p> Version control <p>78% of respondents mention some kind of version control – almost all being a variant of Git.</p><p>Here is the split of vendor mentions:</p> <a href="https://substackcdn.com/image/fetch/$s_!cODb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dcc3c9b-121a-4fdd-981a-4a2ba5b8a2ea_1018x886.png">          </a>Version control tools mentioned across all responses <p>In the “Other” category, the mentions are:</p>  <p><a href="https://subversion.apache.org/">SVN</a> (18 responses, 0.6%)</p>  <p><a href="https://en.wikipedia.org/wiki/Azure_DevOps_Server#TFVC">TFS</a> (10, 0.3%)</p>  <p><a href="https://www.perforce.com/">Perforce</a> (8, 0.25%)</p>  <p><a href="https://www.mercurial-scm.org/">Mercurial SCM</a> (6, 0.2%)</p>  <p>If we zoom into Git vendors, we see:</p> <a href="https://substackcdn.com/image/fetch/$s_!h4Ou!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65b7e2d5-1add-4b7d-abc6-2be842790fc3_908x886.png">          </a>Version control responses mentioning Git vendors (2,311 responses) <p>GitLab and BitBucket are alive and well – and this survey is a reminder of that. For some reason, I assumed GitHub would absolutely dominate this category, and that it would have a market share of close to 90%. And this is potentially true for hobby projects, but most respondents share their work stack.</p><p>In the <a href="https://survey.stackoverflow.co/2022/">Stack Overflow 2022 survey</a>, we saw similar results among professional developers:</p> <a href="https://substackcdn.com/image/fetch/$s_!S7Qr!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed17aa2c-adfc-475f-bf03-55016f9f845c_1238x1002.png">          </a>GitHub, GitLab, and Bitbucket results are similar to those in our Stack Overflow survey of 2022 <p>GitLab’s self-hosted offering still seems popular. Around 10% of GitLab mentions are for GitLab’s self-hosted offering, and it seems to be the last major Git provider offering self hosting.</p> CI/CD <p>In the category of continuous integration and continuous deployment, the results are:</p> <a href="https://substackcdn.com/image/fetch/$s_!PSI3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5de6e1a2-31b1-4036-b324-e62a79f5d149_1296x1068.png">          </a>Most-mentioned CI/CD solutions <p>GitHub Actions being the most-used continuous integration (CI) tool by so much is somewhat surprising. GitHub Actions launched as generally available (GA) in 2019, but many other vendors in this list have been around much longer. That GitLab CI and Bitbucket Pipelines are popular is less surprising because both are offerings from GitHub, GitLab, and Atlassian (the vendor behind Bitbucket). It seems that many customers prefer to use the CI/CD offering from the same company as they use for version control.</p><p>Vendors mentioned:</p>  <p><a href="https://github.com/features/actions">GitHub Actions</a>: CI/CD and build automation by GitHub</p>  <p><a href="https://www.jenkins.io/">Jenkins</a>: the leading open source build server</p>  <p><a href="https://azure.microsoft.com/en-us/products/devops">Azure DevOps</a>: Microsoft’s offering, which includes CI/CD</p>  <p><a href="https://circleci.com/">CircleCI</a>: a popular CI/CD tool</p>  <p><a href="https://argo-cd.readthedocs.io/en/stable/">Argo CD</a>: continuous deployment for Kubernetes</p>  <p><a href="https://www.travis-ci.com/">Travis CI</a>: another CI/CD tool, and competitor to CircleCI</p>  <p><a href="https://www.atlassian.com/software/bitbucket/features/pipelines">Bitbucket Pipelines</a>: the CI/CD offering of Atlassian</p>  <p><a href="https://www.jetbrains.com/teamcity/">Teamcity</a>: CI/CD service from JetBrains</p>  <p><a href="https://buildkite.com/">Buildkite</a>: run CI/CD on your own infrastructure, as used by large companies like Uber, Slack, Canva and others</p>  <p><a href="https://gradle.org/">Gradle</a>: CI/CD for Android, Java and Kotlin. Open source</p>  <p><a href="https://bazel.build/">Bazel</a>: a build tool open sourced by Google. It performs well with large codebases</p>  <p><a href="https://octopus.com/">Octopus Deploy</a>: CI/CD for Kubernetes and other platforms. Built for scale</p>  <p><a href="https://nx.dev/">NX</a>: an “AI-first” build platform</p>  <p><a href="https://bitrise.io/">Bitrise</a>: CI/CD for anything mobile (native iOS/Android, React Native, Flutter, Cordova, Ionic, Unity. We covered more about many of these technologies in <a href="https://newsletter.pragmaticengineer.com/p/cross-platform-mobile-development">Cross-platform mobile development</a>)</p>  <p>Honorable mentions:</p>  <p><a href="https://www.harness.io/">Harness</a>: continuous delivery</p>  <p><a href="https://www.atlassian.com/software/bamboo">Atlassian Bamboo</a>: Atlassian’s self-hosted CI/CD platform</p>  <p><a href="https://spinnaker.io/">Spinnaker</a>: mutli-cloud continuous delivery</p>  <p><a href="https://tekton.dev/">Tekton</a>: cloud-native CI/CD</p>  <p>In-house solutions are also a thing for CI/CD. Many of the Jenkins mentions refer to companies running their own Jenkins setup. Also, larger companies like Google, Meta, and Uber, have custom-build CI/CD systems, and often use custom feature flag systems that might be integrated with CI/CD systems, in-house experimentation stack, and lots of custom infrastructure, in general.</p> 7. Cloud providers, PaaS, IaaS <p>Here is how backend providers lines up, based on survey responses:</p> <a href="https://substackcdn.com/image/fetch/$s_!Z1L3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbdd44d2-c26b-4c0c-9025-739051a34fe4_984x988.png">          </a>Split of backend provider mentions across all respondents (2,866 responses) <p>Zooming in on mentions of the three largest cloud providers (AWS, Azure, GCP):</p> <a href="https://substackcdn.com/image/fetch/$s_!7wJe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F704b7799-3ddf-449f-b5d4-7443a3542f0a_1052x964.png">          </a>Split of cloud provider mentions across the “Big 3” clouds (2,631 mentions) <p>Interestingly, VC firm Coatue <a href="https://x.com/bg2clips/status/1943735453573951544">estimates</a> that total revenue in the cloud market splits like this:</p>  <p>AWS: 44%</p>  <p>Azure: 30%</p>  <p>GCP 19%</p>  <p>The difference in numbers suggests our survey results are likely to overrepresent AWS and underrepresent Azure. I’d speculate that Azure is more popular at “traditional” companies, while Pragmatic Engineer readers tend toward Big Tech workplaces and startups.</p><p>Looking deeper, here are smaller cloud providers and platform-as-a-service (PaaS), or infrastructure-as-a-service (IaaS), mentions:</p> <a href="https://substackcdn.com/image/fetch/$s_!1OpG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea0c498-233f-4553-8fcd-d55326663a07_1038x968.png">          </a>Other infrastructure providers mentioned <p>Vercel is the clear leader in infrastructure choice beyond the “Big 3”. By comparison, Vercel has one fifth of the mentions that GCP gets (100 vs 502).</p><p>Details about other providers:</p>  <p><a href="https://vercel.com/">Vercel</a>: a PaaS optimized for web applications. Founded in 2015, it raised $563M in funding. Vercel is the creator of the popular Next.js React framework, and the Vercel v0 “vibe coding” tool.</p>  <p><a href="https://www.heroku.com/">Heroku</a>: a PaaS that used to be very popular in the early 2010s. Founded in 2007, Salesforce <a href="https://techcrunch.com/2010/12/08/breaking-salesforce-buys-heroku-for-212-million-in-cash/">acquired</a> Heroku in 2010 for $212M.</p>  <p><a href="https://www.hetzner.com/">Hetzner</a>: IaaS provider headquartered in Germany. Offers <a href="https://en.wikipedia.org/wiki/Virtual_private_server">VPSes</a> (virtual private servers). Known for affordable pricing. The company was founded in 1997, never took VC funding, and remains a private company, making it one of the larger bootstrapped companies.</p>  <p><a href="https://render.com/">Render</a>: a PaaS scaleup. Founded in 2018, the scaleup has raised $157M from VCs since. In January this year, the company <a href="https://x.com/render/status/1881740128454168877">shared</a> that more than 2 million developers use its platform.</p>  <p><a href="https://www.digitalocean.com/">DigitalOcean</a>: an IaaS provider where you can spin up VPSes (called “Droplets” in DigitalOcean lingo). The company is publicly traded, valued at $2.5B.</p>  <p><a href="https://www.netlify.com/">Netlify</a>: a PaaS with strong support for frontend projects. Founded in 2014, the scaleup raised $212M in funding.</p>  <p><a href="http://fly.io">Fly.io</a>: somewhere between a PaaS and an IaaS. Fly.io abstracts away most infrastructure management, allowing deploying applications as Docker containers (where the platform takes care of scaling). It also offers “Fly Machines” (microVMs) where customers have more control over regions and instances, which is similar to what IaaS providers offer. Founded in 2016, the startup has raised $110M.</p>  <p><a href="https://railway.com/">Railway</a>: a PaaS scaleup. Founded in 2020, the company raised $25M in funding. By 2023, the company <a href="https://www.linkedin.com/feed/update/urn:li:activity:7046551111528288256/?updateEntityUrn=urn:li:fs_feedUpdate:(V2,urn:li:activity:7046551111528288256)">already had</a> more than 300,000 users(!)</p>   Takeaways <p>This tech stack survey has been the biggest piece of reader-based research to date by the Pragmatic Engineer, and I hope you find the results interesting! We certainly do. Here are my biggest takeaways:</p><p>Us devs are not shy to try out new tools in innovative areas. AI tools are a model example of how quickly things can change in just a year. Twelve months ago, GitHub Copilot seemed to dominate this category, but since then, smaller startups have gained mindshare and market share. This explains why VC investors are pouring so much money into areas seen as innovative: it’s still easy enough to have developers try out new tools, and perhaps stick with them!</p><p>In 2025, AI dev tools are the hotbed of innovation, and in the AI tooling category, companies founded only 2-3 years ago can already be leaders, like Cursor is – at least for now.</p><p>In more established categories, change is slower. In areas like programming languages and CI/CD systems – where each language has been around 10+ years or more – it’s harder for new startups to gain customers and make the kind of “splash” Cursor has.</p><p>JetBrains taking on Microsoft in IDEs is a remarkable highlight. Microsoft is a Big Tech giant with developer tools in its DNA: it’s been a major focus of the company <a href="https://newsletter.pragmaticengineer.com/p/50-years-of-microsoft">for 50 years</a>. And yet, JetBrains keeps being a strong competitor to it in IDEs, and is one of the most loved tools – right behind VS Code.</p><p>Do devs dislike JIRA, or just hate being micromanaged? For as long as I’ve been a dev, I’ve seen fellow developers getting mad at JIRA because it’s slow, clunky, not customizable enough, etc. Another big reason is that we just don’t like being pushed to use it! That “push” has always come from product managers, project managers, or some other kind of manager.</p><p>But I wonder if the root problem is really with JIRA itself, or whether any project management tool idolized by managers would encounter the same push back? It is rare to find a dev who loves creating and updating tickets, and writing documentation. Those who do tend to develop into PMs or TPMs (<a href="https://newsletter.pragmaticengineer.com/p/what-tpms-do">Technical Program Managers</a>), and do more of “higher-level”, organizational work, and less of the coding. Perhaps this in turn makes them biased to something like JIRA?</p><p>We hope you get some value from this article about tools devs use in 2025. We intend to follow up with some more findings from this survey in a few weeks’ time. In the meantime, please leave a comment below if you have any thoughts about the state of tooling today. Thanks again to everyone who took the time to be part of this survey.</p><p>Updates to the article on 16 July, after reader feedback (thank you!)</p>  <p>Added PHP and Clojure as languages (how could I miss a popular language like PHP - my apologies!)</p>  <p>Added Replit and Raycast to the list of AI tools</p>  <p>Added a section on GitHub Copilot possibly being underrepresented on social media</p>  
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!b6ox!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46e94069-6e5c-4de3-814a-df6bbf992cd1_1402x970.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:30 &bull;
   13 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/15/an-hour-of-trouble-shooting.html'> An hour of trouble shooting zoom and iCal synching for Jax t</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   An hour of trouble shooting zoom and iCal synching for Jax to learn that Zoom have decided to deprecate support for said synch as of this month … 
 SERIOUSLY? 
 FTW - truly useless - told Jax she should stop sending money to Zoom and move to Cal.com 
 No notifications. No reduction in price. #BOYCOT 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>An hour of trouble shooting zoom and iCal synching for Jax to learn that Zoom have decided to deprecate support for said synch as of this month …</p>
<p>SERIOUSLY?</p>
<p>FTW - truly useless - told Jax she should stop sending money to Zoom and move to Cal.com</p>
<p>No notifications. No reduction in price. #BOYCOT</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   07:22 &bull;
   13 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 14. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/15/083413.html'> 💬 
</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   💬 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/e70d49a128.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>💬</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/e70d49a128.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:34 &bull;
   13 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 13. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/14/hcr.html'> 💬 HCR … 
</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   💬 HCR … 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/0d49e5a981.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>💬 HCR …</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/0d49e5a981.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:27 &bull;
   14 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/why-big-tech-is-threatened-by-a-global-push-for-data-sovereignty/'>Why Big Tech is threatened by a global push for data sovereignty</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Global Majority nations are building ways to store their citizens' data locally. But will they own the datacenters themselves?
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>[<a href="https://restofworld.org/2025/big-tech-data-sovereignty/?ref=werd.io">Damilare Dosunmu at Rest of World</a>]</p><p>The devolution of data (and with it, software) from the United States to the rest of the world continues apace:</p><blockquote>“Developing nations are challenging Big Tech’s decades-long hold on global data by demanding that their citizens’ information be stored locally. The move is driven by the realization that countries have been giving away their most valuable resource for tech giants to build a trillion-dollar market capitalization.”</blockquote><p>This has always been true, but was perhaps less of an issue when the US government wasn’t both unpredictable and antagonistic. Now, it isn’t just Europe that is seeking to own its own technology future: Global Majority countries are too. And they’re taking steps to make sure it happens.</p><p>Africa is building new data centers. While some more of the wealth and investment will flow into the continent as a result, it doesn’t appear that many of them are actually owned by Africans, which feels like a necessary next step. Otherwise they’re essentially being colonized by tech companies from places like the US and China.</p><p>Countries like Nigeria have also become adept at building software. What’s going to be really interesting is when these countries also pull out of tagging and moderating AI data; much of this work has been outsourced to the Global Majority, and companies like OpenAI depend on it. That may have seemed like smart arbitrage from their points of view to begin with, but with the right strategy and underlying legislation, it could begin to look like leverage.</p><p>[<a href="https://restofworld.org/2025/big-tech-data-sovereignty/?ref=werd.io">Link</a>]</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:00 &bull;
   15 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/fell-in-a-hole-got-out/'>Fell in a hole, got out.</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Tony Stubblebine's account of saving Medium is remarkable in its transparency - and in its execution.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>[<a href="https://medium.com/the-coach-life/fell-in-a-hole-got-out-381356ec8d7f?ref=werd.io">Tony Stubblebine at Medium</a>]</p><p>This account of Medium’s resurgence is remarkable on multiple levels: the transparency of Tony Stubblebine’s account is rare, but the story itself is astonishing. This kind of turnaround almost never happens, and it’s a testament to Tony that it worked in this case.</p><blockquote>“Ev had two eras here. The first was the design era where the team reinvented what a writing platform could look like, both simplifying and beautifying every part of the experience. The second was the invention of a new business model, moving away from the toxic incentives of ads and instead offering a single bundled subscription that any writer could share in.”</blockquote><p>I’ve spoken to a few other people who worked at Medium in one of its earlier eras. For the folks who were around during the business model era, it feels a bit weird that much of their work was to dig the “hole” as he describes it here — but it’s also not wrong (and, to be clear, not their fault). Building Medium with a Twitter-scale sensibility was the wrong thing; building it with profitability in mind, and with a no-nonsense approach to promoting really interesting writing, was always the way.</p><p>I think Tony’s self-described superpowers were really key here:</p><blockquote>“One is that running small businesses gave me a chance to see how every part of a company operates, often because I had to be the one doing it. Two, there really is no such thing as a social media platform having a bigger power user than I was with Medium. I had been a prolific user in every capacity, from hobby writer, to thought leader, to promoting a business, to daily newsletter writer, to building three of the biggest publications here. Nearly 2% of page views on Medium were going to my publications and writing.”</blockquote><p>There’s a lot here for startup founders and anyone who cares about tech and media business models to digest. This account is generous in its transparency and detail, and it deserves to be read and understood by everyone in the space.</p><p>[<a href="https://medium.com/the-coach-life/fell-in-a-hole-got-out-381356ec8d7f?ref=werd.io">Link</a>]</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   12:59 &bull;
   15 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/13/indie-culture-is-great-but.html'> 🔗 Indie Culture Is Great - But What’s Coming Next is Better </a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   🔗 Indie Culture Is Great - But What’s Coming Next is Better 
 .. Ted 
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://open.substack.com/pub/tedgioia/p/indie-culture-is-greatbut-whats-coming">🔗 Indie Culture Is Great - But What’s Coming Next is Better</a></p>
<p>.. Ted</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   00:54 &bull;
   15 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 12. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://idm-thoughtplace.blogspot.com/'>IdM Thoughtplace</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://idm-thoughtplace.blogspot.com/2025/07/from-toll-roads-to-tokens-road-rules-of.html'>From Toll Roads to Tokens: The Road Rules of Identity</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

    Recently, I found myself comparing Identity Management to the New Jersey Turnpike—a stretch of infrastructure that demands continuous maintenance, monitoring, and support. The more I thought about it, the more the analogy seemed to hold up on multiple levels.  Consider this: when you enter the Turnpike, you're authenticated—thanks to your EZ-Pass RFID reader. You authorize yourself to use the ser
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<br><br>Recently, I found myself comparing <a href="https://idm-thoughtplace.blogspot.com/2025/05/identity-as-kitchens-and-driving-on-new.html">Identity Management to the New Jersey Turnpike</a>—a stretch of infrastructure that demands continuous maintenance, monitoring, and support. The more I thought about it, the more the analogy seemed to hold up on multiple levels.<br><br>Consider this: when you enter the Turnpike, you're authenticated—thanks to your EZ-Pass RFID reader. You authorize yourself to use the service by paying the toll<a>1</a>. Your presence on the road is uniquely identified through a combination of your EZ-Pass ID and your vehicle’s license plate. Similarly, in Identity Management, we combine multiple identifiers to authenticate users and authorize access.<br><br>There's even a form of fine-grained authorization at play. Your driver's license determines which type of vehicle you’re allowed to operate—semi-trucks, motorcycles, passenger cars—all of which come with their own set of permissions. Identity systems do the same by assigning entitlements and roles based on user attributes and context.<br><br>We can stretch the analogy further. Think about drivers from other states or countries using the Turnpike. They bring their own credentials, but the system recognizes and allows them to operate—a real-world version of Single Sign-On (SSO). Once authenticated, drivers manage their journey: choosing routes, switching lanes, adjusting speed—just like identities that evolve, shift roles, or gain new permissions over time.<br><br>But perhaps the most vital component in this infrastructure? The on-ramps and off-ramps.<br><br>In our analogy, these represent connectors to other roads—other systems. On-ramps lead drivers onto the Turnpike (onboarding), and off-ramps take them to their destination (offboarding). In identity terms, they’re links to enterprise applications. Some lead to robust, high-speed interstates (modern apps), while others connect to older, more narrow routes (legacy systems). Despite their differences, all are part of the same interconnected digital landscape.<br><br>If these ramps are blocked or broken, people can’t get where they need to go. The same is true in Identity Management. Disrupted connectors—whether due to outages, outdated protocols, or rigid infrastructure—can prevent users from accessing critical resources. That’s why flexibility is key.<br><br>Just as highways need multiple lanes, alternate routes, and regular maintenance, identity infrastructure must be resilient. It needs to support remote access, cloud redundancy, and failover mechanisms. Whether through replicated data centers, leveraging SaaS service, or just having a well-designed backup plan, your identity architecture must ensure users can always reach their destinations.<br><br>In short: smooth identity operations are just like smooth traffic flow. It's all about seamless access, clear pathways, and ensuring the road is always open.<br><br><br><br><br><br><br><br><a>1</a>&nbsp;In the pre-EZ-pass era, one paid the toll on the Garden State Parkway, another important piece of infrastructure with a token, but we won’t get into yet another roadway and it’s analogies here ☺.
   
  
  
  

</div>
</div>

<div class='item-footer'>
   21:40 &bull;
   15 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/12/164249.html'> 💬 
</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   💬 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/cf42bcab49.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>💬</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/cf42bcab49.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   04:42 &bull;
   16 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/12/the-cuts-risk.html'> The cuts ‘risk’ .. 
</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   The cuts ‘risk’ .. 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/3ca32d61d1.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The cuts ‘risk’ ..</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/3ca32d61d1.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   04:41 &bull;
   16 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 11. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/12/nothing-to-see-here-and.html'> Nothing to see here … and to remind that in 2018 the world w</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Nothing to see here … and to remind that in 2018 the world was amazed to learn that Apple had become the first trillion dollar company …  not that it was Apple .. but that it had a hit a trillion. Fast forward just 7 years and ‘a chip’ company hitting 4 trillion barely registers surprise. 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/6c20fad1c6.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Nothing to see here … and to remind that in 2018 the world was amazed to learn that Apple had become the first trillion dollar company …  not that it was Apple .. but that it had a hit a trillion. Fast forward just 7 years and ‘a chip’ company hitting 4 trillion barely registers surprise.</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/6c20fad1c6.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:37 &bull;
   16 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2025/07/7330/'>OpenID for Verifiable Presentation 1.0 最終版が発行されました。</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  欧州のデジタルアイデンティティウォレット関係で使われることになっている規格である、OpenID for Verifiable Presentations 1.0　(https://openid.net/specs/openid-4-verifiable-presentations…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>欧州のデジタルアイデンティティウォレット関係で使われることになっている規格である、OpenID for Verifiable Presentations 1.0　(<a href="https://openid.net/specs/openid-4-verifiable-presentations-1_0-final.html">https://openid.net/specs/openid-4-verifiable-presentations-1_0-final.html</a>) の最終版が発行されました。この仕様は、クレデンシャルのプレゼンテーションを要求し配信するためのOAuth 2.0 [RFC6749]上のメカニズムを定義します。クレデンシャルとプレゼンテーションは任意のフォーマットであることができ、W3C Verifiable Credentials Data Model、ISO mdoc [ISO.18013-5]、IETF SD-JWT VC を含みますが、これらに限定されません。これは&nbsp;<a href="https://openid.net/wg/digital-credentials-protocols/">OpenID DCP Working Group</a> のアウトプットです。</p>



<p>共同議長である</p>



 
 Kristina Yasuda (SPRIND) 



 Torsten Lodderstedt (SPRIND) 



 Joseph Heenan (Authlete) 
 



<p>の各氏、およびWGメンバーのみなさんにこころからお礼を申し上げたいと思います。</p>



<p>なお、同WGは引き続き OpenID for Verifiable Credential Issuance 1.0 の発行に向けて急ピッチで作業を進めています。また、すでに取り残し事項になったことの取り込みのための ver.1.1 の作業も予想されます。</p>



<p>引き続きみなさまのご協力をお願いいたします。</p>



<p>WGへの参加は簡単です。費用はかかりません。知財提供同意書にサインするだけです。ぜひご検討お願い申し上げます。</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   04:56 &bull;
   17 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 10. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-139'>The Pulse #139: Section 174 is reversed! Or is it?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Finally, relief: tax regulation hurting the US tech industry is striked off for good - for the most part, that is. Also: why Meta is “going to war” to poach AI researchers from OpenAI, and more.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering events, insights, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Today, we cover:</p>  <p>Tax rule Section 174 reversed! Or is it..? The much-criticized tax change that forbids expensing of developer salaries in the same year, has been put on hold for five years. This is welcome, bu…</p>  
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-139">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:49 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/if-i-ran-x/'>If I ran X</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  How to transform the internet's most toxic platform into essential infrastructure.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>“After two incredible years,” Linda Yaccarino <a>tweeted</a>, “I’ve decided to step down as CEO of 𝕏.” It was the day after the company’s AI chatbot, Grok, <a href="https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content?ref=werd.io">began calling itself MechaHitler and started spewing antisemitic slurs and advocating for Adolf Hitler in public</a> until the company eventually <a href="https://www.reuters.com/technology/musk-chatbot-grok-removes-posts-after-complaints-antisemitism-2025-07-09/?ref=werd.io">had to remove them</a>. Many of the users still posting to X greeted the antisemitism warmly; some tweeted, “finally”.</p><p>No new CEO has been named.</p><p>I’ve previously written about what I’d do if <a href="https://werd.io/if-i-ran-bluesky-product/">I ran Bluesky product</a>, <a href="https://werd.io/if-i-ran-mastodon/">Mastodon</a>, <a href="https://werd.io/if-i-started-fresh/">my own platform</a>, and <a href="https://werd.io/lets-fund-the-open-social-web/">a fund supporting the open social web</a>. I’d intended to stop the series there.</p><p>But as there’s an opening: let’s imagine I was handed the reins to the most toxic mainstream social network. What would I do if I was the new CEO of X?</p><p>I have some history with the people behind Twitter, the site that eventually became X. I consider multiple people who were there at the site’s inception, as well as many people who worked there in its later years, to be friends; Biz Stone sat on my first startup’s advisory board; I worked for Ev Williams at Medium for a little while. I’ve heard various pieces of founding stories from each of them.</p><p>And I really loved using Twitter. Long before Elon Musk bought it, I was an addicted user who posted many times a day. It opened up communities for me: I attended Twestivals (and hosted one), <a href="https://www.youtube.com/watch?v=dYP-wBaqQAI&amp;ref=werd.io">saw Ben Walker play his Twitter Song live</a>, took part in countless hashtag games and still own a tote bag I won from the literary magazine Granta for writing a tweet-length horror story. Every time I went to an industry event or a conference, I’d be in the Twitter backchannel, making and reading observations, and building new connections.</p><p>And then, one day, <a href="https://www.theguardian.com/technology/2022/oct/26/elon-musk-twitter-visit-sink?ref=werd.io">Elon Musk walked into the building carrying a kitchen sink</a> and began to refashion it around his need to amass power and influence to bring about his edgelord, <a href="https://www.npr.org/2025/04/25/nx-s1-5371718/pronatalist-birth-rate-musk-natal-conference?ref=werd.io">pronatalist</a>, <a href="https://www.theguardian.com/technology/2025/jan/21/the-gesture-speaks-for-itself-germans-divided-over-musks-apparent-nazi-salute?ref=werd.io">Hitler-saluting</a> view of the world. The name changed to X, Musk’s original name for PayPal, a company <a href="https://en.wikipedia.org/wiki/X.com_(bank)?ref=werd.io">whose board voted to fire him as CEO</a> due to mismanagement. Musk fired 80% of the staff, cratered the morale of the people who remained, <a href="https://www.cnn.com/2022/12/05/business/nightcap-twitter-neo-nazi?ref=werd.io">reinstated Nazi accounts</a>, <a href="https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0313293&amp;ref=werd.io">hate speech surged</a> — and the rest is history.</p><p>I’ve been given the reins. I’m CEO now. Can it be saved?</p><p>Throughout this tumult, X’s active user numbers have remained remarkably steady. 2024’s 335.7M monthly active users represented a 9% drop from 2022, the year Musk acquired Twitter. TechCrunch recently reported that <a href="https://techcrunch.com/2025/07/07/threads-is-nearing-xs-daily-app-users-new-data-shows/?ref=werd.io">its web views remain vastly higher than new competitors like Threads and Bluesky</a>. Famously, many brands and journalists have been reticent to leave the site, despite its near-constant onslaught of white supremacy and hate speech. In other words: the traffic is there. These users may be a mix of bots, literal Nazis, and people who don’t mind posting on a site that is heavily used by literal Nazis, but there are hundreds of millions of them. So that’s something.</p><p>On the other hand, <a href="https://finance.yahoo.com/news/x-just-revealed-glimpse-dramatic-160249796.html?ref=werd.io">its revenue tanked following the acquisition</a>, with some reports showing a drop as big as 66%. More recently, it saw 17.5% growth between 2024 and 2025, but it needs to claw back a lot of ground if it wants to return to its 2021 revenue peak — and even then, <a href="https://sites.lsa.umich.edu/mje/2022/12/21/twitter-and-its-fight-for-profitability/?ref=werd.io">it posted a loss for most of the decade that preceded it</a>.</p><p>In short: its online culture is toxic, its finances are decimated, and morale at the company is through the floor. But there is a great deal of remaining traffic, and many stalwarts have refused to give up followings they spent over a decade building.</p><p>If we want to turn it around, the only place it’s possible to start is the culture of the company itself. Everything else flows from there.</p><p>First, let’s address the ketamine in the room: there is no possible cultural turnaround at X while Elon Musk owns the site or has a significant stake. It must be acquired from him. That may seem far-fetched, but <a href="https://www.businessinsider.com/elon-musk-spacex-tesla-stock-stakes-value-net-worth-2025-3?ref=werd.io">much of his wealth is stored in Tesla stock</a>; in a world where <a href="https://www.thewrap.com/trump-musk-train-wreck-70-billion-tesla-stock/?ref=werd.io">his political machinations tank the price of his cash cow</a>, he may need liquidity through other means. <a href="https://www.cnn.com/2024/10/02/business/elon-musk-twitter-x-fidelity?ref=werd.io">X’s valuation had already declined by 80%</a>; <a href="https://www.reuters.com/markets/deals/musks-xai-buys-social-media-platform-x-45-billion-2025-03-28/?ref=werd.io">his own xAI acquired it at a relative discount</a>. As X’s culture continues to degrade, it may prove to be a drag on the valuation of his AI startup. The benefits of offloading it, in other words, may begin to outweigh the benefits to keeping it.</p><p>A coalition of hands-on investors would be needed to perform this acquisition, but consider the upside: a de-Nazified Twitter would be worth considerably more than its current incarnation. Musk had previously taken it private; the revived company could once again be floated on the stock market. The potential gains would be significant.</p><p>A newly-independent X would then be ripe for change.</p><p>Executives and managers who were loyal to Musk would need to be removed. It’s not a given that other employees were fully on board with his <a href="https://fortune.com/2022/11/16/elon-musk-email-twitter-extremely-hardcore-long-hours-high-intensity/?ref=werd.io">“extremely hardcore”</a>, hard-right ideology: some may have needed to remain to maintain their visas or to ensure a paycheck in a tough hiring environment. Internally, diversity policies and mandatory inclusion training would return. Protections for work-life balance would return, and remote work would once again be allowed, with a staged roll-out. Work would be undertaken to establish an internal culture of empathy, which is a hard requirement for building non-toxic communities; experts like <a href="https://leadersintech.org/team?ref=werd.io">Carole Robin</a> and <a href="https://vernamyers.com/?ref=werd.io">Vernā Myers</a> would be heavily drawn on for ongoing manager training and culture guidance, and experts like <a href="https://www.poetofcode.com/?ref=werd.io">Dr Joy Buolamwini</a> and <a href="https://www.dair-institute.org/team/?ref=werd.io">Timnit Gebru</a> would be drawn on to help the company be mindful the implications of ongoing work. This is the work of years, but the process would begin immediately.</p><p>To be clear, it’s not that employees would need to be left-wing. This isn’t an inquisition, and a company culture that only allows one political alignment is more harmful than good. But basic human decency would once again be a value: hate speech and bigotry would be explicitly disallowed, and it would be made obvious that the aim of Twitter’s platform is to create a community where voices from a diversity of communities should feel safe to post, which starts with internal cultural expectations.</p><p>Nazi accounts would need to be banned again, swiftly, with their posts preserved internally in a vault in a de facto legal hold. The messages would be made available as a dataset to organizations that research and investigate hate groups, although users would have the ability to have their content removed in accordance with relevant privacy legislation for their jurisdiction.</p><p>Significant investments would be made into non-profits and on-the-ground organizations that support the vulnerable communities harmed by the hate speech, doxxing, and other activities previously enabled on X. A commitment would be made for ongoing support: it couldn’t be a one-and-done gesture.</p><p>A permanent, inclusive trust and safety council would be established, modeled after the Supreme Court, with real, encoded power to affect company policies around content and moderation. This council and its permanence would be defined in the articles of association of the new company; it would be almost impossible to remove them. Its members would be guaranteed a salary (but could not own stock) and required to both adjudicate on content moderation issues and intervene during strategic lapses. The number of members of the council would increase based on active users but could never decrease.</p><p>Employees who objected to these measures would be free to take a buyout. Some would likely just quit. Meanwhile, the company would begin to aggressively hire across the board, including but not limited to engineering, product, research, design, partnerships, communications, and trust and safety. Public statements and heavy PR would signal the internal cultural changes in order to attract new applicants.</p><p>The name and brand would be symbolically restored to Twitter, indicating a clean break from the Musk years. It’s a trustworthy brand, and the change back would likely turn heads. But it would be a mistake to simply return the company to its old self. The new Twitter needs to be something else entirely.</p><p>It’s now clear that Musk’s acquisition was about amassing power, not providing value. <a href="https://www.axios.com/2024/11/07/elon-musk-twitter-takeover-math-changed?ref=werd.io">As Dan Primack wrote in Axios last year</a>:</p><blockquote>Musk used the platform to help shape public opinion during the election, artificially amplifying his political messages and giving him influence with Donald Trump that money alone wouldn't have bought.</blockquote><p>While this might seem like a selling point to a would-be autocrat, it serves as a weakness to Twitter as a business. Not only does this property make it vulnerable to takeovers from terrible people, but it elevates the company’s business decisions to a level where a mistake could significantly impact democracy or even public safety around the world. (This is also the lesson <a href="https://www.amnesty.org/en/latest/news/2022/09/myanmar-facebooks-systems-promoted-violence-against-rohingya-meta-owes-reparations-new-report/?ref=werd.io">Meta failed to learn in Myanmar</a>.) In turn, this could subject the company to greater regulatory scrutiny and oversight. It may seem like value at first sight, but it’s actually a business risk.</p><p>So how can the new Twitter be incredibly valuable but also safer from regulatory oversight?</p><p>The core value of Twitter is not its social network as such. That’s always been a means to an end. <a href="https://www.pewresearch.org/internet/2019/04/24/sizing-up-twitter-users/?ref=werd.io">The vast majority of content on the site has historically been created by just 10% of users</a>, but people don’t log in to post: <a href="https://www.pewresearch.org/internet/2021/11/15/1-the-views-and-experiences-of-u-s-adult-twitter-users/?ref=werd.io">they want to see what other people are saying</a>. The point of Twitter is to find out what’s going on right now and get an insight into the conversation around it.</p><p>My new Twitter would retain the social network but expand upon the idea that people want to know what’s happening right now. The feed would expand to not just include Twitter’s own conversations, but every public conversation across the social web. Search, trending topics, and live dashboards would represent the entire addressable real-time web, using open standards and protocols like AT Protocol, ActivityPub, and RSS. Twitter would become the way to find out what’s happening everywhere.</p><p>Some social networks — like Facebook — would not be available via this feed. The addressable social web would consist of sites that embrace open protocols (best), or otherwise make a content deal directly with Twitter (less good). However, a Twitter that aims to provide the pulse of the internet and not necessarily a social network in itself is less of a competitor to closed networks; in fact, the tools may provide new engagement and users for them. The aim would be, over time, to make not being represented on the feed more of a business liability for closed social networking platforms than being included. Twitter would provide libraries, ecosystem support, and even funding for open social web protocols: its value increases as the the social web pie gets bigger.</p><p>Monetization would be three-prong. Advertising would remain on public feeds, and a renewed focus on trust and safety would improve brand safety, bringing advertising prices back up. Twitter Blue premium profiles on the social network would remain. But most differently, enterprise tools for tracking public sentiment and yielding unique insights from the global conversation would be made available for corporations, media companies, and governments. These tools would also be made available to public research institutions, with some restrictions, for free.</p><p>Using open protocols should help reduce regulatory risk. In this world, trust and safety is as much about de-listing external content as it is about policing the in-house social network. Aggregating across the entire social web is an enormous infrastructure task that requires real resources and expertise to carry out; the compliance demands across global jurisdictions are complex but fully within Twitter’s wheelhouse. The moat for the business is, in part, the combination of the difficulty of building the tools and their ease of use for end users. In-house expertise would create proprietary machine learning algorithms for sentiment and insight analysis, and integrations with third-party tools like CRMs and analytics platforms could embed the platform in existing enterprise ecosystems. It’s a very similar model to how Google originally became the dominant search engine: simplicity of use combined with the sophistication of its internal technology, all layered over the open web.</p><p>It’s not even particularly unprecedented: Blaine Cook, then CTO of Twitter, <a href="https://x.com/blaine/status/1460182020194279424?ref=werd.io">was fired for demonstrating interoperability with Jaiku, another social network</a>. For this new Twitter, interoperability would be core to its value.</p><p>This transformation wouldn’t happen overnight. A still-aggressive but more realistic timeline might look like:</p>  Culture change: biggest changes in the first six months, then ongoing  Platform redevelopment and rebuilding the team: 1-2 years  Building up aggregation and partnerships: 2-3 years  <p>Once established, the new Twitter would set the tone for the social web itself. By implication, it wouldn’t just be the way that conversations were discovered; people would find platforms this way, too. By not hyper-focusing on its own social network, it has the potential to become a much bigger and more valuable company: one that people use to unlock the entire online social conversation. By using open protocols, the company ensures the whole internet isn’t locked into its services, but by caring for trust and safety, design, and the quality of its feeds, it provides best-in-class options for people to choose. And it fosters a bigger, more interesting social web in the process.</p><p>Does X or Twitter actually need to be saved? Absolutely not. There are plenty of new social networks that are more interesting, more fun to use, and show more promise. But there’s interesting potential here, both to remove a major source of toxicity on the internet, to diffuse a centralized source of power that can be abused by autocrats and oligarchs, and to create a way to elevate the entire social web.</p><p>So that’s what I would do if I was CEO.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://images.unsplash.com/photo-1690563762819-bf3ae53ab662?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDV8fHR3aXR0ZXJ8ZW58MHx8fHwxNzUyMDc3ODk3fDA&ixlib=rb-4.1.0&q=80&w=2000'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   13:54 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/10/i-hope-they-tell-him.html'> I hope they tell him where to get off - and real quick. 
 It</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   I hope they tell him where to get off - and real quick. 
 It would prove they have more gumption than corporate America. 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/3bd284c566.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I hope they tell him where to get off - and real quick.</p>
<p>It would prove they have more gumption than corporate America.</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/3bd284c566.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   05:36 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/10/perplexity-promoting-its-new-browser.html'> Perplexity promoting its new browser on the Nazi’s social me</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Perplexity promoting its new browser on the Nazi’s social media platform. 
 I was thinking of trying it out. Not now. 
 Maybe time to start calling out software companies that promote I’m Muskland rather than all the other places they could. 
 Like HERE! 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/620eefd2f9.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Perplexity promoting its new browser on the Nazi’s social media platform.</p>
<p>I was thinking of trying it out. Not now.</p>
<p>Maybe time to start calling out software companies that promote I’m Muskland rather than all the other places they could.</p>
<p>Like HERE!</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/620eefd2f9.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   01:01 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 09. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2742'>JOSE and COSE HPKE specifications updated in preparation for IETF 123</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The working group last calls for the JOSE and COSE Hybrid Public Key Encryption (HPKE) specifications resulted in actionable feedback on both specs. Both were updated to incorporate the feedback when the actions to take were clear. That said, I expect substantive discussions to occur on the few remaining issues for both specifications at IETF […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The <a href="https://self-issued.info/?p=2721">working group last calls for the JOSE and COSE Hybrid Public Key Encryption (HPKE) specifications</a> resulted in actionable feedback on both specs.  Both were updated to incorporate the feedback when the actions to take were clear.  That said, I expect substantive discussions to occur on the few remaining issues for both specifications at <a href="https://www.ietf.org/meeting/123/">IETF 123 in Madrid</a>.</p>
<p>The current versions are:</p>
 
 <a href="https://www.ietf.org/archive/id/draft-ietf-jose-hpke-encrypt-11.html">draft-ietf-jose-hpke-encrypt-11</a> 
 <a href="https://www.ietf.org/archive/id/draft-ietf-cose-hpke-15.html">draft-ietf-cose-hpke-15</a> 
 
<p>The specifications entering WGLC together were:</p>
 
 <a href="https://www.ietf.org/archive/id/draft-ietf-jose-hpke-encrypt-08.html">draft-ietf-jose-hpke-encrypt-08</a> 
 <a href="https://www.ietf.org/archive/id/draft-ietf-cose-hpke-13.html">draft-ietf-cose-hpke-13</a> 
 
<p>Thanks to the work that <a href="https://www.linkedin.com/in/or13b/">Orie Steele</a>, Hannes Tschofenig, and <a href="https://www.linkedin.com/in/tirumaleswar-reddy-k-2a915946/">Tirumal Reddy</a> put in over the past weeks to get us ready for IETF 123!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:56 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2740'>&#8220;Split Signing Algorithms for COSE&#8221; and &#8220;ARKG&#8221; updated in preparation for IETF 123</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Emil Lundberg and I have published the Split Signing Algorithms for COSE specification. This is an update to the spec formerly called COSE Algorithms for Two-Party Signing. The new draft incorporates feedback received during IETF 122, preparing for discussions at IETF 123 in Madrid. As recorded in the History entries, the changes made were: Renamed […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Emil Lundberg and I have published the <a href="https://datatracker.ietf.org/doc/draft-lundberg-cose-two-party-signing-algs/">Split Signing Algorithms for COSE</a> specification.  This is an update to the spec formerly called <a href="https://self-issued.info/?p=2621">COSE Algorithms for Two-Party Signing</a>.  The new draft incorporates feedback received during IETF 122, preparing for discussions at <a href="https://www.ietf.org/meeting/123/">IETF 123 in Madrid</a>.</p>
<p>As recorded in the <a href="https://www.ietf.org/archive/id/draft-lundberg-cose-two-party-signing-algs-02.html#name-document-history">History entries</a>, the changes made were:</p>
 
 Renamed document from “COSE Algorithms for Two-Party Signing” to “Split signing algorithms for COSE” and updated introduction and terminology accordingly. 
 Dropped definitions for HashML-DSA, as split variants of ML-DSA are being actively discussed in other IETF groups. 
 Changed “Base algorithm” heading in definition tables to “Verification algorithm”. 
 Remodeled COSE_Key_Ref as COSE_Sign_Args. 
 Dropped definitions of reference types for COSE Key Types registry. 
 
<p>Emil also published an update to the <a href="https://datatracker.ietf.org/doc/draft-bradleylundberg-cfrg-arkg/">Asynchronous Remote Key Generation (ARKG)</a> specification, with some assistance from me.  See the <a href="https://www.ietf.org/archive/id/draft-lundberg-cose-two-party-signing-algs-02.html#name-document-history">History entries</a> there for details of the updates made.  Some of the changes made were for alignment with the Split Signing Algorithms specification.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:45 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2737'>Major updates to JSON Web Proof specifications in preparation for IETF 123</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  David Waite and I made significant updates to the JSON Web Proof, JSON Proof Algorithms, and JSON Proof Token and CBOR Proof Token specifications in preparation for presentation and discussions in the JOSE working group at IETF 123 in Madrid. The most significant updates were: Changed the Single Use algorithm representations to use a common […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://www.linkedin.com/in/davidwaite/">David Waite</a> and I made significant updates to the <a href="https://datatracker.ietf.org/doc/draft-ietf-jose-json-web-proof/">JSON Web Proof</a>, <a href="https://datatracker.ietf.org/doc/draft-ietf-jose-json-proof-algorithms/">JSON Proof Algorithms</a>, and <a href="https://datatracker.ietf.org/doc/draft-ietf-jose-json-proof-token/">JSON Proof Token and CBOR Proof Token</a> specifications in preparation for presentation and discussions in the <a href="https://datatracker.ietf.org/wg/jose/about/">JOSE working group</a> at <a href="https://www.ietf.org/meeting/123/">IETF 123 in Madrid</a>.  The most significant updates were:</p>
 
 Changed the Single Use algorithm representations to use a common presentation proof format for both the Compact and CBOR serializations. 
 Defined a new binary “Presentation Internal Representation” so that the holder signature protects the entire presentation. 
 Changed the MAC algorithm to directly sign the binary Combined MAC Representation rather than convert it to a JWS. 
 Added step-by-step instructions for verification of a presentation. 
 Added CBOR examples. 
 Use JSON Proof Token and CBOR Proof Token terminology. 
 Aligned media type names and added media type suffixes. 
 Removed the JSON Serialization (leaving the Compact Serialization and the CBOR Serialization). 
 Made terminology changes to make the meanings of terms more intuitive. 
 
<p>These changes went into the -09 and -10 drafts of <a href="https://github.com/ietf-wg-jose/json-web-proof">the specifications</a>.  See more details in the History entries of each spec.</p>
<p>The current drafts are available at:</p>
 
 <a href="https://www.ietf.org/archive/id/draft-ietf-jose-json-web-proof-10.html">JSON Web Proof</a>
  <a href="https://www.ietf.org/archive/id/draft-ietf-jose-json-proof-algorithms-10.html">JSON Proof Algorithms</a> 
 <a href="https://www.ietf.org/archive/id/draft-ietf-jose-json-proof-token-10.html">JSON Proof Token and CBOR Proof Token</a> 
 
<p>Thanks to <a href="https://www.linkedin.com/in/davidwaite/">David Waite</a> for doing the heavy lifting to make the bulk of these architectural changes, and especially for writing the code that makes the examples real!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:31 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2734'>More SPICEyness</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  In April, I wrote about several useful developments in the IETF Secure Patterns for Internet CrEdentials (SPICE) working group. I’ve recently contributed to progressing several specifications in preparation for the SPICE working group meeting at IETF 123 in Madrid. Here’s a tour… I’ve become a contributor to the Selective Disclosure CWT (SD-CWT) specification. The draft […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>In April, <a href="https://self-issued.info/?p=2650">I wrote about several useful developments</a> in the IETF <a href="https://datatracker.ietf.org/wg/spice/about/">Secure Patterns for Internet CrEdentials (SPICE)</a> working group.  I’ve recently contributed to progressing several specifications in preparation for the SPICE working group meeting at <a href="https://www.ietf.org/meeting/123/">IETF 123 in Madrid</a>.  Here’s a tour…</p>
<p>I’ve become a contributor to the <a href="https://datatracker.ietf.org/doc/draft-ietf-spice-sd-cwt/">Selective Disclosure CWT (SD-CWT)</a> specification.  The <a href="https://www.ietf.org/archive/id/draft-ietf-spice-sd-cwt-04.html">draft we just published</a> in preparation for IETF 123 contains significant enhancements, including better alignment with both <a href="https://datatracker.ietf.org/doc/draft-ietf-oauth-selective-disclosure-jwt/">SD-JWT</a> and <a href="https://www.rfc-editor.org/rfc/rfc8392">CWT</a>, clearer and simpler specification of the use of encryption, creation of the Verifiable Credential Type Identifiers registry, using a CBOR simple value for redacted claims, and numerous editorial improvements.  See the <a href="https://www.ietf.org/archive/id/draft-ietf-spice-sd-cwt-04.html#name-draft-ietf-spice-sd-cwt-04">history entry</a> for more details.  This was joint work with <a href="https://www.linkedin.com/in/rohan-mahy-70985/">Rohan Mahy</a> and <a href="https://www.linkedin.com/in/or13b/">Orie Steele</a>.</p>
<p>I’ve become an editor of the <a href="https://datatracker.ietf.org/doc/draft-ietf-spice-oidc-cwt/">OpenID Connect Standard Claims Registration for CBOR Web Tokens</a> specification, along with <a href="https://www.linkedin.com/in/beltram-%F0%9F%A6%80-maldant-2930b880/">Beltram Maldant</a>.  It creates <a href="https://www.rfc-editor.org/rfc/rfc8392">CWT</a> equivalents of the <a href="https://openid.net/specs/openid-connect-core-1_0.html#StandardClaims">standard JWT claims defined by OpenID Connect</a>.  The <a href="https://www.ietf.org/archive/id/draft-ietf-spice-oidc-cwt-01.html">draft we just published</a> in preparation for IETF 123 aligns the terminology used with <a href="https://openid.net/connect">OpenID Connect</a>.  I believe it’s ready for working group last call.</p>
<p><a href="https://www.linkedin.com/in/bzundel/">Brent Zundel</a> and I updated the <a href="https://datatracker.ietf.org/doc/draft-ietf-spice-glue-id/">GLobal Unique Enterprise (GLUE) Identifiers</a> specification to fix some links and update his association to <a href="https://tradeverifyd.com/">Tradeverifyd</a>.  I believe this one is also ready for working group last call.</p>
<p>Finally, Brent and I updated the <a href="https://datatracker.ietf.org/doc/draft-prorock-spice-cwt-traceability-claims/">Traceability Claims</a> specification to tighten up many of the claim definitions.  See the <a href="https://www.ietf.org/archive/id/draft-prorock-spice-cwt-traceability-claims-02.html#name-document-history">history entries</a> for details.</p>
<p>I’m looking forward to continued progress at the SPICE meeting in two weeks!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:12 &bull;
   18 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/what-is-a-principal-engineer-at-amazon'>What is a Principal Engineer at Amazon? With Steve Huynh</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Former Amazon Principal Engineer Steve Huynh shares what it takes to reach the Principal level, why the jump Principal is so tough at Amazon, and how Amazon’s scale and culture shaped his career.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Stream the Latest Episode <p>Listen and watch now on <a href="https://youtu.be/vZGycBUc1vM">YouTube</a>,  <a href="https://open.spotify.com/episode/4kAEWb0N1HEVpbVM8WvLwg">Spotify</a> and <a href="https://podcasts.apple.com/us/podcast/the-pragmatic-engineer/id1769051199">Apple</a>. See the episode transcript at the top of this page, and timestamps for the episode at the bottom.</p> Brought to You By   <p><a href="http://statsig.com/pragmatic">Statsig</a> <a href="https://modal.com/pragmatic">⁠</a> — <a href="https://modal.com/pragmatic">⁠</a> The unified platform for flags, analytics, experiments, and more.</p>  <p><a href="https://gt.dev/pragmatic">Graphite</a> — The AI developer productivity platform.</p>  <p><a href="http://augmentcode.com/pragmatic">Augment Code</a> — AI coding assistant that pro engineering teams love.</p>  <p>—</p> In This Episode <p><a href="https://www.linkedin.com/in/a-life-engineered/">Steve Huynh</a> spent 17 years at Amazon, including four as a Principal Engineer. While in Seattle, I stopped by at Steve in his studio to record this episode of The Pragmatic Engineer. We went into what the Principal role involves at Amazon, why the path from Senior to Principal is so tough, and how even strong engineers can get stuck. Not because they’re unqualified, but because the bar is exceptionally high.</p><p>We discuss what’s expected at the Principal level, the kind of work that matters most, and the trade-offs that come with the title. Steve also shares how Amazon’s internal policies shaped his trajectory, and what made the Principal Engineer community one of the most rewarding parts of his time at the company.</p><p>We also go into:</p>  <p>Why being promoted from Senior to Principal at Amazon is one of the hardest jumps in tech</p>  <p>How Amazon’s freedom of movement policy helped Steve work across multiple teams, from Kindle to Prime Video</p>  <p>The scale of Amazon: handling 10k–100k+ requests per second and what that means for engineering</p>  <p>Why latency became a company-wide obsession at Amazon —and the research that tied it directly to revenue</p>  <p>Why companies should start with a monolith, and what led Amazon to adopt microservices</p>  <p>What makes the Principal Engineering community so special</p>  <p>Amazon’s culture of learning from its mistakes, including COEs (correction of errors)</p>  <p>The pros and cons of the Principal Engineer role</p>  <p>What Steve loves about the leadership principles at Amazon</p>  <p>Amazon’s intense writing culture and 6-pager format</p>  <p>Why Amazon patents software and what that process looks like</p>  <p>… and much more!</p>       An interesting topic: "brownouts” at Amazon <p>“Brownout” is internal Amazon lingo. At Amazon’s scale, service failures are frequent, and cascading failures can happen if dumping load onto services in a “brownout” state. Steve explained what this means, and why it was important at the e-commerce giant:</p><blockquote><p>Gergely (at <a href="https://youtu.be/vZGycBUc1vM?si=FUOpyvFQoBqzqSNA&amp;t=717">11:56</a>): What does “brownout” mean?</p><p>Steve: I'm using some jargon. Suppose you are <a href="https://www.cloudflare.com/en-gb/learning/ddos/what-is-a-ddos-attack/">DDoS’ing</a> a service or sending a lot of requests over to them: you can just take them down! That would be a blackout. With a blackout: yo you send a request, you can't establish a connection, it immediately comes back as failed. </p><p>But there's a type of outage where they ‘brown out’. So the service reachable, they might accept the connection, but they'll time out or they might return partial results or bad results. Or perhaps the only thing that they do return is a 500 for some percentage of requests.</p><p>So now we start talking about availability and resilience in the face of all of this DDoSing that you're doing to yourself. Let’s say your service is a dependency of some of the process that's going on. </p><p>If there's a failure for a primary dependency and that dependency comes back up: how do you make sure you don't just inundate it with a bunch of requests as it's trying to recover? And so now you have all of these sort of odd dynamics that occur. I used a brownout as something that is recurring problem. There might be some increased latency that may cause a chain reaction of a dependency going down. And then one of these sort of middle tier services would brown out. So you're an owner of the services for your team. And so then it's like, okay, what do we do in those situations? </p><p>How do we know that they're browning out? What do we do in the face of a dependency outage? And then critically, if there is an outage and then the service comes back up: </p><p>How do we make sure that we give it enough space so that [the service] can ‘breathe’? So that as they're trying to recover from some sort of outage, we don't just take them down immediately again.</p></blockquote><p>What Steve describes reminded me of what the Cursor engineering team described as the “<a href="https://newsletter.pragmaticengineer.com/i/165641889/cold-start-problem-at-scale">Cold start problem at scale</a>” in the deepdive <a href="https://newsletter.pragmaticengineer.com/p/cursor">How Cursor is built</a>:</p><blockquote><p>An unappreciated challenge is how hard it is to do a “cold start” for a massive service. As Sualeh [Cursor cofounder] explains:</p><p>“Imagine you’re doing 100,000 requests per second and suddenly, all your nodes die. When restarting your system, your nodes usually go up one after the other. Say you’ve managed to restart 10 nodes from a fleet of 1,000. If you don’t prohibit people from making requests, these 10 nodes will get smashed by all the incoming requests. Before these 10 nodes could have become healthy, you’ve just overloaded those nodes!</p><p>This has bitten us many times in the past. Whenever you have a bad incident that needs a cold start, you need to figure out how to do it well.</p><p>Many of the large providers you probably use have various ‘tricks’ to kill traffic while they perform a cold start. We ended up doing a setup where we either fully halt traffic until the cold start is complete, or prioritize a small subset of our users during a cold start, until the service is back at being healthy.”</p></blockquote> The Pragmatic Engineer deepdives relevant for this episode   <p><a href="https://newsletter.pragmaticengineer.com/p/amazon">Inside Amazon’s engineering culture</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/engineering-career-paths">Engineering career paths at Big Tech and scaleups</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/engineering-leadership-skillset-overlaps">Engineering leadership skill set overlaps</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa">How Big Tech does Quality Assurance (QA)</a></p>   Timestamps <p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM">00:00</a>) Intro</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=71s">01:11</a>) What Steve worked on at Amazon, including Kindle, Prime Video, and payments</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=278s">04:38</a>) How Steve was able to work on so many teams at Amazon</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=552s">09:12</a>) An overview of the scale of Amazon and the dependency chain</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=1000s">16:40</a>) Amazon’s focus on latency and the tradeoffs they make to keep latency low at scale</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=1560s">26:00</a>) Why companies should start with a monolith</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=1604s">26:44</a>) The structure of engineering at Amazon and why Amazon’s Principal is so hard to reach</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=1844s">30:44</a>) The Principal Engineering community at Amazon</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=2166s">36:06</a>) The learning benefits of working for a tech giant</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=2324s">38:44</a>) Five challenges of being a Principal Engineer at Amazon</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=2990s">49:50</a>) The types of managing work you have to do as a Principal Engineer</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=3107s">51:47</a>) The pros and cons of the Principal Engineer role</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=3299s">54:59</a>) What Steve loves about Amazon’s leadership principles</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=3555s">59:15</a>) Amazon’s intense focus on writing</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=3671s">1:01:11</a>) Patents at Amazon</p><p>(<a href="https://www.youtube.com/watch?v=vZGycBUc1vM&amp;t=4078s">1:07:58</a>) Rapid fire round</p> References <p>Where to find Steve Huynh:</p><p>• <a href="https://x.com/alengineered">X</a></p><p>• <a href="https://www.linkedin.com/in/a-life-engineered/">LinkedIn</a></p><p>• <a href="https://www.youtube.com/c/ALifeEngineered/">YouTube</a></p><p>• Steve's course: <a href="https://go.alifeengineered.com⁠">Speedrun to Promotion</a></p><p>• Newsletter: </p>  <a href="https://alifeengineered.substack.com?utm_source=substack&amp;utm_campaign=publication_embed&amp;utm_medium=web">A Life Engineered Your weekly dose of practical and realistic tech career advice and insights. Leverage the combined wisdom of industry leaders and my 20 years of experience navigating the tech landscape to unlock your full potential.  By Steve Huynh </a>  <p>Mentions during the episode:</p><p>• Building Reddit’s iOS and Android app: <a href="https://newsletter.pragmaticengineer.com/p/building-reddits-ios-and-android">https://newsletter.pragmaticengineer.com/p/building-reddits-ios-and-android</a></p><p>• Casey Muratori’s website: <a href="https://caseymuratori.com">https://caseymuratori.com</a></p><p>• Ethan Evans on LinkedIn: <a href="https://www.linkedin.com/in/ethanevansvp/">https://www.linkedin.com/in/ethanevansvp/</a></p><p>• Why you should develop a correction of error (COE): <a href="https://aws.amazon.com/blogs/mt/why-you-should-develop-a-correction-of-error-coe/">https://aws.amazon.com/blogs/mt/why-you-should-develop-a-correction-of-error-coe/</a></p><p>• Bhavik Kothari on LinkedIn: <a href="https://www.linkedin.com/in/bhavik-kothari-5768b42a/">https://www.linkedin.com/in/bhavik-kothari-5768b42a/</a></p><p>• Bhavik’s LinkedIn Posts about Principal Engineering challenges Part 1: <a href="https://www.linkedin.com/posts/bhavik-kothari-5768b42a_some-obvious-and-not-so-obvious-challenges-activity-7303872281674465281-s5Mc?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAABf37PYBgozFf00ihr4fkqjRtMnFajHkQ5E">https://www.linkedin.com/posts/bhavik-kothari-5768b42a_some-obvious-and-not-so-obvious-challenges-activity-7303872281674465281-s5Mc?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAABf37PYBgozFf00ihr4fkqjRtMnFajHkQ5E</a></p><p>• Bhavik’s Principal Engineering challenges Part 2: <a href="https://www.linkedin.com/posts/bhavik-kothari-5768b42a_principal-engineer-challenges-continued-activity-7309228144324972544-_eUF?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAABf37PYBgozFf00ihr4fkqjRtMnFajHkQ5E">https://www.linkedin.com/posts/bhavik-kothari-5768b42a_principal-engineer-challenges-continued-activity-7309228144324972544-_eUF?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAABf37PYBgozFf00ihr4fkqjRtMnFajHkQ5E</a></p><p>• Leadership Principles: <a href="https://www.amazon.jobs/content/en/our-workplace/leadership-principles">https://www.amazon.jobs/content/en/our-workplace/leadership-principles</a></p><p>• Perl: <a href="https://www.perl.org/">https://www.perl.org/</a></p><p>• Rust: <a href="https://www.rust-lang.org/">https://www.rust-lang.org/</a></p><p>• Java: <a href="https://www.javascript.com/">https://www.javascript.com/</a></p><p>• So Good They Can't Ignore You: Why Skills Trump Passion in the Quest for Work You: <a href="https://www.amazon.com/Good-They-Cant-Ignore-You/dp/1455509124">https://www.amazon.com/Good-They-Cant-Ignore-You/dp/1455509124</a></p><p>• AI Engineering: Building Applications with Foundation Models: <a href="https://www.amazon.com/AI-Engineering-Building-Applications-Foundation/dp/1098166302">https://www.amazon.com/AI-Engineering-Building-Applications-Foundation/dp/1098166302</a></p><p>• AI Engineering with Chip Huyen: <a href="https://newsletter.pragmaticengineer.com/p/ai-engineering-with-chip-huyen">https://newsletter.pragmaticengineer.com/p/ai-engineering-with-chip-huyen</a></p><p>• Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems: <a href="https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321">https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321</a></p><p>• Inside Amazon's Engineering Culture: <a href="https://newsletter.pragmaticengineer.com/p/amazon">https://newsletter.pragmaticengineer.com/p/amazon</a></p><p>• A Day in the Life of a Senior Manager at Amazon: <a href="https://newsletter.pragmaticengineer.com/p/a-day-in-the-life-of-a-senior-manager">https://newsletter.pragmaticengineer.com/p/a-day-in-the-life-of-a-senior-manager</a></p><p>—</p><p>Production and marketing by <a href="https://penname.co/">Pen Name</a>. </p><p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!ygmM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32d706ec-1c20-4867-8425-30e9c28ca598_800x800.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   18:44 &bull;
   19 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2731'>OpenID Connect RP Metadata Choices is an Implementer&#8217;s Draft</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I’m happy to report that the OpenID Connect Relying Party Metadata Choices specification has been approved by the OpenID Foundation membership as an Implementer’s Draft. An Implementer’s Draft is a stable version of a specification providing intellectual property protections to implementers of the specification. The need for this was independently identified by Roland Hedberg and […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I’m happy to report that the <a href="https://openid.net/specs/openid-connect-rp-metadata-choices-1_0.html">OpenID Connect Relying Party Metadata Choices</a> specification has been <a href="https://openid.net/openid-connect-relying-party-metadata-choices-implementers-draft-approved/">approved by the OpenID Foundation membership as an Implementer’s Draft</a>.  An Implementer’s Draft is a stable version of a specification providing intellectual property protections to implementers of the specification.</p>
<p>The need for this was independently identified by <a href="https://www.linkedin.com/in/rolandhedberg/">Roland Hedberg</a> and <a href="https://www.linkedin.com/in/stefan-santesson-951890101/">Stefan Santesson</a> while implementing <a href="https://openid.net/specs/openid-federation-1_0.html">OpenID Federation</a>.  The contents of the specification were also validated by <a href="https://www.linkedin.com/in/filipskokan/">Filip Skokan</a>, who implemented it.  Filip has been added as an author.</p>
<p>The abstract of the specification is:</p>
<blockquote><p>
This specification extends the OpenID Connect Dynamic Client Registration 1.0 specification to enable RPs to express a set of supported values for some RP metadata parameters, rather than just single values. This functionality is particularly useful when Automatic Registration, as defined in OpenID Federation 1.0, is used, since there is no registration response from the OP to tell the RP what choices were made by the OP. This gives the OP the information that it needs to make choices about how to interact with the RP in ways that work for both parties.
</p></blockquote>
<p>Thanks to all who contributed to reaching this milestone!</p>
   
  
    
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/openid-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   18:07 &bull;
   19 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 08. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/skills-useful-to-learn-for-robotics'>Skills useful to learn for robotics engineering</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Helpful software engineering, AI engineering, and robotics fundamentals to know for getting into robotics. Also: advice about studying this exciting discipline at university
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Robotics is a very hot industry, and today, the hottest place within it is humanoid robotics. We previously published two deepdives on this topic with Sandor Felber, who’s been a Robot Learning Researcher at MIT, and a Robotics R&amp;D Engineer at Tesla in California, among other roles. The articles cover relevant topics at the intersection of AI, robotics, and software engineering. Earlier this year, Sandor has cofounded <a href="https://www.nyrohumanoids.com/">Nyro Humanoids</a>, an early-stage startup headquartered in San Francisco that builds intelligence for humanoid robots to deploy them in rugged environments.</p><p>In the third and final deepdive in this series, we take a close look at skills useful for joining this field, covering:</p>  <p>Software engineering skills</p>  <p>AI skills</p>  <p>Robotics fundamentals</p>  <p>Advice for studying a Master’s in Robotics</p>  <p>Influential perspectives</p>  <p>Why get into robotics?</p>  <p>Previous issues cover:</p><p><a href="https://newsletter.pragmaticengineer.com/p/robotics">Robotics basics for software engineers</a> (part 1):</p>  <p>Getting into robotics</p>  <p>Robotics industry overview</p>  <p>Planning a robotics project</p>  <p>Development phase</p>  <p>Tech stack and tools (and how Python, C, C++, and Rust are popular)</p>  <p>Testing, demoing, and shipping</p>  <p>Day-to-day as a robot builder</p>  <p><a href="https://newsletter.pragmaticengineer.com/p/humanoid-robots">Robotics for software engineers: humanoid robots</a> (part 2):</p>  <p>Why the humanoid form?</p>  <p>Hardware challenges</p>  <p>Software engineering challenges</p>  <p>Show me the code! Real-time robotics optimization</p>  <p>Real-world optimization techniques</p>  <p>How AI is changing robotic optimization</p>  <p>With this, it’s over to Sandor:</p>   <p>We're standing on the threshold of a robotics revolution. Just as OpenAI's ChatGPT “moment” transformed how we think about artificial intelligence (AI), the robotics industry is approaching its own breakthrough. This looks less like a step-like change, and more of a gradual transformation – one that will fundamentally change how we approach physical AI.</p><p>At Nyro Humanoids, we're creating the intelligence that powers humanoid systems capable of operating where humans cannot – or should not – go. From disaster response missions to save lives, to potentially dangerous construction sites, and toxic industrial environments that require hazardous activities which can put health at risk, our autonomous humanoid robots represent the cutting edge of what we call ‘physical AI’.</p><p>Our mission is to deploy intelligent humanoid robots in high-risk environments to protect human life and expand the boundaries of what's possible. Whether it's navigating collapsed buildings during search and rescue operations, handling hazardous materials, or operating in challenging conditions, we are developing the cognitive capabilities that enable robots to think, adapt, and act autonomously when every second counts.</p><p>The same breakthroughs that have revolutionized language models are now being applied to physically embodied intelligence. There are computers with arms and legs – robots! – which can understand their environment, make complex decisions, and execute precise physical actions in real time.</p><p>What follows is a comprehensive guide to the skills, technologies, and mindset that I’ve developed on my journey. Whether you're a software engineer looking to make the leap into robotics, a student considering the next move, or you’re simply curious about this rapidly-evolving field, this deepdive is a roadmap for becoming part of the robotics revolution.</p><p>The future isn't just about smarter software, it's about intelligence that can move, manipulate, and operate in the physical world. At Nyro Humanoids, we are building it one training run at a time – and <a href="https://www.nyrohumanoids.com/careers">we’re also hiring</a>.</p><p>Building robots is a multidisciplinary endeavour that blends pragmatic software engineering, AI expertise, and a deep understanding of robotics fundamentals. What follows is a breakdown of the key skills that have proven invaluable to me every day in engineering robotics software and hardware.</p> 1. Software engineering skills <p>Software, electrical, and mechanical engineering are the backbone of robotics. Let’s consider software engineering, where skills that prioritize performance, scalability, and reliability, are critical catalysts required to build robots that succeed in real-world applications. Depending on the kind of robotics you get into, some areas of interest might be:</p><p>Communication protocols, such as:</p>  <p><a href="https://en.wikipedia.org/wiki/CAN_bus">CAN</a> Bus (Controller Area Network Bus)</p>  <p>The <a href="https://simple.wikipedia.org/wiki/TCP/IP_model">TCP/IP</a> networking stack and peer-to-peer, or multiple node connections including <a href="https://en.wikipedia.org/wiki/Graph_theory">graph theory</a>. Both are important for designing robust communication systems between single and multi-robot systems, and their hardware and software components.</p>  <p><a href="https://wiki.ros.org/ROS/Introduction">ROS2 middleware</a>: Robot Operating System 2: a middleware acting as a meta-operating system for a robot and its controller, or a remote controller.</p>    <p>Performance optimization. Coding algorithms are efficient in power consumption by minimizing CPU, GPU and/or memory usage. One example of the necessity of performance optimization is batteries. With overly resource-intensive software, the CPU could require thermal throttling or else overheat. If your code isn’t efficient and the CPU draws lots of power, then the robot’s battery won’t last as long, and might require additional cooling heat sinks to be installed. Basically, high-performance and efficient code are must haves, not nice-to-haves, for real-world robotics.</p>  <p>Multithreading and multiprocessing: managing parallel processes in C/C++, Python, or Rust, for robotics systems, is crucial. Often, you may want to stream in two, or with high latency sensitivity.</p><p>Vectorization: leveraging parallelization in modern CPU/GPU architectures such as NVIDIA’s RTX <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/50-series/rtx-5090/">5090s</a> graphics card, with GPUs to speed up computationally-heavy tasks. Some pragmatic examples:</p>  <p>Using PyTorch or NumPy libraries to parallelize computations for more efficient resource usage</p>  <p>Significantly accelerating training and inference processes</p>  <p>Visualize trainings in real time to inspect the behavior of half-baked neural networks (often referred to as “policies” in robot learning)</p>  <p>CUDA and cuDNN: <a href="https://developer.nvidia.com/cuda-toolkit">CUDA</a> is NVIDIA’s parallel computing platform and API. <a href="https://developer.nvidia.com/cudnn">cuDNN</a> stands for CUDA Deep Neural Network. These frameworks allow for:</p>  <p>Using NVIDIA GPUs (the current market leader in the kind of workloads required for robot learning) to accelerate for deep learning use cases.</p>  <p>Making <a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping">SLAM</a> (Simultaneous Localization and Mapping) more efficient. It involves constructing and updating a map of an unknown environment, which is traditionally part of the stack for mobile robots.</p>  <p>Real-time robotics using parallel processing, tensor cores, and optimized <a href="https://www.datacamp.com/blog/what-is-machine-learning-inference">inference</a>, which is the process of using a trained model to make predictions or decisions based on new, unseen data – which you could think of as generating the next output tokens.</p>  <p>Here’s a plain-english cheat sheet for speeding up robot ML and onboarding:</p> <a href="https://substackcdn.com/image/fetch/$s_!6dVb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c76badd-fccf-4237-8135-e64dad8834e0_1014x1008.png">          </a> <p>Rules of thumb:</p>  <p>Start with <a href="https://onnx.ai/">ONNX</a> (Open Neural Network Exchange) if you want portability<br></p>  <p>Stick to <a href="https://docs.pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html">TorchScript</a> if you’re working fully using PyTorch<br></p>  <p>Use <a href="https://github.com/apache/tvm">Apache TVM</a> for weird chips<br></p>  <p>Use micro stacks like <a href="https://github.com/tensorflow/tflite-micro">TensorFlow Lite for Microcontrollers</a> (TFL-Micro), <a href="https://tvm.apache.org/docs/v0.9.0/topic/microtvm/index.html">microTVM</a> or <a href="https://utensor.github.io/website/">uTensor</a> for coin-cell robots (coin-cell robots are miniature robots powered by a flat, round battery cell called a coin cell)</p>  <p>Complexity analysis for resource-constrained devices. It’s necessary to ensure the coded algorithms can scale efficiently, as a system’s complexity expands to multiple tasks, or sets of tasks.</p><p>For example, if the model-based or learned controller (one that controls a robot using some sort of a neural network) requires 50ms to execute a small subset of potential tasks, it will probably be hard to scale it to process many other tasks, while maintaining a sufficiently high control frequency for agile ones. Control frequency is how often a robot's control system updates or executes its control loop. Being able to maintain control frequency while processing additional tasks is often related to robustness, agility, or speed-related metrics.</p> 2. AI skills <p>As mentioned above, robotics increasingly intersects with AI, and this is especially true of tasks that require autonomy, perception, and decision making. I personally found online resources from <a href="https://www.youtube.com/andrejkarpathy">Andrej Karpathy</a>, <a href="https://www.youtube.com/c/pieterabbeel">Pieter Abbeel</a>, and some other greats of robotics to be more useful than many books which quickly become obsolete in this rapidly transforming field – no pun intended. Areas it’s good to be proficient in:</p><p>Machine Learning (ML) basics: Core principles for training models and extracting insights from data. For more, check out our deepdive, <a href="https://newsletter.pragmaticengineer.com/p/the-machine-learning-toolset">The machine learning toolset</a>.</p><p>Data science and <a href="https://en.wikipedia.org/wiki/Probability_theory">probability theory</a>: both are used to understand uncertainty, and how to calculate and make probabilistic decisions. Much of robotics runs on uncertainty that must be tamed.</p><p>Decision-making systems and cognitive science: modelling behaviour, navigation, and task planning. <a href="https://en.wikipedia.org/wiki/Cognitive_science">Cognitive science</a> is the study of the mind and its processes, which can be highly relevant, especially when constructing humanoid robots.</p><p>Deep learning and representational learning: useful for developing perception systems for vision or audio. <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep learning</a> is a subset of machine learning utilizing neural networks for tasks like <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> and <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression analysis</a>. <a href="https://paperswithcode.com/task/representation-learning">Representational learning</a> is the process of extracting meaningful patterns from raw machines. This allows robots to develop useful abstractions for their environments and tasks. A book I enjoyed reading on multi-agent reinforcement learning is <a href="https://www.marl-book.com/">“Multi-Agent Reinforcement Learning: Foundations and Modern Approaches”.</a></p><p>Reinforcement learning (<a href="https://en.wikipedia.org/wiki/Reinforcement_learning">RL</a>) and <a href="https://en.wikipedia.org/wiki/Imitation_learning">imitation learning</a>: used to teach robots to learn optimal actions through trial and error and via human demonstrations. A good resource on this is <a href="https://spinningup.openai.com/en/latest/">Spinning Up </a>by OpenAI.</p><p><a href="https://assemblyai-new.webflow.io/blog/diffusion-models-for-machine-learning-introduction">Diffusion models</a> and multi-agent systems: Leveraging cutting-edge approaches for multi-robot collaboration and planning for more efficient routing and trajectories.</p><p><a href="https://huggingface.co/docs/optimum/en/concept_guides/quantization">Quantization</a> and <a href="https://medium.com/biased-algorithms/what-is-pruning-in-machine-learning-afcaab7fcbcf">pruning</a>: Reducing model size and inference latency by lowering precision (e.g., <a href="https://www.mathworks.com/company/technical-articles/what-is-int8-quantization-and-why-is-it-popular-for-deep-neural-networks.html">INT8 quantization</a>) and removing redundant weights for efficient deployment on edge devices. Quantization and pruning complement each other: prune redundant weights, then store the survivors in INT8 to slash model size and latency. Train with quantization-aware training, where every forward and backward pass mimics 8-bit math, so the network learns weight values and activation ranges that hold up after real quantization, giving a compact, edge-friendly model, with almost zero accuracy loss.</p><p>Note from Gergely: There are plenty of books and online resources on these topics, while search engines like Perplexity or Kagi can provide recommendations. For example, for the query:</p><p>“What are books on Diffusion models and multi-agent systems?”</p><p>The search engine <a href="https://www.perplexity.ai/search/what-are-books-on-diffusion-mo-0a0VWBDUQfGZDGKyFBhQmQ">returns several suggestions</a> that can be good starting points, if books are your thing. Search by the format you want. Full subscribers to Pragmatic Engineer get <a href="https://newsletter.pragmaticengineer.com/p/free-kagi-and-perplexity-access">a year of Perplexity Pro, and 3 months of Kagi Ultimate</a> for free.</p> 3. Robotics fundamentals <p>A solid grounding in mathematics, physics, and hands-on engineering, is non-negotiable for designing, building and deploying robots:</p>  <p>Advanced mathematics and physics: Kinematics, dynamics, thermodynamics, mechanisms, and electromechanics, energy systems, sensors, biomechanics, structural mechanics, power systems.</p>  <p>Realistic simulators: Proficiency in tools like Mujoco, IsaacSim, or PyBullet, to iterate before real-world deployment.</p>  <p>Signal processing and real-time filtering: Ensuring accurate sensor data acquisition, filtering, transmission, processing, and interpretation.</p>  <p>Systems engineering: Designing and integrating complex hardware-software architectures in a scalable way. Many projects fall victim to improper project management and lack of compartmentalization, which can make the debugger’s life very hard when hardware’s also in play.</p>  <p>Human-robot interaction: Building systems that operate effectively alongside humans, with appreciation for – and careful consideration of – how a robot is actually going to be used at deployment, how humans will use and potentially misuse it, and ensuring it’s foolproof.</p>  <p>These varied skills combine to overcome the inherent complexity of robotics. Each contributes to the ultimate goal of creating functional, scalable, and reliable robots that perform effectively in the real world.</p> 4. Advice for studying a Master’s in Robotics <p>Pursuing a postgraduate degree in robotics is a strategic move for mastering interdisciplinary skills, preparing for this rapidly-evolving field, and unlocking opportunities in academia, industry, and entrepreneurial ventures. Opting for university could be a worthwhile investment if you’re serious about getting involved, regardless of age. If that sounds appealing, I have some tips for making the most of it.</p>
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/skills-useful-to-learn-for-robotics">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!6dVb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c76badd-fccf-4237-8135-e64dad8834e0_1014x1008.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:04 &bull;
   20 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/the-future-of-forums-is-lies-i-guess/'>The Future of Forums is Lies, I Guess</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  "I do not have the time or emotional energy to screen out regular attacks by Large Language Models, with the knowledge that making the wrong decision costs a real human being their connection to a niche community."
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>[<a href="https://aphyr.com/posts/389-the-future-of-forums-is-lies-i-guess?ref=werd.io">Kyle Kingsbury</a>]</p><p>This account of how LLMs are used to create spam accounts across Mastodon servers is likely a glimpse into both the future of spam and the future of online communities.</p><blockquote>"In some sense, this is a wildly sophisticated attack. The state of NLP seven years ago would have made this sort of thing flatly impossible. It is now effective. There is no way for moderators to robustly deny these kinds of applications without also rejecting real human beings searching for community."</blockquote><p>Back in the old days, I ran communities that ran on classic forum software. Spam was an arms race then: a successful spammer could make a ton of money (and increase the search engine ranking of their clients' sites) by injecting links on community sites all over the web, so they were incentivized to get past any blockers we erected. These days, LLMs make these attacks a great deal more sophisticated - and a great deal more possible.</p><p>Then, as now, it was a numbers game. As Kyle points out:</p><blockquote>"These attacks do not have to be reliable to be successful. They only need to work often enough to be cost-effective, and the cost of LLM text generation is cheap and falling."</blockquote><p>The attack laid out here is fairly naive. It uses the same username and spams the same content across communities. That's actually an illustration of how low-effort it is. A higher-effort attack would see the posts customized for each community, with different usernames and profiles to fit the community's theme and culture. Those attacks are doubtless already here.</p><p>Attempts to spot LLM writing are not very reliable and have the potential to create false positives - not least because they probably rely on an LLM themselves, and therefore are prone to inaccuracies and bias. So for now this requires human moderation. That's potentially fine for a giant megacorporation (although even they struggle with the tide of AI-enabled spam accounts), but for individual, niche communities maintained by enthusiasts and hobbyists, that may be the beginning of the end.</p><p>[<a href="https://aphyr.com/posts/389-the-future-of-forums-is-lies-i-guess?ref=werd.io">Link</a>]</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   13:54 &bull;
   20 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/threads-is-nearing-xs-daily-app-users-new-data-shows/'>Threads is nearing X's daily app users, new data shows</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  It's wild to me how many people are still engaging with X.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>[<a href="https://techcrunch.com/2025/07/07/threads-is-nearing-xs-daily-app-users-new-data-shows/?ref=werd.io">Sarah Perez at TechCrunch</a>]</p><p>There's a graph in the middle of this article which is fairly sobering for those of us who are really hoping for Bluesky and Mastodon to succeed. In it, we see a gentle decline in X usage, a healthy increase in Threads usage, and a straight, slightly-declining line representing Bluesky usage that seems to stubbornly cling to the X axis.</p><p>For me, this is in contrast to the experience of actually using each network. Every so often, I'll dip into X to see what the discourse is like, but it's universally the most toxic feed I've ever seen. (I reset my account some time ago and don't follow anyone or have any followers.) My Threads feed has become unreadable, with a few really bright spots who prefer to post there. On Bluesky, I interact with some very smart and insightful people I like very much, and get a bunch of interactions from political posters who, while potentially human, are indistinguishable from bots.</p><p>The growth of Threads is, of course, tethered to Instagram: there are calls to action to sign up and to look at your Threads feed all over that app. Which is why its app usage is flourishing but its web usage is not; Instagram doesn't have much of a life on the web.</p><p>It's worth saying that the raw numbers are not the important thing: instead, it's worth considering the difference in interactions across networks. Here, Bluesky and Mastodon absolutely outshine the competition. People are much more likely to reshare and engage in conversation - or donate to non-profits - on those networks, and that counts for a lot.</p><p>[<a href="https://techcrunch.com/2025/07/07/threads-is-nearing-xs-daily-app-users-new-data-shows/?ref=werd.io">Link</a>]</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   13:28 &bull;
   20 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/08/nz-firm-vantaset-sets-sights.html'> 🔗 NZ firm Vantaset sets sights on UK, US law enforcement age</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   🔗 NZ firm Vantaset sets sights on UK, US law enforcement agencies 
 .. enjoy. 
 Particularly interesting to see that despite running a very VERY NZ centric campaign over the past month - the interest from UK, Germany and Australia has all exceeded local interest - at least based on the analytics on the web site. 
 A true reminder that no matter how local you think you are trying to be - the fact i
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://www.rnz.co.nz/news/business/565770/nz-firm-vantaset-sets-sights-on-uk-us-law-enforcement-agencies">🔗 NZ firm Vantaset sets sights on UK, US law enforcement agencies</a></p>
<p>.. enjoy.</p>
<p>Particularly interesting to see that despite running a very VERY NZ centric campaign over the past month - the interest from UK, Germany and Australia has all exceeded local interest - at least based on the analytics on the web site.</p>
<p>A true reminder that no matter how local you think you are trying to be - the fact is that we live in a global world.</p>
<p>It’s also interesting to see how those countries are more interested in exploring organisational performance and productivity from a long time Kiwi company than Kiwi organisations themselves seem to be. #Thinking</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   05:00 &bull;
   20 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 07. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/improved-grok-criticizes-democrats-and-hollywoods-jewish-executives/'>‘Improved’ Grok criticizes Democrats and Hollywood’s ‘Jewish executives’</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  As more people look to AI to learn about the world, the people who control how it's trained and how it responds will control our prevailing narratives. That's wildly dangerous.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://techcrunch.com/2025/07/06/improved-grok-criticizes-democrats-and-hollywoods-jewish-executives/?ref=werd.io">Link: Anthony Ha at TechCrunch</a>.</p><p>Elon Musk's definition of "improved" might be a little bit different to mine:</p><blockquote>"[...] another user asked whether there’s a “particular group that runs Hollywood that injects these subversive themes,” to which Grok replied, “Yes, Jewish executives have historically founded and still dominate leadership in major studios like Warner Bros., Paramount, and Disney. Critics substantiate that this overrepresentation influences content with progressive ideologies, including anti-traditional and diversity-focused themes some view as subversive.”"</blockquote><p>Musk tweaking his AI model to be more aligned with right-wing edgelords was inevitable, but there's a broader point to be made: each AI model is a black box that supposedly gives objective answers but in reality is shaped by its owners. As more people look to AI to learn about the world, the people who control how it's trained and how it responds will control our prevailing narratives.</p><p>That's a very dangerous future: one where it's impossible for democracy to properly function, because citizens cannot be properly informed. Given that it currently seems like AI models will likely be run by large multinational corporations that cross borders, rather than there being a tapestry of much smaller, more specific models, the potential for people to gain power that they can then abuse horribly is immense.</p><p>It just so happens that Elon Musk is a grotesque caricature of those kinds of power dynamics:</p><blockquote>"Even before these recent changes, Grok raised eyebrows after appearing to briefly censor unflattering mentions of Musk and his then-ally President Donald Trump, repeatedly bringing up “white genocide” without prompting, and expressing skepticism about the number of Jews killed in the Holocaust."</blockquote><p>But make no mistake, they're present in every AI company.</p><p>[<a href="https://techcrunch.com/2025/07/06/improved-grok-criticizes-democrats-and-hollywoods-jewish-executives/?ref=werd.io">Link</a>]</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:29 &bull;
   21 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/08/nice-to-be-back-in.html'> 📸 Nice to be back in the old ‘hood … 
</a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   📸 Nice to be back in the old ‘hood … 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/d7fa737df3.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>📸 Nice to be back in the old ‘hood …</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/d7fa737df3.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   12:10 &bull;
   21 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/07/233952.html'></a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/086509c196.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>


   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/086509c196.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   11:39 &bull;
   21 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2025/07/7237/'>【7月17日開催】MyData Japan 2025: 個人データ活用の「実践」を深掘り！進化するデータガバナンスの最前線</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  今年もMyDataJapanカンファレンスの季節がやってまいりました。「MyData Japan 2025 ~ MyData in Practice ~」は、個人を起点とした持続可能かつ倫理的なデータ活用のあり方を追求する、日本最大級のカンファレンスです。今年のテーマは「MyDa…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>今年もMyDataJapanカンファレンスの季節がやってまいりました。<a href="https://mydatajapan.org/events/mydata-japan-2025/">「MyData Japan 2025 ~ MyData in Practice ~」</a>は、個人を起点とした持続可能かつ倫理的なデータ活用のあり方を追求する、日本最大級のカンファレンスです。今年のテーマは「MyData in Practice」。2024年の「MyData by Design」で議論されたMyData原則の「設計」から一歩進んで、具体的な実装と運用に関する実践的な取り組みに焦点を当てます。国内外のMyData関連プロジェクトの事例発表に加え、多様なステークホルダー間の相互交流と協働・共創の機会を促進することを目的としています。わたくしも冒頭15分お話させていただきます。</p>



 今年のテーマ：「MyData in Practice」 



<p>2024年は「MyData by Design」をテーマに、MyDataの原則を制度や技術、ビジネスの設計にどう組み込むかを議論しました。2025年はその次のステップとして、設計を現実世界でどう機能させるかに注目し、国内の実例を通してMyDataの実装における可能性と課題を探ります。（なお、わたくしも冒頭で15分ほどお話をさせていただきます。）</p>



 今年の特徴 



<p>従来の事例発表に加え、参加者同士の相互交流を重視し、一般参加者の皆様にも懇親会にご参加いただけます。企業、行政機関、研究機関、非営利団体、教育機関、市民社会など多様な立場を越えた対話の場として活用いただければと思います。</p>



<p>個人を中心とする新たなデータ利活用の潮流を実社会に根づかせていくために、皆様のご参加をお待ちしております。チケットのお申し込みなどは<a href="https://mydatajapan.org/events/mydata-japan-2025/">オフィシャルサイト</a>からお願い致します。</p>



 開催概要 



日時：2025年07月17日(木)　10:00～18:45（9:40開場）会場：一橋講堂（<a href="http://www.hit-u.ac.jp/hall/accessjp.html">詳細</a>） 〒101-8439 東京都千代田区一ツ橋2-1-2 学術総合センター内（<a href="https://goo.gl/maps/4QgPgK8NRES2">GoogleMap</a>）定員：通常チケット（お弁当なし）：500名<br>通常チケット（お弁当あり）：100名<br>懇親会(19時～20時半)：50名主催：<a href="https://mydatajapan.org/">一般社団法人MyDataJapan</a>特別協力：<a href="http://privacybydesign.jp/">一般社団法人Privacy by Design Lab&nbsp;</a>後援：<br>※順次追加<a href="https://mydata.org/">MyData Global&nbsp;</a>



 主要テーマと目的 



<p>本カンファレンスの中心的なテーマは「MyData in Practice」であり、MyDataの概念を「現実世界でどう機能させるか」に注目しています。2024年の「MyData by Design」がBLTS（ビジネス・法律・技術・社会）の多面的な観点から「MyDataの実現に必要な『設計』」を議論したのに対し、2025年は「その議論を一歩先へ進め、具体的な実装と運用に関する実践的な取り組みに焦点を当てます。」と明記されており、より実践的な側面に重点が置かれています。</p>



<p>具体的な目的としては、以下の点が挙げられます。</p>



 
 国内で進行中の実例を通して「MyDataの実装」の可能性と課題を深掘りする。 



 国内外のMyData関連プロジェクトに関する事例を発表する。 



 「参加者同士の相互交流を促進する機会として、一般参加者の皆様も懇親会に参加できるように」し、「多様な専門性・立場を越えて対話し、今後の協働や共創の可能性を広げる場」を提供する。 



 「個人を中心とする新たなデータ利活用の潮流を実社会に根づかせていく」ために、企業、行政機関、非営利団体、研究機関、教育機関、市民社会といった多様なステークホルダーの参加を促す。 
 



 

 



 プログラムのハイライト 



<p>プログラムは、MyDataの実装と関連する多岐にわたるトピックをカバーしています。</p>



 プライバシーと個人データ保護の未来 



 
 セッション「これからのプライバシーと個人データ保護」では、個人情報保護委員会事務局長、法律事務所の弁護士、大学教授など、法務・政策の専門家が登壇し、現代におけるプライバシーと個人データ保護の課題と展望について議論します。 
 



 DID/VC (分散型識別子/検証可能クレデンシャル) の社会実装 



 
 セッション「DID/VCの社会実装に向けた取り組み」では、株式会社VESS Labsやトヨタ・ブロックチェーン・ラボの専門家が、DID/VC技術が社会でどのように実装されうるか、その可能性と課題について掘り下げます。お昼休憩中の5分ピッチでは、DID/VCプラットフォーム「proovy」やマイナンバーカード対応のプライバシー管理ツール「サニーズ」といった具体的なソリューションが紹介されます。 
 



 特定分野におけるデータ利活用とガバナンス 



 
 カメラ画像データ: 「カメラ画像データの保護と活用-街中を走る車のカメラ画像を起点に考える-」では、トヨタ自動車やヤマハ発動機の担当者が、自動車のカメラ画像データのプライバシー保護と活用について、実務的な視点から議論します。 



 AI・データ活用: 「AI・データ活用の進展と求められるガバナンス」では、プライバシーテック協会、経済産業省、AIガバナンス協会の専門家が、AIの進展に伴うデータ活用とガバナンスの重要性について意見を交わします。 



 子どもの保護: 「オンライン空間における子どもの識別と保護」では、法律事務所の弁護士やメルカリのシニア・プライバシー・オフィサーが、オンライン環境における子どものデータ保護という喫緊の課題について議論します。 
 



 パーソナルデータの同意管理・利活用 



 
 セッション「パーソナルデータの同意管理・利活用(仮題)」では、LBMA Japan、NTTデータ、フェアビルドの専門家が、パーソナルデータの同意取得と適切な利活用の方法について、具体的な取り組みを紹介します。 
 



 参加と交流の機会 



 
 「懇親会」が設けられ、多様な専門性を持つ参加者間のネットワーキングと協働の促進が図られます。「多様な専門性・立場を越えて対話し、今後の協働や共創の可能性を広げる場としてご活用いただければ幸いです。」とされています。 



 「Call for Proposal」を通じて、昼休憩中の5分ピッチとポスター展示の機会が提供され、より多くの参加者が自身の取り組みを発表できる場が設けられています。 



 会場では、「AI学会パーソナルAI研究会 兼理研AIP分散型ビッグデータチーム成果報告会」というサイドイベントも開催されます。 
 



 スポンサーと展示 



<p>本カンファレンスは、株式会社DataSign、株式会社電通総研、富士通株式会社などの企業がスポンサーとして参加しています。ポスター展示では、NHK放送技術研究所が「放送メディアにおけるユーザー主体のデータ活用に向けた取り組み」を、早稲田大学/佐古研究室が「佐古研究室でのセキュリティ・プライバシに関する研究の紹介」を行う予定であり、研究機関による具体的な取り組みも紹介されます。</p>



 結論 



<p>「MyData Japan 2025 ~ MyData in Practice ~」は、MyDataの理念を現実の社会に根付かせるための実践的な議論と具体的な事例共有の場として位置づけられています。法制度、技術、ビジネス、社会という多角的な視点から、個人中心のデータ利活用を推進するための具体的なステップと課題が深掘りされることが期待されます。多様なステークホルダーが参加し、知見を共有し、協働の機会を創出することで、持続可能で倫理的なデータ社会の実現に向けた重要な一歩となるでしょう。</p>



 ソーシャルメディア上の反響 



<p>ソーシャルメディア上でもちらほらと見かけるようになってきましたので、それも合わせてお知らせします。</p>



 
 <blockquote><p>MyData Japan Conference 2025 に登壇します！<br>主に同意におけるダークパターンについて話をします<br><br>2025年7月17日(木) 一橋講堂で開催されますで、ぜひご参加ご検討ください。<br><br>(裏ではあの人も登壇！？)<a href="https://t.co/Vi3CHvBCCs">https://t.co/Vi3CHvBCCs</a> <a href="https://t.co/w66AMVIsHm">pic.twitter.com/w66AMVIsHm</a></p>— 坂本一仁 | Takahito Sakamoto (@_taka_sakamoto) <a href="https://twitter.com/_taka_sakamoto/status/1940982539054899601?ref_src=twsrc%5Etfw">July 4, 2025</a></blockquote> 
 



 
 <blockquote><p>MyData Japan 2025 では結構いろいろとすっ飛ばして話をすると思うので、私の考えの前提となっているオンラインプライバシーの資料を公開します！<a href="https://t.co/6tKqEGrrYL">https://t.co/6tKqEGrrYL</a>… <a href="https://t.co/DXUqzGxNx6">https://t.co/DXUqzGxNx6</a></p>— 坂本一仁 | Takahito Sakamoto (@_taka_sakamoto) <a href="https://twitter.com/_taka_sakamoto/status/1941840495514435782?ref_src=twsrc%5Etfw">July 6, 2025</a></blockquote> 
 



 
 <blockquote><p>今年も行く申し込んだ～<br>MyData Japan 2025 ~ MyData in Practice ~ | MyDataJapan <a href="https://t.co/lKapDE6E3d">https://t.co/lKapDE6E3d</a></p>— 長倉克枝 (@kaetn) <a href="https://twitter.com/kaetn/status/1940023097559851159?ref_src=twsrc%5Etfw">July 1, 2025</a></blockquote> 
 



 
 <blockquote><p>人工知能学会パーソナルAI研究会 (SIG-PAI) 兼 理研AIP分散型ビッグデータチーム成果報告会 のご案内です。MyData Japanカンファレンス2025の一部として7月17日(木)の午後に一橋講堂の会議室でやります。一般発表も少しだけ募集してます。参加無料。リモート参加も可能です。 <a href="https://t.co/rIEVptrz6h">https://t.co/rIEVptrz6h</a></p>— HASIDA Koiti (橋田浩一) (@hasida_k) <a href="https://twitter.com/hasida_k/status/1932990277960204434?ref_src=twsrc%5Etfw">June 12, 2025</a></blockquote> 
 



 
 <blockquote><p>個人を起点とした持続可能かつ倫理的なデータ活用のあり方を追求する、日本最大級のカンファレンスMyData Japan カンファレンスです。<br>2025年07月17日(木)　10:00～18:45<br>一橋講堂<a href="https://t.co/e7wKQigwCy">https://t.co/e7wKQigwCy</a></p>— JDMC (@JDMC2011) <a href="https://twitter.com/JDMC2011/status/1941458485725835589?ref_src=twsrc%5Etfw">July 5, 2025</a></blockquote> 
 



 
 <blockquote><p>MyData Japan 2025に登壇の機会をいただきました。<br>名だたる専門家の皆さまの中に私がいて本当に良いのか…？という畏れ多さを感じつつも、新参者ならではの示唆をお届けできればと思います。<br>7月17日（木）@一橋講堂で開催です。<br>ぜひご参加検討ください。<a href="https://twitter.com/hashtag/MyDataJapan?src=hash&amp;ref_src=twsrc%5Etfw">#MyDataJapan</a><a href="https://t.co/wrihLVKAg8">https://t.co/wrihLVKAg8</a> <a href="https://t.co/QAxpfD8yYi">pic.twitter.com/QAxpfD8yYi</a></p>— MORIDaisuke (@MORIDaisukePub) <a href="https://twitter.com/MORIDaisukePub/status/1942169962334044432?ref_src=twsrc%5Etfw">July 7, 2025</a></blockquote> 
 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://s.w.org/images/core/emoji/16.0.1/72x72/1f60e.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   00:00 &bull;
   21 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 06. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://john.philpin.com'>John Philpin : Lifestream</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

  
    
    <a href='https://john.philpin.com/2025/07/07/jax-and-i-took-a.html'> Jax and I took a little tiki tour of our ' ‘hood favourites </a>
  

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Jax and I took a little tiki tour of our ' ‘hood favourites ’ on Saturday - including an extended session in my ‘favourite of the favourites’ … The Tinted Rose … who have extended their range of available 🔗 Lady H Gins. 

   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn.uploads.micro.blog/2529/2025/eeb95746e5.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Jax and I took a little tiki tour of our ' ‘hood favourites ’ on Saturday - including an extended session in my ‘favourite of the favourites’ … The Tinted Rose … who have extended their range of available <a href="https://ladyhspirits.com">🔗 Lady H</a> Gins.</p>

   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn.uploads.micro.blog/2529/2025/eeb95746e5.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   23:07 &bull;
   21 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.webistemology.com/public-safety-and-data-privacy/'>Public Safety and Data Privacy</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This is an updated version of a post initially written for Data Privacy Day in 2012, but it still seems relevant today in the context of Prime Minister Carney’s Bill C-2.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>According to a press release from the Minister’s office at the time, “The Government of Canada is proud to promote an open, resilient and trusted cyberspace." This is, of course, not borne out by the evidence. As the <a href="https://nationalpost.com/opinion/vic-toews-draws-line-on-lawful-access-youre-with-us-or-the-child-pornographers?ref=webistemology.com">National Post </a>pointed out,</p><blockquote>Public Safety Minister Vic Toews’ attempt to demonise opponents of his new Internet law — you’re either with the government, or with the “child pornographers,” he said on Monday — isn’t the first time he’s sought to demonise critics. In November, he said Liberals should “finally stop putting the rights of child pornographers and organised crime ahead of the rights of law-abiding citizens.” Two weeks ago, he urged the NDP “to listen to the police, listen to the provinces, and support these balanced measures that protect law-abiding Canadians and their children.”</blockquote><p>That government was then the least open Canadian government, at least in living memory. When was the last time you saw Prime Minister Harper in a scrum - risking facing the press in an unscripted or uncontrolled environment? This government reflexively denies or delays Freedom of Information requests. Ministers and senior bureaucrats must route all communications through the Prime Minister’s Office. With a reduced, and possibly cowed, CBC and an increasingly fact-free mainstream media, ‘open’ is a quaint notion from a bygone century.</p><p>Similarly, the ‘Lawful Access’ legislation, which has been promised to be reintroduced this spring, will reduce the resiliency of Canadian information infrastructure due to the deep distrust of citizens that the Harper&nbsp;government has. This legislation will dramatically increase the amount of&nbsp;information about citizens that the police have ready access to without judicial or civilian oversight. It ramps up the state’s surveillance capabilities, in other words.&nbsp;Needless to say, or rather what must be said, is that increased surveillance by the state of its population is the hallmark of authoritarian regimes, not democracy.</p><p>It used to be that Freedom of Information meant that citizens had a right to know what the powerful were doing behind closed doors. Privacy meant that citizens were free to express their opinions and live their lives with some assurances of anonymity from the state. In other words, the Freedom of Information and&nbsp;Privacy Act served to address the imbalance of power between the state and its citizens.</p><p>In Harper’s world, these concepts have been reversed. The Harper government needs privacy to conduct its business away from the prying eyes of the public, who can’t be trusted with the kind of information that the government has to deal with. Similarly, as citizens, we are all suspects, and the state must have the freedom to have access to whatever information it deems necessary about us to ensure that we are harmless.</p><p>If we are not careful, Data Privacy Day may become a day of remembrance instead of a day of celebration.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://www.webistemology.com/content/images/2025/07/canada-politics-1.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:46 &bull;
   22 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/family-zoning-actually-means-something-cool/'>Family zoning actually means something cool</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Forget San Francisco - I wish every single US city would do this. We're too car-dependent, too isolated, too unhealthy. (Honestly, European mixed-use development should be the model.)
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://sfstandard.com/opinion/2025/07/06/family-zoning-may-be-a-euphemism-its-also-a-great-idea/?ref=werd.io">Link: Thomas Goetz in The San Francisco Standard</a>.</p><p>Beyond San Francisco, I wish every single US city would do this. We're too car-dependent, too isolated, too unhealthy:</p><blockquote>"The “family zoning” concept offers a way to say yes to a whole new vision of healthy housing. We need a smart mix of retail, office, housing, and parks to create the sorts of neighborhoods that encourage walking to the store and bumping into your neighbors. Healthy housing also means more than parks or exercise equipment. We need spaces that foster socializing and fun almost by accident."</blockquote><p>We need the kind of mixed-use neighborhoods that <a href="https://en.wikipedia.org/wiki/Jane_Jacobs?ref=werd.io">Jane Jacobs</a> would have approved of. Honestly, European mixed-use development should be the model, with plenty of public art, shared spaces, and amenities the author suggests like fitness circuits and lots of kinds of libraries added into the mix. That sounds like the kind of place I'd like to live.</p><p>If San Francisco wants to be the model for this, great! But it's one of the most expensive places to live in the world. For this to be effective and interesting, it needs to be expanded to cities that are more affordable for everyone, too.</p><p>[<a href="https://sfstandard.com/opinion/2025/07/06/family-zoning-may-be-a-euphemism-its-also-a-great-idea/?ref=werd.io">Link</a>]</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:03 &bull;
   22 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 04. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/america-is-a-myth/'>America is a myth</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  We have to decide which story we believe in.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>My father’s first memories are of a concentration camp. I grew up hearing the stories, and hearing my grandmother’s screams at the hands of the nightmares she faced every single night. I am able to exist because she secretly gathered and cooked snails in the camp, and because my aunt was able to sneak out through the sewers to gather more food.</p><p>It wasn’t a death camp, technically, but they killed people in high numbers. The way people died was through malnourishment, or the terrible conditions, or through direct punishment or torture at the hands of the guards. The Japanese-run camps were the site of many atrocities, forced labor, and medical experiments.</p><p>Of course, in camps run by Nazi Germany, they systematically executed people. That wasn’t always the plan: first, they intended to deport people in large numbers. It was only later on, once these detainments and deportations proved to be too slow, that they moved on to the “final solution”.</p><p>That was eighty years ago. Today, we’re building, running, and partnering with camps that are both visually and thematically similar. We own the sociopathically-named Alligator Alcatraz in South Florida; we partner with the Center for Terrorism Confinement in El Salvador; we’re trying to make deals with places like South Sudan and Rwanda to take deportees that echo plans from long ago. ICE, our anti-immigrant federal agency, is now better funded than most national militaries.</p><p>We are still at the foothills of a bigger fascist movement in the United States. Notable experts in totalitarianism have already left the country. It seems big compared to where we’ve been, but it’s still small compared to where we’re going.</p><p>I reject nativism completely. I unapologetically think people who use phrases like “un-American” are dumb as rocks, but if anything seems un-American, it’s that. I’m a natural-born American citizen who hails, on my mother's side, from a long-standing American family who arrived here on the Mayflower. My ancestors include diplomats who shaped American foreign policy and authors who are a part of the American canon. Yet I’m no more American than someone who naturalized today; I’m no more important on this or any soil than any other person. Why would I be?</p><p>The stories I’ve always been told about America, the ones that really make it sound great, are the ones where immigrants arrived here, perhaps via Ellis Island, and could make a life for themselves, with great union jobs, and make a mark on what the country is. The ones where our cities are homes to people of every ethnicity and every background, where every culture and religion is welcome. They’re myths, of course. The history is far more dark and complicated than that — we’re still dealing with the legacy of slavery and the civil war, let alone Jim Crow, the Tulsa race massacre, the assassination of Dr Martin Luther King, Jr, or COINTELPRO, to pick just some examples. But as far as myths go, they’re good ones; something to aim for.</p><p>We’re busy building a new myth: one that looks a lot closer to the stories that led to death and suffering that was so traumatic that it defined the century that followed it. Those are also the stories that led my grandfather to lead his local resistance movement in Indonesia, and to my cousins in Northern Europe to lead theirs. Back then, good people around the world stood up for inclusion, equality, parity, life, and dignity for all. Now, we have to decide which story about America we want to believe, hold onto, and stand up for — and we have to decide what really matters.</p><p>There are still people who think this is about partisanship, and that our norms will endure. I simply don’t agree. It’s about democracy itself, and our norms are already gone.</p><p>Happy Fourth of July. I’m not feeling very patriotic. But given the choice between a multicultural democracy that welcomes all and one that seeks to tear us back to the worst part of the twentieth century, I’m damn sure where I stand.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://images.unsplash.com/photo-1499200493734-6ba25a83f77c?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fGZsYWd8ZW58MHx8fHwxNzUxNjA4MDAyfDA&ixlib=rb-4.1.0&q=80&w=2000'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   05:47 &bull;
   24 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 03. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-10x-overlemployed-engineer'>The 10x “overemployed” engineer</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A software engineer worked at several AI startups at the same time, without his employers knowing anything about it. Then one frustrated founder who hired him went public…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The 2002 movie <a href="https://en.wikipedia.org/wiki/Catch_Me_If_You_Can">Catch Me if You Can</a> stars Leonardo Di Caprio as a talented 19-year-old fraudster, who forges millions of dollars in checks while pretending to be a doctor, a lawyer, and even an airline pilot. That Hollywood film now has echoes in the tech industry, in a story which has come to light in Silicon Valley.</p><p>A software engineer in their mid-twenties, based in India, successfully scammed approximately a million dollars annually from tech startups by excelling in interviews, getting hired remotely, and then not doing their assigned work, all while being simultaneously employed by many companies.</p><p>As in ‘Catch Me if You Can’, in this story there’s an unusually talented main character who gets into a dramatic showdown once exposed.</p><p>Today’s issue covers what happened, and some learnings from this highly unusual incident:</p>  <p>Exposé. An engineer named Soham Parekh is accused on social media of working at multiple Y Combinator startups at the same time by one employer, and other workplaces come forward.</p>  <p>23 companies that hired, or nearly hired this engineer. Almost all are recently-founded AI startups.</p>  <p>Excuses, excuses… This engineer was hard to contact, and had bizarre reasons as to why.</p>  <p>Skyrocketing demand for AI engineers. One thing is clear: demand for AI engineers is through the roof, and capable software engineers who can integrate LLMs are in high demand.</p>  <p>How did a dev outside the US get hired by hot Silicon Valley startups? False US work authorization status, Silicon Valley startups desperate to hire, and “business-minded 10x engineers” being in high demand, all helped.</p>  <p>Spotting “overemployed” software engineers. Suspicion of ‘overly green’ GitHub profiles, the importance of background checks, and recruitment advice from an ex-Facebook and ex-Uber founder of scaleup hiring business, <a href="https://dynosaurlabs.com/">Dynosaur Labs</a>. Additionally, <a href="https://digger.dev/">Digger</a> founder Igor Zalutski confirms that Soham was genuinely among the top 0.1% of software engineers, in terms of both technical skills and business acumen.</p>  <p>What does it mean for remote work? Remote work makes being “overemployed” much easier – and AI tooling will make it even more so. Are more in-person interviews and hybrid work patterns a solution?</p>  <p>Update: shortly after publishing this article, Soham appeared in the TBPN podcast where he <a href="https://x.com/tbpn/status/1940834500457779337">admitted</a> to having worked multiple jobs in parallel since 2022, confirming the allegations in the below article. In a follow-up email, he also told me he previously spent time living in the US “with some of the founders (at Sync Labs and Antimetal.)”</p> 1. Exposé <p>Yesterday (2 July), startup founder Suhail Doshi made an accusation: that a software engineer named Soham Parekh was working at several Y Combinator startups at once, and had been doing so for over a year, all while failing to do the work he was hired to do:</p> <a href="https://substackcdn.com/image/fetch/$s_!pOKk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc04778c-7c78-4381-9775-56c0ace88a24_794x258.png">          </a>Source: Suhail Doshi <a href="https://x.com/Suhail/status/1940287384131969067">on X</a> <p>Initially, the post got a bit of pushback. After all, in California – where most startups which hired Parekh are based – it’s not forbidden to have a second job (aka ‘moonlighting’), as long as it doesn’t overlap with other commitments. Indeed, many leaders founded startups on the side of their main job, and an employer cannot claim intellectual property ownership of a new project which is fully separate from someone’s primary job.</p><p>What makes this story stand out is the unusually high number of parallel jobs this one dev took on. All together, the combined workload of all these roles was evidently impossible to maintain, and would inevitably lead to questions being asked by individual employers, who wondered why a clearly-talented engineer was unable to deliver their work.</p><p>Suhail said his issue was not that Parekh had a side job; it was something more fundamental:</p> <a href="https://substackcdn.com/image/fetch/$s_!OH0R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e53a5e-3a16-4d12-ba38-35f422745cb5_792x376.png">          </a>Source: Suhail Doshi <a href="https://x.com/Suhail/status/1940289719268712720">on X</a> <p>Following Suhail’s post, reports from other startups began to emerge, with accusations that they had hired Parekh, and then fired him not long afterward.</p><p>He was exposed thanks to the Y Combinator founder community. Doshi went public with accusations after confirming with several other Y Combinator founders who’d had the same experience with Parekh. Garry Tan – president and CEO of Y Combinator – also <a href="https://x.com/garrytan/status/1940474344758825065">shared his belief</a> that without the Y Combinator founder community sharing information with one another, Parekh might have not been publicly exposed.</p> 2. 23 companies that hired, or nearly hired him <p>As the drama played out online, things turned bad fast for Parekh. Other companies at which he worked reportedly started taking action, <a href="https://x.com/Suhail/status/1940491369409401268">as per</a> Suhail:</p> <a href="https://substackcdn.com/image/fetch/$s_!rbDO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4cc803a-14a5-415a-bd64-301a36adb497_1600x882.png">          </a>Showdown: founders learned their employee worked at multiple companies at once. Source: Suhail Doshi <a href="https://x.com/Suhail/status/1940491369409401268">on X</a> <p>Many companies went on record about the fact they had employed and quickly dismissed Parekh. Links to companies below lead to their job pages, if they have one. All are hiring for fullstack software engineers or AI engineers.</p>  <p><a href="https://www.ycombinator.com/companies/playground">Playground AI</a> (YC19, AI-first graphics editor.) Hired Soham in 2024 and <a href="https://x.com/Suhail/status/1940287384131969067">fired</a> him after a week.</p>  <p><a href="https://www.ycombinator.com/companies/leaping-ai">Leaping AI</a> (YC W25, building voice AI agents that are human-like and self improve): hired him, then <a href="https://x.com/akyshnik/status/1940488291134189783">fired him</a> when they found out he also worked elsewhere</p>  <p><a href="https://www.ycombinator.com/companies/fuseai">Fuse AI</a> (YC W25, agentic sales platform for GTM teams): <a href="https://x.com/akyshnik/status/1940488291134189783">fired him</a> when they found out that Soham was working parallel at Leaping AI</p>  <p><a href="https://www.nomic.ai/">Nomic AI</a> (AI to understand data). Hired him in March 2024. Founder Andriy Mulyar <a href="https://x.com/andriy_mulyar/status/1940722853399474186">said</a> “we were paying you [Soham] a lot of money. We were sending you at least $10K per month while you were working at other companies.” </p>  <p><a href="https://app.dover.com/apply/Digger/3545b32f-2ddb-4bea-8c60-4f12a08aec99">Digger</a> (Terraform pull request automation) onboarded him, but terminated him a few days into the job, founder Igor Zalutski told me. The company terminated Parekh when background checks raised red flags. Digger was conscious about the risk of “moving fast” in hiring before background checks completed, but this was a calculated risk.</p>  <p><a href="https://cal.com/jobs">Cal.com</a> (open source Calendly alternative). Founder Peer Richelsen <a href="https://x.com/peer_rich/status/1940452247781187766">shared</a>: “this guy got a trial contract but ghosted us after signing. We fired him immediately.” </p>  <p><a href="https://www.fleetai.com/">Fleet AI</a> (agent engineering platform). Hired and <a href="https://x.com/nicolas_ouporov/status/1940496144595144752">fired</a> within a week.</p>  <p><a href="https://careers.lindy.ai/">Lindy</a> (AI assistant platform). Hired and <a href="https://x.com/Altimor/status/1940432517586374670">fired</a> two weeks later. Aced the interview, but then “the impact of his work was negative.” Founder Flo Crivello said </p>  <p><a href="https://jobs.ashbyhq.com/create.xyz">Create</a> (creating software with AI): hired him and promptly <a href="https://x.com/marcuslowe_/status/1940381045628788748">fired</a> him. Create is an in-person startup. Amusingly, Soham showed up for one day at their SF office, then did not deliver work, and denied vehemently that he was employed at Sync Labs (below) when Create founders confronted him. But he was!</p>  <p><a href="https://www.ycombinator.com/companies/sync-2">Sync Labs</a> (AI lipysnc tool). Soham was featured in their team video in the summer of 2024: and it’s how Create (listed above) <a href="https://x.com/dhruvtruth/status/1940867072625717443">learned</a> Soham lied about not having other employment.</p>  <p><a href="https://antimetal.com/careers">Antimetal</a> (AI platform for infrastructure) Hired in 2022 and fired shortly thereafter. They realized he must be working multiple jobs.</p>  <p><a href="http://ponder.ai">Ponder.ai</a> (video AI). Worked at the startup until the news broke, when he <a href="https://x.com/timwangyc/status/1940675163751584170">was fired</a>.</p>  <p><a href="https://www.comfy.org/">ComfyUI</a> (open source, <a href="http://node.js">Node.js</a>-based app for GenAI) hired Parekh for 3 weeks in June. The interview went well, then things started to <a href="https://x.com/RobinJHuang/status/1940530421311447252">fall apart</a>.</p>   <a href="https://substackcdn.com/image/fetch/$s_!VSIU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ccdf05c-4d84-43cb-a6d4-90c835e6b665_1878x1352.png">          </a>The video made by Sync Labs about their team that showed how he was working at this company, while also being employed by Create. Soure: <a href="https://youtu.be/kK3CKyU5xAY?si=UkNItriuVAtP7Vo-&amp;t=38">Sync Labs</a> <p>And how it all started:</p> <a href="https://substackcdn.com/image/fetch/$s_!XEhq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e599ae0-edff-4939-a8fc-06d0fb547f24_798x704.png">          </a>One of the first AI companies to hire Parekh back in 2022. Source: Matt Parkhurst, founder of Antimetal <a href="https://x.com/mprkhrst/status/1940443347581337925">on X</a> <p>Then there are companies which interviewed Soham, but decided against employing him:</p>  <p><a href="https://www.ycombinator.com/companies/aivideo-com/jobs">AIVideo.com</a> (YC S23, AI-powered video production) They were very close to hiring him after he <a href="https://x.com/AI_Vidz/status/1940287618572537939">did very well</a> on the interview.</p>  <p><a href="https://www.ycombinator.com/companies/pally">Pally AI</a> (YC S25, AI relationship management) They offered him a founding engineer role, which he rejected because the in-person requirement was non-negotiable.</p>  <p><a href="http://r">Mosaic AI</a> (YC W25, video editing AI agents). Spent a month interviewing Soham. Did great in interviews, but the company <a href="https://x.com/_adishj/status/1940293414878421400">did not hire</a> him.</p>  <p><a href="https://www.ycombinator.com/companies/reworkd">Reworkd</a> (YC S23, extracting data at scale). Interviewed him and rejected it after confirming he lied about his location during the interview - as the startup <a href="https://x.com/khoomeik/status/1940576509980479745">used</a> an IP logger during the interview (Soham said he was in the US at the moment, but his IP was in Mumbai, India)</p>  <p><a href="https://www.ycombinator.com/companies/weave-3/jobs">Weave</a> (YC W25, ML-powered tool to measure engineering). Cofounder Andrew Churchill confirmed they interviewed him in May, and he was impressive. They rejected him after hearing rumors from fellow YC founders on holding multiple jobs.</p>  <p><a href="https://www.bestever.ai/">Bestever AI</a> (AI-powered, stunning ads). Interviewed him, but <a href="https://x.com/Appyg99/status/1940514369655460327">rejected</a> after the initial screening.</p>  <p><a href="https://jobs.ashbyhq.com/Cassidy">Cassidy AI</a> (AI automation powered by company data). Soham <a href="https://x.com/JustinFineberg/status/1940516025310892089">reached out</a> to this company with an email discussed below.</p>  <p><a href="https://railway.com/careers">Railway</a> (infrastructure as a service). <a href="https://x.com/JustJake/status/1940499131640033736">Rejected</a> in 2023 during early screening.</p>  <p>Graphite (the AI developer productivity platform. Disclaimer: I’m an investor). Cofounder Jacob Gold interviewed Sohan and <a href="https://www.linkedin.com/feed/update/urn:li:activity:7346627457921261569?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7346627457921261569%2C7346642132276822016%29&amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287346642132276822016%2Curn%3Ali%3Aactivity%3A7346627457921261569%29">said</a> it “was one of the best system design interviews I’ve conculded.” Graphite has strict reference checks: Soham did not procees with these — likely suspecting he could get caught. A reminder that reference checks work, and are important! </p>  <p>An early-stage startup cofounded by former Warp engineer Michelle Lim <a href="https://x.com/michlimlim/status/1940472293379162273">booked him</a> for a 3-day paid work trial, but cancelled it when the story came to light.</p>  <p>A startup advocating for “cheating on everything” also nearly got duped. Cluey is a startup building an AI tool to “cheat on everything”, and as fate would have it, the company’s founder and CEO <a href="https://x.com/im_roy_lee/status/1940489514415542351">shared</a> that they were at the second-round interview stage with Parekh. The company <a href="https://app.cluely.com/share/ZTE5MTAxMmItNTU5ZS00MTIwLWEwMmItZTViMzljZjkzYzhi">shared</a> interview notes taken during their session:</p><blockquote><p>“Candidate Evaluation:</p>  <p>Candidate seen as knowledgeable in React and frontend optimization.</p>  <p>High-level answers are considered good; some fumbling through details noted.</p>  <p>Strong devops and infrastructure experience inferred from GitHub and past projects.</p>  <p>Candidate’s GitHub is reviewed, showing significant contributions and experience.</p>  <p>Comparison to other candidates in terms of technical strength and fit.</p>  <p>Discussion of candidate’s background (e.g., internships at Meta, Waterloo education).</p>  <p>General consensus that the candidate is a good fit, with a strong, "startery" profile.</p>  <p>Plan to schedule a follow-up interview for further evaluation.”</p>  </blockquote> 3. Excuses, excuses and lies <p>Parekh consistently delayed his supposedly-imminent relocation to the US. Most startups that employed him usually knew he was based in India, but all assumed he wanted to relocate to the US. He assured several employers that he was taking care of his O1 visa (discussed below). For example, here is an email sent to the CEO of Fleet AI:</p><blockquote><p>“Ideally, I’d love to be in person. However I am also undergoing the renewal process for the current visa. Is it okay if I let you know earliest by this Tuesday if it would be possible to be in person in New York for this [the team working together]. At this point, I’d say it is 80% likely that I should be able to come in person.”</p></blockquote><p>Excuses were frequent. One recurring feature was Parekh’s creative excuses for why he was slow at getting things done. Arkadiy Telegin, cofounder of Leaping AI, wondered why it took him so long to do pull requests. Parekh replied that a military drone had damaged his house during fighting between India and Pakistan. Later, it turned out he was not near the conflict zone:</p> <a href="https://substackcdn.com/image/fetch/$s_!tZdg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2596c957-4300-41ce-9ba9-6852d87f6152_1230x938.png">          </a>Why the work was slow, allegedly. Source: Arkadiy Telegin <a href="https://x.com/akyshnik/status/1940509741350113657">on X</a> <p>Some lies were caught. Soham used excuses like having been diagnosed with a chronic illness to explain his lack of work — when, in reality, he was busy working for other companies. When confronted about working at another startup, Soham lied about his involvement — but to his misfortune, the other startup publicly posts a team video that includes Soham working in their team. Shruv Amin, cofounder of <a href="https://www.create.xyz/">Create.xyz</a> (AI app builder) <a href="https://x.com/dhruvtruth/status/1940867072625717443">shared</a> his frustrating experience of hiring Soram:</p>  <p>Did well on the onsite (in-person) programming challenge in New York</p>  <p>Monday (first day at work) and Tuesday: calls in sick</p>  <p>Wednesday: cofounder notices that he made lots of GitHub commits in the middle of the day. This feels like he’d be working at another company! Also: Soham did not even clone Create’s repo while clearly working on something else, in private. More red flags! </p>  <p>Thursday: they call him up to ask what happened with commits the previous day. Offered he can come clean and move on. Soham denies doing other work and claims he “couldn’t sleep so was playing with DeepSeek in his own repos.” Too sick to work on his main job but not too sick to “play with DeepSeek?” Sure...</p>  <p>Friday: comes into the office in New York, does a great job, and stays late.</p>  <p>Next weeks: all falls apart. Says he is diagnosed with chronic condition and is “really scared.” Work doesn’t get done, excuses keep piling up.</p>  <p>Create cofounders start to become suspicious that Soham might be employed by another startup called Sync Labs.</p>  <p>Confronted Sohan about this suspicion. Sohan denies being an employee at Sync.</p>  <p>Sync Labs shares an end-of-month video of their team, which includes Sohan: making it clear he is employed at the company.</p>  <p>Another time Soham caught lying was in an interview, where interviewers asked him about his location. Rohan Pandey, cofounder of Reworkd <a href="https://x.com/khoomeik/status/1940576509980479745">shared</a>:</p><blockquote><p>“Call 1: says he’s in US, but we suspect he’s in India</p><p>Call 2: we call his BS, he admits “was in India last week to visit family, but now back in US”</p><p>Call 3: @asimdotshrestha puts an IP logger on Zoom link and it shows up in Mumbai 💀The thing with soham is that all his lies are so believable and he delivers them so innocently</p><p>Each time we’d go into a call thinking he’s sus and come out 30 min later saying he’s great.</p><p>Until we’d discuss it among ourselves and be unable to pinpoint any fact supporting him.”</p></blockquote><p>It’s alleged Soham misled at least one employer about his location. A founder said they thought he was US-based, and even showed <a href="https://x.com/Suhail/status/1940441569276158190">a company laptop</a> sent to a US address. The laptop was later returned – Soham claimed it was sent to his sister. However, he never had a US address, and could not have one legally since he did not have any USA visa.</p><p><a href="https://x.com/IgorZIJ">Igor Zalutski</a>, founder of Digger (AI that understands infrastructure) gave me examples of what it was like working with Soham:</p><blockquote><p>“He said he applied for o1 visa and paid for premium processing so he was "few weeks at most" away from getting it. He said that his lawyers told him it's just better to be out of the US while the application is ongoing, and after it's done he's back, he "got an apartment in North Beach (San Francisco)".</p><p>Later, a founder I background checked with told me that he [Soham] was telling them the same for over a year.</p><p>And funnily enough, Soham started his dirty tricks from the very first day: he asked to push his onboarding meeting to the next day. He wrote how “this is unusual but it’s been unusually windy and rainy so I think that might have messed with the internet wire, I will have someone look over it tomorrow.”</p></blockquote><p>Digger AI terminated Soham’s contract a day or two into his onboarding. By that time, they had placed an order for an Apple laptop to be shipped to Parekh’s home address in India. Luckily, they could still cancel the order.</p> 4. Skyrocketing demand for AI engineers <p>One puzzling part of the story is how he got hired by Silicon Valley startups, particularly by “cream of the crop” places like Y Combinator AI startups. These businesses are well-funded, offer good base salaries close to the $200K/year range for senior engineers, and also attract more candidates than average. But the reality is complicated, and also explains why some parts of the tech market are still “candidate’s markets”, compared to the norm. </p><p>This is a reminder that becoming an “AI engineer” as a software engineer is straightforward enough: we covered <a href="https://newsletter.pragmaticengineer.com/p/ai-engineering-in-the-real-world">Seven examples of software engineers picking up AI engineering</a>, <a href="https://newsletter.pragmaticengineer.com/p/the-ai-engineering-stack">The “AI engineering stack”</a> with Chip Huyen, and H<a href="https://newsletter.pragmaticengineer.com/p/from-software-engineer-to-ai-engineer">ow Janvi Kalra became an AI Engineer at Coda</a> — which later helped her get hired at OpenAI.</p><p>This is why the market is so hot for AI engineers:</p>
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-10x-overlemployed-engineer">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!pOKk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc04778c-7c78-4381-9775-56c0ace88a24_794x258.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   18:20 &bull;
   25 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 02. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/what-are-we-actually-for-the-mamdani-case-for-positive-resistance/'>What are we actually for? The Mamdani case for positive resistance</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Why effective opposition requires more than just saying no
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I’m on vacation this week, which means I’d taken some time away from the internet to breathe. The Oregon coast is beautiful; if you’ve never had a chance, I recommend that you visit.</p><p>Let me amend that: if you’re not already within the United States, I recommend that you wait to visit. <a href="https://www.nbcnews.com/news/world/trump-immigration-detained-visitors-border-search-device-visa-passport-rcna197736?ref=werd.io">Incidents of visitors being detained at the border are on the rise.</a> When I reconnected, I learned that we’ve <a href="https://www.cbsnews.com/news/trump-alligator-alcatraz-detention-center-florida/?ref=werd.io">opened a concentration camp in South Florida</a> and that a funding bill that will serve to create, among other things, <a href="https://www.techpolicy.press/republican-budget-bill-signals-new-era-in-federal-surveillance/?ref=werd.io">an unprecedented surveillance state</a> has <a href="https://www.npr.org/2025/07/01/nx-s1-5450367/senate-republicans-trump-tax-bill-medicaid?ref=werd.io">been passed by the Senate and sent back to the House for approval</a>. Meanwhile, <a href="https://msmagazine.com/2025/06/30/supreme-court-nationwide-injunctions-immigrant-birthright-citizenship-casa-trump/?ref=werd.io">a Supreme Court decision stripped courts of the ability to block executive actions nationwide</a>, making it much harder to pose legal challenges to the ongoing <a href="https://www.msn.com/en-us/news/us/trump-s-immigration-team-goes-full-fascist-with-threat-to-visa-holders/ar-AA1Hq5dq?ref=werd.io">descent into fascism</a> at the hands of the current President.</p><p>I know, I know. We’ve all got dystopia fatigue: the stories are relentless and <a href="https://www.theguardian.com/media/2025/apr/01/i-cant-cope-with-it-any-more-newsrooms-scramble-to-retain-audiences-amid-the-big-switch-off?ref=werd.io">more people are turning away from the news in order to protect their mental health</a>. The temptation is not to say anything about it at all, because what could we possibly add to the conversation? And how can we say something about the current moment that doesn’t sound like something from a paranoid science fiction novel? But at the same time, silence is acquiescence. The overwhelming narrative is one of racism, nativism, and bigotry; by not raising our voices to counter it, we effectively help it along.</p><p>The government knows this, which is why it’s attempting to create a chilling effect on free speech, in part by demanding to see the social media accounts of immigrants and flagging people who might be critical of the current administration. <a href="https://academic.oup.com/jhrp/article/16/1/397/7234270?ref=werd.io">Surveillance always creates a chilling effect on speech and organizing</a>: clearly, immigrants who want a visa but know their social media profiles may prevent them from obtaining or maintaining one will watch what they say.</p><p>But, as bad as this already is, this climate of intimidation affects more than immigrants: it’s designed to make everyone think twice before speaking out. For the rest of us who might find staying silent more comfortable, we need to understand that <a href="https://www.theguardian.com/world/2025/feb/21/say-it-loud-this-is-fascism-and-we-must-all-resist-it?ref=werd.io">the bystander effect is real</a>:</p><blockquote>In this study, when a person is alone in a room that begins to fill with smoke, three-quarters raise the alarm within minutes. Yet, when surrounded by others who remain passive, only 10% take action. This is the “bystander effect”, identified by Latané and Darley in the 1970s, which reveals how a silent, unresponsive crowd can stifle our instincts – even when our own lives are at stake.</blockquote><p>It’s not just about speaking out: a resistance with no message of its own is doomed to repeat empty slogans. The hashtag resistance movement of the first Trump administration is a joke for a reason: it was largely performative, with few actions or real alternatives to back up its words. There needs to be something to say beyond, “not this guy”. Beyond what we’re against, what are we for?</p><p>This moment demands a robust counter-movement, but the formal opposition has been profoundly disappointing. I don’t know what established Democrats are doing. In response to the Republican spending bill that will give funding superpowers to ICE, Senate Minority Leader <a href="https://www.yahoo.com/news/schumer-forces-name-change-big-184428548.html?ref=werd.io">forced the bill to change its name</a> and claimed a victory for it on social media. Meanwhile, the game remains the same: pass measures and repeal regulations to <a href="https://www.cnbc.com/2025/06/30/trump-bill-helps-wealthy-hurts-low-earners-yale-report.html?ref=werd.io">give as much wealth and power as possible to the ruling class, stripping it from lower income people</a>, while <a href="https://www.cnbc.com/2025/06/30/trump-bill-helps-wealthy-hurts-low-earners-yale-report.html?ref=werd.io">scapegoating immigrants for their misfortune</a>.</p><p>Much of Trump’s support this time around was <a href="https://navigatorresearch.org/2024-post-election-survey-the-reasons-for-voting-for-trump-and-harris/?ref=werd.io">about the cost of living</a>. For some reason, Democrats insist on messaging that supports “the middle class”, leaving low-wage earners and the working class behind. To be clear, this was reason number two; reason number one was racist anti-immigrant sentiment, but that can also be tied into the economy when you consider how immigrants have been scapegoated for high costs and job losses. Removing the name from a terrible bill isn’t going to help here, but some inroads have been made. Many of Trump’s social media posts over the last few days have highlighted the price of gas, food, and taxes on social security.</p><p>This week, <a href="https://abc7ny.com/post/zohran-mamdani-nyc-mayoral-primary-ranked-choice-trump-reaction/16903542/?ref=werd.io">Zohran Mamdani won the NYC mayoral primary over Andrew Cuomo by a landslide</a>, with more votes being cast than in any NYC mayoral primary since 1989. Voters were excited, and many precincts switched their allegiance from the Republican Party. (<a href="https://www.youtube.com/watch?v=NjgI-nK1PHE&amp;ref=werd.io">His video about why he won</a> is a masterclass.) His core policies are <a href="https://www.npr.org/2025/07/01/nx-s1-5449040/zohran-mamdani-nyc-mayoral-candidate?ref=werd.io">progressive answers to cost of living problems</a>: things like expanded childcare, cheaper groceries, and free public transit that will benefit people on lower incomes more than anyone else. They’re based on working policies from elsewhere or pilots that yielded great results.</p><p>We need to clearly oppose the cruelty of the second Trump administration, from <a href="https://www.aclu.org/press-releases/groups-take-legal-action-to-halt-trump-fast-track-deportation-policy?ref=werd.io">unconstitutional deportations</a> and <a href="https://www.theguardian.com/us-news/2025/apr/28/trump-immigration-people-detained-deported-cases?ref=werd.io">detentions without due process</a> through <a href="https://www.msnbc.com/top-stories/latest/trump-palantir-surveillance-americans-rcna210017?ref=werd.io">unprecedented surveillance</a> and <a href="https://19thnews.org/2025/03/trump-anti-trans-executive-orders/?ref=werd.io">his anti-trans crusade</a> to <a href="https://www.politico.eu/article/trumps-plan-to-ethnically-cleanse-gaza-is-illegal-says-un-backed-judge/?ref=werd.io">ethnic cleansing in Gaza</a>. This goes far beyond partisan politics to simply being a position that fascism should always be opposed.</p><p>But we also need to have our own vision: one that addresses the core issues felt by ordinary people who are struggling to live in an increasingly cruel American society. I think Zohran Mamdani has clearly shown a road ahead: one that is embraced by a coalition of voters across cultural and ideological barriers.</p><p>I think Trump does too, which is why he posted:</p><blockquote>As President of the United States, I’m not going to let this Communist Lunatic destroy New York. Rest assured, I hold all the levers, and have all the cards. I’ll save New York City, and make it “Hot” and “Great” again, just like I did with the Good Ol’ USA!</blockquote><p>It would be better if the Democratic Party embodied its current role as an opposition force and embraced a progressive approach to the financial hardships faced by many Americans. Instead, both Chuck Schumer and Hakeem Jeffries have <a href="https://www.axios.com/2025/06/26/democrats-zohran-mamdani-meltdown-new-york?ref=werd.io">declined to endorse Mamdani</a> and other major figures have expressed concerns, in part because big-dollar donors favor a more centrist approach. But that approach is entirely at odds with what the electorate wants and needs — and the path forward is now clear. Establishment Democrats need to adjust accordingly or get the hell out of the way.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://werd.io/content/images/2025/07/NYTWA_wednesday-67.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:31 &bull;
   25 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/how-ai-is-changing-software-engineering'>How AI is changing software engineering at Shopify with Farhan Thawar</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Shopify's Head of Engineering, Farhan Thawar, shares how the company is using internal LLMs, unlimited AI tokens, and custom tooling to figure out how to use AI tools more efficiently - faster
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Stream the Latest Episode <p>Listen and watch now on <a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;feature=youtu.be">YouTube</a>,  <a href="https://open.spotify.com/episode/3XTdfnHeQlROSmlAHNqEUs">Spotify</a> and <a href="https://podcasts.apple.com/us/podcast/how-ai-is-changing-software-engineering-at-shopify/id1769051199?i=1000715464113">Apple</a>. See the episode transcript at the top of this page, and timestamps for the episode at the bottom.</p> Brought to You By   <p><a href="https://workos.com/">WorkOS</a> — The modern identity platform for B2B SaaS.</p>  <p><a href="http://statsig.com/pragmatic">Statsig</a> <a href="https://modal.com/pragmatic">⁠</a> — <a href="https://modal.com/pragmatic">⁠</a> The unified platform for flags, analytics, experiments, and more.</p>  <p><a href="http://sonarsource.com/pragmaticsecurity">Sonar</a> — Code quality and code security for ALL code.</p>  <p>—</p> In This Episode <p>What happens when a company goes all in on AI?</p><p>At Shopify ,engineers are expected to utilize AI tools, and they’ve been doing so for longer than most. Thanks to early access to tools like GitHub Copilot (Shopify was the first company outside of GitHub to use it), and models from OpenAI and Anthropic, the company had a head start in figuring out what works.</p><p>In this live episode from <a href="https://leaddev.com/leaddev-london/">LDX3 in London</a>. — recorded two weeks ago — I spoke with <a href="https://www.linkedin.com/in/fnthawar/">Farhan Thawar</a>, Head of Engineering, about how Shopify is building with AI across the entire stack. We cover the company’s internal LLM proxy, its policy of unlimited token usage, and how interns help push the boundaries of what’s possible.</p> <a href="https://substackcdn.com/image/fetch/$s_!-hVl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F065c9f3b-373f-4859-b18c-10b6e6a50562_4000x2667.jpeg">          </a>With Farhan Thawar at LDX3, during the live podcast <p>In this episode, we cover:</p>  <p>How Shopify works closely with AI labs</p>  <p>The story behind Shopify’s recent Code Red</p>  <p>How non-engineering teams are using Cursor for vibe coding</p>  <p>Tobi Lütke’s viral memo and Shopify’s expectations around AI</p>  <p>A look inside Shopify’s LLM proxy—used for privacy, token tracking, and more</p>  <p>Why Shopify places no limit on AI token spending</p>  <p>Why AI-first isn’t about reducing headcount—and why Shopify is hiring 1,000 interns</p>  <p>How Shopify’s engineering department operates and what’s changed since adopting AI tooling</p>  <p>Farhan’s advice for integrating AI into your workflow</p>  <p>And much more!</p>       Interesting detail: Shopify’s interview process <p>Two interesting details on how Shopify hires, as discussed in this conversation:</p> #1: Directors-and-above still need to do a coding interview <blockquote><p>Gergely(at <a href="https://youtu.be/u-3IILWQPRM?si=PpwuRSSvlkdwE-8R&amp;t=2431">40:29</a>): You mentioned something super interesting to me. When you're hiring engineering directors and above: in the past it was the usual interview. Culture, fit, strategy, all that stuff. You added a coding interview for every single engineering director and above hire. Can you tell me about this?</p><p>Farhan: What happens is it is shocking for VPs. To be like “whoa, there's a coding interview!” I'm like: “yeah, because we believe that they are still deeply in love with technology. And so it worked out super well. </p><p>Our whole idea is that you're not running away: you're running towards technology and it's just a better way for you to express it. So I pair with the candidates and they also see that even though I'm not writing code every day, I'm still deep in the weeds of technology. I still love technology and I still want to talk about technical topics. </p><p>And there are lots of great companies out there where that's not the requirement. But at Shopify, we believe people should be as close to the details as possible. And so that doesn't mean coding day to day, but you should understand code and how code works. The muscle memory of coding will come back in these pairing interviews.” </p></blockquote> #2: Shopify doesn’t “ban” AI tools during their full-remote interview process  <p>They actually encourage using them! This is one of the first examples I’ve heard this approach: and it seems to be working for the company:</p><blockquote><p>Gergely (<a href="https://youtu.be/u-3IILWQPRM?si=jXGp73WyJC6RVWS5&amp;t=2527">42:07</a>): “Hold on. So you're using AI during your interview process?”</p><p>Farhan: “Yes!”</p><p>Gergely: “Oh, you’re not running away from it.”</p><p>Farhan: “We're embracing it.”</p><p>Gergely: “How's it working? Tell me.”</p><p>Farhan: “ I love it. Because what happens now is the AI will sometimes generate pure garbage.”</p><p>Gergely: “So you’re screen sharing and you say, literally use anything?”</p><p>Farhan: “You let them use whatever they want. Here's what I'll say. </p><p>If they don't use a copilot, they usually get creamed by someone who does. So they will have no choice. But it’s a copilot! </p><p>Sometimes I will shadow an interview and do the questions myself. I've never seen them with a copilot and send it to the interviewer and say, please mark my assignment as well against a candidate. I have not lost yet. If they don't have a copilot, they will lose. </p><p>But when they do have a copilot, I love seeing the generated code because I want to ask them, what do you think? Is this good code? Is this not good code? Are there problems? And I've seen engineers for example: when there's something very easy to fix, they won't fix it. They will try to prompt to fix it. </p><p>And I ask: “are you really an engineer?” I get the nuance of just prompt and prompt and prompt. I’m like: “just change the one character!” — and they won't change it! And I'm like, okay, I don't want you to be 100% percent using AI. I want you be like 90 or 95%. I want you to be able to go in and look at the code and say, oh yeah, there's a line that's wrong.”</p></blockquote> The Pragmatic Engineer deepdives relevant for this episode   <p><a href="https://newsletter.pragmaticengineer.com/p/shopify-black-friday">How Shopify built its Live Globe for Black Friday</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/vibe-coding-as-a-software-engineer">Vibe coding as a software engineer</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/inside-shopifys-leveling-split">Inside Shopify's leveling split</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/cursor">Real-world engineering challenges: building Cursor</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/how-anthropic-built-artifacts">How Anthropic built Artifacts</a></p>   Timestamps <p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM">00:00</a>) Intro</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=127s">02:07</a>) Shopify’s philosophy: “hire smart people and pair with them on problems”</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=382s">06:22</a>) How Shopify works with top AI labs</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=530s">08:50</a>) The recent Code Red at Shopify</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=647s">10:47</a>) How Shopify became early users of GitHub Copilot and their pivot to trying multiple tools</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=769s">12:49</a>) The surprising ways non-engineering teams at Shopify are using Cursor</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=893s">14:53</a>) Why you have to understand code to submit a PR at Shopify</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=1002s">16:42</a>) AI tools' impact on SaaS</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=1190s">19:50</a>) Tobi Lütke’s AI memo</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=1306s">21:46</a>) Shopify’s LLM proxy and how they protect their privacy</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=1380s">23:00</a>) How Shopify utilizes MCPs</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=1619s">26:59</a>) Why AI tools aren’t the place to pinch pennies</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=1802s">30:02</a>) Farhan’s projects and favorite AI tools</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=1970s">32:50</a>) Why AI-first isn’t about freezing headcount and the value of hiring interns</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=2180s">36:20</a>) How Shopify’s engineering department operates, including internal tools</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=2431s">40:31</a>) Why Shopify added coding interviews for director-level and above hires</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=2620s">43:40</a>) What has changed since Spotify added AI tooling</p><p>(<a href="https://www.youtube.com/watch?v=u-3IILWQPRM&amp;t=2680s">44:40</a>) Farhan’s advice for implementing AI tools</p> References <p>Where to find Farhan Thawar:</p><p>• X: <a href="https://x.com/fnthawar">https://x.com/fnthawar</a></p><p>• LinkedIn: <a href="https://www.linkedin.com/in/fnthawar/">https://www.linkedin.com/in/fnthawar/</a></p><p>Mentions during the episode:</p><p>• LDX3 2025: <a href="https://leaddev.com/leaddev-london/agenda/">https://leaddev.com/leaddev-london/agenda/</a></p><p>• Shopify: <a href="https://www.shopify.com/">https://www.shopify.com/</a></p><p>• Claude Code: <a href="https://www.anthropic.com/claude-code">https://www.anthropic.com/claude-code</a></p><p>• Codex: <a href="https://openai.com/index/introducing-codex/">https://openai.com/index/introducing-codex/</a></p><p>• Ruby: <a href="https://www.ruby-lang.org">https://www.ruby-lang.org</a></p><p>• MySQL: <a href="https://www.mysql.com/">https://www.mysql.com/</a></p><p>• Thomas Dohmke on LinkedIn: <a href="https://www.linkedin.com/in/ashtom/">https://www.linkedin.com/in/ashtom/</a></p><p>• The present, past and future of GitHub: <a href="https://newsletter.pragmaticengineer.com/p/github">https://newsletter.pragmaticengineer.com/p/github</a></p><p>• GitHub Copilot: <a href="https://github.com/features/copilot">https://github.com/features/copilot</a></p><p>• Cursor: <a href="https://www.cursor.com/">https://www.cursor.com/</a></p><p>• VS Code: <a href="https://code.visualstudio.com/">https://code.visualstudio.com/</a></p><p>• Devin: <a href="https://devin.ai/">https://devin.ai/</a></p><p>• AI tools for software engineers, but without the hype – with Simon Willison (co-creator of Django): <a href="https://newsletter.pragmaticengineer.com/p/ai-tools-for-software-engineers-simon-willison">https://newsletter.pragmaticengineer.com/p/ai-tools-for-software-engineers-simon-willison</a></p><p>• Jevons paradox: <a href="https://en.wikipedia.org/wiki/Jevons_paradox#Conditions">https://en.wikipedia.org/wiki/Jevons_paradox#Conditions</a></p><p>• Tobi’s memo: Reflexive AI usage is now a baseline expectation at Shopify: <a href="https://x.com/tobi/status/1909251946235437514">https://x.com/tobi/status/1909251946235437514</a></p><p>• Librechat: <a href="https://www.librechat.ai/">https://www.librechat.ai/</a></p><p>• Farhan’s post on X about spending on AI tools: <a href="https://x.com/fnthawar/status/1930367595670274058">https://x.com/fnthawar/status/1930367595670274058</a></p><p>• Mikhail Parakhin on LinkedIn: <a href="https://www.linkedin.com/in/mikhail-parakhin/">https://www.linkedin.com/in/mikhail-parakhin/</a></p><p>• Gumloop: <a href="https://www.gumloop.com/">https://www.gumloop.com/</a></p><p>• Jira: <a href="https://www.atlassian.com/software/jira">https://www.atlassian.com/software/jira</a></p><p>• Linear: <a href="https://linear.app/">https://linear.app/</a></p><p>• How Shopify Built Its Live Globe for Black Friday: <a href="https://newsletter.pragmaticengineer.com/p/shopify-black-friday">https://newsletter.pragmaticengineer.com/p/shopify-black-friday</a></p><p>• The Pulse #86: Is Shopify’s new “mastery” framework a response to higher interest rates?: <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-86">https://newsletter.pragmaticengineer.com/p/the-pulse-86</a></p><p>• Inside Shopify's Leveling Split: Exclusive: <a href="https://newsletter.pragmaticengineer.com/p/inside-shopifys-leveling-split">https://newsletter.pragmaticengineer.com/p/inside-shopifys-leveling-split</a></p><p>• The Pulse #138: Ban or embrace AI tools in technical interviews: <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-138">https://newsletter.pragmaticengineer.com/p/the-pulse-138</a></p><p>—</p><p>Production and marketing by <a href="https://penname.co/">Pen Name</a>. </p><p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!-hVl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F065c9f3b-373f-4859-b18c-10b6e6a50562_4000x2667.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:45 &bull;
   26 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://werd.io'>Ben Werdmüller</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://werd.io/republican-budget-bill-signals-new-era-in-federal-surveillance/'>Republican Budget Bill Signals New Era in Federal Surveillance</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  "The looming rapid expansion of federal surveillance may signal a step change on a trajectory set in motion after September 11, 2001, with broad implications for the rights and privacy of all Americans."
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://www.techpolicy.press/republican-budget-bill-signals-new-era-in-federal-surveillance/?ref=werd.io">Link: Dean Jackson and Justin Hendrix at TechPolicy.Press</a>.</p><p>TechPolicy.Press looks at Trump's big bill from a technology-enabled civil rights perspective. This much has become very clear:</p><blockquote>"The current form of the Republican budget bill <a href="https://www.nytimes.com/article/house-gop-tax-bill-trump.html?ref=werd.io">commits</a> as much as $175 billion to enforce President Trump’s anti-immigration agenda. While the final number will depend on reconciliation between the chambers of Congress, the next budget will almost certainly include billions of dollars for administration priorities, such as <a href="https://migrantinsider.com/p/gop-megabill-gives-ice-30-billion?ref=werd.io">new technological capabilities</a> for the Department of Homeland Security (DHS), including Customs and Border Patrol (CBP) and ICE."</blockquote><p>What's important to understand is what this bill enables in terms of aggregation between data stores owned by individual states and departments. Trump's executive order to “ensure the Federal Government has unfettered access to comprehensive data from all State programs that receive Federal funding, including, as appropriate, data generated by those programs but maintained in third-party databases” is hugely important - not only have local government databases like this never been aggregated before, but that mention of third-party databases may require unfettered access to private services.</p><p>Data brokers already provide warrantless access to the information we're sharing with online services for a low price. It'll all be connected and - you can put money on this - used to train AI models that will supposedly provide information about us to law enforcement, ICE, and other agencies, but will also hallucinate and be subject to systemic biases.</p><p>It's fair to assume that all data that is collected anywhere will be eventually used by the worst possible actor. The best way to avoid this kind of surveillance is to never collect this kind of data in the first place. Failing those needed restrictions, we need stronger controls on how data is stored, used, and redacted.</p><p>That ship has now completely sailed. It's all out there, being used by the worst possible actor to enact policies that have the potential to lead to the worst possible things. Just as the civil liberties violations of the PATRIOT Act never really went away, the cat is out of the bag forever. Even when this administration is behind us, we can never really go back. For that, we can blame the failure of imagination of every system designer, every digital policy leader, every decision-maker who led us here.</p><p>[<a href="https://www.techpolicy.press/republican-budget-bill-signals-new-era-in-federal-surveillance/?ref=werd.io">Link</a>]</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:59 &bull;
   26 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2025/07/02/implement-asp-net-core-openid-connect-with-keykloak-to-implement-level-of-authentication-loa-requirements/'>Implement ASP.NET Core OpenID Connect with Keycloak to implement Level of Authentication (LoA) requirements</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This post looks at implementing an OpenID Connect client in ASP.NET Core and require a level of authentication (LoA) implemented using Keycloak. The applications are hosted using Aspire. The LoA is requested in Keycloak using the acr_values claim. Code: https://github.com/damienbod/IdentityExternalErrorHandling Setup The applications are implemented using Aspire. An ASP.NET Core application uses an
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This post looks at implementing an OpenID Connect client in ASP.NET Core and require a level of authentication (LoA) implemented using Keycloak. The applications are hosted using Aspire. The LoA is requested in Keycloak using the acr_values claim.</p>



<p>Code: <a href="https://github.com/damienbod/IdentityExternalErrorHandling">https://github.com/damienbod/IdentityExternalErrorHandling</a></p>



 Setup 



<p>The applications are implemented using Aspire. An ASP.NET Core application uses an OpenID Connect client to authenticate against a Keycloak server. The client application should be required to use passkeys to authenticate. This is forced using the acr_values claim with the LoA3 value. The LoA3 value is only specific to my implementation, this can be implemented using any definitions on the Keycloak server. If using a different identity provider, some require pre-defined values. If implementing this in Microsoft Entra, authentication contexts with continuous access policies can be used.</p>







<p></p>



 OpenID Connect setup 



<p>The OpenID Connect client is implemented using the standard ASP.NET Core interfaces. The client does not use OAuth PAR for this demo but OAuth PAR should be used, if the identity provider supports this. When using OAuth PAR, the PAR event must be used.</p>


  .AddOpenIdConnect("keycloak", "keycloak", options =&gt;
{
  options.SignInScheme = IdentityConstants.ExternalScheme;
  options.SignOutScheme = IdentityConstants.ApplicationScheme;
  options.RemoteSignOutPath = new PathString("/signout-callback-oidc-keycloak");
  options.SignedOutCallbackPath = new PathString("/signout-oidc-keycloak");
  options.CallbackPath = new PathString("/signin-oidc-keycloak");

  options.Authority = builder.Configuration["AuthConfiguration:IdentityProviderUrl"];
  options.ClientSecret = builder.Configuration["AuthConfiguration:ClientSecret"];
  options.ClientId = builder.Configuration["AuthConfiguration:Audience"];
  options.ResponseType = OpenIdConnectResponseType.Code;

  options.Scope.Clear();
  options.Scope.Add("openid");
  options.Scope.Add("profile");
  options.Scope.Add("email");
  options.Scope.Add("offline_access");

  options.ClaimActions.Remove("amr");
  options.ClaimActions.MapJsonKey("website", "website");

  options.GetClaimsFromUserInfoEndpoint = true;
  options.SaveTokens = true;

  options.PushedAuthorizationBehavior = PushedAuthorizationBehavior.Disable;

  options.TokenValidationParameters = new TokenValidationParameters
  {
	  NameClaimType = "name",
	  RoleClaimType = "role",
  };

  options.Events = new OpenIdConnectEvents
  {
	  // Add event handlers
  };
});
  


<p></p>



 Using the acr_values 



<p>The OnRedirectToIdentityProvider is used to set and send the level of authentication requirement to the identity provider. The acr_values claim is used for this. If using OAuth PAR, the par event should be used.</p>


  OnRedirectToIdentityProvider = async context =&gt;
{
    // Require passkeys
    context.ProtocolMessage.AcrValues = "LoA3";

    var logger = context.HttpContext.RequestServices.GetRequiredService&lt;ILogger&lt;Program&gt;&gt;();
    logger.LogInformation("OnRedirectToIdentityProvider to identity provider. Scheme: {Scheme: }", context.Scheme.Name);

    await Task.CompletedTask;
},
  


<p></p>



 Keycloak setup 



<p>Keycloak is used to implement the OpenID Connect server and implement the level of authentication requirement. Keycloak have really good docs for this:</p>



<p><a href="https://www.keycloak.org/docs/latest/server_admin/index.html#features">https://www.keycloak.org/docs/latest/server_admin/index.html#features</a></p>



<p>The application requirements are setup as follows:</p>



 
 LoA1, Level 1, password 



 LoA2, Level 2, authenticator app 



 LoA3, Level 3, passkeys 
 



<p>The alias can be set on the Realm settings:</p>







<p></p>



<p>The client browser flow must be changed to support the LoA flow. In the flow the value of the level is setup as required. This was implemented following the Keycloak documentation.</p>







<p></p>



 Notes 



<p>The Level of authentication can be implemented and forced using Keycloak and the acr_values. If using different identity providers, it will need a different client implementation. All servers force this different.</p>



<p>It is important to validate the the correct level of authentication is returned to the client application. You should also validate the amr claim as well. </p>



<p></p>



<p>Some identity provider return errors if this is incorrect, some identity providers return a weaker value for this claim. The client must be implemented depending on the used identity provider.</p>



 Links 



<p><a href="https://www.keycloak.org/docs/latest/server_admin/index.html#features">https://www.keycloak.org/docs/latest/server_admin/index.html#features</a></p>



<p><a href="https://learn.microsoft.com/en-us/aspnet/core/security/authentication/configure-oidc-web-authentication">https://learn.microsoft.com/en-us/aspnet/core/security/authentication/configure-oidc-web-authentication</a></p>



<p><a href="https://docs.duendesoftware.com/identityserver/fundamentals/openid-connect-events/">https://docs.duendesoftware.com/identityserver/fundamentals/openid-connect-events/</a></p>



<p><a href="https://learn.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.authentication.openidconnect.openidconnectevents">https://learn.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.authentication.openidconnect.openidconnectevents</a></p>



<p><a href="https://datatracker.ietf.org/doc/html/rfc9126">https://datatracker.ietf.org/doc/html/rfc9126</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2025/05/oidc-confidential-pkce-flow-drawio.png?w=640'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   04:40 &bull;
   26 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 01. July 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://identitywoman.net'>Identity Woman</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://identitywoman.net/so-you-want-an-unconference-you-can-hire-me/'>So you want an Unconference? You can Hire me!</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   I have been reflecting on Ikigai a Japanese concept blending passion, talent, purpose, and fulfillment. For me, the magical intersection is in the design and facilitation of interactive events for high performance collaboration communities.&nbsp; My offerings are now updated on the unconference.net website.&nbsp; Check out the three new case studies!.&nbsp; I am keen to work […] 
 The post So you 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>I have been reflecting on Ikigai a Japanese concept blending passion, talent, purpose, and fulfillment. For me, the magical intersection is in the design and facilitation of interactive events for high performance collaboration communities.&nbsp; My offerings are now updated on the unconference.net website.&nbsp; Check out the three new case studies!.&nbsp; I am keen to work […]</p>
<p>The post <a href="https://identitywoman.net/so-you-want-an-unconference-you-can-hire-me/">So you want an Unconference? You can Hire me!</a> appeared first on <a href="https://identitywoman.net">Identity Woman</a>.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:31 &bull;
   27 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://identitywoman.net/joining-the-sideways-verifiable-communities-project/'>Joining the Sideways Verifiable Communities project</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   I’m excited to share that I have joined the Verifiable Community project led by Grace Rachmany.&nbsp; This is a full circle moment for me, returning me to the roots of why I devoted 20 years of my life to the digital identity field. My passion has been in fostering the creation of distributed identity protocols […] 
 The post Joining the Sideways Verifiable Communities project appeared first on Id
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>I’m excited to share that I have joined the Verifiable Community project led by Grace Rachmany.&nbsp; This is a full circle moment for me, returning me to the roots of why I devoted 20 years of my life to the digital identity field. My passion has been in fostering the creation of distributed identity protocols […]</p>
<p>The post <a href="https://identitywoman.net/joining-the-sideways-verifiable-communities-project/">Joining the Sideways Verifiable Communities project</a> appeared first on <a href="https://identitywoman.net">Identity Woman</a>.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:24 &bull;
   27 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/software-engineering-with-llms-in-2025'>Software engineering with LLMs in 2025: reality check</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  How are devs at AI startups and in Big Tech using AI tools, and what do they think of them? A broad overview of the state of play in tooling, with Anthropic, Google, Amazon, and others
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Hi – this is Gergely with the monthly, free issue of the Pragmatic Engineer Newsletter. In every issue, I cover challenges at Big Tech and startups through the lens of engineering managers and senior engineers. If you’ve been forwarded this email, you can <a href="https://newsletter.pragmaticengineer.com/about">subscribe here</a>.</p><p>Two weeks ago, I gave <a href="https://leaddev.com/leaddev-london/agenda/">a keynote</a> at LDX3 in London, “Software engineering with GenAI.” During the weeks prior, I talked with software engineers at leading AI companies like Anthropic and Cursor, in Big Tech (Google, Amazon), at AI startups, and also with several seasoned software engineers, to get a sense of how teams are using various AI tools, and which trends stand out.</p><p>If you have 25 minutes to spare, check out <a href="https://www.youtube.com/watch?v=EO3_qN_Ynsk">an edited video version</a>, which was just published on my YouTube channel. A big thank you to organizers of the <a href="https://leaddev.com/leaddev-london/agenda/">LDX3 conference</a> for the superb video production, and for organizing a standout event – including the live podcast recording (released tomorrow) and a book signing for <a href="https://www.engguidebook.com/">The Software Engineer’s Guidebook</a>.</p><p><a href="https://youtu.be/EO3_qN_Ynsk">Watch the recording of the keynote</a></p> <a href="https://substackcdn.com/image/fetch/$s_!gpz7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05f7fc-3743-4340-9acc-6ed6a606c86b_1600x1067.jpeg">          </a>My keynote at LDX3, in London <p>This article covers:</p>  <p>Twin extremes. Executives at AI infrastructure companies make bold claims, which developers often find fall spectacularly flat.</p>  <p>AI dev tooling startups. Details from Anthropic, Anysphere, and Codeium, on how their engineers use Claude Code, Cursor, and Windsurf.</p>  <p>Big Tech. How Google and Amazon use the tools, including how the online retail giant is quietly becoming an MCP-first company.</p>  <p>AI startups. Oncall management startup, incident.io, and a biotech AI, share how they experiment with AI tools. Some tools stick and others are disappointments.</p>  <p>Seasoned software engineers. Observations from experienced programmers, Armin Ronacher (creator of Flask), Peter Steinberger (founder of PSPDFKit), Birgitta Böckeler (Distinguished Engineer at Thoughtworks), Simon Willison (creator of Django), Kent Beck (creator of XP), and Martin Fowler (Chief Technologist at Thoughtworks).</p>  <p>Open questions. Why are founders/CEOs more bullish than devs about AI tools, how widespread is usage among developers, how much time do AI tools really save, and more.</p>  <p>The bottom of this article could be cut off in some email clients. <a href="https://newsletter.pragmaticengineer.com/p/software-engineering-with-llms-in-2025">Read the full article uninterrupted, online.</a></p><p><a href="https://newsletter.pragmaticengineer.com/p/software-engineering-with-llms-in-2025">Read the full article online</a></p> 1. Twin extremes <p>There’s no shortage of predictions that LLMs and AI will change software engineering – or that they already have done. Let’s look at the two extremes.</p><p>Bull case: AI execs. Headlines about companies with horses in the AI race:</p>  <p>“Anthropic’s CEO said all code will be AI-generated in a year.” (<a href="https://www.inc.com/joe-procopio/anthropics-ceo-said-all-code-will-be-ai-generated-in-a-year/91163367">Inc Magazine</a>, March 2025).</p>  <p>“Microsoft's CEO reveals AI writes up to 30% of its code — some projects may have all code written by AI” (<a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/microsofts-ceo-reveals-that-ai-writes-up-to-30-percent-of-its-code-some-projects-may-have-all-of-its-code-written-by-ai">Tom’s Hardware</a>, April 2025)</p>  <p>“Google chief scientist predicts AI could perform at the level of a junior coder within a year” (<a href="https://tech.yahoo.com/ai/articles/google-chief-scientist-predicts-ai-084702595.html">Business Insider</a>, May 2025)</p>  <p>These are statements of confidence and success – and as someone working in tech, the last two might have some software engineers looking over their shoulders, worrying about job security. Still, it’s worth remembering who makes such statements: companies with AI products to sell. Of course they pump up its capabilities.</p><p>Bear case: disappointed devs. Two amusing examples about AI tools not exactly living up to the hype: the first from January, when coding tool Devin generated a bug that cost a team $733 in unnecessary costs by generating millions of <a href="https://posthog.com/">PostHog</a> analytics events:</p> <a href="https://substackcdn.com/image/fetch/$s_!G_bE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F315cf361-6603-409b-a85a-bd230c098b65_790x746.png">          </a>Source: <a href="https://x.com/abhagsain/status/1876362355870994538">Anurag Bhagsain</a> <p>While responsibility lies with the developer who accepted a commit without closer inspection, if an AI tool’s output is untrustworthy, then that tool is surely nowhere near to taking software engineers’ work.</p><p>Another case enjoyed with self-confessed schadenfreude by those not fully onboard with tech execs’ talk of hyper-productive AI, was the public preview of GitHub Copilot Agent, when the agent kept stumbling in the .NET <a href="https://www.reddit.com/r/ExperiencedDevs/comments/1krttqo/my_new_hobby_watching_ai_slowly_drive_microsoft/">codebase</a>.</p> <a href="https://substackcdn.com/image/fetch/$s_!lNMv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F681f8e4e-5daf-46c9-bd25-037b0ce4c429_1522x1150.png">          </a>Source: <a href="https://www.reddit.com/r/ExperiencedDevs/comments/1krttqo/my_new_hobby_watching_ai_slowly_drive_microsoft/">Reddit</a> <p>Fumbles included the agent adding tests that failed, with Microsoft software engineers needing to tell the agent to restart:</p> <a href="https://substackcdn.com/image/fetch/$s_!BwwO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6692b559-0635-4bb2-a45f-85aa7f80956a_1600x1147.png">          </a>Source: <a href="https://github.com/dotnet/runtime/pull/115733">GitHub</a> <p>Microsoft deserves credit for not hiding away the troubles with its agent: the .NET repository has several pull requests opened by the agent which were closed because engineers gave up on getting workable results from the AI.</p><p>We cover more on this incident in the deepdive, <a href="https://newsletter.pragmaticengineer.com/i/164565996/dogfood-thats-not-tasty-copilot-agent-fumbles-in-real-world-with-net">Microsoft is dogfooding AI dev tools’ future</a>.</p><p>So between bullish tech executives and unimpressed developers, what’s the truth? To get more details, I reached out to engineers at various types of companies, asking how they use AI tools now. Here’s what I learned…</p> 2. AI dev tools startups <p>It’s harder to find more devs using AI tools for work than those at AI tooling companies which build tools for professionals, and dogfood their products.</p> Anthropic <p>The Anthropic team told me:</p><blockquote><p>“When we gave Claude Code to our engineers and researchers, they all started using it every day, which was pretty surprising.”</p></blockquote><p>Today, 90% of the code for Claude Code is written by Claude Code(!), Anthropic’s Chief Product Officer Mike Krieger says. And usage has risen sharply since 22 May – the launch day of Claude Sonnet 4 and Claude Code:</p>  <p>40%: how much Claude Code usage increased by on the launch day of Claude Sonnet 4</p>  <p>160%: Userbase growth in the month after launch</p>  <p>These numbers suggest Claude Code and Claude Sonnet 4 are hits among developers. Boris Cherny, creator of Claude Code, said this on the <a href="https://www.latent.space/p/claude-code">Latent Space podcast</a>:</p><blockquote><p>"Anecdotally for me, it's probably doubled my productivity. I think there are some engineers at Anthropic for whom it's probably 10x-ed their productivity."</p></blockquote><p>MCP (Model Context Protocol) was created by Anthropic in November 2024. This is how it works:</p> <a href="https://substackcdn.com/image/fetch/$s_!uLip!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8af6f15a-db16-41d3-aa00-908a0d1aeb4b_1456x1090.png">          </a>Image source: <a href="https://newsletter.pragmaticengineer.com/p/mcp">MCP Protocol: a new AI dev tools building block</a> <p>MCP is gaining popularity and adoption across the industry:</p>  <p>November 2024: Anthropic <a href="https://www.anthropic.com/news/model-context-protocol">open sources MCP</a></p>  <p>December 2024 – February 2025: Block, Replit, Windsurf, and Sourcegraph, adopt the protocol</p>  <p>March, April: OpenAI, Google, Microsoft also adopt it</p>  <p>Today: Thousands of active MCP servers operate, and adoption continues</p>  <p>We cover more about the protocol and its importance in <a href="https://newsletter.pragmaticengineer.com/p/mcp">MCP Protocol: a new AI dev tools building block</a>.</p> Windsurf <p>Asked how they use their own product to build Windsurf, the team told me:</p><blockquote><p>“~95% of our code is written using Windsurf’s agent Cascade and the passive coding feature, Windsurf Tab.”</p></blockquote><p>Some non-engineers at the company also use Windsurf. Gardner Johnson, Head of Partnerships, <a href="https://www.youtube.com/watch?feature=shared&amp;v=BjzpSLtVwU4">used</a> it to build his own quoting system, and replace an existing B2B vendor.</p><p>We previously covered <a href="https://newsletter.pragmaticengineer.com/p/building-windsurf-with-varun-mohan">How Windsurf is built with CEO Varun Mohan</a>.</p> Cursor <p>~40-50% of Cursor’s code is written from output generated by Cursor, the engineering team at the dev tools startup estimated, when I asked. While this number is lower than Claude Code and Windsurf’s numbers, it’s still surprisingly high. Naturally, everyone at the company dogfoods Cursor and uses it daily.</p><p>We cover more on how Cursor is built in <a href="https://newsletter.pragmaticengineer.com/p/cursor">Real-world engineering challenges: building Cursor</a>.</p> 3. Big Tech <p>After talking with AI dev tools startups, I turned to engineers at Google and Amazon.</p> Google <p>From talking with five engineers at the search giant, it seems that when it comes to developer tooling, everything is custom-built internally. For example:</p>  <p>Borg: the Google version of Kubernetes. It predates Kubernetes, which was built by Google engineers, with learnings from Borg itself. We cover more on <a href="https://newsletter.pragmaticengineer.com/p/how-kubernetes-is-built-with-kat">the history of Kubernetes with Kat Cosgrove</a>.</p>  <p>Cider: the Google version of their IDE. Originally, it started off as a web-based editor. Later, a VS Code fork was created (called Cider-v). Today, this VS Code version is the “main” one and is simply called “Cider.”</p>  <p>Critique: in-house version of GitHub’s code review</p>  <p>Code Search: the internal Sourcegraph, which Code Search predates. Sourcegraph was inspired by Code Search. We previously covered <a href="https://newsletter.pragmaticengineer.com/p/inside-sourcegraphs-engineering-culture">Sourcegraph’s engineering culture.</a></p>  <p>The reason Google has “custom everything” for its tooling is because the tools are integrated tightly with each other. Among Big Tech, Google has the single best development tooling: everything works with everything else, and thanks to deep integrations, it’s no surprise Google added AI integrations to all of these tools:</p>  <p>Cider:</p>  <p>Multi-line code completion</p>  <p>Chat with LLM inside IDE for prompting</p>  <p>Powered by Gemini</p>  <p>As a current engineer told me: “Cider suggests CL [changelist – Google’s version of pull requests] descriptions, AI input on code reviews, AI auto complete. It has a chat interface like Cursor, but the UX is not as good.”</p>    <p>Critique: AI code review suggestions</p>  <p>CodeSearch: AI also integrated</p>  <p>An engineer told me that Google seems to be taking things “slow and steady” with developer tools:</p><blockquote><p>“Generally, Google is taking a very cautious approach here to build trust. They definitely want to get it right the first time, so that software engineers (SWEs) can trust it.”</p></blockquote><p>Other commonly-used tools:</p>  <p>Gemini: App and Gemini in Workspace features are usually dogfooded internally, and are available with unlimited usage for engineers</p>  <p>LLM prompt playground: works very similarly to OpenAI’s dev playground, and predates it</p>  <p>Internal LLM usage: various Gemini models are available for internal use: big and small, instruction-tuned, and more creative ones, thinking models and experimental ones.</p>  <p>MOMA search engine: knowledge base using LLMs. This is a chatbot fine-tuned with Google’s inside knowledge. The underlying model is based on some version of the Gemini model, but what it provides is pretty basic: answers to direct questions. Devs tell me MOMA is promising, but not as useful as some hoped, likely due to how dependent it is on internal documentation. For example, if a team’s service is badly documented and lacks references, the model wouldn’t do well on questions about it. And since all Google’s services are custom, the generic model knowledge doesn’t help (e.g., details about Kubernetes don’t necessarily apply to Borg!)</p>  <p>NotebookLM: heavily used. One use case is to feed in all product requirement documents / user experience researcher documents, and then ask questions about the contents. <a href="https://notebooklm.google.com/">NotebookLM</a> is a publicly available product.</p>  <p>Google keeps investing in “internal AI islands.” A current software engineer told me:</p><blockquote><p>“There are many org-specific and team-specific GenAI tooling projects happening everywhere. This is because it’s what leadership likes to see, these days!</p><p>Cynically: starting an AI project is partly how you get more funding these days. As to how effective this tooling is, who knows!”</p></blockquote><p>I’d add that Google’s strategy of funding AI initiatives across the org might feel wasteful at first glance, but it’s exactly how successful products like NotebookLM were born. Google has more than enough capacity to fund hundreds of projects, and keep doubling down on those that win traction, or might generate hefty revenue.</p><p>Google is preparing for 10x more code to be shipped. A former Google Site Reliability Engineer (SRE) told me:</p><blockquote><p>“What I’m hearing from SRE friends is that they are preparing for 10x the lines of code making their way into production.”</p></blockquote><p>If any company has data on the likely impact of AI tools, it’s Google. 10x as much code generated will likely also mean 10x more:</p>  <p>Code review</p>  <p>Deployments</p>  <p>Feature flags</p>  <p>Source control footprint</p>  <p>… and, perhaps, even bugs and outages, if not handled with care</p>   Amazon <p>I talked with six current software development engineers (SDEs) at the company for a sense of the tools they use.</p><p>Amazon Q Developer is Amazon’s own GitHub Copilot. Every developer has free access to the Pro tier and is strongly incentivized to use it. Amazon leadership and principal engineers at the company keep reminding everyone about it.</p><p>What I gather is that this tool was underwhelming at launch around two years ago because it only used Amazon’s in-house model, <a href="https://newsletter.pragmaticengineer.com/i/155552919/amazons-ai-models-fail-to-impress">Nova</a>. Nova was underwhelming, meaning Q was, too.</p><p>This April, that changed: Q did away with the Nova dependency and became a lot better. Around half of devs I spoke with now really like the new Q; it works well for AWS-related tasks, and also does better than other models in working with the Amazon codebase. This is because Amazon also trained a few internal LLMs on their own codebase, and Q can use these tailored models. Other impressions:</p>  <p>Limited to files. Amazon Q can currently only understand one file at a time — a limitations SDEs need to work around.</p>  <p>Works well with Java. If Amazon runs on one thing, it’s Java, so this is a great fit.</p>  <p>Finetuned models are only marginally better. Even models trained on Amazon’s own codebase feel only moderately better than non-trained models, surprisingly.</p>  <p>Cline hooked up to Bedrock is a popular alternative: A lot of SDEs prefer to use Cline hooked up to AWS Bedrock where they run a model (usually Sonnet 4)</p>  <p>Q CLI: the command line interface (CLI) is becoming very popular very quickly internally, thanks to this tool using the AWS CLI being able to directly hook up to MCP servers, of which Amazon has hundreds already (discussed below)</p>  <p>Q Transform: used for platform migrations internally, migrating from one language version (e.g. Java 8) to another (e.g. Java 11). It’s still hit-and-miss, said engineers: it works great with some internal services, and not others. Q transform is <a href="https://aws.amazon.com/q/developer/transform/">publicly available</a>.</p>  <p>Amazon Q is a publicly available product and so far, the feedback I’m hearing from non-Amazon devs is mixed: it works better for AWS context, but a frequent complaint is how <a href="https://x.com/andrewvijay/status/1939926424283984010">slow autocomplete is</a>, even for paying customers. Companies paying for Amazon Q Pro are exploring snappier alternatives, like Cursor.</p><p>Claude Sonnet is another tool most Amazon SDEs use for any writing-related work. Amazon is a partner to Anthropic, which created these models, and SDEs can access Sonnet models easily – or just spin up their own instance on Bedrock. While devs could also use the more advanced Opus model, I’m told this model has persistent capacity problems – at least at present.</p><p>What SDEs are using the models for:</p>  <p>Writing <a href="https://newsletter.pragmaticengineer.com/i/49561596/vocabulary">PR/FAQ documents</a> (also called “working backwards” documents). These documents are a big part of the culture, as covered in <a href="https://newsletter.pragmaticengineer.com/p/amazon">Inside Amazon’s engineering culture</a>.</p>  <p>Writing performance review feedback for peers, and to generate self-reviews</p>  <p>Writing documentation</p>  <p>…any writing task which feels like a chore!</p>  <p>It’s worth considering what it would mean if more devs used LLMs to generate “mandatory” documents, instead of their own capabilities. Before LLMs, writing was a forcing function of thinking; it’s why Amazon has its culture of “writing things down.” There are cases where LLMs are genuinely helpful, like for self-review, where an LLM can go through PRs and JIRA tickets from the last 6 months to summarize work. But in many cases, LLMs generate a lot more text with much shorter prompts, so will the amount of time spent thinking about problems reduce with LLMs doing the writing?</p> Amazon to become “MCP-first?” <p>In 2002, Amazon founder and CEO Jeff Bezos introduced an “API mandate.” As former Amazon engineer Steve Yegge <a href="https://gist.github.com/chitchcock/1281611">recalled</a>:</p><blockquote><p>“[Jeff Bezos’] Big Mandate went something along these lines:</p><p>1. All teams will henceforth expose their data and functionality through service interfaces.</p><p>2. Teams must communicate with each other through these interfaces.</p><p>3. There will be no other form of interprocess communication allowed: no direct linking, no direct reads of another team's data store, no shared-memory model, no back-doors whatsoever. The only communication allowed is via service interface calls over the network. (...)</p><p>6. Anyone who doesn't do this will be fired.</p><p>7. Thank you; have a nice day!</p><p>Ha, ha! Ex-Amazon folks will of course realize immediately that #7 was a little joke I threw in, because Bezos most definitely does not give a s**t about your day.</p><p>#6 was real, so people went to work.”</p></blockquote><p>Since the mid-2000s, Amazon has been an “API-first” company. Every service a team owned offered APIs for any other team to use. Amazon then started to make several of its services available externally, and we can see many of those APIs as today’s AWS services. In 2025, Amazon is a company with thousands of teams, thousands of services, and as many APIs as services.</p><p>Turning an API into an MCP server is trivial, which Amazon does at scale. It’s simple for teams that own APIs to turn them into MCP servers, and these MCP servers can be used by devs with their IDEs and agents to get things done. A current SDE told me:</p><blockquote><p>“Most internal tools and websites already added MCP support. This means it’s trivial to hook up automation with an agent and the ticketing agent, email systems, or any other internal service with an API. You can chain pretty much everything!”</p></blockquote><p>Another engineer elaborated:</p><blockquote><p>“There’s even an internal amazon MCP server that hooks into our wiki, ticketing system, and Quip. The internal MCP also works with Q CLI. This integration steadily increased in popularity internally.”</p></blockquote><p>Developers are often selectively lazy, and some have started to automate previously tedious workflows.</p><p>Amazon is likely the global leader in adopting MCP servers at scale, and all of this can be traced back to that 2002 mandate from Bezos pushing everyone to build APIs.</p> 4. AI startups <p>Next, I turned to engineers working at startups building AI products, but not AI developer tools. I was curious about how much cutting-edge companies use LLMs for development.</p> incident .io <p>The startup is a platform for oncall, incident response, and status pages, and became AI-first in the past year, given how useful LLMs are in this area. (Note: I’m <a href="https://blog.pragmaticengineer.com/investing/">an investor</a> in the company.)</p><p>Software engineer <a href="https://blog.lawrencejones.dev/about/">Lawrence Jones</a> said:</p><blockquote><p>“Our team is massively into using AI tools to accelerate them. Over the last couple of years we’ve…</p>  <p>Seen many engineers adopt IDEs like Cursor and use them for both writing code and understanding it</p>  <p>Built Claude Code 'Projects' which contain our engineering documentation, so people can draft code in our style, according to our conventions and architecture preferences</p>  <p>Lots of the team use <a href="https://www.granola.ai/">Granola</a> to track notes from calls, sometimes grabbing a room to just talk to their phone about plans which they’ll later reformat into a doc</p>  <p>Claude Code has been the biggest change, though. Our entire team are regular users. Claude Code is the interactive terminal app that runs an Anthropic agent to explore and modify your codebase.”</p></blockquote><p>The team has a Slack channel where team members share their experience with AI tools for discussion. Lawrence shared a few screenshots of the types of learnings shared:</p> <a href="https://substackcdn.com/image/fetch/$s_!zSH4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F964e5827-9721-4541-9aa2-6cc2aa612230_1222x552.png">          </a>Using Linear MCP: sharing learnings with the team  <a href="https://substackcdn.com/image/fetch/$s_!qOE0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddca236-e685-4a58-9e4c-17762c63bc11_1600x691.png">          </a>Using Claude for research: sharing what worked with the rest of the team  <a href="https://substackcdn.com/image/fetch/$s_!CXfq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb98d631-0311-4d6f-8708-d4cadb5db62c_1600x512.png">          </a>Asking Claude for options: a few things that worked for an engineer <p>The startup feels like it’s in heavy experimentation mode with tools. Sharing learnings internally surely helps devs get a better feel for what works and what doesn’t.</p> Biotech AI startup <p>One startup asked not to be named because no AI tools have “stuck” for them just yet, and they’re not alone. But there’s pressure to not appear “anti-AI”, especially as theirs is a LLM-based business.</p><p>The company builds ML and AI models to design proteins, and much of the work is around building numerical and automated ML pipelines. The business is doing great, and has raised multiple rounds of funding, thanks to a product gaining traction within biology laboratories. The company employs a few dozen software engineers.</p><p>The team uses very few AI coding tools. Around half of devs use Vim or <a href="https://helix-editor.com/">Helix</a> as editors. The rest use VS Code or PyCharm – plus the “usual” Python tooling like Jupyter Notebooks. Tools like Cursor are not currently used by engineers, though they were trialled.</p><p>The company rolled out an AI code review tool, but found that 90% of AI comments were unhelpful. Despite the other 10% being good, the feedback felt too noisy. Here’s how an engineer at the company summarized things:</p><blockquote><p>“We've experimented with several options with LLMs, but little has really stuck.</p><p>It's still faster to just write correct code than to review LLM code and fix its problems, even using the latest models.</p><p>Given the hype around LLMs, I speculate that we might just be in a weird niche.”</p></blockquote><p>An interesting detail emerged when I asked how they would compare the impact of AI tools to other innovations in the field. This engineer said that for their domain, the impact of the <a href="https://docs.astral.sh/uv/">uv</a> project manager and <a href="https://github.com/astral-sh/ruff">ruff</a> linter has been greater than AI tools, since uv made their development experience visibly faster!</p> <a href="https://substackcdn.com/image/fetch/$s_!Als6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc899f2e8-30be-4017-a7f8-6a4e638b7f26_585x167.png">   </a> <p><a href="https://github.com/astral-sh/ruff">Ruff</a> is 10-100x faster than existing Python linters. Moving to this linter created a noticeable developer productivity gain for the biotech AI startup</p><p>It might be interesting to compare the impact of AI tools to other recent tools like ruff/uv. These have had a far greater impact.</p><p>This startup is a reminder that AI tools are not one-size-fits-all. The company is in an unusual niche where ML pipelines are far more common than at most companies, so the software they write will feel more atypical than at a “traditional” software company.</p><p>The startup keeps experimenting with anything that looks promising for developer productivity: they’ve found moving to high-performance Python libraries is a lot more impactful than using the latest AI tools and models; for now, that is!</p> 5. Seasoned software engineers <p>Finally, I turned to a group of accomplished software engineers, who have been in the industry for years, and were considered standout tech professionals before AI tools started to spread.</p> Armin Ronacher: from skeptic to believer <p><a href="https://lucumr.pocoo.org/">Armin</a> is the creator of Flask, a popular Python library, and was the first engineering hire at application monitoring scaleup, Sentry. He has been a developer professionally for 17 years, and was pretty unconvinced by AI tooling, until very recently. Then, a month ago he published a blog post, <a href="https://lucumr.pocoo.org/2025/6/4/changes/">AI changes everything</a>:</p><blockquote><p>“If you would have told me even just six months ago that I'd prefer being an engineering lead to a virtual programmer intern over hitting the keys myself, I would not have believed it. I can go and make a coffee, and progress still happens. I can be at the playground with my youngest while work continues in the background. Even as I'm writing this blog post, Claude is doing some refactorings.”</p></blockquote><p>I asked what changed his mind about the usefulness of these tools.</p><blockquote><p>“A few things changed in the last few months:</p>  <p>Claude Code got shockingly good. Not just in the quality of the code, but in how much I trust it. I used to be scared of giving it all permissions, now it's an acceptable risk to me – with some hand holding.</p>  <p>I learned more. I learned from others, and learned myself, about how to get it to make productivity gains</p>  <p>Clearing the hurdle of not accepting it, by using LLMs extensively. I was very skeptical; in particular, my usage of Cursor and similar code completion actually went down for a while because I was dissatisfied. The agentic flow, on the other hand, went from being not useful at all, to indispensable.</p>  <p>Agents change the game. Tool usage, custom tool usage, and agents writing their own tools to iterate, are massive game changers. The faults of the models are almost entirely avoided because they can run the code and see what happens. With Sonnet 3.7 and 4, I noticed a significant step up in the ability to use tools, even if the tools are previously unknown or agent created.”</p>  </blockquote> Peter Steinberger: rediscovering a spark for creation <p><a href="https://steipete.me/">Peter Steinberger</a> has been an iOS and Mac developer for 17 years, and is founder of PSPDFKit. In 2021, he sold all his shares in the company when PSPDFKit raised €100M in funding. He then started to tinker with building small projects on the side. Exactly one month ago, he published the post <a href="https://steipete.me/posts/2025/finding-my-spark-again">The spark returns</a>. He writes:</p><blockquote><p>“Right now, we are at an incredible crossroads in technology. AI is moving so fast and is changing the way we work in software development, but furthermore, it’s going to change the world. I haven’t been as excited, astounded, and amazed by any technology in a very long time.”</p></blockquote><p>Indeed, something major did change for Pete: for the first time in ages he started to code regularly.</p> <a href="https://substackcdn.com/image/fetch/$s_!ELSh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7c3aef0-456a-442c-bed9-f19a186de2c3_788x498.png">          </a>Source: Peter Steinberger <a href="https://x.com/steipete/status/1925983535958999393">on X</a> <p>I asked what the trigger was that got him back to daily coding. Peter’s response:</p><blockquote><p>“Tools got better. Models reached a stage where they are really capable, pricing went down: we're at this inflection point where suddenly things "just work", and especially with Cursor and Claude Code they became easy. Everyone can just open that thing on their project, type in what they want and it just happens.</p><p>I see more and more folks getting infected by it. Once they see how capable this new generation of tools is, it doesn't take long before their excitement is through the roof. These tools fundamentally change how we build software.</p><p>Suddenly, every side project is just a few sentences away, code becomes cheap, languages and frameworks matter less because it got incredibly simple to just switch. Combine that power with a capable engineer, and you easily create 10-20x the output.</p><p>I see people left and right quitting their jobs to dedicate all their time to AI. My friend just said "it's the most exciting time since I started to learn programming”. Suddenly, I feel I can build anything I want.”</p></blockquote><p>Pete emphasized:</p><blockquote><p>“I’m telling you, [agentic AI tools] are the biggest shift, ever. Been talking to a bunch of engineers who wanna quit their job just because they wanna go all in on doing stuff with AI!”</p></blockquote> Birgitta Böckeler: a new “lateral move” in development <p><a href="https://www.linkedin.com/in/birgittaboeckeler/">Birgitta</a> is a Distinguished Engineer at Thoughtworks, and has been writing code for 20 years. She has been experimenting with and researching GenAI tools for the last two years, and last week published <a href="https://newsletter.pragmaticengineer.com/p/two-years-of-using-ai">Learnings from two years of using AI tools for software engineering</a> in The Pragmatic Engineer. Talking with me, she summarized the state of GenAI tooling:</p><blockquote><p>“We should embrace that GenAI is a lateral move and opportunity for something new, not a continuation of how we've abstracted and automated, previously. We now have this new tool that allows us to specify things in an unstructured way, and we can use it on any abstraction level. We can create low code applications with it, framework code, even Assembly.</p><p>I find this lateral move much more exciting than thinking of natural language as "yet another abstraction level". LLMs open up a totally new way in from the side, which brings so many new opportunities.”</p></blockquote> Simon Willison: “coding agents” actually work now <p><a href="https://simonwillison.net/">Simon</a> has been a developer for 25 years, is the creator of Django, and works as an independent software engineer. He writes an <a href="https://simonwillison.net/">interesting tech blog,</a> documenting learnings from working with LLMs, daily. He was also the first-ever guest on The Pragmatic Engineer Podcast in <a href="https://newsletter.pragmaticengineer.com/p/ai-tools-for-software-engineers-simon-willison">AI tools for software engineers, but without the hype</a>. I asked how he sees the current state of GenAI tools used for software development:</p><blockquote><p>“Coding agents are a thing that actually work now: run an LLM in a loop, let it execute compilers and tests and linters and other tools, give it a goal, and watch it do the work for you. The models’ improvement in the last six months have tipped them over from fun toy demos, to being useful on a daily basis.”</p></blockquote> Kent Beck: Having more fun than ever <p><a href="https://tidyfirst.substack.com/about">Kent Beck</a> is the creator of Extreme Programming (XP), an early advocate of Test Driven Development (TDD), and co-author of the Agile Manifesto. In <a href="https://newsletter.pragmaticengineer.com/p/tdd-ai-agents-and-coding-with-kent">a recent podcast episode</a> he said:</p><p>“I’m having more fun programming than I ever had in 52 years.”</p><p>AI agents revitalized Kent, who says he feels he can take on more ambitious projects, and worry less about mastering the syntax of the latest framework being used. I asked if he’s seen other “step changes” for software engineering in the 50 years of his career, as what LLMs seem to provide. He said he has:</p><blockquote><p>“I saw similar changes, impact-wise:</p>  <p>Microprocessors (1970s): the shift from mainframe computing</p>  <p>The internet (2000s): changed the digital economy</p>  <p>iPhone and Android (2010s): suddenly things like live location sharing is possible, and the percentage of time spent online sharply increased”</p>  </blockquote> Martin Fowler: LLMs are a new nature of abstraction <p>Martin Fowler is Chief Scientist at Thoughworks, author of the book Refactoring, and a co-author of the Agile Manifesto. This is what he told me about LLMs:</p><blockquote><p>“I think the appearance of LLMs will change software development to a similar degree as the change from assembler to the first high-level programming languages did.</p><p>The further development of languages and frameworks increased our abstraction level and productivity, but didn't have that kind of impact on the nature of programming.</p><p>LLMs are making the same degree of impact as high-level languages made versus the assembler. The distinction is that LLMs are not just raising the level of abstraction, but also forcing us to consider what it means to program with non-deterministic tools.”</p></blockquote><p>Martin expands on his thoughts in the article, <a href="https://www.martinfowler.com/articles/2025-nature-abstraction.html">LLMs bring a new nature of abstraction</a>.</p> 6. Open questions <p>There are plenty of success stories in Big Tech, AI startups, and from veteran software engineers, about using AI tools for development. But many questions also remain, including:</p> #1: Why are founders and CEOs much more excited? <p>Founders and CEOs seem to be far more convinced of the breakthrough nature of AI tools for coding, than software engineers are. One software engineer-turned-founder and executive who runs <a href="https://www.warp.dev/warp-ai">Warp</a>, an AI-powered command line startup, posted for help in convincing devs to stop dragging their feet on adopting LLMs for building software:</p> <a href="https://substackcdn.com/image/fetch/$s_!Ni5k!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a633fc6-13e1-469b-8445-ff5d5dccb5b7_802x666.png">          </a>Source: Zach Lloyd <a href="https://www.linkedin.com/posts/zachlloyd_anyone-else-having-a-hard-time-getting-their-activity-7333511252666662913-RK7g?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAIk0KwBsmE3oBadWSg2ettxmEyKbqZKG34">on LinkedIn</a>  #2: How much do devs use AI? <p>Developer intelligence platform startup <a href="https://getdx.com/">DX</a> recently ran a study with 38,000 participants. It’s still not published, but I got access to it (note: I’m <a href="https://blog.pragmaticengineer.com/investing/">an investor</a> at DX, and advise them). They asked developers whether they use AI tools at least once a week:</p>  <p>5 out of 10 devs use AI tools weekly across all companies (50%)</p>  <p>6 out of 10 devs use them weekly at “top” companies (62%)</p>  <p>On one hand, that is incredible adoption. GitHub Copilot launched with general availability 3 years ago, and Cursor launched just 2 years ago. For 50% of all developers to use AI-powered dev tools in such a short time feels like faster adoption than any tool has achieved, to date.</p><p>On the other hand, half of devs don’t even use these new tools once a week. It’s safe to assume many devs gave them a try, but decided against them, or their employer hasn’t invested.</p> #3: How much time does AI save devs, really? <p>In the same study, DX asked participants to estimate how much time these tools saved for them. On the median, it’s around 4 hours per week:</p> <a href="https://substackcdn.com/image/fetch/$s_!12-U!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a02fd0f-7693-48e2-be3e-f7605631e2f2_1600x901.png">          </a>Source: a study by DX. Study yet to be published <p>Is four hours lots? It’s 10% of a 40-hour workweek, which is certainly meaningful. But it is nowhere near the amounts reported in the media: like Sam Altman’s <a href="https://www.windowscentral.com/software-apps/sam-altman-ai-will-make-coders-10x-more-productive-not-replace-them">claim</a> that AI could make engineers 10x as productive.</p><p>Google CEO Sundar Pichai also estimated that the company is seeing 10% productivity increase thanks to AI tools on a <a href="https://lexfridman.com/sundar-pichai-transcript">Lex Fridman podcast episode</a>, which roughly matches the DX study.</p><p>This number feels grounded to me: devs don’t spend all their time coding, after all! There’s a lot of thinking and talking with others, admin work, code reviews, and much else to do.</p> #4: Why don’t AI tools work so great for orgs? <p>Laura Tacho, CTO at DX told me:</p><blockquote><p>“These GenAI tools are great for the individual developer right now, but not yet that good at the organizational level.”</p></blockquote><p>This observation makes sense: increasing coding output will not lead to faster software production, automatically; not without increasing code review throughout, deployment frequency, doing more testing (as more code likely means more bugs), and adapting the whole “software development pipeline” to make use of faster coding.</p><p>Plus, there’s the issue that some things simply take time: planning, testing, gathering feedback from users and customers, etc. Even if code is generated in milliseconds, other real-world constraints don’t just vanish.</p> #5: Lack of buzz among devs <p>I left this question to last: why do many developers not believe in LLMs’ usefulness, before they try it out? It’s likely to do with the theory that LLMs are less useful in practice, then they theoretically should be.</p><p>Simon Willison has an interesting observation, which he shared on the podcast:</p><blockquote><p>“Right now, if you start with the theory, it will hold you back. With LLMs, it's weirdly harmful to spend too much time trying to understand how they actually work, before you start playing with them, which is very unintuitive.</p><p>I have friends who say that if you're a machine learning researcher, if you've been training models and stuff for years, you're actually more disadvantaged when starting to use these tools, than if you come in completely fresh! That’s because LLMs are very weird; they don't react like you expect from other machine learning models.”</p></blockquote> Takeaways <p>Summarizing the different groups which use LLMs for development, there’s surprising contributions from each:</p> <a href="https://substackcdn.com/image/fetch/$s_!nfev!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b99b231-c717-4130-81c1-1ebaf5e4e364_1498x696.png">          </a> <p>I’m not too surprised about the first three groups:</p>  <p>AI dev tools startups: their existence depends on selling tools to devs, so it’s only natural they’d “eat their own dogfood”</p>  <p>Big Tech: companies like Google and Amazon are very profitable and want to protect their technology advantage, so will invest heavily in any technology that could disrupt them, and incentivize engineers to use these tools; especially home grown ones, like Google’s Gemini and Amazon’s Q.</p>  <p>AI startups: these are innovative companies, so it’s little surprise they experiment with AI dev tools. I found it refreshing to talk to a startup where the new tools don’t work that well, yet.</p>  <p>The last one is where I pay a lot more attention. For seasoned software engineers: most of these folks had doubts, and were sceptical about AI tools until very recently. Now, most are surprisingly enthusiastic, and see AI dev tools as a step change that will reshape how we do software development.</p><p>LLMs are a new tool for building software that us engineers should become hands-on with. There seems to have been a breakthrough with AI agents like Claude Code in the last few months. Agents that can now “use” the command line to get feedback about suggested changes: and thanks to this addition, they have become much more capable than their predecessors.</p><p>As Kent Beck put it in our conversation:</p><blockquote><p>“The whole landscape of what's ‘cheap’ and what's ‘expensive’ has shifted.</p><p>Things that we didn't do because we assumed they were expensive or hard, just got ridiculously cheap.</p><p>So, we just have to be trying stuff!”</p></blockquote><p>It’s time to experiment! If there is one takeaway, it would be to try out tools like Claude Code/OpenAI Codex/Amp/Gemini CLI/Amazon Q CLI (with AWS CLI integration), editors like Cursor/Windsurf/VS Code with Copilot, other tools like Cline, Aider, Zed – and indeed anything that looks interesting. We’re in for exciting times, as a new category of tools are built that will be as commonplace in a few years as using a visual IDE, or utilizing Git as a source control, is today.</p><p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!gpz7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05f7fc-3743-4340-9acc-6ed6a606c86b_1600x1067.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:07 &bull;
   27 days ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 24. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.sakimura.org'>@_Nat Zone</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.sakimura.org/2025/06/7204/'>iPhoneにマイナンバーカードを搭載してみた！2024年6月24日スタート</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  2024年6月24日からついにiPhoneでマイナンバーカードが利用できるようになりました！実際にセットアップして、どんな機能が使えるのか、そしてAndroidスマホに与える影響についても検証してみました。 📱 動画の内容 🔑 必要なもの ⚠️ 重要なポイント 🎯 タイムスタンプ…
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>2024年6月24日からついにiPhoneでマイナンバーカードが利用できるようになりました！実際にセットアップして、どんな機能が使えるのか、そしてAndroidスマホに与える影響についても検証してみました。 </p>



 

 



  動画の内容 



 
 iPhoneマイナンバーカード搭載の手順を詳しく解説 



 必要なもの：iPhone、マイナンバーカード、各種暗証番号 



 マイナポータルアプリのダウンロードから設定まで 



 Apple Walletへの追加方法 



 既存のAndroidスマホへの影響を検証 
 



  必要なもの 



 
 iPhone（対応機種） 



 マイナンバーカード（実物） 



 券面入力用暗証番号（4桁） 



 署名用パスワード（英数字6-16文字） 
 



  重要なポイント 



 
 マイナンバーカードは1人1台のスマートフォンにのみ搭載可能 



 iPhoneのマイナンバーカードは運転免許証の代わりにはならない 



 他のスマートフォンに搭載していた場合は削除される 
 



<p> タイムスタンプ付き目次 </p>



<p><a href="https://www.youtube.com/watch?v=EZImfN58yU0">00:00</a> イントロダクション <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=28s">00:28</a> 必要なものの確認 <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=36s">00:36</a> マイナポータルアプリのダウンロード <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=57s">00:57</a> アプリの初期設定と注意事項の確認 <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=90s">01:30</a> 利用規約の確認（UI改善点も指摘） <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=146s">02:26</a> 顔認証の撮影手順 <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=195s">03:15</a> 暗証番号・パスワードの入力 <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=228s">03:48</a> マイナンバーカードの読み取り <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=241s">04:01</a> iPhone用暗証番号・パスワードの設定 <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=277s">04:37</a> Apple Walletへの追加 <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=294s">04:54</a> 身分証明書の使用方法 <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=333s">05:33</a> 設定完了と通知設定 <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=375s">06:15</a> Apple Walletでの確認 <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=411s">06:51</a> Android側への影響を検証 <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=491s">08:11</a> 関連する不具合について <br><a href="https://www.youtube.com/watch?v=EZImfN58yU0&amp;t=522s">08:42</a> まとめ・終了 </p>



  検証結果  



<p>Android側では利用者証明用証明書が無効になる一方、署名用証明書は有効のまま残るという興味深い現象を確認しました（追記：翌朝になったら失効していました。が、その後確認するとまた有効になっていたりと不思議な状態です。）また、一時的にnanacoやSuicaが使用できなくなる問題も発生しましたが、再起動で解決しました。</p>



<p>— </p>



<p> デジタルIDチャンネルでは、デジタル身分証明書や認証技術について分かりやすく解説しています。  この動画が参考になったら「Good」ボタンをお願いします！  チャンネル登録もよろしくお願いいたします！ </p>



<p><a href="https://www.youtube.com/hashtag/%E3%83%9E%E3%82%A4%E3%83%8A%E3%83%B3%E3%83%90%E3%83%BC%E3%82%AB%E3%83%BC%E3%83%89">#マイナンバーカード</a><a href="https://www.youtube.com/hashtag/iphone"> #iPhone</a><a href="https://www.youtube.com/hashtag/%E3%83%87%E3%82%B8%E3%82%BF%E3%83%ABid"> #デジタルID</a><a href="https://www.youtube.com/hashtag/%E3%83%9E%E3%82%A4%E3%83%8A%E3%83%9D%E3%83%BC%E3%82%BF%E3%83%AB"> #マイナポータル</a><a href="https://www.youtube.com/hashtag/applewallet"> #AppleWallet</a></p>



  Android上のスマホ用（署名用）電子証明書のその後：謎は深まる 



<p>なお、翌朝になったらAndroid上のスマホ用（署名用）電子証明書は失効していました。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2025/06/Screenshot_20250625-093956-scaled.png?ssl=1"></a>



<p>ところがその15分後に確認するとまた有効に…。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2025/06/Screenshot_20250625-095503-scaled.png?ssl=1"></a>



<p>さらにその20分後に確認するとまた無効に。</p>



<a href="https://i0.wp.com/www.sakimura.org/wp-content/uploads/2025/06/Screenshot_20250625-101534-scaled.png?ssl=1"></a>



<p>なんか、わたしバグ踏んでますかね…。</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://s.w.org/images/core/emoji/16.0.1/72x72/1f4f1.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:23 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/two-years-of-using-ai'>Learnings from two years of using AI tools for software engineering</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  How to think about today’s AI tools, approaches that work well, and concerns about using them for development. Guest post by Birgitta Böckeler, Distinguished Engineer at Thoughtworks
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>It feels like GenAI is changing software engineering fast: first, it was smarter autocomplete, and now there’s ever more agentic tools that many engineers utilize. But what are some practical approaches for using these tools?</p><p>To find out more, I turned to <a href="https://birgitta.info/">Birgitta Böckeler</a>, Distinguished Engineer at Thoughtworks, who has been tackling this question full time for the past two years. She still writes production code at Thoughtworks, but her main focus is developing expertise in AI-assisted software delivery.</p><p>To stay on top of the latest developments, Birgitta talks to Thoughtworks colleagues, clients, and fellow industry practitioners, and uses the tools. She tries out tools, and figures out how they fit into her workflow. Today, Birgitta walks us through what she’s learned the last two years of working with AI tools:</p>  <p>Evolution from “autocomplete on steroids” to AI agents. From the early days of autocompete, through AI chats and IDE integration, to the agentic step change.</p>  <p>Working with AI: a practical mental model of your “AI teammate,” beware of cognitive biases where GenAI can “manipulate” you, and emerging workflows with AI</p>  <p>Impact on team effectiveness. AI coding assistants increase the speed of software delivery – though it’s complicated to measure by exactly how much. Without close supervision, the impact on quality could be negative. Team dynamics will most likely be impacted when rolling out these tools quickly.</p>  <p>The future. LLMs are not the next compilers: they are something different, the future of AI coding is unevenly distributed, and we will take on tech debt while figuring out how to use these AI tools the right way.</p>  <p>To learn more, check out additional thoughts by Birgitta in the <a href="https://martinfowler.com/articles/exploring-gen-ai.html">Exploring Generative AI</a> collection on her colleague Martin Fowler's website.</p><p>Programming note: this week, I’m in Mongolia for the launch of The Software Engineer’s Guidebook translated into Mongolian, so there will be no podcast episode or The Pulse this week: see you for the next issue, next Tuesday!</p><p>With that, it’s over to Birgitta. Note, the terms AI, Generative AI, and LLM are used interchangeably throughout this article.</p>   <p>Almost precisely 2 years ago in July 2023, Thoughtworks decided to introduce a full-time, subject-matter expert role for "AI-assisted software delivery". It was when the immense impact that Generative AI can have on software delivery was becoming ever more apparent, and I was fortunate enough to be in the right place at the right time, with the right qualifications to take on the position. And I’ve been drinking from the firehose ever since.</p><p>I see myself as a domain expert for effective software delivery who applies Generative AI to that domain. As part of the role, I talk to Thoughtworks colleagues, clients, and fellow industry practitioners. I use the tools myself and try to stay on top of the latest developments, and regularly <a href="https://martinfowler.com/articles/exploring-gen-ai.html">write</a> and <a href="https://birgitta.info/">talk</a> about my findings and experiences.</p><p>This article is a round-up of my findings, experiences, and content, from the past 2 years.</p> 1. Evolution from “autocomplete on steroids” to AI agents <p>AI coding tools have been developing at breakneck speed, making it very hard to stay on top of the latest developments. Therefore, developers not only face the challenge of adapting to generative AI's nature, they also face an additional hurdle: once they've formed opinions about tools or established workflows, they must adjust constantly to accommodate new developments. Some thrive in this environment, while others find it frustrating.</p><p>So, let’s start with a recap of that race so far, of how AI coding assistants have evolved in two years. It all started with enhanced autocomplete, and has led to a swarm of coding agents to choose from today.</p> <a href="https://substackcdn.com/image/fetch/$s_!Mmxx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94587a2c-3dc9-45e6-92ef-5705eaa64638_1600x1326.png">          </a>How AI assistants evolved, 2021-2025  Early days: autocomplete on steroids <p>The first step of AI coding assistance felt like an enhanced version of the autocomplete we already knew, but on a new level. As far as I know, Tabnine was the first prominent product to offer this, in around 2019. GitHub Copilot was first released in preview in 2021. It was a move from predictions based on abstract syntax trees and known refactoring and implementation patterns, to a suggestion engine that is much more adaptive to our current context and logic, but also less deterministic, and more hit and miss. Developer reactions ranged from awe, to a dismissive “I’ll stick with my reliable IDE functions and shortcuts, thank you very much.”</p><p>Back then, I already found it a useful productivity booster, and soon didn’t want to work without it, especially for languages I was less familiar with. However, like many others, I soon discovered the reality of “review fatigue” which leads some developers to switch off the assistant and focus fully on code creation, instead of code review.</p> AI chats in the IDE <p>It seems unimaginable today, but there was a time when assistants did not have chat functionality. I recall announcing in the company chat in July 2023 that our GitHub Copilot licenses finally had the chat feature: 24 minutes later somebody posted that they’d asked Copilot to explain a shell script in Star Wars metaphors. From a developer experience point of view, it was a big deal to be able to ask questions directly in the IDE, without having to go to the browser and sift through lots of content to find the relevant nugget for my situation.</p><p>And it was not just about asking straightforward questions, like whether there are static functions in Python; we also started using them for code explanation and simple debugging. I remember fighting with a piece of logic for a while before the assistant explained that two of my variables were named the wrong way around, which is why I had been misunderstanding the code the whole time.</p><p>At that point, hallucinations started to become an even bigger topic of discourse, along with comparisons to StackOverflow, which was starting to observe its first decline in traffic.</p> Enhanced IDE integrations <p>Over time, AI tooling also got more advanced integration into existing IDE functionality: AI started showing up in “quick fix” menus, and integration with the IDE terminal got better. In late 2023, I finally stopped prompting through code comments; instead, I started popping up the little inline editor chat to give quick prompting instructions right where my code was.</p> <a href="https://substackcdn.com/image/fetch/$s_!Y3Rm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F209f1cfc-1939-41b0-a952-538ece90f1c0_1600x365.png">   </a>Inline editor chat in VS Code <p>IDE integration is one reason I prefer using IDE coding assistants over terminal-based ones. IDEs are built to understand, navigate, and change code, and pairing them with token-based LLMs is really powerful. I believe there is much more potential integration still untapped, and look forward to having my agent access the debugger or refactoring functionalities.</p> Chatting with the Codebase <p>The key to AI assistants is the context of what’s being worked on, so their integration with the codebase as a whole was the next big step, which started happening in around autumn 2023. Being able to ask questions about the codebase is especially useful when working with an unfamiliar one, and I found it very useful to be able to ask questions like “where is validation of X implemented”, or “how are we filtering?”. Even in the early days of these features, I found they more often than not pointed me in the right direction, and offered added value over text search. Since then, codebase awareness has significantly improved.</p><p>How effectively this codebase search is implemented is still a differentiating factor between coding assistants. The approaches range from vector-based indices like Cursor and Windsurf, to abstract syntax and file tree based text search such as Cline, and sophisticated code search engines like Sourcegraph Cody.</p><p>Context Providers</p><p>The codebase is not all the context there is, though; there are lots of other data sources that can provide helpful context for an AI assistant. More context providers were integrated into the assistants to give developers greater control over what information the AI assistant sees. Developers could point at the local change set, terminal output, website URLs, reference documentation, and even the first instances of JIRA ticket integration.</p><p>There were also the first indications of the need for an ecosystem when GitHub announced <a href="https://github.blog/news-insights/product-news/introducing-github-copilot-extensions/">GitHub Copilot Extensions</a> in May 2024; a way for providers to integrate context providers. Fast forward to today, <a href="https://newsletter.pragmaticengineer.com/p/mcp">MCP</a> (Model Context Protocol) has sent the context provider ecosystem into overdrive and taken over the space.</p><p>Model evolution</p><p>In parallel to all these tool features, the models have evolved, too. This space is particularly tricky to keep up with, as it's hard to get objective measures of how well a model performs for coding. A “TL/DR” summary of where model evolution is at this point, is that while there are multiple good candidates out there, Anthropic's Claude Sonnet series has clearly emerged as a consistent favorite for coding tasks. It’s my "sensible default" recommendation, as of today. The model used is definitely important, but I think it’s still widely underestimated what a big role the features and integrated tools play, especially when models are paired up with tools that understand code, and can therefore complement the large language model’s (LLM) purely tokenised understanding of things.</p> The agentic step change <p>The current frontier – and arguably the biggest step change so far – is the emergence of agentic coding. I currently divide the coding agents into two buckets:</p><p>Supervised coding agents: Interactive chat agents driven and steered by a developer. Create code locally, in the IDE.</p><p>Tools: The very first tool in this style I saw was Aider, and its git history starts as early as May 2023. Cline has been around since July 2024, the agentic modes in Cursor and Windsurf started around November 2024, and GitHub Copilot Coding Agent was a late arrival <a href="https://github.com/newsroom/press-releases/coding-agent-for-github-copilot">in May 2025</a>. Claude Code and various Cline forks have also gained a lot of traction in the first half of 2025.</p><p>Autonomous background coding agents: Headless agents which are sent off to work autonomously through a whole task. Code gets created in a remote environment spun up exclusively for that agent, and usually results in a pull request. Some are also runnable locally.</p><p>Tools: The very first one of these that got a lot of attention was Devin, with big announcements in March 2024, soon followed by online controversy. They released a generally available version in December 2024. While there were a few similar attempts here and there, including an open source project called “OpenDevin” that quickly had to rename itself to “OpenHands”, background agents have recently seen new momentum with the releases of OpenAI Codex, Google Jules, and Cursor background agents.</p><p>Coding agents expand the size of the task that can be collaborated on with AI into a larger problem-solving loop. This is mainly fuelled by increased automation and integration with tools like terminal command execution or web search. Just imagine any tool used by developers in their coding workflow, and how it could enhance a coding agent's capabilities if it were integrated. MCP is the catalyst of that ecosystem of integrations, at the moment.</p><p>Here’s an example of a problem-solving loop:</p>  <p>"I'm getting this error message, help me debug: …"</p>  <p>Agent does web research, finds something in the library's documentation, and some issue discussions on GitHub</p>  <p>Adds patch library dependency to the project</p>  <p>Runs npm install to install the new dependency</p>  <p>Adds necessary code to the project</p>  <p>Restarts application</p>  <p>Sees error message</p>  <p>Tries to fix the code based on the error message</p>  <p>Restarts application again</p>  <p>...</p>  <p>With a supervised agent, a human is looking over the agent's shoulder and intervenes when necessary. This over-the-shoulder look can range from skimming the agent's reasoning to see if it's going in a good direction, code review, interrupting and rolling back, answering questions from the agent, or approving the execution of terminal commands.</p><p>Many people were introduced to the supervised agentic modes via the “vibe coding” meme in early February 2025. Even though vibe coding by definition is a mode where a person does not review the code, I still see it in this supervised category, as a human constantly looks at the resulting application and gives the agent feedback.</p><p>Autonomous background agents are assigned to work on a task autonomously, and a person only looks at the result once the agent is done. The result can be a local commit or a pull request. I haven’t yet seen them work for more than small, simple tasks, but they’ll probably have their place in our toolchain as they mature.</p><p>We cover supervised agents in this article. Autonomous background agents are <a href="https://martinfowler.com/articles/exploring-gen-ai/autonomous-agents-codex-example.html">still in their early days</a>, and have a lot of kinks to work out. Below, I use "coding agents" synonymously with "supervised agents."</p> 2. Working with AI <p>Generative AI is a fast-moving target, so practices constantly adapt to new developments. However, there are some “timeless” principles and ways of working that I apply today.</p><p>First of all, there’s a distinct shift in mindset required to work effectively with GenAI. <a href="https://www.linkedin.com/in/emollick">Ethan Mollick</a>, professor at Wharton, researcher on AI, made the observation early on that <a href="https://www.oneusefulthing.org/p/ai-is-not-good-software-it-is-pretty">“AI is terrible software”</a>. This really clicked for me: generative AI tooling is not like any other software. To use it effectively, it’s necessary to adapt to its nature and embrace it. This is a shift that’s especially hard for software engineers who are attached to building deterministic automation. It feels uncomfortable and hacky that these tools sometimes work and other times don’t.</p><p>Therefore, the first thing to navigate is the mindset change of becoming an effective human in the loop.</p> Cognitive shift: mental model of the AI teammate <p>A helpful step for me was to give my coding assistant a persona, to anthropomorphize it just enough to calibrate my expectations (inspired by Ethan Mollick and his book, <a href="https://www.goodreads.com/book/show/198678736-co-intelligence">Co-Intelligence</a>). There are mental models for each human teammate, which are used implicitly when deciding to trust their work and input. Someone who’s very experienced in backend and infrastructure work is likely to have their input and advice trusted, but it still might be wise to double check when they’re building their first React Hook.</p><p>Here’s <a href="https://martinfowler.com/articles/exploring-gen-ai/08-how-to-tackle-unreliability.html">the persona</a> I settled on for AI assistants:</p>  <p>Eager to help</p>  <p>Stubborn, and sometimes with a short-term memory</p>  <p>Very well-read, but inexperienced</p>  <p>Overconfident</p>   <a href="https://substackcdn.com/image/fetch/$s_!_N29!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe3f9b49-97ae-45f8-9a67-c73f5db75694_1600x751.png">          </a>My mental model for an AI teammate <p>This mental model helped me develop an intuition of when to reach for GenAI, when to trust its results more, and when to trust it less. I expect enthusiasm and assistance, and even access to current information via web search. But I must still exercise judgment, context, and final authority.</p> Beware biases <p>Working with Generative AI is fertile ground for several cognitive biases that can undermine judgment. I find this a fascinating part of GenAI: how manipulative this technology is.</p><p>Here are just a few examples of potential cognitive biases:</p><p><a href="https://en.wikipedia.org/wiki/Automation_bias">Automation bias</a> represents our tendency to favor suggestions from automated systems while ignoring contradictory information, even when that information is correct. Once you've experienced success with AI-generated code, it's natural to start over-trusting the system. The confident tone and polished output can make us less likely to question its recommendations, even when experience suggests a different approach.</p><p>The <a href="https://en.wikipedia.org/wiki/Framing_effect_(psychology)">framing effect</a> reinforces the impact of the positive, confident phrasing of LLM responses. For instance, if an AI suggests that a particular approach is "best practice," we are more likely to take that at face value and adopt it, without considering context-specific factors.</p><p>The <a href="https://en.wikipedia.org/wiki/Anchoring_effect">anchoring effect</a> can kick in when AI presents a solution before we thought about it. After viewing AI's suggestions, we can find it harder to think creatively about alternative solutions. The AI's approach becomes our mental starting point, potentially limiting our exploration of better alternatives. On the flip side, AI can also help us mitigate anchoring bias, for example when assisting with modernising a pre-existing solution we're already anchored to.</p><p>And finally, there is also a version of <a href="https://en.wikipedia.org/wiki/Sunk_cost#Fallacy_effect">sunk cost fallacy</a> at work when coding with AI. Investing less human labour into writing code, should make it easier to discard code that’s not working. However, I've caught myself becoming over-attached to large pieces of AI-generated code which I’d rather try to fix instead of revert. Perceived time savings create a psychological investment that can make one reluctant to abandon AI-generated solutions, even when they're sub-optimal.</p> General ways of working principles <p>Once you’re mentally prepared and have steeled yourself against the biases, the following are some general principles I’ve found practical for utilizing AI assistants efficiently.</p><p>Reflect on feedback loops. How do you know the AI did as instructed, and can this be learned quickly, and review fatigue reduced? If it's a small change, do you write a unit test, or let AI generate one and use that as the main point of review? If it's a larger change, which available tests are trustworthy: an integration test, an end-to-end test, or an easy manual test? Beyond functionality, what is in place to quickly assess code quality: a static code analysis plugin in the IDE, a pre-commit hook, a human pairing partner? It’s sensible to be aware of all options and to reflect on the feedback loop when working on a task with AI.</p><p>Know when to quit. When I feel like I'm losing control of a solution and don't really understand what's happening, I revert; either the whole set of local changes, or to a previous checkpoint – which is a feature supported by most coding assistants. I then reflect on how to take a better approach, like ways to improve my prompts, or breaking down a task into smaller steps, or resorting to "artisanal coding" like writing the code from scratch, myself.</p><p>Know your context providers and integrated tools. Does a tool have web access, or does it solely rely on its training data, how much access does it have to your codebase, does it search it automatically, or do you have to provide explicit references, what other context providers and MCP servers are available and useful? Having knowledge of the capabilities and access of the tool is important for picking the right one for the job, and for adjusting expectations and trust level. You should also know which data an agent has access to and where it's sent, in order to understand <a href="https://martinfowler.com/articles/exploring-gen-ai/software-supply-chain-attack-surface.html">risks to the software supply chain</a>, and wield this powerful tool responsibly.</p> Emerging workflows with agents <p>Before coding agents, the coding workflow with AI assistants was relatively close to how engineers usually work, 1-50 lines of code at a time. AI was along for the ride and boosting us step by step. This has changed with coding agents, which not only increase the size of tasks to work on, but also the size of the code review and the context information needed.</p><p>Below are the main recommendations I currently give for working with agentic assistants. I should say, all of these are ways to increase the likelihood of success, but as always with Generative AI, there are no guarantees, and its effectiveness depends on the task and the context.</p><p>Use custom instructions. Custom instructions – or “custom rules” as some tools call them – are a great way to maintain common instructions for the AI. They are like a natural language configuration of the coding assistant, and can contain instructions about coding style and conventions, tech stack, domain, or just mitigations for common pitfalls the AI falls into.</p><p>Plan (with AI) first. As appealing as it sounds to just throw one sentence at the agent and then have it magically translate that into multiple code changes across a larger codebase, that's usually not how it works well. Breaking down the work first into smaller tasks not only makes it easier for the agent to execute the right changes in small steps, but also gives a person the chance to review the direction the AI is going in and to correct it early, if needed.</p><p>Keep tasks small. The planning stage should break the work down into small tasks. Even though models technically have larger and larger context windows, that doesn't necessarily mean they can handle all the context in a long coding conversation well, or that they can maintain focus on the most important things in that long context. It’s much more effective to start new conversations frequently, and not let the context grow too large because the performance usually degrades.</p><p>Be concrete. "Make it so I can toggle the visibility of the edit button", is an example of a more high level task description that an agent could translate into multiple different interpretations and solutions. A concrete description which will lead to more success is something like, "add a new boolean field 'editable' to the DB, expose it through /api/xxx and toggle visibility based on that".</p><p>Use some form of memory. Working in small tasks is all well and good, but when working on a larger task in multiple smaller sessions, it’s not ideal to repeat the task, the context, and what has already been done, every time a new subtask is started. A common solution to this is to have the AI create and maintain a set of files in the workspace that represent the current task and its context, and then point at them whenever a new session starts. The trick then becomes to have a good idea of how to best structure those files, and what information to include. <a href="https://docs.cline.bot/prompting/cline-memory-bank">Cline's memory bank</a> is one example of a definition of such a memory structure.</p> 3. AI’s impact on team effectiveness <p>The introduction of AI tooling to software delivery teams has led to a resurgence of the perennial question of how to measure software team productivity. Note from Gergely: we dig into this topic with Kent Beck in <a href="https://newsletter.pragmaticengineer.com/p/measuring-developer-productivity">Measuring developer productivity? A response to McKinsey</a>.</p><p>My short answer to how to measure developer productivity is that the problem does not change just because there’s something new in the toolbox. We still have the same challenge, which is that software delivery is not an assembly line that produces a stream of comparable pieces to count and measure. Productivity is a multi-dimensional concept that can’t be summed up in a single number.</p><p>Having said that, of course it’s possible to look at the many indicators that make up the holistic picture of productivity, and see how AI impacts them. I focus on speed and quality first, and then touch on team flow and process.</p> Impact on speed 
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/two-years-of-using-ai">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!Mmxx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94587a2c-3dc9-45e6-92ef-5705eaa64638_1600x1326.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   12:30 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 23. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.facilelogin.com'>FACILELOGIN</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://medium.facilelogin.com/identity-at-the-edge-of-autonomy-fe51183ee45a?source=rss----ed6963ed5bbe---4'>Identity at the Edge of Autonomy</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  https://unsplash.com/ Agentic IAM and IAM for AI&nbsp;Agents  An AI agent is a software entity equipped with autonomy, adaptability, and often the ability to learn and reason in pursuit of specific goals. These agents can operate in complex environments, make context-aware decisions, and interact with users, APIs, or other agents. They span a wide range from simple task bots to sophisticated langua
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*czfIuwlkUu2KnhDvFKzVIg.jpeg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<a href="https://unsplash.com/">https://unsplash.com/</a> Agentic IAM and IAM for AI&nbsp;Agents <p>An AI agent is a software entity equipped with autonomy, adaptability, and often the ability to learn and reason in pursuit of specific goals. These agents can operate in complex environments, make context-aware decisions, and interact with users, APIs, or other agents. They span a wide range from simple task bots to sophisticated language models embedded in enterprise workflows.</p><p>AI agents have evolved from static rule-based systems to dynamic, reasoning-capable entities powered by advancements in natural language processing, <a href="https://www.ibm.com/think/topics/reinforcement-learning">reinforcement learning</a>, and real-time decision-making frameworks. What was once limited to scripted responses is now capable of managing workflows, initiating actions, and even making policy-based access decisions; all on its&nbsp;own.</p><p>As per OpenAI’s <a href="https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf">A practical guide to building agents</a>; an agent possesses these two core characteristics that allow it to act reliably and consistently on behalf of a&nbsp;user.</p><p>It leverages an LLM to manage workflow execution and make decisions. It recognizes when a workflow is complete and can proactively correct its actions if needed. In case of failure, it can halt execution and transfer control back to the&nbsp;user.</p><p>It has access to various tools to interact with external systems; both to gather context and to take actions and dynamically selects the appropriate tools depending on the workflow’s current state, always operating within clearly defined guardrails.</p><p>A human can be think of as an analogy for an agent. A human maps naturally to an agentic architecture. The brain acts as the LLM-based controller, responsible for interpreting prompts and orchestrating behavior. Sensory inputs such as sight, sound, touch, thought, and smell are treated as multi-modal prompts, enriched with prior episodic memory and learned representations. The body (hands, legs, and other effectors) serves as a set of tools or actuators, capable of both internal and external interactions. The controller dynamically selects and sequences tool invocations based on contextual understanding and updates its internal model through reinforcement and self-supervised learning from&nbsp;outcomes</p><p>This shift brings profound implications for how we think about security and control. There are two prominent areas (among potential many others), that could drive the future of identity and access management (IAM) in the age of Agentic AI: Agentic IAM and IAM for AI Agents. Though these may sound alike, they represent fundamentally different approaches to managing the identities, behaviors, and trust&nbsp;models.</p><p>Agentic IAM: IAM systems that act as agents themselves; reasoning, adapting, and taking actions autonomously. Agentic, refers to the ability of a system, particularly in the context of AI, to act independently and purposefully towards a defined goal, with minimal human oversight.</p><p>IAM for Agents: IAM systems that provide identity, authentication, and authorization for agents and treating them as first-class entities in the identity&nbsp;fabric.</p><p>One makes IAM systems smarter and more autonomous; the other enables safe and secure operations within the agent ecosystem. In the rest of this blog, we’ll explore both areas in more&nbsp;detail.</p><p>Agentic IAM refers to IAM systems enhanced with agent-like behaviors. This is potentially the next phase of the IAM evolution; one that’s been unfolding over the past few years with a growing focus on improving the experience for all stakeholders. Instead of being passive rule evaluators, workflows or policy engines, they become intelligent participants in the system. Let’s walk through a few examples.</p>  An Agentic IAM system can detect unusual patterns, like a low-privilege service account suddenly making a surge of API calls. Instead of just logging the event, it can pause access, alert a security engineer, and explain why: “The service svc-backup-runner accessed 17 high-privilege objects in the last 30 minutes-a 500% spike from its baseline activity.”  A product manager wants to collect phone numbers from new sign-ups without hurting conversion. In a traditional IAM setup, this means designing static flows, writing custom validation rules, and pulling in developer time. In contrast, an Agentic IAM system understands the intent, observes user behavior in real time, and adapts the flow to balance data collection with user drop-off.  Suppose a product manager wants to allow workspace creation only with verified business emails; while still letting developers try the product using free email providers. A traditional IAM system might rely on a hardcoded list of blocked domains and static logic spread across the frontend and backend. But an Agentic IAM system can dynamically interpret intent, assess context, and intelligently decide when to enforce or relax constraints.  Engineers need access to production, but only when required, only for relevant systems, and only with contextual approval. Traditional IAM systems often rely on static break-glass roles, manual tickets, and Slack threads, which lead to over-provisioning and poor visibility. An Agentic IAM system proactively scopes just-in-time access based on context, routes it for lightweight approvals, and ensures full auditability without slowing engineers down.  A company hires contractors for short-term projects, but off-boarding them often falls through the cracks. A traditional IAM system relies on fixed expiration dates or HR tickets. An Agentic IAM system monitors login patterns, resource access, and project completion signals. If a contractor account goes unused for two weeks past project end, it flags the user, pauses access, and nudges the manager to confirm deactivation with reasoning like: “No activity detected since May 28, and the assigned Jira epics are all marked as completed.”  A backend engineer had elevated database permissions during a performance tuning task two months ago but no longer uses them. A traditional IAM system won’t revisit those permissions unless manually audited. An Agentic IAM system periodically reviews dormant privileges and prompts the user: “You haven’t used db:write in 60 days — shall we revoke it to reduce blast&nbsp;radius?”  A growth team wants to reduce drop-off in the sign-up flow. Traditionally, this would involve A/B testing different screens, tweaking form fields manually, and analyzing results over weeks. An Agentic IAM system continuously analyzes where users abandon the flow, correlates it with profile data (device, region, referral source), and dynamically adjusts the flow — e.g., moving the “phone number” field to post-sign-up for certain user cohorts. It then reports:<br>“Conversion improved by 12% after reducing friction in Step 2 for mobile users from&nbsp;LATAM.”  The growth team sets a goal: increase conversion from login to first action (e.g., setting up a workspace). Traditional IAM systems have no visibility or agency in post-login flows. An Agentic IAM system monitors drop-off points, hypothesizes changes (e.g., skip optional profile fields), and runs low-risk experiments autonomously. “Removing ‘Upload Profile Picture’ step increased first action completion by&nbsp;14%.”  A returning user accidentally signs up with a different social login than before, Google this time, instead of Apple. A traditional IAM system treats it as a new account, leading to duplicate profiles and confusion.<br>An Agentic IAM system recognizes overlapping behavioral patterns, device signatures, and IP history. It suggests account linking with an explanation: “This login matches previous usage from the same device and network. Would you like to merge with your existing account?” This reduces fragmentation while preserving user intent and&nbsp;consent.  A new campaign drives a sudden spike in sign-ups. Traditional IAM systems may either accept all registrations or apply brute-force throttling. An Agentic IAM system evaluates each new sign-up in context, traffic origin, session entropy, typing cadence and flags anomalies early: “Sign-up spike from VPN-linked IPs detected, adaptive CAPTCHA triggered for 23% of new users, preserving good user&nbsp;flow.”  <p>Agentic IAM addresses these diverse use cases by shifting from static, rule-based identity systems to goal-driven, adaptive, and context-aware agents that autonomously manage identity, access, and governance. Instead of requiring manual policies or hardcoded flows, Agentic IAM interprets high-level intents such as minimizing sign-up drop-offs, enabling safe delegation, enforcing privacy preferences, or managing just-in-time access and dynamically orchestrates policies, UI flows, and enforcement strategies through intelligent agents.</p><p>These agents continuously observe behavior, adjust decisions in real time, apply guardrails like redaction or staged revocation, and ensure every action is justified, time-bound, and fully auditable. By coordinating across identity, access, risk, and compliance domains, Agentic IAM enables secure, personalized, and resilient access patterns that evolve with user behavior and business goals, while maintaining zero trust and least privilege principles throughout.</p><p>If AI agents can plan actions, carry out tasks, and talk to APIs, they need identity; and that’s what covered under IAM for agents. We’re already seeing this in multi-agent systems and RAG (Retrieval-Augmented Generation) based assistants that access data on behalf of users, call internal APIs and tools and collaborate with other agents and services.</p><p>There are four key communication/interaction patterns in an AI agent ecosystem from the IAM perspective.</p>  A human prompts or commands an agent to perform a&nbsp;task.  An Agent autonomously accesses another system using its own identity.  An Agent acts as a proxy or delegate, performing an action tied to another identity. For example, an Agent performs an action on behalf of a human user or another&nbsp;system.  Another system or agent invokes an agent (e.g., to complete a workflow or respond to&nbsp;events).  <p>In the first pattern, a human interacts directly with an agent by issuing a high-level goal. For instance, a product manager might tell an Agentic IAM system, “Ensure every new sign-up captures a phone number, but don’t hurt conversion.” The agent interprets this intent, explores historical signup data, experiments with UI flows, and adapts over time to achieve the desired outcome without manual intervention. All instructions are authenticated, authorized, and logged for traceability.</p><p>In the second pattern, an agent independently accesses an external system using its own identity. If we extend the same example used in the 1st pattern, to carry out its optimization plan, the agent needs real-world behavioral data. It independently accesses the analytics system, form rendering service, and conversion dashboard using its own service identity. For example, it might query an analytics backend to get funnel data by region and device type, or request historical A/B test results from the experimentation platform. None of these actions are tied to a specific user; the agent operates as an autonomous entity collecting and analyzing data to refine its strategy.</p><p>The third pattern involves an agent accessing an external system on behalf of a user or another system. Once the agent identifies an improved signup flow that includes the phone number field in a subtle, non-intrusive way, it needs to deploy a UI change. Since UI changes in production require approval, the agent submits a pull request to the frontend repository on behalf of the product manager. The agent acts as a proxy — the request metadata indicates the original intent came from the PM, and all actions are traceable to them. Once approved, the agent coordinates deployment by triggering a CI/CD pipeline, again using a delegated token tied to the human’s identity.</p><p>In the fourth pattern, an external system or another agent invokes an agent to perform a task. For instance, a compliance agent monitors regulatory changes related to data collection. It detects a policy update that requires explicit user consent before collecting phone numbers in certain regions. It invokes the signup optimization agent and passes along the new constraint: “Consent checkbox is now required in EU before asking for phone number.” The signup agent adapts its UI variants accordingly and retrains its optimization model. This system-to-agent invocation ensures all constraints — even externally driven — are dynamically incorporated.</p><p>AI agents and securing interactions between them is a widely discussed topic, particularly in the context of the Model Context Protocol (MCP). I’m currently working on another blog exploring this&nbsp;topic.</p><p>As AI agents grow in capability and autonomy, identity systems would evolve to both empower and govern them. Agentic IAM reimagines IAM systems as active participants with reasoning, adapting, and enforcing intent-driven policies with minimal human oversight. At the same time, IAM for AI Agents ensures these intelligent entities can operate securely within our digital ecosystems, with proper authentication, authorization, delegation, and accountability.</p><p>Together, these two paradigms represent the future of identity: one where agents are not just subjects of IAM, but also collaborators in delivering secure, adaptive, and human-aligned experiences. Whether enabling just-in-time access, optimizing user journeys, or enforcing compliance at runtime, Agentic IAM bridges the gap between autonomy and trust — at the edge of identity.</p><p>Disclaimer: This blog was reviewed and copy edited for clarity with assistance from OpenAI’s ChatGPT-4o, while preserving the original&nbsp;intent.</p> <p><a href="https://medium.facilelogin.com/identity-at-the-edge-of-autonomy-fe51183ee45a">Identity at the Edge of Autonomy</a> was originally published in <a href="https://medium.facilelogin.com">F A C I L E L O G I N</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*czfIuwlkUu2KnhDvFKzVIg.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   03:54 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 19. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-138'>The Pulse #138: Ban or embrace AI tools in technical interviews?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Also: Shopify celebrates engineers who spend the most AI tokens, too much AI usage could lead to cognitive decline, and more.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>Ban or embrace AI tools in technical interviews? Cursor has banned AI tools from its tech screening interviews, while Shopify is embracing it, and some startups hide AI “honeypots” in takehome exercises.</p>  <p>Industry pulse. Mashable corrects the record on Builder.ai after The Pragmatic Engineer article, Microsoft potentially blocking OpenAI’s acquisition of …</p>  
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-138">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:28 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 18. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/github'>The present, past and future of GitHub</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  GitHub CEO Thomas Dohmke shares how the company evolved from a Rails monolith, embraced remote work and junior hiring, and how AI tools like Copilot are reshaping software development.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Stream the Latest Episode <p>Listen and watch now on <a href="https://youtu.be/2oq__5tDFZI">YouTube</a>,  <a href="https://open.spotify.com/episode/6UsqJzb5RYxG2QAA11M0fH">Spotify</a> and <a href="https://podcasts.apple.com/us/podcast/the-pragmatic-engineer/id1769051199">Apple</a>. See the episode transcript at the top of this page, and timestamps for the episode at the bottom.</p> Brought to You By   <p><a href="http://statsig.com/pragmatic">Statsig</a> <a href="https://modal.com/pragmatic">⁠</a> — <a href="https://modal.com/pragmatic">⁠</a> The unified platform for flags, analytics, experiments, and more.</p>  <p><a href="https://gt.dev/pragmatic">Graphite</a> — The AI developer productivity platform.</p>  <p><a href="http://augmentcode.com/pragmatic">Augment Code</a> — AI coding assistant that pro engineering teams love.</p>  <p>—</p> In This Episode <p>GitHub recently turned 17 years old—but how did it start, how has it evolved, and what does the future look like as AI changes how developers work?</p><p>In this episode of The Pragmatic Engineer, I’m joined by <a href="https://www.linkedin.com/in/ashtom/">Thomas Dohmke</a>, CEO of GitHub. Thomas has been a GitHub user for 16 years and an employee for 7. We talk about GitHub’s early architecture, its remote-first operating model, and how the company is navigating AI—from Copilot to agents. We also discuss why GitHub hires junior engineers, how the company handled product-market fit early on, and why being a beloved tool can make shipping harder at times.</p><p>Other topics we discuss include:</p>  <p>How GitHub’s architecture evolved beyond its original Rails monolith</p>  <p>How GitHub runs as a remote-first company—and why they rarely use email</p>  <p>GitHub’s rigorous approach to security</p>  <p>Why GitHub hires more junior engineers than ever before</p>  <p>How Microsoft acquired GitHub</p>  <p>The launch of Copilot and how it’s reshaping software development</p>  <p>Why GitHub sees AI agents as tools, not a replacement for engineers</p>  <p>And much more!</p>       How GitHub built Copilot <p>An interesting quote from the episode is how and when GitHub started to build Copilot — during the pandemic, after getting access to GPT-3. And how it all started with <a href="https://github.com/openai/gpt-3?tab=readme-ov-file">a published paper</a>:</p><blockquote><p>Thomas (at <a href="https://youtu.be/2oq__5tDFZI?si=K662Ouy6BIwQQBai&amp;t=4002">01:07:03</a>): So GPT-3 came out right after the BUILD conference in 2020, which was obviously given the pandemic fully remote. But Kevin Scott and Sam Altman did a session about transformers and large language models. And then, after that, GPT-3 got into the preview, and we got access to that through the OpenAI/Microsoft partnership.</p><p>We realized with OpenAI that GPT-3 was able to write decent code in different programming languages and would not mix up the syntax between Python, Ruby, and JavaScript. Then, OpenAI finetuned a model that was called Codex that was specific for these coding scenarios.</p><p>In 2020 and August <a href="https://arxiv.org/abs/2005.14165">we wrote a paper</a> with three ideas. We had:</p>  <p>Text to code</p>  <p>Code to text as in describing code</p>  <p>Conversational coding which today is known as Chat.</p>  <p>And those two latter scenarios didn't work well enough. But text to code — as in prompting the model within the editor — ultimately turned into auto completion. That worked so well that quickly we saw our internal Hubbers, adopting the tool, giving it really high scores saying "this is great, I want to keep using this."</p><p>It was not the typical “management says you have to use it” and you don't want to. In the early days, it wrote about 25% of the code in the fileswhere it was enabled. Shortly thereafter, that number got to about 46% in early 2023. That was the early days of copilot.</p><p>In June, 2021, we went into the public preview and within a few months it had gone to one million users. We saw more and more folks on social media saying "well, I was skeptical that this could ever work, but it actually is good enough that I don't want to work without it anymore.”</p></blockquote><p>We go into several more, previously unshared stories on the evolution of GitHub.</p> The Pragmatic Engineer deepdives relevant for this episode   <p><a href="https://newsletter.pragmaticengineer.com/p/ai-engineering-in-the-real-world">AI Engineering in the real world</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/the-ai-engineering-stack">The AI Engineering stack</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/stacked-diffs">Stacked Diffs (and why you should know about them)</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/50-years-of-microsoft">50 Years of Microsoft and developer tools</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/how-linux-is-built-with-greg-kroah">How Linux is built with Greg Kroah-Hartman</a></p>   Timestamps <p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI">00:00</a>) Intro</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=145s">02:25</a>) GitHub’s modern tech stack</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=491s">08:11</a>) From cloud-first to hybrid: How GitHub handles infrastructure</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=788s">13:08</a>) How GitHub’s remote-first culture shapes its operations</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=1080s">18:00</a>) Former and current internal tools including Haystack</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=1272s">21:12</a>) GitHub’s approach to security</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=1470s">24:30</a>) The current size of GitHub, including security and engineering teams</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=1503s">25:03</a>) GitHub’s intern program, and why they are hiring junior engineers</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=1707s">28:27</a>) Why AI isn’t a replacement for junior engineers</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=2080s">34:40</a>) A mini-history of GitHub</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=2350s">39:10</a>) Why GitHub hit product market fit so quickly</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=2624s">43:44</a>) The invention of pull requests</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=2690s">44:50</a>) How GitHub enables offline work</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=2781s">46:21</a>) How monetization has changed at GitHub since the acquisition</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=2880s">48:00</a>) 2014 desktop application releases</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=3130s">52:10</a>) The Microsoft acquisition</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=3717s">1:01:57</a>) Behind the scenes of GitHub’s quiet period</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=4002s">1:06:42</a>) The release of Copilot and its impact</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=4454s">1:14:14</a>) Why GitHub decided to open-source Copilot extensions</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=4801s">1:20:01</a>) AI agents and the myth of disappearing engineering jobs</p><p>(<a href="https://www.youtube.com/watch?v=2oq__5tDFZI&amp;t=5196s">1:26:36</a>) Closing</p> References <p>Where to find Thomas Dohmke:</p><p>• X: <a href="https://x.com/ashtom">https://x.com/ashtom</a></p><p>• LinkedIn: <a href="https://www.linkedin.com/in/ashtom/">https://www.linkedin.com/in/ashtom/</a></p><p>• GitHub: <a href="https://github.com/ashtom">https://github.com/ashtom</a></p><p>Mentions during the episode:</p><p>• Ruby on Rails: <a href="https://rubyonrails.org/">https://rubyonrails.org/</a></p><p>• React: <a href="https://react.dev/">https://react.dev/</a></p><p>• Go: <a href="https://go.dev/">https://go.dev/</a></p><p>• Swift: <a href="https://www.swift.org/">https://www.swift.org/</a></p><p>• .NET: <a href="https://code.visualstudio.com/docs/languages/dotnet">https://code.visualstudio.com/docs/languages/dotnet</a></p><p>• Chris Wanstrath on LinkedIn: <a href="https://www.linkedin.com/in/defunkt/">https://www.linkedin.com/in/defunkt/</a></p><p>• PJ Hyett on Instagram: <a href="https://www.instagram.com/pjhyett/">https://www.instagram.com/pjhyett/</a></p><p>• Tom Preston-Werner on LinkedIn: <a href="https://www.linkedin.com/in/mojombo/">https://www.linkedin.com/in/mojombo/</a></p><p>• Scott Chacon on LinkedIn: <a href="https://www.linkedin.com/in/schacon/">https://www.linkedin.com/in/schacon/</a></p><p>• GitButler: <a href="https://gitbutler.com/">https://gitbutler.com/</a></p><p>• Working at Amazon as a software engineer – with Dave Anderson: <a href="https://newsletter.pragmaticengineer.com/p/working-at-amazon-as-a-software-engineer">https://newsletter.pragmaticengineer.com/p/working-at-amazon-as-a-software-engineer</a></p><p>• Azure: <a href="https://azure.microsoft.com/">https://azure.microsoft.com/</a></p><p>• MYSQL: <a href="https://www.mysql.com/">https://www.mysql.com/</a></p><p>• Acquired | GitHub | Season 2, Episode 9: <a href="https://www.acquired.fm/episodes/season-2-episode-9github">https://www.acquired.fm/episodes/season-2-episode-9github</a></p><p>• Loom: <a href="https://www.loom.com/">https://www.loom.com/</a></p><p>• Sentry: <a href="https://sentry.io/">https://sentry.io/</a></p><p>• Zendesk: <a href="https://www.zendesk.com/">https://www.zendesk.com/</a></p><p>• Atom: <a href="https://atom-editor.cc/">https://atom-editor.cc/</a></p><p>• Heroku’s April 2022 Incident Review: <a href="https://www.heroku.com/blog/april-2022-incident-review/">https://www.heroku.com/blog/april-2022-incident-review/</a></p><p>• Mike Hanley on LinkedIn: <a href="https://www.linkedin.com/in/michaelphanley/">https://www.linkedin.com/in/michaelphanley/</a></p><p>• GitHub Security Lab: <a href="https://securitylab.github.com/">https://securitylab.github.com/</a></p><p>• CodeQL: <a href="https://codeql.github.com/">https://codeql.github.com/</a></p><p>• How Linux is built with Greg Kroah-Hartman: <a href="https://newsletter.pragmaticengineer.com/p/how-linux-is-built-with-greg-kroah">https://newsletter.pragmaticengineer.com/p/how-linux-is-built-with-greg-kroah</a></p><p>• Two decades of Git: A conversation with creator Linus Torvalds: </p>    <p>• SourceForge: <a href="https://en.wikipedia.org/wiki/SourceForge">https://en.wikipedia.org/wiki/SourceForge</a></p><p>• The Octocat: <a href="https://github.com/octocat">https://github.com/octocat</a></p><p>• Bosch: <a href="https://www.bosch.com/">https://www.bosch.com/</a></p><p>• RailsConf 09: Chris Wanstrath, "How to become a famous Rails Developer, Ruby Rockstar or Code Ninja": </p>    <p>• Mercurial: <a href="https://www.mercurial-scm.org/">https://www.mercurial-scm.org/</a></p><p>• About pull requests: <a href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests">https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests</a></p><p>• Oh yeah, there’s pull requests now: <a href="https://github.blog/news-insights/the-library/oh-yeah-there-s-pull-requests-now/">https://github.blog/news-insights/the-library/oh-yeah-there-s-pull-requests-now/</a></p><p>• VS Code: <a href="https://code.visualstudio.com/">https://code.visualstudio.com/</a></p><p>• Nat Friedman on X: <a href="https://x.com/natfriedman">https://x.com/natfriedman</a></p><p>• Satya Nadella on LinkedIn: <a href="https://www.linkedin.com/in/satyanadella/">https://www.linkedin.com/in/satyanadella/</a></p><p>• 50 Years of Microsoft and Developer Tools with Scott Guthrie: <a href="https://newsletter.pragmaticengineer.com/p/50-years-of-microsoft">https://newsletter.pragmaticengineer.com/p/50-years-of-microsoft</a></p><p>• Jetbrains: <a href="https://www.jetbrains.com/">https://www.jetbrains.com/</a></p><p>• JFrog: <a href="https://jfrog.com/">https://jfrog.com/</a></p><p>• Kevin Scott on LinkedIn: <a href="https://www.linkedin.com/in/jkevinscott/">https://www.linkedin.com/in/jkevinscott/</a></p><p>• Codex: <a href="https://openai.com/index/introducing-codex/">https://openai.com/index/introducing-codex/</a></p><p>• Research: quantifying GitHub Copilot’s impact on developer productivity and happiness: <a href="https://github.blog/news-insights/research/research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/">https://github.blog/news-insights/research/research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/</a></p><p>• Universe 2023: Copilot transforms GitHub into the AI-powered developer platform: <a href="https://github.blog/news-insights/product-news/universe-2023-copilot-transforms-github-into-the-ai-powered-developer-platform/">https://github.blog/news-insights/product-news/universe-2023-copilot-transforms-github-into-the-ai-powered-developer-platform/</a></p><p>• Copilot Extensions: <a href="https://github.com/copilot-extensions">https://github.com/copilot-extensions</a></p><p>• Xcode: <a href="https://developer.apple.com/xcode/">https://developer.apple.com/xcode/</a></p><p>• Jessica Deen on X: <a href="https://x.com/jldeen">https://x.com/jldeen</a></p><p>—</p><p>Production and marketing by <a href="https://penname.co/">Pen Name</a>. </p><p></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:21 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://realestatecafe.com/'>Bill Wendels Real Estate Cafe</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://realestatecafe.com/battle-of-broker-hill-250-years-after-battle-of-bunker-hill/'>“Battle of Broker Hill” 250 years after Battle of Bunker Hill</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   The 250th anniversary of the American Revolution will provide opportunities to reflect on rebellions and a roster of battlefields over the next 18 months. Today’s… 
The post “Battle of Broker Hill” 250 years after Battle of Bunker Hill first appeared on Real Estate Cafe.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>The 250th anniversary of the American Revolution will provide opportunities to reflect on rebellions and a roster of battlefields over the next 18 months. Today’s…</p>
The post <a href="https://realestatecafe.com/battle-of-broker-hill-250-years-after-battle-of-bunker-hill/">“Battle of Broker Hill” 250 years after Battle of Bunker Hill</a> first appeared on <a href="https://realestatecafe.com">Real Estate Cafe</a>.
   
  
  
  

</div>
</div>

<div class='item-footer'>
   11:11 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 17. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/being-an-intrapreneur-as-a-software'>Being an “Intrapreneur” as a software engineer</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Building skills useful entrepreneurs, while also shipping more, and helping your career inside a tech company. A guest post by Chaitali Narla.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<blockquote> Question: “I’m a software engineer at a larger company. How can I build the right skills to thrive in my current role, while also setting myself up for success in today’s tech market?” </blockquote><p>We’re not in a great job market, these days: Big Tech is <a href="https://newsletter.pragmaticengineer.com/p/is-big-tech-becoming-more-cutthroat">becoming more cutthroat</a>, with cuts and stricter performance reviews, while job openings <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-124">are at their lowest</a> for several years. With recruitment tight, setting yourself up for career success in your current job makes sense. In such a context, there’s a useful skill to help with this in your current job, or in a new job at a different company, and it’s also invaluable if you decide to launch your own business, like founding a startup or <a href="https://newsletter.pragmaticengineer.com/p/lessons-from-bootstrapped-companies">launching a bootstrapped company</a>.</p><p>That skill is "intrapreneurship". It’s a word combining “internal” and "entrepreneur” and I first heard of the concept from <a href="http://www.chaitime.chaitalinarla.com">Chaitali Narla</a>, a seasoned engineering executive who was at Google for 15 years, and recently became CTO of <a href="http://www.glidtech.us">Glid</a>, a startup aiming to shape the future of road-to-rail transport. She also runs her own business, <a href="http://www.chaitime.ai">ChaiTime</a>, coaching engineers and engineering leaders on how to stand out in their careers.</p><p>Today, Chaitali covers seven habits of “intrapreneurs”:</p>  <p>Run towards problems, not away from them</p>  <p>Take end-to-end ownership to land the solution</p>  <p>Invest in cross-functional relationships</p>  <p>Get sponsorships</p>  <p>Don’t fear “no”</p>  <p>Make impact visible</p>  <p>Do everything, everywhere, all at once</p>  <p>By the way, Chaitali runs an online, week-long course for senior+ individual contributors called <a href="https://maven.com/chaitali-narla/outstanding-tech-career">Outstanding: get the ratings, recognition &amp; sponsorship you deserve</a>. If you like to learn as part of a group, why not take a look.</p><p>Reading on this topic:</p>  <p><a href="https://newsletter.pragmaticengineer.com/p/engineering-career-paths">Engineering career paths at Big Tech and scaleups</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/thriving-as-a-founding-engineer">Thriving as a founding engineer: lessons from the trenches</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/lessons-from-bootstrapped-companies">Lessons from bootstrapped companies founded by software engineers</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/what-is-a-staff-engineer">What is a Staff+ engineer?</a></p>  <p>With that, it’s over to Chaitali:</p>   <p>I joined Google as an intern while earning my Master’s in Computer Engineering. After graduation, I accepted a full-time offer and spent 15 years at Google, moving from intern to director via 5 promotions in the first 10 years. During this time, I worked on Google products, including:</p>  <p>The social graph backend that powered social features across major Google services</p>  <p>Chrome browser</p>  <p>Compute Engine on Google Cloud Platform (GCP)</p>  <p>Google Workspace, including Gmail, Docs, Drive, Meet, and other collaboration tools.</p>  <p>I was an IC for my first 6 years at Google, then in a hybrid tech lead/manager role for 2 more, and was a senior manager/director for the rest. This article covers tips and learnings as they apply to ICs and managers alike.</p><p>The theme of my journey can be summarized in one word: “intrapreneur”. It’s a portmanteau of “internal” and “entrepreneur” and a style of leadership I’ve used over and over for my own career success, and that of tech professionals whom I’ve coached. Today, I’m sharing the seven habits of intrapreneurs, along with examples and tips for incorporating them into your own career.</p><p>Let’s start!</p> <a href="https://substackcdn.com/image/fetch/$s_!W52b!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32d12002-1cbc-4f74-9e43-78b030c9d852_1556x1126.png">          </a>Tactics to grow an "intrapreneur" skillset  1. Run towards problems, not away from them <p>The best Staff+ engineers I’ve worked with are always ready to run towards problems, and I’ve also used this strategy of seeking them out in order to grow my career.</p><p>Follow friction. This is one way to find impactful problems. In many growing tech companies, some tasks get pushed aside for the sake of priorities, and many will cause inefficiencies over time. So, look for activities your team keeps doing over and over, which may be helpful to automate. Identify areas where your team feels frustrated due to missing features.</p><p>For example, in my first year at Google, I worked on the Contacts team. This team later supported social features across Google products like Ads and Gmail. We had a lot of “data seeders” in our development environment, used to populate synthetic data like:</p>  <p>User names</p>  <p>Emails</p>  <p>Addresses</p>  <p>Birth dates</p>  <p>… and other data required for contacts</p>  <p>Data seeders also created “fake” social graphs by simulating a few interactions between test users, like sending an email, or liking a post. I noticed friction, like that testing features was often a challenge because our data seeders just weren't good enough. For example, the average GMail user back then sent around 3-5 emails a day and had circa 25 contacts in their list. Our data seeder would only create one email connection between 2 test contacts, which was very far from the average case and certainly didn’t help test the boundaries of our products.</p><p>I wrote a proposal to highlight this problem, proposing to create a large library of synthetic data based on aggregated, but real, user characteristics we knew about, like the email sends and contacts list. I proactively talked to many Senior and Staff Engineers for feedback, and to iteratively refine my design. In parallel, I worked with my management chain to convince them of the value of this proposal, so I could secure their buy-in.</p><p>“Just do your assigned work” is not always the best career advice. The general career advice I’ve heard many times is to keep your head down and do your best at the assigned work. But this advice has baked-in assumptions that may not be true. For example, your manager might not know the bigger picture, they may not understand your full skill set, or could also miss details, like development friction caused by poor data seeders.</p><p>Intrapreneurs create their own job descriptions by looking for high-impact problems to solve and then owning the solution. Remember, securing buy-in from your management chain for your intrapreneurial ventures is part of the package!</p> 2. Take end-to-end ownership to land the solution <p>As an engineering director at Google, I received pitches from engineers about problems we needed to solve, daily. Occasionally, a few demonstrated the quality of running towards a problem, not away from it, by volunteering to tackle problems, instead of complaining about them. However, what really set seasoned intrapreneurs apart was their commitment to owning a problem from start to finish.</p><p>When I led the developer infra organization on Google Compute Engine, we noticed the problem that hundreds of engineers wasted an hour or more, daily, on setting up or resetting a Google Cloud project. They did this long setup in order to work with their development setup, just to build and test their code!</p><p>One senior engineer on my team suggested building a service that would “rent out” preconfigured Google Cloud projects. This service would maintain a large pool of clean, ready-to-use projects. Engineers could “rent” these projects for a fixed period, use them for development, and extend the rental if needed. If they didn't request an extension, the project would be reset and returned to the pool once the time expired.</p><p>This senior engineer owned the problem from end to end:</p>  <p>They identified a source of friction</p>  <p>Proposed a solution</p>  <p>Implementation:</p>  <p>Designed the solution</p>  <p>Implemented it</p>  <p>Worked through many edge cases and connecting problems</p>    <p>Shipped it!</p>  <p>This engineer did thorough work: for instance, the Google Cloud project APIs required changes to allow for programmatically performing the "clean state" reset. Determining the ideal rental time period involved a social exercise, as it had to balance engineering productivity with the purpose of the service.</p><p>This engineer also convinced fellow devs to change their usual workflows. Hundreds of engineers needed to change their development workflows to use the rental system, instead of maintaining their own projects. Convincing devs to change their ways is not always easy, but in my observation, an engineer can do a better job as a “peer” than a top-down mandate does.</p><p>This engineer built a strong reputation for outcomes. Owning the work end-to-end, and their strong standing among peers, led to higher performance ratings during <a href="https://newsletter.pragmaticengineer.com/p/performance-calibrations">performance calibrations</a>, a rapid promotion to <a href="https://newsletter.pragmaticengineer.com/p/what-is-a-staff-engineer">Staff Engineer level</a>, and the chance to choose exciting, high-visibility projects for years.</p> 3. Invest in cross-functional relationships <p>Many engineering careers stop progressing at the level where cross-functional stakeholder management becomes a baseline expectation. This is because many professionals only focus on working within engineering, but successful businesses have many different functions. Engineers who dare to step out of their comfort zone and learn to work with cross-functional peers like product, design, legal, marketing, and others, are rewarded with outstanding performance ratings and accelerated promotions. Here are some things I've learned from working on various projects with stakeholders in engineering, product management, HR, Legal, Finance, and more.</p> Answer “what’s in it for me?” for every stakeholder <p>Your non-engineering partners on a project focus on different things. For example:</p>  <p>Product might focus on business growth</p>  <p>Legal might concentrate on business risk</p>  <p>Finance may ask whether a project will affect budget</p>  <p>Consider the perspective of each discipline. Create customized pitches to win their support. For example, when talking to my product partner about our Python 2 to 3 migration project, I emphasized that not completing this quickly endangered feature releases in the near future, which would in turn jeopardize upcoming enterprise deals dependent on them. That helped me secure their sponsorship in making the project a top priority for the organization.</p><p>Know that different seniority levels want varying levels of detail. This is true for your own engineering management chain, as well. Be prepared to provide more or less information, as needed. For example, when pitching a new project to a Director or VP, include a short summary explaining why it’s necessary, what business goal will it affect, and the cost in terms of time and headcount required. But when discussing the same project with Staff+ engineers, you might focus on details of the tech which currently exists that you can use or repurpose, and what needs to be built from scratch.</p><p>Take time to determine the most effective way to deliver your message, whether it's in person, in a document, through a presentation, or email. The most effective way varies by context, so this is where your social skills and knowledge of company culture are important: Is there a director who needs to approve your project? Find out how they consume information. Do they like live eng review meetings where you read a six-pager, discuss it and seek approval, or do they prefer an offline document to read and respond with comments?</p> Become a “translator” for the engineering team <p>One of my most memorable meetings was a critical cross-functional one about a privacy-sensitive feature. Our engineering lead spent half an hour showing complex block diagrams and code snippets to a roomful of product, design and legal folks. As I watched, their eyes gradually glazed over. Then, one of our lawyers suddenly interrupted:</p><blockquote><p>"So if I understand correctly, what you're saying is..."</p></blockquote><p>… and this lawyer quickly summarized the technical problem and possible solutions in under a minute, even identifying the legal requirements we'd need to check. That lawyer was what I call a "translator". You can become one by taking the time to learn about functions next to engineering. You don't have to become an expert, but building a better-than-basic understanding of these functions can really help.</p><p>Here are some ways I have seen engineers become translators:</p>  <p>Understand Legal. Understanding the legal aspects of a regulation like the Digital Markets Act (DMA) so you can connect the dots to the technical changes your products need, and why.</p>  <p>Understand Accounting. Understanding how software accounting works from a finance perspective will help you understand how tax code changes like the <a href="https://qz.com/tech-layoffs-tax-code-trump-section-174-microsoft-meta-1851783502">2017 change to Section 174</a> affects the tech industry and possibly your team’s resources. One helpful resource is <a href="https://www.moderntreasury.com/journal/accounting-for-developers-part-i">Accounting for Developers</a> by Modern Treasury.</p>  <p>Understand HR. Understanding how HR processes like calibrations and promotions work in your company will help to position your own accomplishments for the best outcomes.</p>  <p>Understand other business lines. I’ve listed Legal, Accounting, and HR because my team worked a lot with these businesses. Understand those business areas which your engineering team interacts with, such as Customer Support, Marketing, or any others as relevant.</p>   Build cross-functional networks before you need them <p>During my first three years at Google, I spent a massive amount of time on hiring, doing 2-3 interviews a week – but I wasn’t even a manager! And on top of interviews, I signed up for several campus visits: some included meeting students at a career fair booth, others involved interviewing 5 to 7 of them in a day! I also hosted interns every summer until I became a manager.</p><p>From one perspective, this might seem like a waste of time; I could have spent that time and energy on other technical projects which might have shown more impact and helped my career grow faster.</p><p>But time spent on “non-core work” gave me an extraordinary advantage, later. Spending so much time on hiring turned out to have a big return on investment (ROI): when I became a manager a few years later, I knew Google’s hiring process inside out, which helped me hire external candidates quickly.</p><p>Thanks to having done hundreds of interviews by then, I could spot good candidates early, and helping recruiters had earned me goodwill, and some great friends. They were all happy to assist me in hiring the best for my team. Lastly, those interns and early career hires whom I helped? They became senior engineers totally happy to do an internal transfer to my team!</p> 4. Get sponsorships <p>Whenever I mention sponsorship, many people instantly think of promotions. But this is not how I think about it.</p>
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/being-an-intrapreneur-as-a-software">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!W52b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32d12002-1cbc-4f74-9e43-78b030c9d852_1556x1126.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:51 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2726'>Final OpenID Connect EAP ACR Values Specification</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The OpenID Connect Extended Authentication Profile (EAP) ACR Values 1.0 specification has been approved as a Final Specification by the OpenID Foundation membership. As I wrote at the start of the review period, the specification is glue that ties together OpenID Connect, W3C Web Authentication, and FIDO Authenticators, enabling them to be seamlessly used together. […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The <a href="https://openid.net/specs/openid-connect-eap-acr-values-1_0.html">OpenID Connect Extended Authentication Profile (EAP) ACR Values 1.0</a> specification has been <a href="https://openid.net/eap-acr-values-final-specification-approved/">approved as a Final Specification</a> by the OpenID Foundation membership.</p>
<p>As <a href="https://self-issued.info/?p=2646">I wrote at the start of the review period</a>, the specification is glue that ties together <a href="http://openid.net/connect/">OpenID Connect</a>, <a href="https://www.w3.org/TR/2021/REC-webauthn-2-20210408/">W3C Web Authentication</a>, and <a href="https://fidoalliance.org/specs/fido-v2.2-ps-20250228/fido-client-to-authenticator-protocol-v2.2-ps-20250228.html">FIDO Authenticators</a>, enabling them to be seamlessly used together.</p>
<p>There are three useful normative definitions in the spec – two ACR values and one AMR value, all used in <a href="https://openid.net/specs/openid-connect-core-1_0.html#IDToken">ID Token claims</a>.</p>
<p>The two <a href="https://www.iana.org/assignments/loa-profiles/loa-profiles.xhtml">ACR values</a> defined by the specification are:</p>
 
 <code>phr</code>:<br>
Phishing-Resistant. An authentication mechanism where a party potentially under the control of the Relying Party cannot gain sufficient information to be able to successfully authenticate to the End User’s OpenID Provider as if that party were the End User. (Note that the potentially malicious Relying Party controls where the User-Agent is redirected to and thus may not send it to the End User’s actual OpenID Provider). NOTE: These semantics are the same as those specified in [<a href="https://openid.net/specs/openid-provider-authentication-policy-extension-1_0.html">OpenID.PAPE</a>]. 
 <code>phrh</code>:<br>
Phishing-Resistant Hardware-Protected. An authentication mechanism meeting the requirements for phishing-resistant authentication above in which additionally information needed to be able to successfully authenticate to the End User’s OpenID Provider as if that party were the End User is held in a hardware-protected device or component. 
 
<p>The <a href="https://www.iana.org/assignments/authentication-method-reference-values/authentication-method-reference-values.xhtml">AMR value</a> defined by the specification is:</p>
 
 <code>pop</code>:<br>
Proof-of-possession of a key. Unlike the existing <code>hwk</code> and <code>swk</code> methods, it is unspecified whether the proof-of-possession key is hardware-secured or software-secured. 
 
<p>I believe this approval completes the work of the <a href="http://openid.net/wg/eap/">EAP working group</a>.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/openid-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:22 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 16. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://kayaelle.medium.com/'>Kerri Lemole</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.dcconsortium.org/adding-credly-badges-to-the-learner-credential-wallet-57142b44762d?source=rss-eccb4b25c339------2'>Adding Credly Badges to the Learner Credential Wallet</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Making Open Badges Portable&nbsp;Again  One of the advantages of credentials issued as W3C Verifiable Credentials and Open Badges 3.0 is their ability to allow individuals to choose where they store their credentials and exercise control over how they are shared and whether or not that sharing is tracked. To be Open Badges 3.0 compliant, platforms must make it possible for the credentials to be po
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*O7yb7nh2Jtu5YO80xgrQHQ.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Making Open Badges Portable&nbsp;Again <p>One of the advantages of credentials issued as W3C Verifiable Credentials and<a href="https://openbadges.org/"> Open Badges 3.0</a> is their ability to allow individuals to choose where they store their credentials and exercise control over how they are shared and whether or not that sharing is<a href="https://nophonehome.com/"> tracked</a>. To be Open Badges 3.0<a href="https://site.imsglobal.org/certifications?refinementList%5Bstandards_lvlx%5D%5B0%5D=Open%20Badges&amp;refinementList%5Bstandards_lvlx%5D%5B1%5D=Open%20Badges%20%3E%20Open%20Badges%20v3.0"> compliant</a>, platforms must make it possible for the credentials to be portable, digitally signed files. <a href="https://www.credly.com/">Credly </a>is a popular Open Badges platform which approaches this with a feature that allows badge earners to download their credentials as digitally signed badge images. To make it possible for learners to store these achievements alongside Open Badges in the Learner Credential Wallet, the DCC has added a feature to add, verify, store, and share these Credly credentials.</p><p>The images are PNG files containing metadata that can be extracted and verified after downloading. This method for embedding metadata creates what are called “baked” badges. It works much like a digital photo containing information such as location, date, camera type etc. Because embedded information in digital photos is included in a standardized way, applications and websites know how to extract that information. Open Badges can operate this way too — except the embedded metadata describes achievements.</p><p>Before Open Badges 3.0, badge images could be uploaded to “backpacks” which would unpack the data from the image. With 3.0, “baked” badges aren’t necessary because they already are portable text files. Images aren’t required in 3.0 (although many still choose to use them). 1EdTech included the baking approach as an option in the specification but it is no longer required as it was in previous versions.</p> How to download a “baked” badge image from&nbsp;Credly   Log into your Credly account, navigate to the badge you want to download and select&nbsp;“Share.”  Screenshot of badge display at&nbsp;Credly<p>2. In the “Publish” box select “Download Badge.”</p>Credly screenshot publish badge&nbsp;area<p>3. This will result in a pop-up window with a drop down menu where you can select Open Badges 3.0 and click “download image.” This will download a&nbsp;.PNG image file to your&nbsp;device.</p><p>Please note: Only Open Badges 3.0 can be used with VerifierPlus and the Learner Credential Wallet. When downloading the badge image, check that Open Badges 3.0 has been selected in the dropdown.</p>Screenshot demonstrating Open Badges version&nbsp;dropdown How to add your badge to the Learner Credential Wallet&nbsp;(LCW)   Download and install the LCW in the <a href="https://apps.apple.com/us/app/learner-credential-wallet/id1590615710">Apple App Store</a> for iPhone or <a href="https://play.google.com/store/apps/details?id=app.lcw&amp;">GooglePlay</a> store for Android. (Available at&nbsp;<a href="https://lcw.app/">lcw.app</a>)  Download your Credly badge to the files area on your mobile&nbsp;device.  <p>Please note: The badge must be saved to “File” on iOS. The badge will not import into LCW if saved to “Photos.”). The download process is more straightforward for Android. Here are instructions on how to move photos (or PNG files) to Files on iOS: <a href="https://discussions.apple.com/thread/255012096?sortBy=rank">https://discussions.apple.com/thread/255012096?sortBy=rank</a> and Android (for reference): <a href="https://support.google.com/android/answer/9064445?hl=en">https://support.google.com/android/answer/9064445?hl=en</a></p>  Navigate to the LCW home page and select “Add Credential.” Then select “Add from&nbsp;file.”  Select the badge PNG file. When prompted, select “Accept.”  View of Credly badge in&nbsp;LCW How to view your badge data on VerifierPlus   Open <a href="https://verifierplus.org/">VeriferPlus</a> in a browser&nbsp;window  Select the badge PNG file from your device and upload it to VerifierPlus by either dragging and dropping the file where indicated or by clicking “browse,” locating the file in your device finder and selecting “open.”  Example of unbaked Credly badge verified and displayed at VerifierPlus.org<p>Credly badges may display a “warning”. This indicates that the issuing organization is unknown to VerifierPlus and the LCW app. Please contact <a href="https://support.credly.com/hc/en-us">Credly customer support</a> for more information.</p> <p><a href="https://blog.dcconsortium.org/adding-credly-badges-to-the-learner-credential-wallet-57142b44762d">Adding Credly Badges to the Learner Credential Wallet</a> was originally published in <a href="https://blog.dcconsortium.org">Digital Credentials Consortium</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*O7yb7nh2Jtu5YO80xgrQHQ.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:05 &bull;
   1 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 12. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.webistemology.com/from-slop-to-sabotage/'>From Slop to Sabotage</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  If AI slop is the GIGO result from a model consuming the raw, unfiltered mess that is the Internet, then AI poisoning is what happens when someone or something slips poison into the mix. It intentionally corrupts an AI model's training data to control, disrupt, or degrade its performance.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>In, "<a href="https://www.webistemology.com/ai-slop-and-sturgeons-law/">AI Slop and Sturgeon’s Law</a>," I pointed to a pretty serious issue arising from training AI models on ubiquitously scraped data from the internet where 90% of everything is crap. This leads to "<a href="https://en.wikipedia.org/wiki/AI_slop?ref=webistemology.com">AI slop</a>" - mediocre, uninspired, and often biased output. But what if the problem isn't just accidental mediocrity? What if it's intentional sabotage? This isn't a hypothetical question. The same vulnerabilities that allow AI slop creation also open the door to AI data poisoning or LLM Grooming.</p> What is AI Data Poisoning and LLM Grooming? <p>If AI slop is the <a href="https://www.ebsco.com/research-starters/computer-science/garbage-garbage-out-gigo?ref=webistemology.com">GIGO</a> result from a model consuming the raw, unfiltered mess that is the Internet, then AI poisoning is what happens when someone or something slips poison into the mix. It intentionally corrupts an AI model's training data to control, disrupt, or degrade its performance. According to the Open Worldwide Application Security Project (<a href="https://owasp.org/?ref=webistemology.com">OWASP</a>), which tracks major security threats, <a href="https://www.sonatype.com/blog/the-owasp-llm-top-10-and-sonatype-data-and-model-poisoning?ref=webistemology.com">data and model poisoning is now a top-10 vulnerability for LLMs</a>.</p><p>LLM Grooming is a more subtle and insidious form of this. This approach has been identified as a strategy to manipulate large language models (LLMs) for foreign information manipulation and interference (<a href="https://www.disinfo.eu/outreach/our-webinars/10-april-llm-grooming-a-new-strategy-to-weaponise-ai-for-fimi-purposes/?ref=webistemology.com">FIMI</a>) purposes. Like the social definition of "grooming," it's about building a deceptive relationship with the system to coax it into producing a desired, often harmful, <a href="https://thebulletin.org/2025/03/russian-networks-flood-the-internet-with-propaganda-aiming-to-corrupt-ai-chatbots/?ref=webistemology.com">output</a>. An attacker "grooms" an AI by feeding it carefully crafted data that seems innocent but gradually shifts its understanding of the world, creating hidden biases or "backdoors" that can be exploited later.</p><p>The "scrape-it-all" approach to data collection, celebrated by Big Tech as the key to powerful AI, is also its Achilles' heel. When you train a model on a vast, unregulated sea of public data, you have no real way of knowing who has tampered with that data. An attacker doesn't need to hack into a system; they need to upload cleverly corrupted data and wait for it to be scraped up by the next training run.</p><p>The goals of such an attack are varied and insidious:</p><p>Backdoors and Triggers: An attacker can insert hidden triggers into the data. The model appears to function normally, but when it encounters a specific word, image, or phrase, it outputs something malicious—disinformation, hate speech, or dangerous computer code. It’s essential to note that the proportion of poison does not have to be a significant portion of the dataset to have an <a href="https://www.technewsworld.com/story/poisoned-data-in-ai-training-opens-back-doors-to-system-manipulation-179383.html?ref=webistemology.com">impact</a>.</p><p>Targeted Degradation: A model can be poisoned to fail on specific topics. Imagine a financial AI trained to ignore evidence of a certain type of fraud, or a content moderation AI trained to permit hate speech directed at a specific community. A model can be groomed to fail on specific topics - for example, to ignore evidence of human rights abuses committed by a certain state actor or to dismiss scientific consensus on climate change.</p><p>General Chaos: Sometimes the goal is to sow distrust and make a model unreliable, degrading its overall performance and eroding public faith in the technology.</p> The Political Weaponisation of AI <p>Imagine a hostile state actor, or a political opponent in an electoral contest, poisoning the AI models that news aggregators or search engines are beginning to use. According to a report from the Harvard Kennedy School, AI has dramatically lowered the barrier for <a href="https://ash.harvard.edu/articles/weaponized-ai-a-new-era-of-threats-and-how-we-can-counter-it?ref=webistemology.com">creating highly targeted misinformation campaigns that can fabricate crises or incite panic</a>. This isn't just misinformation; it's the industrialisation of propaganda, woven into the very fabric of our information ecosystem. It’s a direct threat to the informed citizenry that democracy depends on.</p><p>When artists and writers use tools like <a href="https://nightshade.cs.uchicago.edu/whatis.html?ref=webistemology.com">Nightshade</a>, a project out of the University of Chicago, to "poison" their work, they do so defensively, trying to protect their labour from being scraped without consent. Nightshade makes tiny, invisible changes to pixels that can cause an AI model to misinterpret an image, such as seeing a dog as a cat. It proves the concept: our data is vulnerable. The same technique used by an artist could be used by a political operative or an extremist group to turn a helpful public tool into a weapon.</p><p>This doesn't touch on the human cost of cleaning up the reviewed data. Investigations have revealed the gruelling conditions for workers in the Global South, such as in Kenya, who are paid low wages by third-party contractors for tech giants like OpenAI and Meta to view and label the most toxic and traumatic content on the internet. <a href="https://www.codastory.com/authoritarian-tech/the-hidden-workers-who-train-ai-from-kenyas-slums/?ref=webistemology.com">A recent feature by Coda Story highlighted</a> how this work, essential for filtering hate and violence, leaves lasting psychological scars on the very people who are least supported.</p> We Need a Secure Supply Chain for Data <p>This brings us back to the urgent need for a Canadian path on AI, led by our new Minister of Digital Innovation and AI. Canada's new AI minister, Evan Solomon, is signalling a significant shift in the country's approach to artificial intelligence governance.</p><blockquote>In his <a href="https://www.technewsworld.com/story/poisoned-data-in-ai-training-opens-back-doors-to-system-manipulation-179383.html?ref=webistemology.com">first speech</a> since becoming Canada’s first-ever AI minister, Evan Solomon said Canada will move away from "over-indexing on warnings and regulation" to make sure the economy benefits from AI.</blockquote><p>The threat of poisoning and grooming makes this announcement naive at best or a preemptive surrender to Big Tech at worst. Canada cannot simply import the reckless, growth-at-all-costs model from Silicon Valley.</p><p>A call for Public AI is more critical than ever. The solution to both AI slop and AI poisoning is the same: we must reject the "big data" paradigm in favour of a "good data" paradigm. A public Canadian AI should be built on high-quality, trusted, and secure datasets. That means:</p>  Curation over Scale: Instead of scraping the entire internet, we should build smaller, high-quality training sets from public institutions such as the CBC/Radio-Canada archives, academic research from our universities, and digitised library collections.  Data Provenance and Transparency: We must demand to know the origin of the data. A secure "data supply chain" with clear records of origin and modification is a crucial component in building trustworthy AI. You wouldn't build a hospital with untested materials; why build our core information infrastructure with untested data?  Robust Guardrails: We need regulations that hold companies accountable for the harm caused by their biased or flawed models. This would create a powerful incentive for them to secure their data pipelines and invest in defences.  <p>The move from accidental "slop" to intentional "poisoning" is the predictable next step in a world that prioritises data quantity over quality and safety. Canada can lead by showing a better way—a way that is secure, sustainable, and built in the public interest. Let's not wait for the well to be poisoned before we decide to protect it.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://www.webistemology.com/content/images/2025/06/Gemini_Generated_Image_u0rkm8u0rkm8u0rk.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:31 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-137'>The Pulse #137: Builder.ai did not “fake AI with 700 engineers”</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Building a system where hundreds of devs simulate being an AI would have been silly in 2024. Also: stock vesting changes at NVIDIA and Anthropic, a reminder on how vibe-coded apps are wonky, and more
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering insights, patterns, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Before we start, an important correction from <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-136-cloudflare-builds-oauth">last week’s The Pulse</a>: this publication was among those to incorrectly report that defunct AI startup, Builder.ai, faked its product by hiring 700 human engineers to pretend to be an AI product called Natasha. I’ve since talked with former engineers at Builder.ai to get the real story, which is the first report in today’s edition. As far as I know, these clarifying details are being surfaced for the first time. I’ve not yet seen the incorrect detail retracted and clarified by other publications, but am happy to do so, here.</p><p>Today, we cover:</p>  <p>No, Builder.ai did NOT fake AI with 700 engineers. The claim that the AI startup “faked AI” with hundreds of engineers went viral – and I also fell for it, initially. The reality is much more sobering: Builder.ai built a code generator on top of Claude and other LLMs; it did not build a so-called “Mechanical Turk.” One reason the startup might have failed – excluding allegations of accounting fraud – was the time and effort it spent on building internal versions of Slack, Zoom, JIRA, and more, when these tools already existed.</p>  <p>Industry pulse. A big push to repeal Section 174, Meta throws money at fixing its AI problems, Google might be preparing for job cuts, ChatGPT could be eating Google Search market share, and Arc launches “AI-browser”, Dia.</p>  <p>Stock vesting changes at NVIDIA and Anthropic. Stock grants at NVIDIA are becoming front-loaded, while Anthropic has gone from options to double-trigger RSUs.</p>  <p>A reminder of vibe coding’s security risks. Readers of this publication proved vibe-coded apps are a security nightmare, by bypassing the upvoting fingerprinting on <a href="https://vibe-check-engineer-says.lovable.app/">a simple “vibe coded” app</a> which I instructed an AI to make secure.</p>   1. No, Builder.ai didn’t fake AI with 700 engineers <p>An eye-catching detail widely reported by media and on social media about the bankrupt business <a href="http://builder.ai">Builder.ai</a> last week, was that the company faked AI with 700 engineers in India:</p>  <p>“Microsoft-backed AI startup chatbots revealed to be human employees” – <a href="https://mashable.com/article/microsoft-backed-ai-startup-chatbot-human-employees">Mashable</a></p>  <p>“Builder.ai used 700 engineers in India for coding work it marketed as AI-powered” – <a href="https://www.msn.com/en-in/news/other/builder-ai-used-700-engineers-in-india-for-coding-work-it-marketed-as-ai-powered-after-hype-now-goes-bust/ar-AA1FZ65K">MSN</a></p>  <p>“Builder.ai faked AI with 700 engineers, now faces bankruptcy and probe” – India’s <a href="https://www.business-standard.com/companies/news/builderai-faked-ai-700-indian-engineers-files-bankruptcy-microsoft-125060401006_1.html">Business Standard</a></p>  <p>In the past week, I’ve been talking with several engineers who worked at Builder.ai, and can confirm that this detail was untrue. But let’s hold that thought for a second, and do a thought experiment about how we could make this headline be true! Something like it has been attempted before…</p> Design challenge: a system with 700 devs pretending to be an AI <p>Okay, we’ve put on our “evil hacker” villain mask and put ethical considerations in the bin: our goal is to build a system where 700 engineers pretend to be a working AI system, all without using any artificial intelligence. Also, it’s the year 2024 in this experiment. So, how would we pull it off?</p><p>The naive approach: have the devs write code and assume there will never be more than 700 parallel sessions in play:</p> <a href="https://substackcdn.com/image/fetch/$s_!eDZT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9354d0de-54c3-4a55-a79c-d96a20d91705_1596x1362.png">          </a>First attempt at a system where 700 devs can pretend to be an AI <p>There is one immediate, major problem: latency. No user will believe it’s a working AI if it takes 10-30 minutes to provide a response. In that scenario, the deception is likely to be quickly exposed. What’s needed is faster response times, so customers could be fooled into believing they’re interacting with a machine. Basically, what’s called for is something akin to the <a href="https://en.wikipedia.org/wiki/Mechanical_Turk">Mechanical Turk</a>:</p><blockquote><p>“The Mechanical Turk, also known as the Automaton Chess Player, or simply The Turk, was a fraudulent chess-playing machine constructed in 1770, which appeared to be able to play a strong game of chess against a human opponent. For 84 years, it was exhibited on tours by various owners as an automaton.</p><p>The machine survived and continued giving occasional exhibitions until 1854, when a fire swept through the museum where it was kept, destroying the machine. Afterwards, articles were published by a son of the machine's owner revealing its secrets to the public: that it was an elaborate hoax, suspected by some, but never proven in public while it still existed.”</p></blockquote><p>The Automaton Chess Player concealed a person inside the machine, which went unnoticed for more than 80 years:</p> <a href="https://substackcdn.com/image/fetch/$s_!wENQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a2adba6-9c9f-42fb-a648-53aa3d5b7dd5_1600x1458.png">          </a>The Automaton Chess Machine in action <p>Back to the current problem, and applying the inspiration of the 18th century chess machine containing a concealed human. To improve latency – and decrease users’ suspicion – we could perhaps stream what the “assigned developer” typed:</p> <a href="https://substackcdn.com/image/fetch/$s_!PPfX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff812727e-7116-4d95-b783-05a381c99869_1600x1127.png">          </a>Reducing latency of the system by streaming typing <p>This is better, but it remains a giveaway that the system is slow to complete basic tasks. So what about incentivizing our developers with a bonus for completing tasks under 3 minutes, and allowing them to use any tool they want? Incentives are powerful, so it’s likely the following would be observed:</p> <a href="https://substackcdn.com/image/fetch/$s_!dxcP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa57feb7a-0203-4a35-b0ba-e999dc7e48d1_1600x1131.png">          </a>Devs complete tasks much faster when they can use their tools! <p>We did it! We managed to fake a good enough AI.</p><p>But wait… how exactly did the devs complete their tasks within the arbitrary time frame of 3 minutes? To find out, questions are asked, and this what we see (remember, it’s 2024):</p> <a href="https://substackcdn.com/image/fetch/$s_!H05c!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d2ac9ce-ab3f-4c7a-b3dd-c5b2697a4f31_1600x1009.png">          </a>How the “700 devs pretending to be AI” would actually work in 2024 <p>Wait… what?! “Devs pretending to be an AI would use an AI to deliver the outputs in time? This is a logical approach for 2024, when LLMs were already more than capable of generating high-quality code. And this is why it would be irrational to hire 700 developers to pretend to be AI last year, when there were already LLMs that did this much better.</p><p>If you hired a competent engineer in 2024 to design a system that takes a prompt and pretends to be an AI, and they could use any tool they liked, and there were 700 devs for the project, what they built would look something like this:</p> <a href="https://substackcdn.com/image/fetch/$s_!tYy3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ff638a0-236e-437b-b6e5-142c421e6b39_1574x834.png">          </a>How to pretend to be an AI in 2024… just use an LLM! <p>Spoiler: Builder.ai did exactly this as well!</p> Natasha’s tech stack <p>Builder.ai <a href="https://www.youtube.com/watch?v=s4YrQsZ03Vg">first showcased</a> the idea of Natasha in 2021, well before ChatGPT was announced. Back then, Natasha was positioned as a “personal app builder,” and it was clear that the solution worked with a “network of geeks” who built apps to spec:</p> <a href="https://substackcdn.com/image/fetch/$s_!C5Ln!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a69572c-cad3-4121-86ca-bb0e10d23793_1420x738.png">          </a>“You tell us your idea, and me [Natasha] and my network of geeks build it, using building blocks that really work.” Source: <a href="https://www.youtube.com/watch?v=s4YrQsZ03Vg">Builder.ai</a> in 2021 <p>The product promised a cost estimate up front, and a schedule. The idea was that by taking on thousands of projects, the team behind Natasha could create reusable building blocks that speed up building websites and mobile apps.</p><p>In December 2023, one year after ChatGPT was released, Builder.ai <a href="https://www.builder.ai/blog/natasha-everywhere">announced</a> Natasha CodeGen as “your always-on software development partner”. In April 2024, the company demoed Natasha CodeGen in <a href="https://www.builder.ai/blog/codegen-with-natasha">a series of videos</a>, which show code generation happening, as well. In the video, there’s a cut, and the video returns when the React code is generated. I’ve confirmed with former engineers at the company that behind the scenes, the system ran for a few minutes before finishing code generation:</p> <a href="https://substackcdn.com/image/fetch/$s_!dhD8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0be4148a-357d-481f-ad45-b77ae8cc7ab9_1600x960.png">          </a>Natasha’s log output in April 2024. Source: <a href="https://www.builder.ai/blog/codegen-with-natasha">Builder.ai</a> <p>Natasha was aimed to be an AI tool for the whole software development cycle:</p>  <p>Idea: refine an idea with a visual UI of what the app’s UI could look like</p>  <p>Planning: create user stories (tasks) inside a dedicated UI. Tasks include creating acceptance criteria.</p>  <p>Code generation planning: feed the task into an LLM to plan steps for code generation</p>  <p>Testing: have the AI add tests first, following a test driven development (TDD) approach, and create a PR only if the tests pass</p>  <p>Generate code: create the code, and run them against the tests</p>  <p>Create a PR: only do this if all the tests pass</p>  <p>A team of 15 engineers worked on Natasha Codegen. Most engineers were based in the UK, with around 3 in India. At its peak, Builder.ai’s AI team was circa 30 people. On top of building Natasha, the team was building and maintaining many AI products and services. One ex-engineer there told me they thought a lack of focus contributed to the company’s demise.</p><p>The tech stack behind Natasha:</p>  <p>Python: for the orchestrator that lines up steps that the agents took</p>  <p>Ruby on Rails: for parts of the backend and frontend</p>  <p>React: for a good part of the frontend</p>  <p>GPT and Claude for LLMs integrated for the code generation step</p>   <a href="https://substackcdn.com/image/fetch/$s_!2id-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fdd29f7-b016-4bd3-80a4-b87cd2642877_1600x812.png">          </a>Some of the web components for Natasha were built using Ruby on Rails. Source: <a href="https://www.builder.ai/blog/codegen-with-natasha">Builder.ai</a> <p>The team built a set of coding benchmarks that they ran whenever a new model came out, and chose the model that worked best for their use cases.</p><p>Natasha had a grander vision than to just be a code generator tool: it was the codename for all AI projects inside <a href="http://builder.ai">Builder.ai</a>, like Microsoft using “Copilot” for all its AI projects, not only GitHub Copilot. Other products using the Natasha brandname:</p>  <p>A chatbot that customers and developers at Builder.ai could talk to about their codebase, or instruct to implement certain features</p>  <p>A knowledge graph: a vector database storing relationships between features, the blocks that implement them and customer use cases</p>  <p>ML models: to predict how long it would likely take to implement a specification requested by a customer</p>   What about the 700 developers? <p>Builder.ai had a working code generator platform built by around 15 engineers, so why did it need to hire hundreds more more in India? For one thing, Builder hired 300 internal engineers and kicked off building internal tools, all of which could have simply been purchased, including:</p>  <p>Builder Home (dashboard for customers)</p>  <p>Builder Meet (similar to Zoom)</p>  <p>Builder Tracker (similar to JIRA)</p>  <p>Builder Whiteboard (inspired based on Figma: designers would import Figma designs to Whiteboard, and then use these designs to create clickable wireframes and prototypes. Later, Whiteboard exported React code and components to the working folder of customer projects.)</p>  <p>Builder Chat (similar to Slack)</p>  <p>SenseiBot (review and merge PRs and deploy apps to Test/Staging/Prod environments)</p>  <p>One reason Builder.ai failed to grow revenue as quickly as investors were told it was doing, was likely due to this lack of focus and rebuilding tools that already existed without building anything novel.</p><p>Builder.ai also sold an “external development network”, on top of Natasha. There were around 500-1,000 engineers employed through outsourcing companies like Globant, TatvaSoft, and others. These devs were based in places like Vietnam, Romania, Ukraine, Poland, and other countries, as well as India. Last year, the company was working on more than 500 client apps. This number of outsourced devs is likely to be the origin of the “700 developers in India” claim that went viral.</p><p>Former engineers at Builder.ai told me there was internal conflict about what was the main product: was it the Natasha ecosystem, including the code generator, or the bespoke software development service that Builder.ai offered to customers?</p><p>The company built Builder IDE with a team of internal 20 devs and Natasha to help the hundreds of outsourced developers build apps for customers. Builder IDE included facial recognition to verify that the developer matched the profile in the system. It also had a fraud detection system that monitored usage. That system flagged cases where contractors billed for 8 hours, but had been active in the IDE for less.</p><p>Fraud around developer hours worked vs recorded was rampant for two years, according to Yash Mittal, former associate product director at <a href="http://builder.ai">Builder.ai</a>. He wrote:</p><blockquote><p>“The primary bottleneck [of scaling the business] was with our external developer network. Another pioneering effort by Builder.ai involved onboarding developers globally to customize solutions on our platform using our IDEs. However, we didn't anticipate the significant fraud that would ensue, leading to a prolonged and resource-intensive ‘cat and mouse’ game lasting nearly two years before we finally got it under control.”</p></blockquote> Downfall <p><a href="http://builder.ai">Builder.ai</a> went bust after the emergence of allegations of accounting fraud. The Financial Times <a href="https://www.ft.com/content/926f4969-fda7-4e78-b106-4888c8704bda">reported</a> that lenders to the company seized remaining funds once a financial audit revealed the company had apparently misled investors about revenue:</p><blockquote><p>“Builder.ai submitted provisional accounts to its auditor showing large reductions to prior revenue estimates, according to people familiar with the matter.</p><p>These figures showed that a prior $220mn estimate for 2024 revenues had been revised to around $55mn, while a previously reported 2023 total sales figure of $180mn would be restated to roughly $45mn, the people added.”</p></blockquote><p>Lenders withdrawing their capital blew a hole in the accounts, and the fraud allegations ensured no new investors wanted to sink money into the business. The company’s fate was sealed.</p><p>I’ve spoken with engineers who worked at <a href="http://builder.ai">Builder.ai</a>, and they feel disappointed and a bit bitter about the experience. I talked with three engineers who were incredibly disappointed at the company’s collapse, and said they didn’t spot any warning signs. After all, Builder.ai raised money from Microsoft <a href="https://www.builder.io/blog/builder-closes-20-million-funding-m12-microsoft">in April 2024</a> – which itself showed a strong vote of confidence. One dev told me he trusted <a href="http://builder.ai">Builder.AI</a>’s leadership because former CEO Sachin Dev Duggal <a href="https://www.ey.com/en_gl/weoy/class-of-2024/united-kingdom">won</a> Ernst and Young’s “World Entrepreneur of the Year” award as recently as last year.</p> <a href="https://substackcdn.com/image/fetch/$s_!9zxS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd20128ae-b96d-40b3-84f8-4a68a6cae9ef_1600x851.png">          </a>A journey: entrepreneur of the Year in 2024, accused of misleading investors in 2025. Source: <a href="https://www.ey.com/en_gl/weoy/class-of-2024/united-kingdom">Ernst and Young</a> <p>These engineers did solid work, created an AI system that felt like it was on par in capability terms with the likes of Devin and Factory. Unfortunately, the viral claim that Builder.ai used human devs to pretend to be an AI, has them fearing an impact upon their career prospects.</p><p>This is why I want to share the truth about Builder.ai’s tech stack: that there was no conspiracy to deceive users into interacting with 700 devs in the mistaken belief they were working with a cutting-edge AI. The devs did solid work, and the company’s demise was totally unrelated to their efforts.</p><p>Also, I find it hard to believe that devs joining the then-high flying AI company could have had knowledge of machinations taking place at the executive management level of the business.</p><p>So, where did the viral claim about 700 devs pretending to be AI, originate. The Financial Times <a href="https://www.ft.com/content/16ee837a-1d89-448b-8a33-9741025334d6">tracked it down</a> to this post from an account on X:</p> <a href="https://substackcdn.com/image/fetch/$s_!K59a!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F094b9a82-6b9b-48b9-85a6-5e4ca6ee641d_796x1340.png">          </a><a href="https://x.com/BernhardEngel_/status/1928033488420184450">This post</a> from a self-proclaimed crypto enthusiast with no reporting history turned out to be false <p>The fake claim in this post caught people’s attention, including finance newsletter writer Linas Beliūnas, who shared it with his more than 500,000 LinkedIn followers, and many publications quoted that post:</p> <a href="https://substackcdn.com/image/fetch/$s_!AcPF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F804f47cf-812b-44bd-aa6f-abd86865b11b_876x904.png">          </a>Shocking claims travel fast, even when untrue. Source: Linas Beliūnas on <a href="https://www.linkedin.com/posts/linasbeliunas_wild-15-billion-ai-unicorn-just-collapsed-activity-7334521571966877696-1GUa?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAIk0KwBsmE3oBadWSg2ettxmEyKbqZKG34">LinkedIn</a> <p>This is a good reminder of the importance of checking sources, and to be extra sceptical about social media posts. This also applies to me because last week this publication was among those which reported the claim. This is why I consider it important to recognise the error, and to go get the full story by talking with people who worked at <a href="http://builder.ai">Builder.ai</a>.</p><p>If your team is looking to hire engineers with experience building real AI systems, the Builder.ai alumni group is likely a great source of such hires. It’s sad to see a startup implode in the AI space over fraud allegations, and good luck to engineers who worked at Builder.ai in finding their next role!</p> 2. Industry Pulse  Push to repeal Section 174 accounting changes <p>More than a year ago, in January 2024, The Pragmatic Engineer was one of the first tech publications <a href="https://blog.pragmaticengineer.com/section-174/">to raise the alarm</a> on the disastrous impact of an accounting change in the US for the tech industry.</p>
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-137">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!eDZT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9354d0de-54c3-4a55-a79c-d96a20d91705_1596x1362.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:39 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 11. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.webistemology.com/ai-slop-and-sturgeons-law/'>AI Slop and Sturgeon's Law</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  In a wonderful bit of serendipity, Sturgeon's Law has become a way to counter AI hype and provide a cautionary note about AI tools and a fundamental problem with them.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://en.wikipedia.org/wiki/Theodore_Sturgeon?ref=webistemology.com">Theodore Sturgeon</a> was a prominent science-fiction author who may be best known for 1953's <a href="https://www.goodreads.com/book/show/541024.More_Than_Human?ref=webistemology.com">More Than Human</a>. If it were written today, it might be characterised as a transhumanist novel. He is also credited with originating what has come to be known as <a href="https://en.wikipedia.org/wiki/Sturgeon'%20s_law?ref=webistemology.com">Sturgeon's Law</a>. When asked to defend the quality of his genre against critics who claimed 90 per cent of it was crud, Sturgeon famously retorted, "Ninety percent of everything is crap."</p><p>He wasn't being cynical but realistic. Any content consumer today will recognise that most of what is produced is mediocre, uninspired, or simply incorrect. This is mainly true across various fields, including literature, film, music, academic research, and the vast array of content on the internet. Only a small fraction, at most 10 per cent, is genuinely excellent.</p><p>For decades, this was just a pithy or curmudgeonly comment on the everyday creation of what we now call content. In a wonderful bit of serendipity, Sturgeon's Law has become a way to counter AI hype and provide a cautionary note about AI tools and a fundamental problem with them.</p> The Training Data Dilemma: An Ocean of Mediocrity <p>Large Language Models (LLMs), the engines behind tools like ChatGPT, Gemini, or Claude, are trained on datasets scraped indiscriminately from the Internet. They then extrude text, mimicking writing, reasoning, and creativity by identifying patterns in the text and images created by humans. This includes everything from digitised books and scientific papers to news articles, Reddit comments, and billions of social media posts. Here's the catch: if Sturgeon's Law holds for the Internet (and anyone who has spent time online knows that 90 per cent is a generous underestimate of the amount of crap out there), then these AI models are being trained on a dataset where at least 90 percent of the material is, for lack of a better word, crap.</p><p>When you train a system on a diet of mediocrity, you shouldn't be surprised when its output reflects that. The model learns to replicate the most common patterns it sees. It's a regression to the mean, a levelling to the lowest common denominator. The result is what many are now calling "AI slop"—content that is superficially plausible, grammatically correct, but ultimately bland, soulless, and often subtly inaccurate. It mimics the style of human writing without the substance, the spark of original thought, or the depth of lived experience.</p><p>This isn't just a matter of quality; it's a matter of truth. The "crap" in the training data includes not just poorly written prose but also misinformation, conspiracy theories, and deeply ingrained societal biases. The AI learns these patterns just as readily as it learns grammar rules.</p> The Consequences for Authentic Creation <p>The proliferation of AI slop presents a clear danger to creators, researchers, and anyone who values authentic human expression.</p>  The Devaluation of Original Work: As our feeds become flooded with cheap, instantly generated content, original research and authentic creativity fade into the background. It becomes increasingly difficult for readers and consumers to distinguish the genuine article from a sea of synthetic text. Why would a company pay a Canadian writer or journalist for a thoughtful article when they can generate a passable, keyword-stuffed equivalent for pennies? The economic foundation of creative labour is eroding.  Poisoning the Well for Future Knowledge: We are creating a self-referential loop of mediocrity. Future AI models will inevitably be trained on the slop produced by today's AI. The internet's dataset is becoming increasingly polluted with synthetic, derivative content. This will render future AI models less reliable, making it harder for humans to find trustworthy information and untainted sources. AI models are crapping into their input funnels. The results will not be good.  Privacy, Data Colonialism, and the Human Cost: Let's Not Forget Where This Training Data Comes From. It's our collective intellectual and creative output—our blog posts, family photos, and late-night forum arguments—scraped and ingested without our meaningful consent to fuel the development of commercial products. This is a new form of data colonialism, where the raw material of our lives is extracted, processed, and turned into a product that benefits a handful of corporations, primarily in the Global North.  But the exploitation runs deeper. The "dirty work" of making these AI systems functional—the painstaking and often traumatising labour of data classification and content moderation—is frequently outsourced to workers in the Global South. These are the people paid pennies to view and label the very worst of the internet's content: hate speech, violence, and abuse. This is the hidden, human cost of our shiny new AI toys, a stark reminder of the global inequalities that underpin the digital economy.    The Climate Connection: The computational power required to train these massive models is immense, contributing to a significant carbon footprint. Are we willing to expend vast amounts of energy and resources, contributing to our climate crisis, to generate a tidal wave of digital mediocrity? Is this a worthy trade-off?   Charting a Different Course🍁 <p>The federal government has just appointed its first-ever Minister of Artificial Intelligence and Digital Innovation, Evan Solomon. This presents a pivotal opportunity to reject the Silicon Valley model of reckless development and instead forge a uniquely Canadian path. This is the moment to champion Public AI—AI systems built in the public interest. We can create open-source models trained not on the internet's sludge, but on high-quality, curated datasets from our world-class cultural and scientific institutions. Imagine AI tools designed to strengthen public services, not just to maximise corporate profit.</p><p>The new ministry must establish robust guardrails for the sustainable development of AI. That means implementing policies that demand energy efficiency and transparency from data centres, protect workers from exploitative labour practices common in the industry, and ensure that the benefits of AI are shared by all Canadians, not just a select few. The new minister has signalled a focus on economic benefits over regulation, which makes it even more critical for activists and citizens to demand a framework that prioritises people and the planet.</p><p>We cannot allow the future of our information ecosystem to be dictated by a model trained on the worst of us. We need a public conversation about the digital world we want to build, led by citizens and not just tech corporations.</p><p>This means demanding transparency in how AI models are trained, fighting for policies that protect the rights and livelihoods of human creators, and investing in and creating spaces for authentic, high-quality information and art to flourish, free from the noise of the slop machine.</p><p>We cannot allow the future of our information ecosystem to be dictated by a model trained on the worst of us. We need a public conversation about the digital world we want to build, led by citizens and not just tech corporations. Sturgeon's Law was a witty observation. It was never meant to be a technical blueprint for the future of knowledge. It's time we focused on curating and celebrating the 10 per cent of excellence rather than building a global infrastructure that endlessly regurgitates the other 90 per cent.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://www.webistemology.com/content/images/2025/06/ben-hadfield-JC-jxc8Wpxg-unsplash.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:11 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.talkingidentity.com'>Talking Identity</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.talkingidentity.com/2025/06/secure-by-design-has-an-incentive-problem.html'>Secure-by-Design has an Incentive Problem</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  In my last blog post, I argued that we don’t need more innovation invention to fix the broken state of SaaS and cloud security that Patrick Opet’s open letter was calling out. Instead, I said that what we need are different priorities. The conversations it triggered basically boiled down to this: if we already know […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>In <a href="https://blog.talkingidentity.com/2025/05/the-innovation-we-need-is-strategic-not-technical.html">my last blog post</a>, I argued that we don’t need more innovation invention to fix the broken state of SaaS and cloud security that Patrick Opet’s open letter was calling out. Instead, I said that what we need are different priorities. The conversations it triggered basically boiled down to this: if we already know what good looks like, why aren’t more vendors and enterprises doing it?</p>



<p>&lt;Slight aside&gt;On LinkedIn, Mike Schwartz <a href="https://www.linkedin.com/feed/update/urn:li:activity:7328465577360900098?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7328465577360900098%2C7333473497727307776%29&amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287333473497727307776%2Curn%3Ali%3Aactivity%3A7328465577360900098%29">called me out</a> for saying we don’t need innovation, which is fair. He pointed out that what I was calling out was not needing more invention (of new tech or standards), but innovating on just getting people to use a lot of what we already have. Thankfully, that lines up with where the rest of this post is going&lt;End aside&gt;</p>



<p>The answer, unfortunately, is as straightforward as it is challenging: the incentives are all wrong.</p>



<a href="https://blog.talkingidentity.com/wp-content/uploads/2025/06/green-prophet-OAJj6_PjvBA-unsplash-scaled-e1749665329428.jpg"></a>



<p></p>



<p>Secure-by-design isn’t failing because the right standards, technology, or architectures don’t exist. It’s failing because there is no systemic reason to prioritize it – neither for the people building software, nor for those buying it. In fact, one could argue that today’s ecosystem actually discourages secure-by-design thinking. Vendors end up optimizing for roadmap velocity, not resilience. Enterprise buyers meanwhile are optimizing for feature parity and price, not control integrity. Security teams are told to meet deadlines instead of challenging system design. And compliance overrides capability as the default benchmark for “good enough.”</p>



 Compliance Is Treated as the Ceiling instead of the Floor 



<p>As many discussions at the recently concluded <a href="https://www.identiverse.com">Identiverse</a> conference made clear, compliance does result in security, but the kind that’s seemingly frozen in time: narrowly scoped, backward-looking, and audit-centric. Compliance-driven security doesn’t adapt as threats evolve. It doesn’t incentivize secure architectures. And it certainly doesn’t reward proactive, defense-in-depth investments like the ones being called for: session integrity, token binding, or real-time access evaluation.</p>



<p>This is what makes Patrick Opet’s open letter so relevant. Despite my reservations with where some of the blame was laid, what it clearly did was call out the need for all of us to stop settling for security theater, and to start building systems that are actually resilient to our ever-evolving threat landscape.</p>



<p>The hard truth is that we can’t expect (security) people to just do the right thing (cue <a href="https://www.linkedin.com/in/sanderiam/">the philosopher</a> in my ear saying “duh!”). We need to create incentives for doing the right thing.</p>



<p>Secure-by-design isn’t rocket science, but it does require effort. It requires time, architectural rethink, cross-functional coordination, and long-term investment. Unfortunately in today’s landscape, it is hard to find places where that is rewarded (though I have seen some examples of teams really trying).</p>



<p>So if we want more secure-by-design adoption, we need to fix the incentive structures that govern behavior, both for vendors as well as buyers.</p>



 What A Good Incentive Structure Could Look Like 



 
 Shift Liability for Insecure Defaults: Right now, when SaaS vendors ship insecure implementations of OAuth or rely on fragile session management, it is the customer who typically pays the price in the event of a breach. Introducing clearer, shared liability standards – especially in cases of negligence or insecure-by-default configurations – would force vendors to take greater ownership of security posture, not just feature completeness. And I say this as someone who’s spent his life mostly on the vendor side of the equation. 



 Make Secure Architectures a Market Advantage: Security is often invisible in the buying process. That has to change. Procurement teams can start by asking tougher/deeper questions in RFPs. Go beyond “Do you support SSO?” and “Do you implement OAuth”, and start asking “How do you manage token lifecycle and session state?” and “are you enforcing DPoP or Mutual TLS?”. Independent benchmarking (think energy efficiency ratings or credit scores) could create a public, competitive metric for software security maturity. As a security industry, we need to make it much, much easier for buyers to do comparative evaluations. 



 Reward Security Investments with Lower Cyber Risk Premiums: Cyber insurance is a rapidly growing space. Providers are already building models to assess risk posture, and are in a perfect position to reward vendors and buyers who implement secure-by-design principles with lower premiums or higher coverage ceilings. This is already done in other domains (drivers that have done a defensive driving course are cheaper to insure). So why can’t we do the same for software with hardened session controls? Of course, the previous point about creating benchmarks and making comparisons easier become relevant here. 



 Measure the Right Things at the Board Level: How many posts have we seen about security and fraud reduction needing to become a board level priority. But it has to be done correctly. If the only metric security leaders are reporting to the board is “number of passed audits,” then secure-by-design will never get the visibility or funding it needs. We need to elevate identity posture, architectural maturity, and integration integrity to the same level as SLAs and NPS. Security isn’t just a function of the CISO. It’s a strategic business risk. And boards should treat it accordingly, giving CISOs the support they need. 



 Embed Security Into Corporate Culture: This one’s less about structure and more about mindset. Just as accessibility and sustainability are becoming table stakes in modern product development (sometimes through regulation), secure-by-design needs the same kind of internal advocacy. That means giving product managers and engineers the time, training, and tooling to make security a design decision, and not something relegated to a post-launch cleanup effort. It means moving security left and up the value chain. 
 



 This Isn’t About Blame. It’s About Incentive Design. 



<p>The clear need of the hour is to realign the system so that secure-by-design becomes the default outcome of doing good business. That means rethinking procurement, regulation, insurance, and organizational measurement. This is asking more of both enterprise buyers and vendors, but also giving them a reason to invest.</p>



<p>The technical playbook already exists. The missing ingredient is the will to change incentives. Until we fix that, we’ll keep pretending that compliance is enough, even as the cracks widen. We must stop rewarding checkbox security. Let’s start building systems that are actually built to withstand the world we live in.</p>



<a href="https://blog.talkingidentity.com/wp-content/uploads/2025/06/getty-images-7dN3b9Xh84Q-unsplash-scaled-e1749665612943.jpg"></a>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://blog.talkingidentity.com/wp-content/uploads/2025/06/green-prophet-OAJj6_PjvBA-unsplash-scaled-e1749665329428-1024x471.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   18:18 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/tdd-ai-agents-and-coding-with-kent'>TDD, AI agents and coding with Kent Beck</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Kent Beck—creator of Extreme Programming and co-author of the Agile Manifesto—reflects on decades of coding, from the birth of TDD to his experiments with AI tools shaping software’s future.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Stream the Latest Episode <p>Listen and watch now on <a href="https://youtu.be/aSXaxOdVtAQ">YouTube</a>,  <a href="https://open.spotify.com/episode/1S28nbYSgRoFwvRrC8w0QI">Spotify</a> and <a href="https://podcasts.apple.com/us/podcast/the-pragmatic-engineer/id1769051199">Apple</a>. See the episode transcript at the top of this page, and timestamps for the episode at the bottom. </p> Brought to You By <p>• <a href="http://sonarsource.com/pragmaticsecurity">Sonar</a> — Code quality and code security for ALL code.</p><p>•⁠ <a href="http://statsig.com/pragmatic">Statsig</a> <a href="https://modal.com/pragmatic">⁠</a> — <a href="https://modal.com/pragmatic">⁠</a> The unified platform for flags, analytics, experiments, and more.</p><p>• <a href="http://augmentcode.com/pragmatic">Augment Code</a> — AI coding assistant that pro engineering teams love</p><p>—</p> In This Episode <p>Kent Beck is one of the most influential figures in modern software development. Creator of Extreme Programming (XP), co-author of <a href="https://agilealliance.org/agile101/the-agile-manifesto/">The Agile Manifesto</a>, and a pioneer of Test-Driven Development (TDD), he’s shaped how teams write, test, and think about code.</p><p>Now, with over five decades of programming experience, Kent is still pushing boundaries—this time with AI coding tools. In this episode of Pragmatic Engineer, I sit down with him to talk about what’s changed, what hasn’t, and why he’s more excited than ever to code.</p><p>In our conversation, we cover:</p>  <p>Why Kent calls AI tools an “unpredictable genie”—and how he’s using them</p>   <a href="https://substackcdn.com/image/fetch/$s_!7uBC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa928f800-c894-46f7-bbbc-f690db6de2f3_1930x1032.png">          </a>Kent’s mental model of AI agents: an “unpredictable genie” that grants your “wishes” — but oftentimes in unexpected (and illogical) ways!   <p>Why Kent no longer has an emotional attachment to any specific programming language</p>  <p>The backstory of The Agile Manifesto—and why Kent resisted the word “agile”</p>  <p>An overview of XP (Extreme Programming) and how Grady Booch played a role in the name</p>  <p>Tape-to-tape experiments in Kent’s childhood that laid the groundwork for TDD</p>  <p>Kent’s time at Facebook and how he adapted to its culture and use of feature flags</p>  <p>And much more!</p>       Takeaways <p>Some of the most interesting topics discussed were these:</p><p>1. Kent is re-energized thanks to using AI agents to build stuff. Kent has been coding for 52 years, and the last decade, he’s gotten a lot more tired of all of it: learning yet another new language or framework, or debugging the issues when using the latest framework.</p><p>What he loves about these AI agents (and AI coding tools) is how he doesn’t need to know exactly all the details: he can now be a lot more ambitious in his projects.</p><p>Currently, Kent is building a server in <a href="https://en.wikipedia.org/wiki/Smalltalk">Smalltalk</a> (that he’s been wanting to do for many years) and a <a href="https://microsoft.github.io/language-server-protocol/">Language Server Protocol</a> (LSP) for Smalltalk</p><p>2. Test driven development (TDD) is a “superpower” when working with AI agents. AI agents can (and do!) introduce regressions. An easy way to ensure this does not happen is to have unit tests for the codebase.</p><p>Kent Beck is one of the biggest proponents of TDD, so it’s no surprise he is using this approach when coding with these agents as well. What is surprising is how he’s having trouble stopping AI agents from deleting tests in order to make them “pass!”</p><p>3. Facebook wrote no unit tests in 2011, and this stunned Kent, back in the day. Kent joined Facebook in 2011, and was taken aback by the lack of testing and how everyone pushed code to production without automated testing.</p><p>What he came to realize – and appreciate! – was how Facebook had several things balancing this out:</p>  <p>Devs took responsibility for their code very seriously</p>  <p>Nothing at Facebook was “someone else’s problem:” devs would fix bugs when they saw them, regardless of whose commit caused it</p>  <p>Feature flags were heavily used for risky code</p>  <p>Facebook did staged rollouts to smaller markets like New Zealand</p>  <p>To this date, Facebook ships code to production in a unique way. We covered more in the deepdive <a href="https://newsletter.pragmaticengineer.com/p/shipping-to-production">Shipping to Production</a>.</p><p>4. The “Extreme” in “Extreme Programming” was a marketing hack (kind of!) Kent shared the hilarious story of how he came up with the name “Extreme Programming.” He came up with a methodology that worked really well for one of his clients (which would become Extreme Programming) and wanted to name it. This is how:</p><blockquote><p>“I wanted to pick a word that Grady Booch would never say that he was doing. Because that was the competition! I didn't have a marketing budget. I didn't have any money. I didn't have that kind of notoriety [that Grady Booch already had]. I didn't have that corporate backing.</p><p>So if I was going to make any kind of impact, I had to be a little bit outrageous. Extreme sports were coming up back then. And I picked that metaphor.</p><p>It's actually a good metaphor because extreme athletes are the best prepared, or they’re dead. People so desperately wanted something kind of like that then it just exploded from there.”</p></blockquote><p>We previously did an episode on <a href="https://newsletter.pragmaticengineer.com/p/software-architecture-with-grady-booch">Software architecture with Grady Booch</a>.</p> Interesting quote: devs should experiment with GenAI <p>From <a href="https://youtu.be/aSXaxOdVtAQ?si=oZkdBP5K4gzseq80&amp;t=3208">53:30</a>:</p><blockquote><p>Gergely: “I wonder if we're going back to discovering things that we you were popularizing in the 2000s.”</p><p>Kent: “People should be experimenting. Try all the things, because we just don't know.</p><p>The whole landscape of what's ‘cheap’ and what's ‘expensive’ has all just shifted. Things that we didn't do because we assumed they were going to be expensive or hard just got ridiculously cheap. Like, what would you do today if cars were suddenly free? Okay, things are going to be different, but what are the second and third-order effects? Nobody can predict that! So we just have to be trying stuff.”</p></blockquote> The Pragmatic Engineer deepdives relevant for this episode   <p><a href="https://newsletter.pragmaticengineer.com/p/facebook">Inside Facebook’s engineering culture</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/shipping-to-production">Shipping to Production</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/software-architecture-with-grady-booch">Software architecture with Grady Booch</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/the-ai-engineering-stack">The AI Engineering Stack</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/the-past-and-future-of-backend-practices">The past and future of modern backend practices</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/ai-engineering-in-the-real-world">AI Engineering in the real world</a></p>  <p><a href="https://newsletter.pragmaticengineer.com/p/paying-down-tech-debt">Paying down tech debt</a></p>   Timestamps <p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ">00:00</a>) Intro</p><p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ&amp;t=147s">02:27</a>) What Kent has been up to since writing Tidy First</p><p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ&amp;t=365s">06:05</a>) Why AI tools are making coding more fun for Kent and why he compares it to a genie</p><p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ&amp;t=821s">13:41</a>) Why Kent says languages don’t matter anymore</p><p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ&amp;t=1016s">16:56</a>) Kent’s current project building a small talk server</p><p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ&amp;t=1071s">17:51</a>) How Kent got involved with The Agile Manifesto</p><p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ&amp;t=1426s">23:46</a>) Gergely’s time at JP Morgan, and why Kent didn’t like the word ‘agile’</p><p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ&amp;t=1585s">26:25</a>) An overview of “extreme programming” (XP)</p><p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ&amp;t=2141s">35:41</a>) Kent’s childhood tape-to-tape experiments that inspired TDD</p><p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ&amp;t=2531s">42:11</a>) Kent’s response to Ousterhout’s criticism of TDD</p><p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ&amp;t=3005s">50:05</a>) Why Kent still uses TDD with his AI stack</p><p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ&amp;t=3266s">54:26</a>) How Facebook operated in 2011</p><p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ&amp;t=3850s">1:04:10</a>) Facebook in 2011 vs. 2017</p><p>(<a href="https://www.youtube.com/watch?v=aSXaxOdVtAQ&amp;t=4344s">1:12:24</a>) Rapid fire round</p> References <p>Where to find Kent Beck:</p><p>• X: <a href="https://x.com/kentbeck">https://x.com/kentbeck</a></p><p>• LinkedIn: <a href="https://www.linkedin.com/in/kentbeck/">https://www.linkedin.com/in/kentbeck/</a></p><p>• Website: <a href="https://kentbeck.com/">https://kentbeck.com/</a></p><p>• Newsletter: </p>  <a href="https://tidyfirst.substack.com?utm_source=substack&amp;utm_campaign=publication_embed&amp;utm_medium=web">Software Design: Tidy First? Software design is an exercise in human relationships. So are all the other techniques we use to develop software. How can we geeks get better at technique as one way of getting better at relationships?  By Kent Beck </a>  <p>Mentions during the episode:</p><p>• Extreme Programming Explained: Embrace Change: <a href="https://www.amazon.com/Extreme-Programming-Explained-Embrace-Change/dp/0321278658">https://www.amazon.com/Extreme-Programming-Explained-Embrace-Change/dp/0321278658</a></p><p>• The Agile Manifesto: <a href="https://agilealliance.org/agile101/the-agile-manifesto/">https://agilealliance.org/agile101/the-agile-manifesto/</a></p><p>• Tidy First?: A Personal Exercise in Empirical Software Design: <a href="https://www.amazon.com/Tidy-First-Personal-Exercise-Empirical/dp/1098151240">https://www.amazon.com/Tidy-First-Personal-Exercise-Empirical/dp/1098151240</a></p><p>• Measuring developer productivity? A response to McKinsey: <a href="https://newsletter.pragmaticengineer.com/p/measuring-developer-productivity">https://newsletter.pragmaticengineer.com/p/measuring-developer-productivity</a></p><p>• Dead Code, Getting Untangled, and Coupling versus Decoupling: <a href="https://newsletter.pragmaticengineer.com/p/dead-code-getting-untangled-and-coupling">https://newsletter.pragmaticengineer.com/p/dead-code-getting-untangled-and-coupling</a></p><p>• Augment: <a href="https://www.augmentcode.com/">https://www.augmentcode.com/</a></p><p>• Smalltalk: <a href="https://en.wikipedia.org/wiki/Smalltalk">https://en.wikipedia.org/wiki/Smalltalk</a></p><p>• Swift: <a href="https://www.swift.org/">https://www.swift.org/</a></p><p>• Go: <a href="https://go.dev/">https://go.dev/</a></p><p>• Rust: <a href="https://www.rust-lang.org/">https://www.rust-lang.org/</a></p><p>• Haskell: <a href="https://www.haskell.org/">https://www.haskell.org/</a></p><p>• C++: <a href="https://isocpp.org/">https://isocpp.org/</a></p><p>• JavaScript: <a href="https://www.javascript.com/">https://www.javascript.com/</a></p><p>• Managing the Development of Large Software Systems: <a href="https://www.praxisframework.org/files/royce1970.pdf">https://www.praxisframework.org/files/royce1970.pdf</a></p><p>• Martin Fowler’s website: https://martinfowler.com/</p><p>• DSDM: <a href="https://en.wikipedia.org/wiki/Dynamic_systems_development_method#">https://en.wikipedia.org/wiki/Dynamic_systems_development_method#</a></p><p>• Crossing the Chasm, 3rd Edition: Marketing and Selling Disruptive Products to Mainstream Customers: <a href="https://www.amazon.com/Crossing-Chasm-3rd-Disruptive-Mainstream/dp/0062292986">https://www.amazon.com/Crossing-Chasm-3rd-Disruptive-Mainstream/dp/0062292986</a></p><p>• Tottenham Hotspur: <a href="https://www.tottenhamhotspur.com/">https://www.tottenhamhotspur.com/</a></p><p>• J.P. Morgan: <a href="https://www.jpmorgan.com/">https://www.jpmorgan.com/</a></p><p>• Software architecture with Grady Booch: <a href="https://newsletter.pragmaticengineer.com/p/software-architecture-with-grady-booch">https://newsletter.pragmaticengineer.com/p/software-architecture-with-grady-booch</a></p><p>• Grady Booch on LinkedIn: <a href="https://www.linkedin.com/in/gradybooch/">https://www.linkedin.com/in/gradybooch/</a></p><p>• The Philosophy of Software Design – with John Ousterhout: <a href="https://newsletter.pragmaticengineer.com/p/the-philosophy-of-software-design">https://newsletter.pragmaticengineer.com/p/the-philosophy-of-software-design</a></p><p>• A Philosophy of Software Design: My Take (and a Book Review): <a href="https://blog.pragmaticengineer.com/a-philosophy-of-software-design-review/">https://blog.pragmaticengineer.com/a-philosophy-of-software-design-review/</a></p><p>• Cursor: <a href="https://www.cursor.com/">https://www.cursor.com/</a></p><p>• Facebook Engineering Process with Kent Beck: <a href="https://softwareengineeringdaily.com/2019/08/28/facebook-engineering-process-with-kent-beck/">https://softwareengineeringdaily.com/2019/08/28/facebook-engineering-process-with-kent-beck/</a></p><p>• Claude Code: <a href="https://www.anthropic.com/claude-code">https://www.anthropic.com/claude-code</a></p><p>• The Timeless Way of Building: <a href="https://www.amazon.com/Timeless-Way-Building-Christopher-Alexander/dp/0195024028">https://www.amazon.com/Timeless-Way-Building-Christopher-Alexander/dp/0195024028</a></p><p>—</p><p>Production and marketing by <a href="https://penname.co/">Pen Name</a>. </p><p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!7uBC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa928f800-c894-46f7-bbbc-f690db6de2f3_1930x1032.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:10 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 10. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.webistemology.com/our-digital-moai/'>Our Digital Moai</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Today, our monuments are less tangible than stone, but no less massive. They are the sprawling, humming, and voraciously hungry data centres that power artificial intelligence.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<blockquote>The Silent, Towering Costs of AI</blockquote><p>They stand silent on the slopes of a remote island, stone giants staring out at an empty sea. The Moai of Rapa Nui (Easter Island) are marvels of human ingenuity and devotion, a testament to a society’s capacity for immense, coordinated effort. They are also silent witnesses to a catastrophic collapse. The society that carved them, that poured its identity, resources, and labour into their creation, seemingly sacrificed its very future for them.</p><p>Today, our monuments are less tangible than stone, but no less massive. They are the sprawling, humming, and voraciously hungry data centres that power artificial intelligence. To their builders, they are the pinnacle of our achievement, the foundation of a future defined by limitless progress. But if we listen closely, past the utopian promises of Silicon Valley, we can hear a warning echoing from the shores of Rapa Nui. Our digital Moai are being erected at a staggering dollar, human, and ecological cost—and we are refusing to see the cliff edge we are racing towards.</p><p>The construction of the Moai was a symptom, not the cause, of the disease. It represented a society’s totalising focus on a single form of prestige and power, a cultural obsession that required the felling of entire forests to transport the monolithic statues. This deforestation led to soil erosion, the collapse of biodiversity, and ultimately, the starvation and warfare that decimated the Rapa Nui people. They likely didn’t see the last tree for the statues.</p> Are we any different? <p>The AI industry is powered by data centres that consume biblical amounts of energy and water. A single query to a generative AI model can use nearly ten times the electricity of a simple Google search. Globally, the electricity consumption of data centres is projected to more than double by 2026, surpassing the entire energy use of countries like France. In Canada, where some pride themselves on their climate commitments, the race to build these facilities threatens to derail our goals, with some projections showing that new data centres could force provinces to rely more heavily on fossil fuels, effectively wiping out hard-won emissions reductions.</p><p>This insatiable thirst extends to water. Billions of litres of fresh water are required to cool these computational behemoths, diverting a precious resource from communities and ecosystems already under strain from a worsening climate crisis. We are, in essence, draining our digital island’s resources to power our statues.</p><p>But the cost is not just ecological. The great stone heads of Rapa Nui required the immense physical labour of their people. Our digital Moai are also built on the backs of a vast, hidden workforce—one located primarily in the Global South.</p><p>Beneath AI's sleek, automated façade lies a sprawling human infrastructure of “ghost workers.” Millions of people in countries like Kenya, the Philippines, and India perform the gruelling, low-paid data annotation and content moderation labour. These are the digital carvers, meticulously labelling images and text to train AI models, or shielding users from the internet's most toxic content by viewing it themselves.</p><p>Reports have exposed a grim reality: workers are paid less than $2 an hour, face precarious employment with no benefits, and suffer significant psychological trauma. This is the dark underbelly of the AI revolution, a system of digital colonialism that concentrates wealth and power in the Global North while outsourcing the human and environmental damage to the South. It is an extractive logic as old as empire, repackaged in the language of innovation.</p><p>Just as the Rapa Nui chiefs likely celebrated each new, larger Moai, our tech titans celebrate every bigger, more powerful AI model. The colossal sums of money, the brightest minds of a generation, and our planet’s finite resources are being poured into this singular pursuit. We are told it is for the good of humanity, that this is the only path to progress.</p><p>However, a critical and honest accounting is long overdue. Could this immense allocation of capital and intellect not be better utilised to address the crises that AI is exacerbating—climate change, resource scarcity, and global inequality?</p><p>The Rapa Nui left us a haunting lesson carved in stone. Their statues were a cultural apex, but also a symbol of unsustainable choices, of a society so fixated on its symbols of status that it failed to protect its foundations.</p><p>We must now look at the humming, windowless data centres rising in our communities and worldwide and ask ourselves the hard questions. What are we sacrificing for these digital giants? Whose labour are they built on? Whose water are they drinking? And when we have strip-mined our planet for the energy and materials to power them, who will be left to admire their cleverness?</p><p>It is time to demand transparency, accountability, and a radical shift in direction. We need sustainable AI, fair labour practices throughout the entire AI supply chain, and a public, democratic conversation about the kind of future we are building. Otherwise, our magnificent digital Moai will become our legacy—towering monuments to our own brilliant, self-inflicted demise, standing silent watch over a world we chose not to save.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://www.webistemology.com/content/images/2025/06/Gemini_Generated_Image_y2gz0uy2gz0uy2gz.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:45 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.webistemology.com/the-carney-liberals-and-lawful-access/'>The Carney Liberals and Lawful Access</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The Carney Liberals have introduced a bill that would permanently damage our digital privacy. Like a zombie, the idea of "lawful access" legislation keeps rising from the dead, no matter how many times Canadians have killed it.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<blockquote>It's the new surveillance act, same as the old surveillance act</blockquote><blockquote>This was originally posted on June 10, 2025, on Substack.</blockquote><p>The Carney Liberals have introduced a bill that would permanently damage our digital privacy. Like a zombie, the idea of "lawful access" legislation keeps rising from the dead, no matter how many times Canadians have killed it. First introduced by previous Liberal governments in the 2000s, then aggressively pushed by Stephen Harper’s Conservatives as the infamous Bill C-30, this toxic idea is now back, championed by the current Liberal government.</p><p>Buried deep inside a massive piece of legislation misleadingly titled the "Strong Borders Act" (Bill <a href="https://www.parl.ca/DocumentViewer/en/45-1/bill/C-2/first-reading?ref=webistemology.com">C-2</a>) is Part 15: the Supporting Authorised Access to Information Act (SAAIS). Let's call it what it is: the same old Government Surveillance Bill in a new disguise.</p> A Zombie Bill That Refuses to Die <p>This isn't a new fight, and it crosses party lines.</p>  In 2012, the Conservative government introduced Bill C-30, the "Protecting Children from Internet Predators Act"—a cynical title for a bill that would have given authorities warrantless access to your data. Public Safety Minister Vic Toews infamously told opponents they could "either stand with us or with the child pornographers."  Canadians from all walks of life rose. The backlash was immense and unified. The government was forced to kill the bill completely.  We won. Canadians across the political spectrum clearly stated that warrantless access and mandated surveillance backdoors have no place in Canada.  <p>Now, the Carney Liberals are trying again, using the classic political tactic of hiding these hugely controversial powers inside an unrelated omnibus bill, hoping we won't notice. But some of us have noticed.</p> Why We Must Fight Bill C-2 Today <p>This new Liberal bill shares the same alarming features as its predecessors. If passed, it would:</p> Force Companies to Build Backdoors <p>The bill would compel companies, from your messaging app to your email provider, to build the technical tools for police and spy agencies to intercept your communications. While it pays lip service to not creating a "systemic vulnerability," the government gives itself the power to define that term, creating a loophole big enough to drive a surveillance truck through.</p> Operate in Secrecy <p>Section 15 of Bill C-2 could impose a permanent gag order on any company served with an order. The company could never tell its users, the media, or the public that it has been forced to become an arm of the state. This is a recipe for a secret law system free from democratic accountability.</p> Sideline Our Courts <p>Instead of requiring a judge to approve these intrusive measures, the bill hands that power to a political minister in the Carney government. It’s a blatant power grab by the executive branch, undermining the checks and balances that protect our rights.</p> Affect Everyone <p>The bill's scope is so broad that it could apply to nearly any online service you use. With massive fines for non-compliance, no company could afford to resist.</p> This Is a Defining Moment <p>The Carney government is betting we've forgotten our history. They are hoping that the “Elbows Up” moment that we are in will distract us from fighting this battle one more time. However, “Elbows Up” is also the right approach to this authoritarian push by the Liberal government. This is not a done deal, and Canadians know that when we stand together, we win.</p> WHAT YOU CAN DO RIGHT NOW: <p>CONTACT YOUR MEMBER OF PARLIAMENT (MP) IMMEDIATELY. This is the most critical thing you can do. You can <a href="https://www.ourcommons.ca/members/en?ref=webistemology.com">find your MP on the Parliament of Canada website</a>. Call them. Email them. You don't need to be an expert. Just say:</p><blockquote>I am your constituent, and I am calling to demand that you vote NO on Bill C-2. Part 15 of this bill is another attempt to pass 'lawful access' surveillance legislation, just like Bill C-30, which Canadians have already rejected. I do not want the Carney Liberals to pass this bill. It threatens everyone's privacy and digital security in this country.</blockquote><p>MOBILIZE YOUR COMMUNITY. Don’t act alone. Are you a member of a union, a professional association, a faith group, or a community organisation? Inform them that the zombie surveillance bill has resurfaced. Ask your organisation to take a public stand against Bill C-2. The more groups that speak out, the stronger our message becomes.</p><p>SPREAD THE WORD. Share this post. Talk to your friends, family, and colleagues. Use the hashtags #StopBillC2, #StopTheZombieBill, and #ProtectOurPrivacyCA on social media. Remind everyone that we have defeated this threat before and will do it again.</p><p>This is our moment to bury this zombie bill once and for all. Let’s ensure every Ottawa MP knows that Canadians will not stand for secret surveillance. Not now, not ever.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://www.webistemology.com/content/images/2025/06/Gemini_Generated_Image_u2hcqku2hcqku2hc.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:38 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/cursor'>Real-world engineering challenges: building Cursor</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Cursor has grown 100x in load in just a year, sees 1M+ QPS for its data layer, and serves billions of code completions, daily. A deepdive into how it’s built with cofounder, Sualeh Asif
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Cursor is an AI-powered IDE which seems the most loved among engineers. In <a href="https://blog.pragmaticengineer.com/ide-that-software-engineers-love/">a survey</a> we ran last year, Cursor was the most common answer to the question <a href="https://blog.pragmaticengineer.com/ide-that-software-engineers-love/">“what is your favorite editor with GenAI features that help with your work?”</a>.</p><p>Anysphere is the startup behind Cursor, founded in 2022, with the first version of Cursor released two years ago, in March 2023. Last week, Anysphere <a href="https://x.com/cursor_ai/status/1931032123038945530">shared</a> they’d raised a $900M Series C round, which values the company at $9.9B(!). The business has crossed $500M in annual revenue(!!) which might be a record: no other dev tools company I know of hit this milestone within 2 years of launching its first product. It helps that Cursor is used by more than half of the 500 largest tech companies on the Fortune 500.</p><p>Also last week, the company launched <a href="https://www.cursor.com/changelog">Cursor 1.0</a>, a major release: notable additions are AI code review (with a tool called BugBot), background agents, and support for memory (remembering details from past chats).</p><p>I sat down with Cursor cofounder, <a href="https://www.linkedin.com/in/sualeh-a-1a4a97116/">Sualeh Asif</a>, to find out how Cursor works, and how the team builds the tool, and he shared new details of its internals:</p>  <p>Tech stack. TypeScript and Rust, cloud providers, Turbopuffer, Datadog, PagerDuty, and others.</p>  <p>How the autocomplete works. A low-latency sync engine passes encrypted context to the server, which runs inference.</p>  <p>How the Chat works without storing code on the server. Clever use of Merkle trees to avoid storing source code on the server, while being able to search source code using embeddings.</p>  <p>Anyrun: Cursor’s orchestrator service. A Rust service takes care of launching agents in the cloud, securely and with the right process isolation, using Amazon EC2 and AWS Firecracker.</p>  <p>Engineering challenges. Usage patterns dictate technology choices, scaling problems, the cold start problem, sharding challenges, and hard-to-spot outages.</p>  <p>Database migrations out of necessity. How and why Cursor moved from Yugabyte (a database that should scale infinitely), to PostgresSQL. Also, the epic effort of moving to Turbopuffer in hours, during a large indexing outage.</p>  <p>Engineering culture and processes. Releases every 2-4 weeks, unusually conservative feature flagging, a dedicated infra team, an experimentation culture, and an interesting engineering challenge they face.</p>  <p>This episode is part of the <a href="https://newsletter.pragmaticengineer.com/t/real-world-engineering-challenges">Real-world engineering challenges series</a>. Read <a href="https://newsletter.pragmaticengineer.com/t/real-world-engineering-challenges">other, similar deepdives</a>.</p>    Cursor by the numbers <p>Before we jump into the tech stack, let’s start with some context on Cursor in numbers:</p>  <p>50: number of engineers working on Cursor</p>  <p>1M transactions per second, and higher at its peak</p>  <p>100x: growth in users and load in 12 months – doubling month-on-month at times.</p>  <p>100M+: lines of enterprise code written per day with Cursor by enterprise clients, such as NVIDIA, Uber, Stripe, Instacart, Shopify, Ramp, Datadog, and others. Cursor claims more than 50% of the 1,000 largest US companies use its products.</p>  <p>$500M+: annual revenue run rate. This was at $300M <a href="https://www.lennysnewsletter.com/p/the-rise-of-cursor-michael-truell">in early May</a>, and $100M <a href="https://x.com/AnjneyMidha/status/1879306784525222193">in January</a>, after being zero a year prior. Could Cursor be setting a revenue growth record?</p>  <p>A billion: just fewer than this many lines of code are written with Cursor daily by enterprise and non-enterprise users</p>  <p>Hundreds of terabytes: scale of indexes stored in Cursor’s databases. This might seem less impressive than the other figures, but code itself is pretty small in terms of storage, compared to images and video. Also, this is not code itself, but embeddings, as Cursor doesn’t store code in its databases.</p>  <p>Cursor may be catching up with GitHub Copilot in revenue generation: Reuters <a href="https://www.reuters.com/business/ai-vibe-coding-startups-burst-onto-scene-with-sky-high-valuations-2025-06-03/">reports</a> GitHub Copilot likely generated $500M in revenue in 2024. Currently, Cursor is on track to generate the same in 2025, or even more if growth continues at the current pace.</p> 1. Tech stack <p>Some stats about the barely-3-years-old codebase behind Cursor:</p>  <p>25,000 files</p>  <p>7 million lines of code</p>  <p>The editor is a fork of Visual Studio Code, meaning it has the same tech stack as VS Code:</p>  <p>TypeScript: most business logic written in this language</p>  <p>Electron: the framework used by Cursor</p>  <p>When starting the company, they had to decide where to build their editor from scratch, similar to what <a href="https://zed.dev/">Zed</a> did, or to start with a fork. Sualeh explains the decision:</p><blockquote><p>“We needed to own our editor and could not ‘just’ be an extension, because we wanted to change the way people program. This meant we either needed to build a brand new IDE, or fork an existing editor.</p><p>We decided to fork because starting from scratch would have taken a massive effort just to build a stable editor. Our value proposition was not building a stable editor, but changing how devs program, doing it in an incremental way. For example, building the magical ‘tab model’ would have been very difficult without forking, and it was trivial with the fork. Forking let us focus on the experience, not the editor.”</p></blockquote><p>Backend:</p>  <p>TypeScript: most business logic written in this.</p>  <p>Rust: all performance-critical components use this language. The Orchestrator, discussed below, is an example.</p>  <p>Node API to Rust: Most business logic is in TypeScript and the performance-intensive parts are in Rust, so there’s a bridge to invoke the Rust code from TypeScript, via Node.js. One example is invoking the indexing logic (written in Rust) that makes heavy use of this bridge.</p>  <p>Monolith: the backend service is mostly a large monolith, and is deployed as one. This is a reminder that monoliths work pretty well for early-stage startups and can help teams move fast.</p>  <p>Databases:</p>  <p><a href="https://turbopuffer.com/">Turbopuffer</a>: a multi-tenant database used to store encrypted files and the Merkle Tree of workspace, covered below. The team prefers this database for its scalability, and not having to deal with the complexity of <a href="https://en.wikipedia.org/wiki/Shard_(database_architecture)">database sharding</a>, like previously. We cover challenges in “Engineering challenges”, below.</p>  <p><a href="https://pinecone.io/">Pinecone</a>: a vector database storing some embeddings for documentation</p>  <p>Data streaming:</p>  <p><a href="https://www.warpstream.com/">Warpstream</a>: an Apache Kafka compatible data streaming service</p>  <p>Tooling:</p>  <p><a href="https://www.datadoghq.com/">Datadog</a>: for logging and monitoring. Sualeh says they’re heavy users and find the developer experience of Datadog vastly superior to the alternatives</p>  <p><a href="https://www.pagerduty.com/">PagerDuty</a>: for oncall management, integrated with their Slack</p>  <p><a href="https://slack.com/intl/en-gb/">Slack</a>: internal comms and chat</p>  <p><a href="https://sentry.io/">Sentry</a>: error monitoring</p>  <p><a href="https://amplitude.com/">Amplitude</a>: analytics</p>  <p><a href="https://stripe.com/en-nl">Stripe</a>: billing and payments when purchasing a plan with Cursor.</p>  <p><a href="https://workos.com/">WorkOS</a>: authentication when logging into Cursor, such as logging in with GitHub or Google Workspace</p>  <p><a href="https://vercel.com/">Vercel</a>: the platform that the Cusor.com website is hosted on</p>  <p><a href="https://linear.app/">Linear</a>: for managing work</p>  <p>Cursor - naturally! The team uses Cursor to build Cursor. In the end, every engineer is responsible for their own checked-in code, whether they wrote it by hand, or had Cursor generate it.</p>  <p>Model training: Cursor uses several providers to train its own models and finetune existing ones:</p>  <p><a href="https://www.voltagepark.com/">Voltage Park</a></p>  <p><a href="https://www.databricks.com/blog/databricks-mosaicml">Databricks MosaicML</a></p>  <p><a href="https://mlfoundry.com/">Foundry</a></p>   Physical infrastructure <p>All the infrastructure runs on cloud providers. Sualeh says:</p><blockquote><p>“We are very much a ‘cloud shop.’ We mostly rely on AWS and then Azure for inference. We also use several other, newer GPU clouds as well.”</p></blockquote><p>Most of the CPU infra runs on AWS. They also operate tens of thousands of NVIDIA H100 GPUs. A good part of GPUs run within Azure.</p> <a href="https://substackcdn.com/image/fetch/$s_!PFOU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ab72d5b-860d-4271-be74-c64a2c7669d5_1536x847.png">          </a>20 NVIDIA H100 GPUs in a Microsoft Azure data center. Cursor’s compute needs equate to thousands of such racks. Source: <a href="https://www.servethehome.com/nvidia-gtc-2023-keynote-product-announcements/microsoft-azure-nvidia-dgx-h100-installation/">NVIDIA</a> <p>Inference is by far the biggest GPU use case for Cursor, which means generating the next tokens, either as autocomplete, or complete code blocks. In fact, Azure GPUs are solely for inference, not other LLM-related work like fine-tuning and training models.</p><p><a href="https://developer.hashicorp.com/terraform">Terraform</a> is what Cursor uses to manage infrastructure such as GPUs and virtual machines, like EC2 instances.</p> 2. How Cursor’s autocomplete works <p>To understand some of the technical challenges of building Cursor, let’s see what happens when booting up the editor for the first time.</p> <a href="https://substackcdn.com/image/fetch/$s_!GApI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F242912d9-7577-4f8d-85e6-e03fbe6f1b3a_1600x1112.png">          </a>Opening Cursor with a project  Low-latency sync engine: autocomplete suggestions <p>Upon opening a project or folder, you’re likely to jump into editing files. This means Cursor needs to generate autocomplete suggestions, which the Cursor team calls tab suggestions.</p><p>A low-latency sync engine powers the “tab model”. This generates suggestions that are greyed out and can be accepted by hitting the “Tab” key. The suggestions need to be generated quickly in ideally less than a second. Here’s what happens behind the scenes:</p> <a href="https://substackcdn.com/image/fetch/$s_!xkWB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F583f8775-d894-4cb5-aa20-940b8284c864_1558x1348.png">          </a>How Cursor’s “tab suggestions” work <p>How it works:</p>  <p>A small part of the current context window (code) is collected locally by the client</p>  <p>The code is encrypted</p>  <p>The encrypted code / context is sent to the backend</p>  <p>The backend decrypts the code / context</p>  <p>A suggestion is generated using Cursor’s in-house LLM model</p>  <p>The suggestion is sent back</p>  <p>The IDE displays the suggestion. Hitting “Tab” accepts the suggestion</p>  <p>…the process repeats for the next suggestion.</p>  <p>This “tab model” must be as fast as possible, and data transfer as low as possible. There’s always a tradeoff between how much context to send, and the quality of the suggestions: the more relevant context Cursor can send, the better the suggestions. However, sending lots of context can slow down the display of suggestions, so getting this right is one challenge for Cursor’s engineers.</p> 3. How Cursor’s Chat works without storing code on the server <p>Cursor supports a chat mode for asking about the codebase, “chatting” with the codebase, or asking Cursor to do things that will kick off an agent to refactor, add some functionality, modify a method, etc. No source code is stored on the backend, but all LLM operations are done there. The way it manages this is through indexes of the codebase. Here’s how it works:</p><p>Asking a question in Chat mode: let’s take the example of asking about the createTodo() method, which is part of the codebase, defined in server.js. To complicate things, I defined a similar method called addTodo() in index.html, inline. Let’s see how Cursor gets on with this one!</p> <a href="https://substackcdn.com/image/fetch/$s_!vuoc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F343ce691-1072-42b0-b066-e051f767d6a5_958x194.png">   </a>Providing a codebase-related prompt to Cursor <p>The prompt is sent to the Cursor server, where it interprets it, and decides it needs to execute a codebase search:</p> <a href="https://substackcdn.com/image/fetch/$s_!zSwZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89b78ca0-5680-4739-8f16-190320090835_974x180.png">   </a> <p>The search starts:</p> <a href="https://substackcdn.com/image/fetch/$s_!VdY_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89f983a-87d8-4b88-9510-aec3db4ca8e3_966x186.png">   </a> <p>Search is done using codebase indexes. Codebase indexes are previously-created embeddings. It tries to locate the embeddings that are best matches for the context using vector search. In this case, the vector search returned two very close results: in server.js, and index.html.</p><p>Requesting the code from the client: the server does not store any source code, but now requests the source code both from <a href="http://server.js">server.js</a> and index.html, so it can analyze both and decide which is relevant:</p> <a href="https://substackcdn.com/image/fetch/$s_!rl5Z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffad62e93-ffab-4615-b5d9-56b63b7569ae_962x334.png">          </a> <p>Finally, after the vector search and requesting relevant source code from the client, the server has the context it needs to answer the question:</p> <a href="https://substackcdn.com/image/fetch/$s_!_w4w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe263225e-bae9-42a4-a649-b17990c57a1d_950x920.png">          </a> <p>There’s a few things Cursor does behind the scenes to make these kinds of searches work.</p> Semantic indexing code with code chunks <p>To allow vector search using embeddings like in the above case, Cursor first needs to break up the code into smaller chunks, create embeddings, and store these embeddings on the server. Here’s how it does this:</p> <a href="https://substackcdn.com/image/fetch/$s_!Qivw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05e037ae-80cb-48a4-873e-9ddf25de77fe_1318x1322.png">          </a>   <p>Create code chunks. Cursor slices and dices the contents of a file into smaller parts. Each part will be an embedding later.</p>  <p>Create embeddings without storing filenames or code. Cursor doesn’t even want to store the filenames on the server because it can be considered confidential information. Instead, it sends over obfuscated filenames and encrypted code chunks to the server. The server decrypts the code, creates an embedding using <a href="https://platform.openai.com/docs/guides/embeddings">OpenAI’s embedding models</a>, or one of their own, and stores the embedding in their vector database, Turbopuffer.</p>  <p>Creating embeddings is computationally expensive and is one reason it’s done on Cursor’s backend, using GPUs in the cloud. Indexing usually takes less than a minute for mid-sized codebases, and can take minutes or longer for large codebases. You can view the status of indexing inside Cursor, at Cursor Settings → Indexing:</p> <a href="https://substackcdn.com/image/fetch/$s_!cC6X!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe0b36de-c456-45d4-816b-b3784d5f3517_1326x692.png">          </a>  Keeping the index up-to-date using Merkle trees <p>As you edit the codebase with Cursor or another IDE, Cursor’s on-server index becomes out of date. A naive solution would be to run the reindexing operation every few minutes. However, because indexing is expensive in compute – and uses bandwidth by transmitting encrypted code chunks – this is not ideal. Instead, Cursor makes clever use of Merkle trees and a high-latency sync engine (the sync engine runs every 3 minutes) to keep on-server indexes up to date.</p><p>A <a href="https://en.wikipedia.org/wiki/Merkle_tree">Merkle tree</a> is a tree whose every leaf is the cryptographic hash of the underlying file (e.g. the hash for the file main.js). And every node is a combination of the hashes of its children. A Merkle tree of a simple project with four files looks like this:</p> <a href="https://substackcdn.com/image/fetch/$s_!B4Yh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F714edd02-f202-4cbb-a442-60c09d06ad14_1600x889.png">          </a>A Merkle Tree based on code in a codebase <p>How this Merkle Tree works:</p>  <p>Every file gets a hash, based on its contents. The leaves of the tree are files.</p>  <p>Every folder gets a hash, based on the hash of its children.</p>  <p>Cursor uses a very similar Merkle tree to this, except it uses obfuscated file names. The Cursor client creates a Merkle tree based on local files, and the server also creates one based on the files it has finished indexing. This means both the client and server store their respective Merkle trees.</p><p>Every 3 minutes, Cursor does an index sync. To determine which files need re-indexing, it compares the two Merkle trees; the one on the client which is the source of truth, and the one on the server which is the state of the index. Let’s take the example of “index.html” changing on the client-side:</p> <a href="https://substackcdn.com/image/fetch/$s_!1JIC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4eef44d8-4fee-436b-9bc6-d5840e2d18a2_1460x1452.png">          </a>Client and server-side Merkle trees out of sync. Cursor uses obfuscated filenames, actual filenames above are for simplicity <p>Tree traversal is used to locate where re-indexing is needed. Tree traversal is not something us developers implement much, but for this use case, Cursor engineers had to. The Merkle Tree makes tree traversal efficient because starting from the root node, it’s easy enough to tell if the hashes match. Where there are differences in the hashes, it’s also easy enough to find files that need to be synced. Just as importantly, the Merkle tree minimizes sync operations to only files that have changed.</p><p>This Merkle tree structure fits nicely into Cursor’s real-world usage. For example, it’s common enough to shut down your computer at the end of the day, and then start the next day by fetching updates from the git repo. In a team, it’s common enough for a bunch of files to change by the next morning. With this Merkle tree, Cursor does as little re-indexing as possible, saving time on the client side and using compute as efficiently as possible on its server side.</p> Secure indexing <p>Even though Cursor doesn’t store code on the server-side, there are sensitive parts of a codebase that are a bad idea to send over, even when encrypted. Sensitive data includes secrets, API keys, and passwords.</p><p>Using .gitignore and .cursorignore is the best way to keep indexing secure. Secrets, API keys, passwords, and other sensitive information, should not be uploaded to source control, and are usually stored as local variables, or in local environment files (.env files) that are added to the .gitgnore. Cursor respects .gitignore and will not index files listed there, nor will it send contents of those files to the server. Additionally, it offers a .cursorignore file where files to be ignored by Cursor should be added.</p><p>Before uploading chunks for indexing, Cursor also scans code chunks for possible secrets or sensitive data, and does not send them.</p> Indexing very large codebases <p>For massive codebases – often monorepos with tens of millions of lines of code – indexing the whole codebase is extremely time consuming, uses a lot of Cursor’s compute resources, and is generally unnecessary. Using the .cursorignore file is the sensible approach at this point. The documentation <a href="https://docs.cursor.com/context/codebase-indexing#working-with-large-monorepos">offers more guidance</a>.</p> 4. Anyrun: Cursor’s orchestrator service <p>Anyrun is the name of Cursor’s orchestrator component, and is written fully in Rust. Fun fact: “Anyrun” is a nod to Cursor’s company name, Anysphere. Anyrun is responsible for this:</p>
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/cursor">
              Read more
          </a>
      </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!PFOU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ab72d5b-860d-4271-be74-c64a2c7669d5_1536x847.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:01 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2025/06/10/experimental-alternative-flow-for-oauth-first-party-applications/'>Experimental alternative flow for OAuth First-Party Applications</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This post looks at an alternative way of implementing a native app authentication and authorization. At present, a web browser is used to implement authentication of native applications when using OAuth and OpenID Connect. The alternative approach implemented in the post is based on the OAuth 2.0 for First-Party Applications draft and adapted to be […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This post looks at an alternative way of implementing a native app authentication and authorization. At present, a web browser is used to implement authentication of native applications when using OAuth and OpenID Connect. The alternative approach implemented in the post is based on the <a href="https://github.com/oauth-wg/oauth-first-party-apps">OAuth 2.0 for First-Party Applications</a> draft and adapted to be focused on the device/application. The aim is to focus on having an authenticated device and onboard to user to the device without gaps in the secure channels. Forcing a phishing resistant authentication can be a native implementation on the device and is responsibility of the business.</p>



<p>A working and experimental PoC can be found here:</p>



<p>Code: <a href="https://github.com/damienbod/OAuthClientAssertionsPerInstance">https://github.com/damienbod/OAuthClientAssertionsPerInstance</a></p>



<p>The example uses <a href="https://duendesoftware.com/">Duende</a> IdentityServer to implement the flow and a console application is used to implement the native application. </p>



 Description 



<p>Principal idea: The application on the device could be authenticated first and the user can be attached to the application.</p>



<p>The device would use private/public keys and the server would create and attach an “auth_session” for the “public_key”. An adaption to the client assertion would be required. The OAuth client credentials flow using client assertions to request the token can be used from this point onwards. All access tokens would include the auth_session and further authorization can be implemented using this.</p>



<p>Once the device can request an access token, the user properties can be attached to this secure instance as required. The business application can decide the level of user authorization after this.</p>



 Advantages (compared to actual draft) 



 
 Application/Device is authenticated first and is unique. 



 Application/Device binding of the auth_session value ensures that the context referenced by the auth_session cannot be stolen and reused by another device. 



 User onboarding can be implemented in an application specific way and use an existing OAuth standard to onboard. No security changes are required for the endpoint. (OAuth client credentials token with auth_session claim in token.) 



 The device is responsible for user onboarding and the results can be attached to the server device session as required in the business. 



 No refresh tokens are required, but can be used. 



 The user recovery process can be implemented in a business specific way. Not part of the security specifications. 
 



 Disadvantages 



 
 Client assertion needs to be adapted on existing implementations. 



 auth_session included in the application access token must be attached to the client assertion request. No gaps. 



 Initial session setup endpoint is unsecure and open to DDoS attacks 
 



<p>The flow could look like this:</p>







<p>Figure: First-Party Client Authorization Device Request</p>



 
 (A) The first-party client starts application for the first time and creates an asymmetric private, public key pair. The client initiates the authorization request by making a POST request to the Device Registration Endpoint using the public key. 



 (B) The Authorization Server determines whether the information provided to the Device Registration Endpoint is sufficient. The server creates an ‘auth_session’ for the public key and returns the ‘auth_session’ in the response. 



 (C) The Device requests an access token with a client assertion and OAuth client credentials created using the private key. The ‘auth_session’ is added to the client assertion using the ‘device_auth_session’ claim. The public key attached to the auth_session is used to validate the client assertion. Optional, DPoP is used to request the token. DPoP does not use the same private, public key pair. 



 (D) The Authorization Server returns an access token from the Token Endpoint. The ‘auth_session’ is returned in the access token. 



 (E) The Authorization Challenge Endpoint is used to attach user authentication properties to the device and the auth_session. The Authorization Server authorises the access token using standard OAuth requirements, including DPoP. The auth_session claim is used to authorize specifics for the user. 



 (F) .. 



 (G) repeat for n-user properties 



 (H) .. 
 



<p></p>



 Flow details 



<p>The flow details can be found here:</p>



<p><a href="https://github.com/damienbod/OAuthClientAssertionsPerInstance/blob/main/OAuth_first_party_adapted_draft.md">https://github.com/damienbod/OAuthClientAssertionsPerInstance/blob/main/OAuth_first_party_adapted_draft.md</a></p>



<p></p>



 Links<a href="https://github.com/damienbod/OAuthClientAssertionsPerInstance#links"></a> 



<p><a href="https://docs.duendesoftware.com/identityserver/v7/tokens/authentication/jwt/">https://docs.duendesoftware.com/identityserver/v7/tokens/authentication/jwt/</a></p>



<p><a href="https://docs.duendesoftware.com/identityserver/v7/reference/validators/custom_token_request_validator/">https://docs.duendesoftware.com/identityserver/v7/reference/validators/custom_token_request_validator/</a></p>



<p><a href="https://docs.duendesoftware.com/identityserver/v7/tokens/authentication/jwt/">https://docs.duendesoftware.com/identityserver/v7/tokens/authentication/jwt/</a></p>



<p><a href="https://docs.duendesoftware.com/foss/accesstokenmanagement/advanced/client_assertions/">https://docs.duendesoftware.com/foss/accesstokenmanagement/advanced/client_assertions/</a></p>



<p><a href="https://www.scottbrady.io/oauth/removing-shared-secrets-for-oauth-client-authentication">https://www.scottbrady.io/oauth/removing-shared-secrets-for-oauth-client-authentication</a></p>



 Specs<a href="https://github.com/damienbod/OAuthClientAssertionsPerInstance#specs"></a> 



<p><a href="https://www.rfc-editor.org/rfc/rfc7636">https://www.rfc-editor.org/rfc/rfc7636</a></p>



<p><a href="https://datatracker.ietf.org/doc/draft-ietf-oauth-first-party-apps">https://datatracker.ietf.org/doc/draft-ietf-oauth-first-party-apps</a></p>



<p><a href="https://github.com/oauth-wg/oauth-first-party-apps">https://github.com/oauth-wg/oauth-first-party-apps</a></p>



<p><a href="https://github.com/oauth-wg/oauth-first-party-apps/blob/main/draft-ietf-oauth-first-party-apps.md">https://github.com/oauth-wg/oauth-first-party-apps/blob/main/draft-ietf-oauth-first-party-apps.md</a></p>



<p><a href="https://datatracker.ietf.org/doc/html/rfc9449">https://datatracker.ietf.org/doc/html/rfc9449</a></p>



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2025/06/flow-native-app.png?w=640'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   04:11 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 09. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.webistemology.com/unleashing-corporate-greed-not-ontarios-potential/'>Unleashing Corporate Greed, Not Ontario's Potential</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Doug Ford's Bill 5, deceptively titled the "Protecting Ontario by Unleashing Our Economy Act, 2025" is a direct assault on our environment, on Indigenous rights, on workers' protections, and on local democracy.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<blockquote>Originally published on June 9th, 2025 on <a href="https://privacycdn.substack.com/p/unleashing-corporate-greed-not-ontarios?ref=webistemology.com">Substack</a>.</blockquote><blockquote>Ford's Bill 5 is yet another stalking horse for unfettered crony capitalism.</blockquote><p>Doug Ford's Bill 5, deceptively titled the "<a href="https://www.ola.org/en/legislative-business/bills/parliament-44/session-1/bill-5?ref=webistemology.com">Protecting Ontario by Unleashing Our Economy Act, 2025</a>" is a direct assault on our environment, on Indigenous rights, on workers' protections, and on local democracy. This legislation is not a path to prosperity but a dangerous blueprint for a corporate free-for-all. Neither is Bill 5 likely to pass the test as a “nation-building project”. The Canadian Centre for Policy Alternatives has already suggested “<a href="https://www.policyalternatives.ca/news-research/ten-trump-proof-nation-building-projects-for-a-strong-independent-canada/?ref=webistemology.com">Ten Trump proof nation building projects"</a> worth considering ahead of Bill 5.</p><p>At the heart of Bill 5 is the power for Ford's cabinet to declare "Special Economic Zones" (SEZs) anywhere in Ontario. Within these zones, the government can waive provincial laws, regulations, and municipal bylaws for specific companies or projects. The term "Special Economic Zone" itself isn't new; the first SEZs appeared in the late 1950s in industrialised nations like Ireland, designed to attract foreign investment, often through tax breaks and looser regulations. China later used SEZs extensively to draw in foreign capital. However, what Ford proposes isn't about thoughtful economic development; it's about creating sacrifice zones where corporate interests take precedence over everything else.</p> A Slap in the Face to Indigenous Rights and Reconciliation <p>Ford has already signalled his intent to designate the Ring of Fire as Ontario's first SEZ. This region, rich in minerals but ecologically vital and the heart of Treaty 9 territory, cannot become a testing ground for overriding Indigenous rights. Many First Nations leaders have rightly decried Bill 5 as violating their treaty rights. The bill was rammed through without the Free, Prior, and Informed Consent (FPIC) of affected First Nations – a core principle of the UN Declaration on the Rights of Indigenous Peoples (UNDRIP), which Canada has endorsed. Consulting after the power to create these zones is already law is not consultation; it's a sham. We must stand in solidarity with First Nations, such as Neskantaga and Attawapiskat, and others who courageously oppose this colonial approach and demand their right to self-determination and meaningful consent regarding any development in their territories.</p> Fueling the Climate Crisis, Not a Green Transition <p>The Ring of Fire isn't just a mining prospect; it's part of the Hudson Bay Lowlands, home to vast peatlands – the second-largest continuous expanse of northern peatland on Earth. These peatlands are colossal carbon sinks, storing an estimated 150 billion tonnes of carbon in Canada alone. Disturbing them for mining, facilitated by SEZ exemptions, would release enormous amounts of greenhouse gases, accelerating the climate emergency. It isn't a "green" transition if we destroy critical ecosystems and carbon stores to get so-called "critical minerals." Furthermore, Bill 5 undermines the Endangered Species Act, allowing politicians to disregard scientific advice, and weakens environmental assessment processes across the province. This is a direct contradiction to any serious climate action.</p> Workers and Communities: Sacrificed at the Altar of Deregulation <p>While Ford's officials offer verbal assurances that labour laws won't be touched in SEZs, the legislation itself is "wide-open," allowing exemptions from any provincial law. This is a recipe for eroding hard-won workers' rights, safety standards, and fair wages. It's an invitation for a race to the bottom. Remember Bill 2 and the assault on nurses and health care workers? Moreover, by allowing the province to override municipal bylaws, Bill 5 silences local communities and democratically elected councils, opening the door to "shady backroom deals."</p> A Better Path Forward: Resistance and a Just Alternative <p>Unless you are a full-throated climate change denier and corporate shill, the following should be top of mind:</p>  Demanding the Full Repeal of Bill 5: This legislation is fundamentally flawed and poses a significant danger.  Upholding Indigenous Sovereignty: Insist on fully implementing UNDRIP, including FPIC, and respecting Treaty Rights in all decisions—no development without consent.  Champion Real Climate Action: Protect our vital carbon sinks, such as peatlands. Invest in a truly just transition rooted in renewable energy and Green New Deal principles, not destructive extraction.  Defend Workers and Local Democracy: Fight for ironclad protections for labour rights and ensure communities have the final say on what happens in their backyards.  <p>The fight against Bill 5 is a fight for Ontario's soul. It’s about choosing a future where people and planet come before corporate profit. Let's organise, mobilise, and make our voices heard. Our collective future depends on it.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:31 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 06. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@rufftimo'>Timothy Ruff</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://rufftimo.medium.com/nophonehome-is-working-but-dont-stop-now-01896d171c88?source=rss-11906019170a------2'>NoPhoneHome is Working, But Don’t Stop Now…</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   On Monday, June 2, 2025, we* launched NoPhoneHome.com with about 75 signers opposed to “phone home” (surveillance) capabilities within digital identity systems, including the ACLU, EFF, EPIC, CDT, Bruce Schneier, Dr. Jan Camenisch, Dr. Anna Lysyanskaya, and more. Now there’s&nbsp;150+.  More importantly, it is beginning to have the desired&nbsp;effect:  Prior to June 2, Utah got the ball rolling w
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*Jc9lJQRiDOniX7oV7HCZPw.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>On Monday, June 2, 2025, we* launched NoPhoneHome.com with about 75 signers opposed to “phone home” (surveillance) capabilities within digital identity systems, including the ACLU, EFF, EPIC, CDT, Bruce Schneier, Dr. Jan Camenisch, Dr. Anna Lysyanskaya, and more. Now there’s&nbsp;150+.</p><p>More importantly, it is beginning to have the desired&nbsp;effect:</p><p>Prior to June 2, Utah got the ball rolling with <a href="https://le.utah.gov/~2025/bills/static/SB0260.html">SB260</a>, which prohibits surveillance within a state-endorsed digital identity system. Utah then turned off “Server Retrieval” (phone home) mode for the 100,000+ mDL (mobile driver’s license) holders in the state and just held its inaugural <a href="https://www.uvu.edu/herbertinstitute/events/2025-events/utah-data-governance-summit.html">Data Governance Summit</a>, strengthening data privacy practices statewide, attended by 700+. The governor spoke, <a href="https://www.linkedin.com/posts/rufftim_you-should-own-your-personal-information-activity-7336871724954591233-DNTK?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAB6Wy4B5ldJqF6Up4oNkg8VKx65JmTo0aA">singing the praises of data privacy and decentralized identity</a>, followed by the Senate and House majority leaders, who did the same. (Utah is rapidly becoming a hotbed for decentralized tech and bold new&nbsp;policy.)</p><p>Now, because of the #NoPhoneHome campaign, we’re learning that other states are discovering that phone home capability is latently present within digital identity systems they’ve implemented or are considering, and they’re justifiably alarmed.</p><p>Key facts to remember:</p><p>1. To their enormous credit, <a href="https://www.aamva.org/assets/best-practices,-guides,-standards,-manuals,-whitepapers/mobile-driver-s-license-implementation-guidelines-1-2">AAMVA has recently banned Server Retrieval mode</a> nationally in the&nbsp;U.S.;</p><p>2. Server Retrieval is “Recommended” within the <a href="https://www.iso.org/standard/69084.html">ISO 18013–5</a> mDL standard, but not required (we originally thought it was required, so this is a positive correction);</p><p>3. Do not trust “privacy by policy”: if phone home capability is activate-able within an identity system, it’s only one ‘emergency’ away from activation… phone home capability should be removed entirely, and no standard where phone home is conformant (such as ISO 18013–5 or OpenID Connect) should be implemented, with the exception of #5&nbsp;below;</p><p>4. The mDL has dominated the discussion since June 2 but it is not the only phone home offender, by a long shot: anything that uses OpenID Connect (OIDC) or almost any other federated identity protocol (OAuth, SAML, etc.) phones home for *every verification, every time*… watch for that discussion to increase in attention and&nbsp;fervor;</p><p>5. Phone home is harmless when used *within* an organization’s trust domain, but when it crosses domain boundaries — like when a citizen uses their state-issued ID to login to a non-state website — it enables Orwellian surveillance;</p><p>6. Phone home enables not only surveillance, it enables control — with phone home an authority can deny permission to proceed, effectively controlling identity-enabled online activities;</p><p>7. Some countries (i.e., Estonia, India, Singapore) have digital identity systems that phone home, and they seem OK with that… our concern is with societies that generally oppose tracking and surveillance and do not realize they may be getting surveillance in latent&nbsp;form.</p><p>Exposing phone home capabilities is already having a significant and growing impact, so if you live in a society that generally opposes surveillance, now is the time to raise the alarm with privacy advocates and policy makers however you can, while there is momentum… Because once phone home is implemented, and authorities get a taste of its immense power over citizens, it will be difficult to ever&nbsp;remove.</p><p>Now is the time. Please keep spreading the #NoPhoneHome message to all who might need to hear it. We may not get a second&nbsp;chance.</p><p>*We includes <a href="https://www.linkedin.com/in/rufftim/">me</a>, <a href="https://www.linkedin.com/in/kimdhamilton/">Kim Hamilton Duffy</a>, <a href="https://www.linkedin.com/in/jay-c-stanley/">Jay Stanley</a>, <a href="https://www.linkedin.com/in/mccown/">Steve McCown</a>, and <a href="https://www.linkedin.com/in/joe-andrieu-a0528/">Joe&nbsp;Andrieu</a>.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*Jc9lJQRiDOniX7oV7HCZPw.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:27 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 05. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2721'>WGLC for JOSE and COSE HPKE Specifications</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Hybrid Public Key Encryption (HPKE) was standardized by RFC 9180 in February 2022. It is “hybrid” in the sense that it combines public key cryptographic operations to establish a symmetric key with symmetric cryptographic algorithms using the established key to do the content encryption. It has its own set of registries where Key Encapsulation Mechanisms […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Hybrid Public Key Encryption (HPKE) was standardized by <a href="https://www.rfc-editor.org/rfc/rfc9180.html">RFC 9180</a> in February 2022.  It is “hybrid” in the sense that it combines public key cryptographic operations to establish a symmetric key with symmetric cryptographic algorithms using the established key to do the content encryption.  It has its <a href="https://www.iana.org/assignments/hpke/">own set of registries</a> where Key Encapsulation Mechanisms (KEMs), Key Derivation Functions (KDFs), and Authenticated Encryption with Associated Data (AEAD) algorithms used with HPKE are registered.  The KEMs registered include post-quantum KEMs.</p>
<p>There’s been a multi-year effort to bring HPKE encryption to applications using <a href="https://www.rfc-editor.org/rfc/rfc7516.html">JSON Web Encryption (JWE)</a> and <a href="https://www.rfc-editor.org/rfc/rfc9052.html">COSE encryption</a>.  As has been done by other protocols using HPKE, such as <a href="https://www.rfc-editor.org/rfc/rfc9420.html">MLS</a>, both the JOSE and COSE HPKE specifications made choices about which cryptographic operations make sense together in the specification’s context, as well as which HPKE features to use.  Making those choices within the working groups is part of what made these specifications take a while.  There’s also been a deliberate effort to keep the specifications aligned where it made sense.</p>
<p>The good news is that both the JOSE and COSE HPKE specifications have matured to the point where Working Group Last Call (WGLC) has started for them.  The two WGLCs are intentionally running concurrently because the drafts are closely related and their functionality is intended to be aligned.  They run until Friday, June 20, 2025.</p>
<p>Please participate in the WGLCs on either the jose@ietf.org or cose@ietf.org mailing lists, respectively.  The messages to reply to are:</p>
 
 <a href="https://mailarchive.ietf.org/arch/msg/jose/Oj8c3wqoOvI0j9u5zH8MF_LIEsU/">[jose] WGLC for draft-ietf-jose-hpke-encrypt-08</a> 
 <a href="https://mailarchive.ietf.org/arch/msg/cose/abU46avuzta6ulSYy0ETvOHPYoQ/">[COSE] WGLC for draft-ietf-cose-hpke-13</a> 
 
<p>The specifications entering WGLC together are:</p>
 
 <a href="https://www.ietf.org/archive/id/draft-ietf-jose-hpke-encrypt-08.html">draft-ietf-jose-hpke-encrypt-08</a> 
 <a href="https://www.ietf.org/archive/id/draft-ietf-cose-hpke-13.html">draft-ietf-cose-hpke-13</a> 
 
<p>Finally, I’ll note that a new IETF <a href="https://datatracker.ietf.org/group/hpke/about/">HPKE working group</a> has recently been formed to make updates to the HPKE specification.  Among the chartered updates are adding post-quantum KEMs and hybrid combined KEMs.</p>
<p>Thanks to all in both working groups who helped us reach this point!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:22 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://newsletter.pragmaticengineer.com/'>The Pragmatic Engineer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://newsletter.pragmaticengineer.com/p/the-pulse-136-cloudflare-builds-oauth'>The Pulse #136: Cloudflare builds OAuth framework mostly with Claude</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Also: new trend of higher base salaries for AI engineers than software engineers, Morgan Stanley shows AI is helpful for rewriting legacy codebases, and more.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Pulse is <a href="https://newsletter.pragmaticengineer.com/s/the-pulse">a series</a> covering insights, patterns, and trends within Big Tech and startups. Notice an interesting event or trend? <a href="https://blog.pragmaticengineer.com/contact/">Send me a message</a>.</p><p>Today, we cover:</p>  <p>AI-assisted coding in practice: Cloudflare and OAuth. A software engineer at Cloudflare built a production-grade OAuth library in five days, making very heavy use of Claude. The full commit history, alongside prompts with Claude are published. This approach sped up development by 2-5x, but feels like an outlier for AI-assisted coding.</p>  <p>Industry pulse. Meta trialing onsite interviews, security risks of AI agents accessing the web, Apple Tax is history for US web payments, Reddit sues Anthropic, which cuts model access for Windsurf, AI startup Builder.ai faked its revenue, and more.</p>  <p>New trend: higher base salaries for AI engineers. Top US AI startups are offering $300-500K base salaries for AI engineers, and €235-355K in the EU. This is pulling up the AI/ML engineering market with salaries that exceed what Senior Director of Engineering roles used to command.</p>  <p>Morgan Stanley and COBOL: can AI speed up the rewriting of legacy code? The answer seems “yes”, based on Morgan Stanley building its own tool on top of ChatGPT. LLMs interpreting and attempting to explain legacy codebases could be a practical use case for these tools.</p>   1. AI-assisted coding in practice: Cloudflare and OAuth <p>Cloudflare has done something interesting: generating a TypeScript library by AI that implements the provider side of the OAuth 2.1 authentication protocol, for use by Cloudflare staff. <a href="https://en.wikipedia.org/wiki/OAuth">O</a><a href="https://en.wikipedia.org/wiki/OAuth">Auth</a> allows third-party applications to obtain limited access to user accounts. OAuth is commonly used for the “Login with Google/GitHub” functionality.”</p><p><a href="https://github.com/kentonv">Kenton Varda</a>, tech lead at Cloudflare used Claude to generate much of the code, and published the <a href="https://github.com/cloudflare/workers-oauth-provider/commits/main/?after=a6e3e06c2642e0fd4c185374753201cffc21ce8a+104">full prompt history</a>, as well as <a href="https://github.com/cloudflare/workers-oauth-provider/">the code itself</a> for all to view.</p>
      <p>
          <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-136-cloudflare-builds-oauth">
              Read more
          </a>
      </p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:39 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 03. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2715'>OpenID Federation draft 43 Incorporating Feedback from Interop Event</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Draft 43 of the OpenID Federation specification has been published. A number of features in draft 42 were discussed during the recent OpenID Federation interop event and the changes made in draft 43 are largely a result of conclusions reached there and resulting discussions that followed. Before the interop, there were 40 open issues. As […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://openid.net/specs/openid-federation-1_0-43.html">Draft 43 of the OpenID Federation specification</a> has been published.  A number of features in <a href="https://openid.net/specs/openid-federation-1_0-42.html">draft 42</a> were discussed during the recent <a href="https://self-issued.info/?p=2697">OpenID Federation interop event</a> and the changes made in draft 43 are largely a result of conclusions reached there and resulting discussions that followed.</p>
<p>Before the interop, there were 40 open issues.  As a result of the progress made at SUNET, and the ongoing engagement of interop participants since then, we’re now down to 17 <a href="https://github.com/openid/federation/issues">open issues</a>.  And 9 of those propose extension specifications, post-final work, or reviewing the text.</p>
<p>The changes made in -43 are detailed in the <a href="https://openid.net/specs/openid-federation-1_0-43.html#name-document-history">Document History</a> section.</p>
<p>Thanks all for the significant progress towards finishing the specification!</p>
   
  
    
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/openid-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:41 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/lets-stop-phoning-home'>Let's Stop Phoning Home</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  When you're the parent of a teenager out late at night, the prospect of them phoning home might seem reassuring.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 <a href="https://substackcdn.com/image/fetch/$s_!DqRi!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc34821a-0ab1-44e3-9bda-74230924b42e_1536x1024.heic">          </a> <p>When you're the parent of a teenager out late at night, the prospect of them phoning home might seem reassuring. But that same action—to check in, to report back—is also the dream of every government that wants to monitor its citizens and every company seeking to surveil its customers.</p><p>This concern sits at the heart of the <a href="https://nophonehome.com">No Phone Home movement</a>, which advocates for digital identity systems that don't phone home—that is, digital credentials that do not silently report back to their issuers or some central authority every time they're used. While this kind of telemetry can be marketed as a security or interoperability feature, in reality, it opens the door to a kind of invisible surveillance infrastructure that undermines privacy and individual freedom.</p><p>I've added my name as a signatory to the No Phone Home campaign, joining a broad coalition of organizations and individuals who believe that digital identity should serve people, not institutions. The signatories include respected organizations like the ACLU, the EFF, and Brave Software, as well as numerous experts with deep experience in digital identity, cryptography, and privacy advocacy.</p> Enabling Surveillance...and Control <p>The phrase "phone home" might conjure nostalgic images of a homesick alien, but in the context of digital credentials, it's far more sinister. When a credential—like a mobile driver's license or digital vaccine certificate—relies on contacting a central authority each time it's presented, it creates a record of where and how it was used. Even if that data isn't stored today, the potential exists. That built-in capacity for surveillance is what the No Phone Home campaign seeks to dismantle.</p><p>What's more, the very architecture of phone-home systems inherently concentrates power. It privileges the issuer over the holder, undermining the principles of user control and consent. It's not hard to imagine a world where access to services—buying a train ticket, checking into a hotel, entering a public building—depends on real-time authorization or permission from a government server or corporate backend.</p><p>Shoshana Zuboff, in <a href="https://www.windley.com/archives/2021/05/can_the_digital_future_be_our_home.shtml">The Age of Surveillance Capitalism</a>, lays bare the business model that feeds off this architecture. Her thesis is chilling: surveillance is no longer a byproduct of digital services—it is the product. As she puts it, "Surveillance capitalism unilaterally claims human experience as free raw material for translation into behavioral data." In this world, "phoning home" isn't a safety feature—it's the toll you pay for participation.</p><p>Against that backdrop, the No Phone Home movement demands digital identity architectures where credentials are presented to verifiers without any need to check back with the issuer. This model aligns with the principles of self-sovereign identity and decentralization. It shifts the balance of power, placing control squarely in the hands of the individual.</p> Systems that Phone Home <p>Many digital identity systems are designed to contact a central server—typically the issuer or identity provider—whenever an identity credential is presented. This is especially true in federated identity systems, where verifying a token often means checking with the original source. OAuth and OpenID Connect, for example, explicitly redirect the user to the identity provider (IdP) as part of the authentication process. SAML can be more opaque, performing these validations through backend calls that may not be obvious to the user. In all these cases, the result is the same: the issuer is aware of the credential's use, creating a trail of user activity that can be observed, logged, and potentially acted upon.</p><p>Some verifiable credential systems can operate similarly, enabling the issuer to learn where and when credentials are used. OpenID for Verifiable Credential Issuance (OpenID4VC), for example, inherits these patterns from OpenID and can allow for issuer visibility into credential presentations. But this is a design choice, not a necessity. For example, the verifiable credential presentation protocol in <a href="https://github.com/hyperledger/anoncreds-spec">Anoncreds</a> is designed to avoid these pitfalls, enabling credential verification and even revocation checks without contacting the issuer—preserving privacy without sacrificing trust.</p><p>Mobile driver's licenses (mDLs) exemplify how this can go wrong. They feel like physical IDs—familiar, simple, and discreet—but unlike handing over a plastic card, an mDL may rely on server retrieval to validate the credential in real time. This means that governments could know when and where you use your license, and in some implementations, could even grant or deny permission for its use. The result is a powerful mechanism for surveillance, packaged in the form of a seemingly benign, everyday artifact.</p><p>The American Association of Motor Vehicle Administrators (AAMVA) has acknowledged the privacy concerns associated with server retrieval mode in mDLs. In their December 2024 Implementation Guidelines (version 1.4), they warned about the tracking potential of this mode. Subsequently, in version 1.5, they prohibited the practice. But, as Timothy Ruff argues in <a href="https://rufftimo.medium.com/phone-home-is-bad-really-bad-eccbbf89fc6e">Phone Home is Bad. Really Bad,</a> many systems still support it, and the prohibition is simply a policy choice that could be reversed.</p><p>The usual justification for "phoning home" is the need to verify that a credential is still valid or hasn't been revoked. But this function doesn't require building surveillance into the architecture. Cryptographic techniques like revocation registries, signed timestamps, and status lists enable real-time verification without ever contacting the issuer. These methods let verifiers check credential status in a privacy-preserving way, ensuring both trust and autonomy. In fact, this is not just possible, it's already being done. Many projects in the self-sovereign identity space routinely demonstrate how to maintain trust without compromising privacy.</p><p>These "phone home" systems risk turning identity into an instrument of control. By embedding surveillance into the plumbing of digital trust, they invert the foundational goal of identity systems: to empower the individual.</p> Build the Future You Want to Live In <p>The choice to build digital identity systems that don't phone home is ultimately a choice about the kind of society we want to live in. Do we want a world where every credential presentation creates a record, where silent connections to central servers allow for invisible oversight, and where the potential for control is built into the foundation of everyday interactions?</p><p>The No Phone Home campaign isn't just about technical standards—it's about civic architecture. It asks us to reject the logic of surveillance and embrace designs that respect human dignity. As our daily lives increasingly rely on digital intermediaries, we have a narrow window to get this right.</p><p>By insisting on architectures that protect privacy by design—not just by policy—we build a future where technology empowers rather than controls. That's a future worth fighting for.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!DqRi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc34821a-0ab1-44e3-9bda-74230924b42e_1536x1024.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:09 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 02. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://reb00ted.org/'>reb00ted</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://reb00ted.org/tech/20250602-orb-tools-for-humanity-in-5-min/'>The Orb / Tools for humanity / Worldcoin in 5 min</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   
 
  
 
 
 “The Orb” is popping up in all sorts of places, most recently in
Time magazine’s “The Orb Will See You Now”.
It’s being heralded as everything from the one and final solution for privacy-protecting
digital identity online, to a civil liberties nightmare, to an indispensable tool for
preventing an AI takeover, to a quick way to make $42, and more. 
 But what it actually does and how it w
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://reb00ted.org/images/posts/20250602/world-human-ai-altman-02.webp'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   
 <a href="https://time.com/7288387/sam-altman-orb-tools-for-humanity/">
  
 </a>
 
<p>“The Orb” is popping up in all sorts of places, most recently in
<a href="https://time.com/7288387/sam-altman-orb-tools-for-humanity/">Time magazine’s “The Orb Will See You Now”</a>.
It’s being heralded as everything from the one and final solution for privacy-protecting
digital identity online, to a civil liberties nightmare, to an indispensable tool for
preventing an AI takeover, to a quick way to make $42, and more.</p>
<p>But what it actually does and how it works is remarkably hard to find out. In a
<a href="https://open.spotify.com/episode/27n5EhUUFnDkxKn0MXm6aB?si=82gAJ1BGTN-ZPg13LMeNOQ&amp;nd=1&amp;dlsi=f2f457f1ac6943bd">recent podcast</a>,
the team behind the project took more than two hours to explain it, and that was only the highlights
with no detail. No wonder theories are all over the place.</p>
<p>I listened to that podcast, and I think I can explain it in 5 min. Read on, and we’ll start
at the place where the value occurs: the Relying Party.</p>
<p>“Relying Party” is a geek term for a website or app into which you want to log in. So:
all the value of the Orb and the app and networks and all of that is because users can
log into websites or apps with this system (which doesn’t even have a good name. It’s not
the Orb, which in the grand scheme of things is not important. It’s not the World Coin.
Maybe it’s the <a href="https://en.wikipedia.org/wiki/World_(blockchain)">World Network</a>, but
that name is so bland to be almost useless. I’m going to call it WN for this post.)</p>
<p>So you can log into a website or app with WN. That feature is of course not unique. You can log into
websites or apps with other things, like usernames and passwords, or your Google or Facebook
account or <a href="https://openid.org/">OpenID</a>. But unlike other ways of logging in, if you
log into a website or app with WN, the website or app is being guaranteed by all the tech
that WN throws at it that a user who already has an account on the website or app they
want to log in, cannot make a second account for the same website or app.</p>
<p>Have multiple Reddit accounts? Multiple Gmail addresses? Or multiple Amazon accounts?
If those sites were to implement WN, you could not do that. Instead, those sites could be
fairly certain that any new account created on their site was 1) created by a human and
2) each account was created by a different human. (Actually, the site can let you create
multiple accounts, but only if it knows that all of them belong to you. You cannot
create separate accounts pretending to be two distinct humans. So I lied, you could have
multiple Reddit accounts, but you could not hide that fact from Reddit.)</p>
<p>I don’t want to talk about here why this may be a good or a bad idea. Only how this is done.</p>
<p>Secondly, if you log into two separate sites with WN, the two sites cannot compare notes and track you
from one site to the other. Unlike with, say, logging in by e-mail address, the two sites
cannot simply compare the e-mail addresses you gave them, and say, Oh, it’s the same Joe!
Let’s show him underware ads on this site too, because he browsed underwear on the first site.
That is impossible because he two sites are being given different identifiers.
(Those seem to be public keys, so there is a different key pair per site.) This is nice
from a privacy perspective, similar to what Kim Cameron popularized twenty years ago with
<a href="https://www.identityblog.com/stories/2005/05/13/TheLawsOfIdentity.pdf">directed identity</a>.</p>
<p>The site-specific key pairs are being generated from the single most important piece of data
in the system, which is your master public key, stored on a blockchain. This public
blockchain acts as a registry for all the identities in the system, i.e. for all humans
registered with WN – but before your freak out, it’s not as
bad as it sounds, because all that the blockchain stores is a public key. There is no
other information on that blockchain or elsewhere associated with that key, such
as your name or blood type. It’s just a long basically random number.</p>
<p>So: the blockchain lists all the public keys of the people whose identity can be asserted
by WN, and the technical machinery can derive relying-party-specific keypairs from any
of those so that the relying party can be certain the user has been verified, but cannot
tell which of the public key on the block chain, or which other keys used with other sites
belong to the same user.</p>
<p>How does the key get onto that block chain? It appears there is a small set of trusted
actors that have the credentials to put keys onto that blockchain, and that small set of
trusted actors are the Orb stations WN has been setting up all over the world to get people
registered. The actual keys being registered are generated on your mobile device, and
only the public key is given to the system, the private one remains on your device only.</p>
<p>So the Orb only exists to make a decision whether or not a public key you’ve created on
your mobile device may or may not be added to the registry of verified identities on
that blockchain. That’s all. The key decision the Orb (and all of its Orb siblings in
other locations) need to make is: have any of us registered that human before? If yes, do
not add their new key to the blockchain. If no, let’s add it.</p>
<p>To determine whether or not WN has registered you before, the Orb (really just a fancy
camera) takes a picture of your retina, figures out its unique characterics, breaks them
into a gazillion data shards (which are entirely useless until you have brought all of
them together – modern cryptography is often counter-intuitive) and distributes them
so that it is hard to bring it back together. When somebody attempts to register again,
the new attempted registration does the same thing, but the system rejects it (using some
fancy multi-party computation distributed over 3 universities) because
the previous shards are there already.</p>
<p>That’s it. (Plus a few add-on’s they have been building. Like verifying, using your
smartphone’s camera, that it is indeed you operating the device when you are attempting
to log into a site or app, so you can’t simply hand your public key to another person
who otherwise could amass or buy thousands of other people’s identities and effectively
create thousands of accounts on the same site, all under their control. Or the crypto
currency that the relying party has to pay in order to get WN to verify an authentication
attempt – that pays the sign-up bonus from for new users, plus for the operation of
the network.)</p>
<p>My impression:</p>
<p>Lots of tech and smarts. I have not looked at the algorithms, but I can
believe that this system can be made to work more or less as described. It has a clearly
unique value proposition compared to the many other authentication / identity systems that
are found in the wild.</p>
<p>My major remaining questions:</p>
 
 
<p>How does this system fail, and how would it be fixed if/when it does? You can be
100% certain of attacks, and 100% certain of very sophisticated attacks if they get
substantial uptake. I have no good intuition about this, and I haven’t seen any
substantive discussion about that either. (That would probably take a lot more than
a two-hour podcast.) Given that one of their stated goal is that in the longer term
no organization, including themselves, can take over the system, how would a rapid
fix for a vulnerability even work?</p>
 
 
<p>And of course: will anybody adopt it? Tech history is full of failed novel authentication
systems – the above quoted Kim’s CardSpace, in spite of being shipped with every
copy of Windows, being a notable example. And there I have my doubts. As it is famously
said, whatever your authentication or identity scheme, the first thing that a relying
party asks for when implementing any of them is your e-mail address. If they continue
to do that, the whole system would be largely pointless. But maybe there are some niche
applications where this is different, I just haven’t seen many of them.</p>
 
 
<p>P.S. I’m sure I got some details wrong. Please correct me if you know better.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://reb00ted.org/images/posts/20250602/world-human-ai-altman-02.webp'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   12:30 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@rufftimo'>Timothy Ruff</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://rufftimo.medium.com/phone-home-is-bad-really-bad-eccbbf89fc6e?source=rss-11906019170a------2'>Phone Home is Bad. Really Bad.</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   What phone home enables is worse than just surveillance and privacy invasion. Much&nbsp;worse.  The ACLU, EFF, EPIC, CDT, Bruce Schneier and nearly 100 others just signed their support to a public statement opposing phone home capabilities in digital identity systems: NoPhoneHome.com.  The statement is simple, clear, and helpful, but it severely soft-pedals the&nbsp;problem.  When Phone Home Is Fa
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*Jc9lJQRiDOniX7oV7HCZPw.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 What phone home enables is worse than just surveillance and privacy invasion. Much&nbsp;worse. <p>The <a href="http://aclu.org">ACLU</a>, <a href="https://www.eff.org/issues/digital-identity">EFF</a>, <a href="http://www.epic.org">EPIC</a>, <a href="http://www.cdt.org">CDT</a>, <a href="https://en.wikipedia.org/wiki/Bruce_Schneier">Bruce Schneier</a> and nearly 100 others just signed their support to a public statement opposing phone home capabilities in digital identity systems: <a href="http://nophonehome.com">NoPhoneHome.com</a>.</p><p>The statement is simple, clear, and helpful, but it severely soft-pedals the&nbsp;problem.</p> When Phone Home Is Far Worse Than Just Surveillance <p>The effects of surveillance are limited by what a surveiler can accomplish by surveilling. In that sense, a phone home to government is dystopian, as it provides the ability to decide whether or not we can proceed with an identity-enabled activity. It literally gives government the ability to say “yes, I will allow you to do that&nbsp;thing.”</p><p>Phone home is ultimately about control over digital activities. It may not start with that objective, but that is the inescapable conclusion when in the hands of government. That is far worse than just watching me — which is bad enough — it’s controlling me.</p> What is Phone&nbsp;Home? <p>Phone home is when you present a digital credential, whether in person or online, and the system you’ve presented to “phones home” to the original issuer (or a proxy) to verify that your credential is valid, enabling identification of both verifier and user. Phone home can also happen at a later time, and/or be triggered by an application on your own device, each with the same ultimate effect of surveillance and control by the&nbsp;issuer.</p><p>You might think such a phone home check is necessary to verify a credential, but it is not. Credential verification — including current revocation status — can now be done without phoning home. The SSI (self-sovereign identity) industry has emerged over the last ten years to solve this very problem, and it did solve the problem. It was first solved using blockchain tech and now there are other, non-blockchain methods that also provide robust verification without phoning home, including revocation.</p> So Why Phone&nbsp;Home? <p>So why does any digital identity system phone home, if it doesn’t have&nbsp;to?</p><p>The federated protocols that phone home are widely adopted within the IAM (Identity &amp; Access Management) industry, because they power the single sign-on capabilities that corporate workers use every day. Many workers have dozens of systems they need to access, and some orgs run thousands of different software applications; single sign-on gives workers a single login instead of many, a benefit that has become a necessity in our software-dominated world. Within an organization’s trust domain, phoning home is harmless.</p><p>The $15 billion IAM industry — powered primarily by single-sign-on solutions — phones home to itself to verify every login, every day. This ubiquity of intra-domain phone home makes the underlying protocols well-worn, so developers, designers, architects, and other tech professionals are intimately familiar with them; OpenID, OAuth, SAML, and other protocols all phone home inherently, it’s how they work. So when government begins to get into the digital ID game and policy makers approach existing ID industry experts for products and advice, those experts simply tweak their existing, phone-home capable products and — voila — government inherits phone home capability.</p> When Phone Home Goes from Helpful to&nbsp;Harmful <p>The problem comes when phone home is used to verify identities across organizational boundaries rather than just within those boundaries. That’s when phone home goes from useful single sign-on to Orwellian surveillance and&nbsp;control.</p><p>Phone home from me to my employer to allow me into a system that I’m entitled to access? Great, that’s “intra-domain” (within the same organization). Phone home from the liquor store to the government to verify that I’m over 18 so I can buy alcohol? No way, that’s “inter-domain” (across organizational boundaries), and none of the state’s business. Both phone home, but one is harmless and the other Orwellian.</p><p>I live in Utah, where we have a <a href="https://www.sltrib.com/news/politics/2023/08/01/judge-dismisses-lawsuit/">law</a> aimed to verify that a user is over 18 before they’re granted access to a pornographic website. While I’m no fan of pornography, I oppose any digital identity system that phones home for permission for those who do participate, and who are trying to follow the age verification law.</p><p>Ultimately, a digital credential from government should function like the physical one: it should have the power and trust of government when I use it, but no built-in way for the government to know if, when, or where I use&nbsp;it.</p> But… “The Phone Home Function Isn’t Activated” <p>As the phone home privacy debate intensifies, you’ll hear this argument a lot: “The system may technically have the capability, but we’ve not implemented it that&nbsp;way.”</p><p>This is the situation we find ourselves in with <a href="https://www.aamva.org/">AAMVA</a> and the mDL (mobile driver’s license). AAMVA is the association behind all the government drivers license departments in the United States, and it publishes recommended guidelines for how states should implement the mDL (mobile drivers license). To AAMVA’s credit, in their Dec. 2024 <a href="https://www.aamva.org/publications-news/aamva-news/now-available-mobile-driver-s-license-(mdl)-implementation-guidelines,-version-1-4">Implementation Guidelines</a> (version 1.4) they did warn about the tracking potential of “Server Retrieval Mode” (phone home mode), and just recently released <a href="https://www.aamva.org/assets/best-practices,-guides,-standards,-manuals,-whitepapers/mobile-driver-s-license-implementation-guidelines-1-2">updated guidance</a> (version 1.5) that finally prohibits the practice.</p><p>Better late than never. I still wonder why they tolerated it as long as they&nbsp;did.</p><p>But while AAMVA is a national nonprofit, it is a private organization not subject to public comment, FOIA, political mandates or other regulations; all they can do is pass policy, it’s up to states to choose to follow them. Even if all 50 states choose to follow this new policy there’s still one enormous problem: the underlying ISO 18013 standard appears to require that all implementations retain Server Retrieval (phone home) capability.</p><p>When government is tempted with such power, it’s akin to giving an ice cream cone to a four year-old and telling him not to lick it, and then walking away. If the four year-old isn’t hungry at the moment maybe he can resist for a while, but if he’s hungry and the adults walk away… all of a sudden it’s an emergency.</p><p>And emergencies are precisely the problem with having latent phone home capabilities available, like those baked into ISO&nbsp;18013.</p> Privacy By&nbsp;Policy <p>Using a phone-home-capable standard like ISO 18013 to implement a digital identity system while promising not to activate that capability is — with credit to <a href="https://www.linkedin.com/in/mccown/">Steve McCown</a> for this insightful phrase — ”privacy by policy.” It’s like telling the four year-old not to lick the cone — that’s the policy — but then walking away to create the conditions where the policy will likely be violated.</p><p>All any government needs is an emergency, and often they don’t need even that. Sometimes they argue a need to monitor (and control) all uses of everyone’s identities so they can catch the bad guys just in case they show up. And that may be true: when all uses of government-issued digital credentials are constantly monitored, they may catch more bad guys, assuming bad guys are stupid enough to use systems with known surveillance. But American society decided at its founding that personal liberty was more important than catching every bad guy, that we should be protected from “unreasonable searches and seizures,” and that we should be presumed innocent instead of&nbsp;guilty.</p><p>As we’ve seen repeatedly, all government needs is an emergency like 9/11 or COVID and we quickly give into demands for intrusive and sometimes surreptitious surveillance. And this is the inherent weakness of privacy by policy; policies and policy makers change — in emergencies very quickly — and never do they change in the direction of greater personal&nbsp;liberty.</p><p>Bottom line: if the capability exists to surveil digital identity and to control it, to selectively deny its use, that power will be activated sooner or later; we’re always just one emergency away. Government may find other ways to still surveil and control, but having latent phone home capability embedded within a broadly used digital identity is like installing a big, shiny, easy ‘press here to surveil’ button… the only protection we have is current policy makers’ promise not to press&nbsp;it.</p> Ubiquitous Use = Ubiquitous Control <p>Another weak argument we’ve heard and will continue to hear: “the government-issued ID will only be used for a few things, like accessing government services, travel,&nbsp;etc.”</p><p>If it really stayed that way I’d have no problem with phone home, but it won’t stay that way, not by a long shot. Mark my words: government-issued credentials will begin to replace usernames and passwords internet-wide.</p><p>Just recently in my home state of Utah, the mDL program <a href="https://cobi.utah.gov/2025/17/issues/23703">was funded</a> to expand its utility for things like “e-banking, online shopping for age-restricted items, online car rental.” That is just the proverbial tip of the iceberg, and here’s&nbsp;why:</p><p>A government-issued verifiable credential is the strongest, most secure, highest-trust credential most of us will ever have. It is far more secure and trustworthy than any username-password combination, and the user experience will be better. Imagine the reduction of friction as websites say the equivalent of “just use your mDL with us, no need for a username or password or even to set up a new account.” It’s a huge win/win for both consumers and websites, as websites reduce friction, gain strongly vetted humans instead of bots, and get a windfall in liability reduction as they no longer have to maintain valuable, breachable honeypots of usernames and passwords. Users win by eliminating usernames and passwords, having a slicker experience, and increasing their security and resistance to ID&nbsp;theft.</p><p>Combine ubiquitous use with a latent phone home capability and you’ve got a tinder box, like a hungry four year-old holding an ice cream cone… as soon as the adult leaves the room that cone will get a lick. And with this kind of surveillance comes the huge bonus of control, literally the ability for government to say “no” to identity verifications for any reason under the&nbsp;sun.</p> Privacy Advocates in Free Societies Must Vigorously Oppose Phone Home Right Now, BEFORE It’s Too&nbsp;Late <p>The current, very dangerous problem with phone home is when it’s implemented unknowingly into a society that would oppose it if they knew what they were getting. Once phone home is embedded and government gets a taste of its power, I don’t see how that power is ever relinquished.</p><p>If phone home is on the table in your state — and it is in almost every state — it’s now or&nbsp;never.</p><p>To the current generation of privacy advocates: if we succumb to privacy by policy, and we allow phone home capabilities to be implemented while we are still above room temperature and standing on the top side of the grass, it’s on us. We must discover phone home capability wherever it might lurk, and activate any and all fellow privacy lovers to oppose it like we would a foreign&nbsp;invader…</p><p>Because if we lose this fight, we likely lose it&nbsp;forever.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*Jc9lJQRiDOniX7oV7HCZPw.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   12:23 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 02. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://identitywoman.net'>Identity Woman</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://identitywoman.net/no-phone-home-what-it-means-and-why-it-is-important/'>No Phone Home: What it means and why it is important </a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   I am a signatory to the No Phone Home Statement being released today, June 2, 2025. This statement concerns a specific design choice that many in the industry consider potentially hazardous and unsuitable for many digital identity systems. The “phone home” capability is where the issuer of the identity is notified every time the identity […] 
 The post No Phone Home: What it means and why it is im
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>I am a signatory to the No Phone Home Statement being released today, June 2, 2025. This statement concerns a specific design choice that many in the industry consider potentially hazardous and unsuitable for many digital identity systems. The “phone home” capability is where the issuer of the identity is notified every time the identity […]</p>
<p>The post <a href="https://identitywoman.net/no-phone-home-what-it-means-and-why-it-is-important/">No Phone Home: What it means and why it is important&nbsp;</a> appeared first on <a href="https://identitywoman.net">Identity Woman</a>.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   12:05 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 02. June 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2025/06/02/handling-openid-connect-error-events-in-asp-net-core/'>Handling OpenID Connect error events in ASP.NET Core</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  ASP.NET Core provides great extension points for handling OpenID Connect error events. This blog looks at implementing error handling in an ASP.NET Core application implemented using ASP.NET Core Identity. Code: https://github.com/damienbod/IdentityExternalErrorHandling Setup The application uses OpenID Connect to implement the authentication of the user identities. This implements a standard OpenI
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>ASP.NET Core provides great extension points for handling OpenID Connect error events. This blog looks at implementing error handling in an ASP.NET Core application implemented using ASP.NET Core Identity.</p>



<p>Code: <a href="https://github.com/damienbod/IdentityExternalErrorHandling">https://github.com/damienbod/IdentityExternalErrorHandling</a></p>



 Setup 



<p>The application uses OpenID Connect to implement the authentication of the user identities. This implements a standard OpenID Connect flow and uses Microsoft Entra ID as the identity provider. The application also uses ASP.NET Core Identity which can be used to implement user management. This is not required and I normally avoid using this in business applications as this logic can be delegated in most cases to the identity provider.</p>







<p></p>



<p>The OpenID Connect logic can be implemented using the default ASP.NET Core OpenID Connect handlers for any OpenID Connect implementation. Almost all products and services provide client implementations for the specific clients and all are just wrappers for the default ASP.NET Core interfaces. Microsoft provides the Microsoft.Identity.Web Nuget packages for Microsoft Entra products. This works fine as long as you do not use any other OAuth or OpenID connect services in the same application.</p>


  // Identity.External
builder.Services.AddAuthentication(options =&gt;
{
    options.DefaultAuthenticateScheme = IdentityConstants.ApplicationScheme;
    options.DefaultChallengeScheme = IdentityConstants.ApplicationScheme;
    options.DefaultSignInScheme = IdentityConstants.ExternalScheme;
})
.AddOpenIdConnect("EntraID", "EntraID", oidcOptions =&gt;
{
    oidcOptions.SignInScheme = IdentityConstants.ExternalScheme;
    oidcOptions.SignOutScheme = IdentityConstants.ApplicationScheme;
    oidcOptions.RemoteSignOutPath = new PathString("/signout-callback-oidc-entra");
    oidcOptions.SignedOutCallbackPath = new PathString("/signout-oidc-entra");
    oidcOptions.CallbackPath = new PathString("/signin-oidc-entra");

    oidcOptions.Scope.Add("user.read");
    oidcOptions.Authority = $"https://login.microsoftonline.com/{builder.Configuration["AzureAd:TenantId"]}/v2.0/";
    oidcOptions.ClientId = builder.Configuration["AzureAd:ClientId"];
    oidcOptions.ClientSecret = builder.Configuration["AzureAd:ClientSecret"];
    oidcOptions.ResponseType = OpenIdConnectResponseType.Code;
    oidcOptions.UsePkce = true;

    oidcOptions.MapInboundClaims = false;
    oidcOptions.SaveTokens = true;
    oidcOptions.TokenValidationParameters.NameClaimType = JwtRegisteredClaimNames.Name;
    oidcOptions.TokenValidationParameters.RoleClaimType = "role";
})
  


<p></p>



 OpenID Connect events 



<p>Where implementing custom logic in the OpenID Connect flows, the ASP.NET Core implementation provides a lot of events. All can be used for a specific need. When implementing error logic being returned from an identity provider, no one event can be used for this logic as every product or service implements and supports this differently. For example some providers don’t return the user authentication errors, others do.</p>


  oidcOptions.Events = new OpenIdConnectEvents
{
	// Add event handlers
	OnTicketReceived = async context =&gt; {}
	OnRedirectToIdentityProvider = async context =&gt; {}
	OnPushAuthorization = async context =&gt; {}
	OnMessageReceived = async context =&gt; {}
	OnAccessDenied = async context =&gt; {}
	OnAuthenticationFailed = async context =&gt; {}
	OnRemoteFailure = async context =&gt; {}
	// ...
};
  


<p></p>



 Handle a remote error 



<p>The OnRemoteFailure can be used to handle flow errors like an incorrect secret in the request. The HandleResponse can be used to prevent further processing for the event of the error and the user can be redirected to a user friendly UI view.</p>


  OnRemoteFailure = async context =&gt;
{
    var logger = context.HttpContext.RequestServices.GetRequiredService&lt;ILogger&lt;Program&gt;&gt;();
    logger.LogInformation("OnRemoteFailure from identity provider. Scheme: {Scheme: }", context.Scheme.Name);

    if (context.Failure != null)
    {
        context.HandleResponse();
        context.Response.Redirect($"/Error?remoteError={context.Failure.Message}");
    }

    await Task.CompletedTask;
}
  


<p></p>



 UI Error Page 



<p>A Razor Page can be used to display the error.</p>


  public class ErrorModel : PageModel
{
    public string? RequestId { get; set; }
    public string? Error { get; set; }
    public string? ErrorDescription { get; set; }
    public bool ShowRequestId =&gt; !string.IsNullOrEmpty(RequestId);

    public void OnGet(string? remoteError)
    {
        if (remoteError != null)
        {
            Error = "Remote authentication error";
            ErrorDescription = remoteError;
        }

        RequestId = Activity.Current?.Id ?? HttpContext.TraceIdentifier;
    }
}
  


<p></p>



 Notes 



<p>The ASP.NET Core APIs for implementing OpenID Connect are excellent. All products and services that implement OpenID Connect servers, handle the error processing differently. Depending on the identity products used in the software, different events are required to handle this.</p>



 Links 



<p><a href="https://learn.microsoft.com/en-us/aspnet/core/security/authentication/configure-oidc-web-authentication">https://learn.microsoft.com/en-us/aspnet/core/security/authentication/configure-oidc-web-authentication</a></p>



<p><a href="https://docs.duendesoftware.com/identityserver/fundamentals/openid-connect-events/">https://docs.duendesoftware.com/identityserver/fundamentals/openid-connect-events/</a></p>



<p><a href="https://learn.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.authentication.openidconnect.openidconnectevents">https://learn.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.authentication.openidconnect.openidconnectevents</a></p>



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2025/05/oidc-confidential-pkce-flow-drawio.png?w=800'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   04:24 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://benwerd.medium.com/building-a-newsroom-technology-culture-40655168c4bf?source=rss-3b16402f5b9d------2'>Building a newsroom technology culture</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

      A guide to hiring, supporting, and empowering the technology competence that journalism needs to survive  Continue reading on Medium »  
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/0*TtyojoUrkCcLJGkP.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   <p><a href="https://benwerd.medium.com/building-a-newsroom-technology-culture-40655168c4bf?source=rss-3b16402f5b9d------2"></a></p><p>A guide to hiring, supporting, and empowering the technology competence that journalism needs to survive</p><p><a href="https://benwerd.medium.com/building-a-newsroom-technology-culture-40655168c4bf?source=rss-3b16402f5b9d------2">Continue reading on Medium »</a></p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/0*TtyojoUrkCcLJGkP.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   02:02 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 29. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2025/05/pg_module_magic_ext/'>Postgres Extensions: Use PG_MODULE_MAGIC_EXT</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Details for extension authors for how to use the new PG_MODULE_MAGIC_EXT
macro introduced in PostgreSQL 18.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>A quick note for PostgreSQL extension maintainers: PostgreSQL 18 introduces a
new macro: <code>PG_MODULE_MAGIC_EXT</code>. Use it to name and version your modules.
Where your module <code>.c</code> file likely has:</p>
  <code>PG_MODULE_MAGIC;
</code>  <p>Or:</p>
  <code>#ifdef PG_MODULE_MAGIC
PG_MODULE_MAGIC;
#endif
</code>  <p>Change it to something like:</p>
  <code>#ifdef PG_MODULE_MAGIC_EXT
PG_MODULE_MAGIC_EXT(.name = "module_name", .version = "1.2.3");
#else
PG_MODULE_MAGIC;
#endif
</code>  <p>Replace the name of your module and the version as appropriate. Note that
<code>PG_MODULE_MAGIC</code> was added in Postgres 8.2; if for some reason your module
still supports earlier versions, use a nested <code>#ifdef</code> to conditionally
execute it:</p>
  <code>#ifdef PG_MODULE_MAGIC_EXT
PG_MODULE_MAGIC_EXT(.name = "module_name", .version = "1.2.3");
#else
#ifdef PG_MODULE_MAGIC
PG_MODULE_MAGIC;
#endif
#endif
</code>  <p>If you manage the module version in your <code>Makefile</code>, as the <a href="https://manager.pgxn.org/howto#neworder">PGXN Howto
suggests</a>, consider renaming the <code>.c</code> file to <code>.c.in</code> and changing the
<code>Makefile</code> like so:</p>
 
 
<p>Replace <code>.version = "1.2.3"</code> with <code>.version = "__VERSION__"</code></p>
 
 
<p>Add <code>src/$(EXTENSION).c</code> to <code>EXTRA_CLEAN</code></p>
 
 
<p>Add this <code>make</code> target:</p>
 <code>src/$(EXTENSION).c: src/$(EXTENSION).c.in
	sed -e 's,__VERSION__,$(EXTVERSION),g' $&lt; &gt; $@
</code>  
 
<p>If you use Git, add <code>/src/*.c</code> to <code>.gitignore</code></p>
 
 
<p>For an example of this pattern, see <a href="https://github.com/theory/pg-semver/commit/3526789">semver@3526789</a>.</p>
<p>That’s all!</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pg_module_magic_ext/">PG_MODULE_MAGIC_EXT</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   22:09 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.jonudell.net'>Jon Udell</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.jonudell.net/2025/05/28/mcp-is-rss-for-ai/'>MCP is RSS for AI</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  We mostly don’t want to read the docs, but we do want to converse with them. When we build search interfaces for our docs, we have always tried to anticipate search intentions. People aren’t just looking for words; they need to use the material to solve problems and get things done. When you create an … Continue reading MCP is RSS for AI
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>We mostly don’t want to read the docs, but we do want to converse with them. When we build search interfaces for our docs, we have always tried to anticipate search intentions. People aren’t just looking for words; they need to use the material to solve problems and get things done. When you create an MCP server, you are forced to make those search intentions explicit. That will be as useful for us as it is for the robots, and will help us work with them more effectively.</p>
<p><a href="https://thenewstack.io/mcp-is-rss-for-ai-more-use-cases-for-model-context-protocol/">MCP Is RSS for AI</a></p>
<p><a href="https://jonudell.info/newstack/archive.html">LLM series at The New Stack</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   05:31 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 26. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2025/05/26/revisiting-using-a-content-security-policy-csp-nonce-in-blazor/'>Revisiting using a Content Security Policy (CSP) nonce in Blazor</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This blog looks at implementing a strong Content Security Policy (CSP) in web applications implemented using Blazor and ASP.NET Core. When implementing CSP, I always recommend using a CSP nonce or at least CSP hashes. If a technical stack does not support CSP nonces, you should probably avoid using this solution when implementing secure and […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This blog looks at implementing a strong Content Security Policy (CSP) in web applications implemented using Blazor and ASP.NET Core. When implementing CSP, I always recommend using a CSP nonce or at least CSP hashes. If a technical stack does not support CSP nonces, you should probably avoid using this solution when implementing secure and professional web applications.</p>



<p>Code: <a href="https://github.com/damienbod/BlazorServerOidc">https://github.com/damienbod/BlazorServerOidc</a></p>







<p></p>



 Older related blogs 



 
 <a href="https://damienbod.com/2024/02/19/using-a-csp-nonce-in-blazor-web/">https://damienbod.com/2024/02/19/using-a-csp-nonce-in-blazor-web/</a> 



 <a href="https://damienbod.com/2023/05/22/blazor-and-csp/">https://damienbod.com/2023/05/22/blazor-and-csp/</a> 



 <a href="https://damienbod.com/2024/04/15/implement-a-secure-blazor-web-application-using-openid-connect-and-security-headers/">https://damienbod.com/2024/04/15/implement-a-secure-blazor-web-application-using-openid-connect-and-security-headers/</a> 



 <a href="https://damienbod.com/2024/01/03/securing-a-blazor-server-application-using-openid-connect-and-security-headers/">https://damienbod.com/2024/01/03/securing-a-blazor-server-application-using-openid-connect-and-security-headers/</a> 



 <a href="https://damienbod.com/2022/01/03/secure-a-blazor-wasm-asp-net-core-hosted-app-using-bff-and-openiddict/">https://damienbod.com/2022/01/03/secure-a-blazor-wasm-asp-net-core-hosted-app-using-bff-and-openiddict/</a> 
 



<p></p>



 Types of Blazor applications 



<p>Before implementing a robust Content Security Policy (CSP) in Blazor, it’s essential to identify the specific type of Blazor application you are working with. Blazor offers various forms and render modes, so it’s crucial to select the one that best aligns with your requirements.</p>



 
 Blazor Web Server (Interactive Server) 



 Blazor Web WASM (Interactive WebAssembly) 



 Blazor Web Mixed mode (Interactive Auto) 



 Blazor WASM hosted in ASP.NET Core (Razor Page host) 



 Blazor WASM standalone 



 Blazor Server , can be updated to Blazor Web Server (Interactive Server) 
 



<p>I only use Blazor application types, render modes that support a CSP nonce. Currently, only three types of Blazor applications offer this support:</p>



 
 Blazor Web Server (Interactive Server) 



 Blazor Web WASM (Interactive WebAssembly) 



 Blazor Web Mixed mode (Interactive Auto) 



 Blazor WASM hosted in ASP.NET Core (Razor Page host) 



 Blazor WASM standalone 



 Blazor Server, can be updated to Blazor Web Server (Interactive Server) 
 



 Blazor Web setup 



<p>When using the latest version of Blazor, the Interactive Server render mode can be used and the Interactive Auto render mode should be avoided, if security is important in the application. This can be setup using the NetEscapades.AspNetCore.SecurityHeaders Nuget package as follows:</p>


  public class Program
{
    public static void Main(string[] args)
    {
        var builder = WebApplication.CreateBuilder(args);

        // Add services to the container.
        builder.Services.AddRazorComponents()
            .AddInteractiveServerComponents();

        builder.Services.AddHttpContextAccessor();

        // ...

        builder.Services.AddSecurityHeaderPolicies()
            .SetDefaultPolicy(SecurityHeadersDefinitions
            .GetHeaderPolicyCollection(oidcConfig["Authority"],
                builder.Environment.IsDevelopment()));

        var app = builder.Build();

        // ...

        app.UseSecurityHeaders();
        app.UseHttpsRedirection();
        app.UseAntiforgery();
        app.UseAuthentication();
        app.UseAuthorization();

        app.MapStaticAssets();
        app.MapRazorComponents&lt;App&gt;()
            .AddInteractiveServerRenderMode();

        app.MapLoginLogoutEndpoints();

        app.Run();
    }
}
  


<p></p>



 Implementing security headers 



<p>The NetEscapades.AspNetCore.SecurityHeaders Nuget package can be used to implement security headers in an ASP.NET Core application. This applies security headers to the responses of the different endpoints. One of the headers is the browser CSP header. The CSP nonce is used as recommended by the latest browsers.</p>


  namespace BlazorWebApp;

using Microsoft.AspNetCore.Builder;

public static class SecurityHeadersDefinitions
{
    public static HeaderPolicyCollection GetHeaderPolicyCollection(string? idpHost, bool isDev)
    {
        ArgumentNullException.ThrowIfNull(idpHost);

        var policy = new HeaderPolicyCollection()
            .AddFrameOptionsDeny()
            .AddContentTypeOptionsNoSniff()
            .AddReferrerPolicyStrictOriginWhenCrossOrigin()
            .AddCrossOriginOpenerPolicy(builder =&gt; builder.SameOrigin())
            .AddCrossOriginResourcePolicy(builder =&gt; builder.SameOrigin())
            // #if !DEBUG // remove for dev if using Visual studio development hot reload 
            .AddCrossOriginEmbedderPolicy(builder =&gt; builder.RequireCorp())
            // #endif
            .AddContentSecurityPolicy(builder =&gt;
            {
                builder.AddObjectSrc().None();
                builder.AddBlockAllMixedContent();
                builder.AddImgSrc().Self().From("data:");
                builder.AddFormAction().Self().From(idpHost);
                builder.AddFontSrc().Self();
                builder.AddStyleSrc().Self().UnsafeInline();
                builder.AddBaseUri().Self();
                builder.AddFrameAncestors().None();

                // #if !DEBUG // remove for Visual studio development
                builder.AddScriptSrc().WithNonce().UnsafeInline();
                // #endif
            })
            .RemoveServerHeader()
            .AddPermissionsPolicyWithDefaultSecureDirectives();

        if (!isDev)
        {
            // maxage = one year in seconds
            policy.AddStrictTransportSecurityMaxAgeIncludeSubDomains();
        }

        return policy;
    }
}
  


<p></p>



<p>The headers can be added to the services.</p>


  builder.Services.AddSecurityHeaderPolicies()
.SetDefaultPolicy(SecurityHeadersDefinitions
.GetHeaderPolicyCollection(oidcConfig["Authority"],
	builder.Environment.IsDevelopment()));
  


<p></p>



<p>The HttpContextAccessor can be used to get the header from the HTTP context and used to load the scripts and the styles in the UI components. The ImportMap is extended with the nonce.</p>


  &lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;

&lt;head&gt;
    &lt;meta charset="utf-8" /&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0" /&gt;
    &lt;base href="/" /&gt;
    &lt;link rel="stylesheet" href="@Assets["lib/bootstrap/dist/css/bootstrap.min.css"]" nonce="@Nonce" /&gt;
    &lt;link rel="stylesheet" href="@Assets["app.css"]" nonce="@Nonce" /&gt;
    &lt;link rel="stylesheet" href="@Assets["BlazorWebApp.styles.css"]" nonce="@Nonce" /&gt;
    
    &lt;ImportMap AdditionalAttributes="@(new Dictionary&lt;string, object&gt;() { { "nonce", Nonce ?? "" }})" /&gt;

    &lt;link rel="icon" type="image/png" href="favicon.png" /&gt;
    &lt;HeadOutlet /&gt;
&lt;/head&gt;

&lt;body&gt;
    &lt;Routes @rendermode="InteractiveServer" /&gt;
    &lt;script src="_framework/blazor.web.js" nonce="@Nonce"&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
@code
{
    public string? Nonce =&gt; HttpContextAccessor?.HttpContext?.GetNonce();
    [Inject] private IHttpContextAccessor? HttpContextAccessor { get; set; }
}
  


<p></p>



 Visual Studio debugging 



<p>When debugging using Visual Studio, it adds two scripts which are blocked by default and should be blocked. This is a script attack and should be blocked in any deployments.</p>



<p>If you would like to allow this in Visual Studio debugging, you can use the #if !DEBUG in the SecurityHeadersDefinitions class to allow the following injected scripts:</p>


  &lt;!-- Visual Studio Browser Link --&gt;
&lt;script type="text/javascript" src="/_vs/browserLink" async="async" id="__browserLink_initializationData" data-requestId="59852cf479154d149a3db2064a0722e6" data-requestMappingFromServer="false" data-connectUrl="http://localhost:63449/fd8b98433c6f43259bb7df9563900638/browserLink"&gt;&lt;/script&gt;
&lt;!-- End Browser Link --&gt;
&lt;script src="/_framework/aspnetcore-browser-refresh.js"&gt;&lt;/script&gt;
  


 Notes 



<p>Using CSP nonces makes it easy to apply, update and maintain an application and use a strong CSP in all environments. I use this in dev, test and production setups. Any web technical stacks which do not support CSP nonces should probably be avoided when building professional web applications. Blazor InteractiveServer render mode has a good solution.</p>



 Links 



<p><a href="https://learn.microsoft.com/en-us/aspnet/core/blazor/security/server/">https://learn.microsoft.com/en-us/aspnet/core/blazor/security/server/</a></p>



<p><a href="https://learn.microsoft.com/en-us/aspnet/core/blazor/hosting-models?view=aspnetcore-9.0">https://learn.microsoft.com/en-us/aspnet/core/blazor/hosting-models</a></p>



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2024/01/brandblazor_nohalo_1000x.png?w=640'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   05:54 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 25. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2708'>Ten Years of JSON Web Token (JWT) and Preparing for the Future</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Ten years ago this week, in May 2015, the JSON Web Token (JWT) became RFC 7519. This was the culmination of a 4.5 year journey to create a simple JSON-based security token format and underlying JSON-based cryptographic standards. The full set of RFCs published together was: RFC 7515: JSON Web Signature (JWS) RFC 7516: JSON […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Ten years ago this week, in May 2015, the JSON Web Token (JWT) became <a href="https://www.rfc-editor.org/rfc/rfc7519.html">RFC 7519</a>.  This was the culmination of a <a href="https://self-issued.info/?p=1387">4.5 year journey</a> to create a simple JSON-based security token format and underlying JSON-based cryptographic standards.  The full set of RFCs published together was:</p>
 
 <a href="http://www.rfc-editor.org/rfc/rfc7515.html">RFC 7515</a>:  JSON Web Signature (JWS) 
 <a href="http://www.rfc-editor.org/rfc/rfc7516.html">RFC 7516</a>:  JSON Web Encryption (JWE) 
 <a href="http://www.rfc-editor.org/rfc/rfc7517.html">RFC 7517</a>:  JSON Web Key (JWK) 
 <a href="http://www.rfc-editor.org/rfc/rfc7518.html">RFC 7518</a>:  JSON Web Algorithms (JWA) 
 <a href="http://www.rfc-editor.org/rfc/rfc7519.html">RFC 7519</a>:  JSON Web Token (JWT) 
 <a href="http://www.rfc-editor.org/rfc/rfc7520.html">RFC 7520</a>:  Examples of Protecting Content Using JSON Object Signing and Encryption (JOSE) 
 <a href="http://www.rfc-editor.org/rfc/rfc7521.html">RFC 7521</a>:  Assertion Framework for OAuth 2.0 Client Authentication and Authorization Grants 
 <a href="http://www.rfc-editor.org/rfc/rfc7522.html">RFC 7522</a>:  Security Assertion Markup Language (SAML) 2.0 Profile for OAuth 2.0 Client Authentication and Authorization Grants 
 <a href="http://www.rfc-editor.org/rfc/rfc7523.html">RFC 7523</a>:  JSON Web Token (JWT) Profile for OAuth 2.0 Client Authentication and Authorization Grants 
 
<p>It’s certainly the case that we co-designed JWT and its underpinnings with <a href="https://openid.net/connect">OpenID Connect</a>, while also attempting to create general-purpose, widely useful standards.  Given the adoption that’s ensued, it seems that we succeeded.</p>
<p>As I wrote in my post <a href="https://self-issued.info/?p=2045">JWTs helping combat fraudulent and unwanted telephone calls</a>, “It’s often said that one sign of a standard having succeeded is that it’s used for things that the inventors never imagined.”  I’m gratified that this applies to JWT and the related specifications.  As was written in the post <a href="https://blog.duendesoftware.com/posts/20250520-essential-moments-in-the-oauth-and-openid-connect-timeline/">Essential Moments in the OAuth and OpenID Connect Timeline</a>, it’s now hard to imagine an online security world without these standards.</p>
<p>That said, there’s work underway to keep JWTs and the use of them secure for the next decade.  Five years ago, the <a href="https://www.rfc-editor.org/rfc/rfc8725.html">JSON Web Token Best Current Practices</a> specification was created.  As <a href="https://self-issued.info/?p=2052">I wrote then</a>:</p>
<blockquote><p>
This Best Current Practices specification contains a compendium of lessons learned from real JWT deployments and implementations over that period. It describes pitfalls and how to avoid them as well as new recommended practices that enable proactively avoiding problems that could otherwise arise.
</p></blockquote>
<p>My coauthors <a href="https://www.linkedin.com/in/yaronf/">Yaron Sheffer</a> and <a href="https://www.linkedin.com/in/dickhardt/">Dick Hardt</a> and I are now updating the JWT BCP to describe additional threats and mitigations that have become known in the last five years.  See the <a href="https://www.ietf.org/archive/id/draft-sheffer-oauth-rfc8725bis-01.html">updated JSON Web Token Best Current Practices</a> specification.</p>
<p>Similarly, my coauthors <a href="https://www.linkedin.com/in/bcampbell/">Brian Campbell</a> and <a href="https://www.linkedin.com/in/cmort/">Chuck Mortimore</a> of the <a href="https://www.rfc-editor.org/rfc/rfc7523.html">JWT Profile for OAuth 2.0 Client Authentication and Authorization Grants</a> are updating it and related specifications to address vulnerabilities caused by <a href="https://openid.net/notice-of-a-security-vulnerability/">ambiguities in the audience values</a> of tokens sent to the authorization server.  See the <a href="https://www.ietf.org/archive/id/draft-ietf-oauth-rfc7523bis-01.html">RFC7523bis</a> specification.</p>
<p>I’m truly grateful that my coauthors <a href="https://www.linkedin.com/in/ve7jtb/">John Bradley</a> and <a href="https://www.linkedin.com/in/natsakimura/">Nat Sakimura</a> and I created something useful and widely used ten years ago, of course with substantial contributions from the OAuth, JOSE, and OpenID Connect working groups.  I look forward to what the next decade will bring!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:17 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://idm-thoughtplace.blogspot.com/'>IdM Thoughtplace</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://idm-thoughtplace.blogspot.com/2025/05/the-goldilocks-syndrome.html'>The Goldilocks Syndrome</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   &nbsp;   “Then Goldenlocks sat down in the chair of the Great,
Huge Bear, and that was too hard for her. And then she sat down in the
chair of the Middle Bear, and that was too soft for her. And then she
sat down in the chair of the Little, Small, Wee Bear, and that was neither too
hard nor too soft, but just right. So she seated herself in it, and
there she sat till the bottom of the chair came o
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>&nbsp;</p><blockquote><p>“Then Goldenlocks sat down in the chair of the Great,
Huge Bear, and that was too hard for her. And then she sat down in the
chair of the Middle Bear, and that was too soft for her. And then she
sat down in the chair of the Little, Small, Wee Bear, and that was neither too
hard nor too soft, but just right. So she seated herself in it, and
there she sat till the bottom of the chair came out, and down she came plump
upon the ground.”<a title="">[i]</a></p></blockquote>

<p>I’ve been making this observation formally ever since I
started in the software field at a company called Magic Solutions back in the
late 90s and probably informally before then. You see, it’s been my experience
that when organizations roll out new enterprise concepts, particularly in IT
and more specifically in IT Security and Governance, it goes through at least three
revisions. I’ve seen this happen in several models whenever there is some sort
of organizational hierarchy. In my Help Desk days, it was about Ticket Subject
Organization, in Identity it’s usually the organization of the Directory
Service (Security Groups and Organization Unit structures) or role/entitlement
hierarchies.</p>

<p>For the record, I’ve been involved in all of the scenarios
listed below, and I’ve been confident I nailed it nearly every time. As I’ve
become more experienced, I mention that these structures will most likely change over time
and that the first time is seldom the charm. </p>

<p>The first one is usually pretty much what the organization
thinks they need. This might be in consultation with experts either during the
sales process or when working with the implementation specialists. This frequently
suffers from a lack of flexibility, in that not all use cases have been
properly considered and weighted. It’s good enough for now, and the project to
review how things are configured is pushed to the next version of the
application / architecture review. </p>

<p>The second time around, the organization is looking to be
flexible, so that any potential scenario can be handled. Now we have the opposite
problem, where different parts of the organization have too much control and
the solution becomes too cumbersome and there is little to no organization. It’s
complete anarchy, audit logs become so incomprehensive that they border on
being meaningless, and nobody is happy.</p>

<p>At the third time through the process, I believe that we are
starting to maybe see a proper solution that has structure, and is somewhat
flexible to new scenarios. In terms of our introduction quote, it’s not too
rigid, and it’s not too open, but just flexible enough.</p>

<p>Sometimes this is
because the structure is more open, or because there’s a stronger change
control process in place. Sometimes it is because the organization itself has
changed, changing in size, complexity, governance needs, or just a plain old
change in culture. Change will still occur, but with the lessons learned the
process should be more manageable.</p>

 <br>

 



 

<p><a title="">[i]</a> <a href="https://en.wikisource.org/wiki/The_Story_of_the_Three_Bears_(Brooke)">https://en.wikisource.org/wiki/The_Story_of_the_Three_Bears_(Brooke)</a>
That’s how this version spelled it. Emphasis is mine.</p>

 

 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   02:18 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 24. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@dkelts.id'>David Kelts on ID</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://medium.com/@dkelts.id/mdl-privacy-annex-e-of-iso-iec-18013-5-2021-2d1e145d19f5?source=rss-9bac94a4f241------2'>mDL Privacy: Annex E of ISO/IEC 18013–5:2021</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   A summary of Privacy and Security Guidance for Mobile Driver’s Licenses; How to make mDL work for mDL Holders&nbsp;(people)  Annex E of ISO/IEC 18013–5 provides informative guidance on privacy and security recommendations for mobile driver’s licenses (mDLs). It tries to ensure the secure and privacy-preserving design and implementation of mDL solutions. The annex offers practical advice to impleme
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/512/1*ow6dTJXegtDaAdPAdzJpsg.jpeg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 A summary of Privacy and Security Guidance for Mobile Driver’s Licenses; How to make mDL work for mDL Holders&nbsp;(people) <p>Annex E of ISO/IEC 18013–5 provides informative guidance on privacy and security recommendations for mobile driver’s licenses (mDLs). It tries to ensure the secure and privacy-preserving design and implementation of mDL solutions. The annex offers practical advice to implementers on how to incorporate privacy and security considerations throughout the mDL lifecycle and all components of the system. [<a href="https://www.ul.com/sites/g/files/qbfpbp251/files/2021-08/mDL%20eBook_Final_0.pdf#:~:text=In%20an%20informative%20annex%2C%20ISO/IEC%2018013%2D5%20lists,rental%20and%20car%20sharing%2C%20and%20airport%20security.">1</a>, <a href="https://www.standards.govt.nz/shop/isoiec-18013-52021">2</a>, <a href="https://www.securetechalliance.org/wp-content/uploads/Mobile-Drivers-License-WP-FINAL-Update-March-2020-4.pdf#:~:text=1%20A%20draft%20international%20standard%20is%20available,implementing%20mDLs%20that%20comply%20with%20this%20standard.&amp;text=The%20Annexes%20provide%20informative%20guidance%20for%20issuing,their%20mDL%20solutions%20for%20privacy%20and%20security.">3</a>]. All of the same privacy principles and guidance apply to ISO/IEC 18013–7:2024 for Internet&nbsp;usage.</p><p>Evaluations of ISO/IEC 18013–5:2021 were done by global [13] groups of privacy engineers and privacy engineering students, including IRB reviews for any implementations by US Federal Agencies. In all cases, the following (or their international equivalents) were used to evaluate the privacy architecture and people’s viewpoints:</p>  NIST Privacy Risk Assessment Methodology (<a href="https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/privacy-risk-assessment">PRAM</a>)  Fair Information Practice Principles (<a href="https://www.cloudflare.com/learning/privacy/what-are-fair-information-practices-fipps/">FIPPs</a>)  Privacy By Design Foundational Principles (<a href="https://privacy.ucsc.edu/resources/privacy-by-design---foundational-principles.pdf">PbD</a>)  Presenting a Mobile Driver’s License or Mobile ID to an in-person Point of Sale reader&nbsp;device<p>Key areas covered in Annex&nbsp;E:</p>  Data Privacy: The annex provides guidance on how to handle personal data within mDL, including secure processing, secure storage, granularity, and user control of disclosure (aka selective disclosure). [<a href="https://www.securetechalliance.org/wp-content/uploads/Mobile-Drivers-License-WP-FINAL-Update-March-2020-4.pdf#:~:text=1%20A%20draft%20international%20standard%20is%20available,implementing%20mDLs%20that%20comply%20with%20this%20standard.&amp;text=The%20Annexes%20provide%20informative%20guidance%20for%20issuing,their%20mDL%20solutions%20for%20privacy%20and%20security.">3</a>,&nbsp;<a href="https://www.securetechalliance.org/wp-content/uploads/Mobile-Drivers-License-WP-FINAL-Update-March-2020-4.pdf#:~:text=1%20A%20draft%20international%20standard%20is%20available,implementing%20mDLs%20that%20comply%20with%20this%20standard.&amp;text=The%20Annexes%20provide%20informative%20guidance%20for%20issuing,their%20mDL%20solutions%20for%20privacy%20and%20security.">3</a>]  Data Security: It offers recommendations on protecting mDL data from unauthorized access, use, disclosure, disruption, modification, or destruction. [<a href="https://www.securetechalliance.org/wp-content/uploads/Mobile-Drivers-License-WP-FINAL-Update-March-2020-4.pdf#:~:text=1%20A%20draft%20international%20standard%20is%20available,implementing%20mDLs%20that%20comply%20with%20this%20standard.&amp;text=The%20Annexes%20provide%20informative%20guidance%20for%20issuing,their%20mDL%20solutions%20for%20privacy%20and%20security.">3</a>, <a href="https://www.securetechalliance.org/wp-content/uploads/Mobile-Drivers-License-WP-FINAL-Update-March-2020-4.pdf#:~:text=1%20A%20draft%20international%20standard%20is%20available,implementing%20mDLs%20that%20comply%20with%20this%20standard.&amp;text=The%20Annexes%20provide%20informative%20guidance%20for%20issuing,their%20mDL%20solutions%20for%20privacy%20and%20security.">3</a>, <a href="https://www.linkedin.com/pulse/convergence-global-identification-standards-examining-ott-sarv-zvuof#:~:text=Security%20Protocols%20and%20Encryption:%20Security%20is%20paramount,mDLs%20from%20unauthorized%20access%2C%20fraud%2C%20and%20tampering.">4</a>]. It provides guidance on using appropriate security mechanisms, such as encryption, digital signatures, and access controls, to protect mDL data. [<a href="https://collateral-library-production.s3.amazonaws.com/uploads/asset_file/attachment/36416/CS676613_-_Digital_Credentials_promotion_campaign-White_Paper_R3.pdf">9</a>, <a href="https://collateral-library-production.s3.amazonaws.com/uploads/asset_file/attachment/36416/CS676613_-_Digital_Credentials_promotion_campaign-White_Paper_R3.pdf">9</a>, <a href="https://www.dock.io/post/iso-18013-5">10</a>, <a href="https://www.dock.io/post/iso-18013-5">10</a>,&nbsp;<a href="https://www.linkedin.com/pulse/convergence-global-identification-standards-examining-ott-sarv-zvuof#:~:text=ISO/IEC%2018013%2D5:%20Given%20that%20mDLs%20exist%20in,mDL%20data%20cannot%20be%20altered%20or%20forged.">11</a>]  Authentication and Authorization: The annex outlines how to ensure the authenticity of the mDL and authorize access to mDL data by authorized parties. [<a href="https://www.securetechalliance.org/wp-content/uploads/Mobile-Drivers-License-WP-FINAL-Update-March-2020-4.pdf#:~:text=1%20A%20draft%20international%20standard%20is%20available,implementing%20mDLs%20that%20comply%20with%20this%20standard.&amp;text=The%20Annexes%20provide%20informative%20guidance%20for%20issuing,their%20mDL%20solutions%20for%20privacy%20and%20security.">3</a>, <a href="https://www.iso.org/obp/ui/ru/#!iso:std:69084:en">5</a>, <a href="https://vidos.id/blog/understanding-iso-18013-5#:~:text=While%20other%20parts%20cover%20physical%20licenses%2C%20this,tool%20for%20secure%20and%20efficient%20identity%20verification.">6</a>, <a href="https://www.linkedin.com/pulse/convergence-global-identification-standards-examining-ott-sarv-zvuof#:~:text=ISO/IEC%2018013%2D5:%20Given%20the%20digital%20nature%20of,how%20and%20when%20their%20data%20is%20shared.">7</a>,&nbsp;<a href="https://standards.iteh.ai/catalog/standards/iso/8b349f37-4a4d-4379-9feb-0061079dba81/iso-iec-18013-5-2021">8</a>]  Privacy-Enhancing Technologies: The annex points to privacy-enhancing technologies, such as pseudonymization, key rotation, and differential privacy to further protect the privacy of mDL holders. [<a href="https://www.securetechalliance.org/wp-content/uploads/Mobile-Drivers-License-WP-FINAL-Update-March-2020-4.pdf#:~:text=1%20A%20draft%20international%20standard%20is%20available,implementing%20mDLs%20that%20comply%20with%20this%20standard.&amp;text=The%20Annexes%20provide%20informative%20guidance%20for%20issuing,their%20mDL%20solutions%20for%20privacy%20and%20security.">3</a>,&nbsp;<a href="https://www.securetechalliance.org/wp-content/uploads/Mobile-Drivers-License-WP-FINAL-Update-March-2020-4.pdf#:~:text=1%20A%20draft%20international%20standard%20is%20available,implementing%20mDLs%20that%20comply%20with%20this%20standard.&amp;text=The%20Annexes%20provide%20informative%20guidance%20for%20issuing,their%20mDL%20solutions%20for%20privacy%20and%20security.">3</a>]  Anti-Surveillance: Explicit instructions on avoiding surveillance tracking by the components of the system (e.g., wallet applications, API entry points, system logs). This includes key rotation that resists 3rd party reassembly of transaction audit trails and tips against surveillance by any party within the ecosystem.  Operation within a Framework of Rules. Any identity system or architecture provides touch points for failure to protect privacy. Mobile Driver’s License systems should always operate within a Trust Framework that pairs regional privacy expectations with enforcement.  <p>In essence, Annex E helps technology providers implement mDL solutions that are not only secure but also respect the privacy of the mDL holders. [<a href="https://www.securetechalliance.org/wp-content/uploads/Mobile-Drivers-License-WP-FINAL-Update-March-2020-4.pdf#:~:text=1%20A%20draft%20international%20standard%20is%20available,implementing%20mDLs%20that%20comply%20with%20this%20standard.&amp;text=The%20Annexes%20provide%20informative%20guidance%20for%20issuing,their%20mDL%20solutions%20for%20privacy%20and%20security.">3</a>]. It provides guidelines document issuers can use to build or&nbsp;procure.</p><p>&lt;end of article content, notes&nbsp;below&gt;</p><p>Contact <a href="https://www.decipher.id/contact/davidkelts">Decipher Identity, LLC</a> for evaluation of your solutions for privacy, identity assurance, or general system&nbsp;design.</p><p>Google AI generated the base for this article and the citations. As one of the lead editors for Annex E of ISO/IEC 18013–5:2021, and the author of “<a href="https://medium.com/@dkelts.id/mobile-driver-licenses-mdl-how-to-use-iso-18013-5-5a1bbc1a37a3">How to Use 18013–5</a>”[12] and “<a href="https://medium.com/@dkelts.id/id-transactions-as-we-open-up-our-world-post-covid-19-395dd548ef13">Contactless ID</a>”, <a href="https://www.linkedin.com/in/dkelts/">I</a> have rewritten the summary&nbsp;above.</p><p>[1] <a href="https://www.ul.com/sites/g/files/qbfpbp251/files/2021-08/mDL%20eBook_Final_0.pdf#:~:text=In%20an%20informative%20annex%2C%20ISO/IEC%2018013%2D5%20lists,rental%20and%20car%20sharing%2C%20and%20airport%20security.">https://www.ul.com/sites/g/files/qbfpbp251/files/2021-08/mDL%20eBook_Final_0.pdf</a></p><p>[2] <a href="https://www.standards.govt.nz/shop/isoiec-18013-52021">https://www.standards.govt.nz/shop/isoiec-18013-52021</a></p><p>[3] <a href="https://www.securetechalliance.org/wp-content/uploads/Mobile-Drivers-License-WP-FINAL-Update-March-2020-4.pdf#:~:text=1%20A%20draft%20international%20standard%20is%20available,implementing%20mDLs%20that%20comply%20with%20this%20standard.&amp;text=The%20Annexes%20provide%20informative%20guidance%20for%20issuing,their%20mDL%20solutions%20for%20privacy%20and%20security.">https://www.securetechalliance.org/wp-content/uploads/Mobile-Drivers-License-WP-FINAL-Update-March-2020-4.pdf</a></p><p>[4] <a href="https://www.linkedin.com/pulse/convergence-global-identification-standards-examining-ott-sarv-zvuof#:~:text=Security%20Protocols%20and%20Encryption:%20Security%20is%20paramount,mDLs%20from%20unauthorized%20access%2C%20fraud%2C%20and%20tampering.">https://www.linkedin.com/pulse/convergence-global-identification-standards-examining-ott-sarv-zvuof</a></p><p>[5] <a href="https://www.iso.org/obp/ui/ru/#!iso:std:69084:en">https://www.iso.org/obp/ui/ru/</a></p><p>[6] <a href="https://vidos.id/blog/understanding-iso-18013-5#:~:text=While%20other%20parts%20cover%20physical%20licenses%2C%20this,tool%20for%20secure%20and%20efficient%20identity%20verification.">https://vidos.id/blog/understanding-iso-18013-5</a></p><p>[7] <a href="https://www.linkedin.com/pulse/convergence-global-identification-standards-examining-ott-sarv-zvuof#:~:text=ISO/IEC%2018013%2D5:%20Given%20the%20digital%20nature%20of,how%20and%20when%20their%20data%20is%20shared.">https://www.linkedin.com/pulse/convergence-global-identification-standards-examining-ott-sarv-zvuof</a></p><p>[8] <a href="https://standards.iteh.ai/catalog/standards/iso/8b349f37-4a4d-4379-9feb-0061079dba81/iso-iec-18013-5-2021">https://standards.iteh.ai/catalog/standards/iso/8b349f37-4a4d-4379-9feb-0061079dba81/iso-iec-18013-5-2021</a></p><p>[9] <a href="https://collateral-library-production.s3.amazonaws.com/uploads/asset_file/attachment/36416/CS676613_-_Digital_Credentials_promotion_campaign-White_Paper_R3.pdf">https://collateral-library-production.s3.amazonaws.com/uploads/asset_file/attachment/36416/CS676613_-_Digital_Credentials_promotion_campaign-White_Paper_R3.pdf</a></p><p>[10] <a href="https://www.dock.io/post/iso-18013-5">https://www.dock.io/post/iso-18013-5</a></p><p>[11] <a href="https://www.linkedin.com/pulse/convergence-global-identification-standards-examining-ott-sarv-zvuof#:~:text=ISO/IEC%2018013%2D5:%20Given%20that%20mDLs%20exist%20in,mDL%20data%20cannot%20be%20altered%20or%20forged.">https://www.linkedin.com/pulse/convergence-global-identification-standards-examining-ott-sarv-zvuof</a></p><p>[12] <a href="https://medium.com/@dkelts.id/mobile-driver-licenses-mdl-how-to-use-iso-18013-5-5a1bbc1a37a3">https://medium.com/@dkelts.id/mobile-driver-licenses-mdl-how-to-use-iso-18013-5-5a1bbc1a37a3</a></p><p>[13] Participants from every inhabited content, 30+ countries, and 50+ companies contributed to the design of ISO/IEC 18013–5:2021</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/512/1*ow6dTJXegtDaAdPAdzJpsg.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   18:54 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 23. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.webistemology.com/whose-mandate/'>Whose mandate?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Prime Minister Mark Carney's first mandate letter to his cabinet reads like a document for boardrooms rather than food banks.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Carney's Mandate Letter Falls Short on Inequality <blockquote>This article was originally published May 23, 2025, on <a href="https://open.substack.com/pub/privacycdn/p/whose-mandate?ref=webistemology.com">Substack</a> </blockquote><p>Prime Minister Mark Carney's first <a href="https://www.pm.gc.ca/en/mandate-letters/2025/05/21/mandate-letter?ref=webistemology.com">mandate letter</a> to his cabinet reads like a document for boardrooms rather than food banks. While the letter acknowledges Canada faces "unprecedented challenges," it fundamentally misdiagnoses the core crisis facing ordinary Canadians: deepening inequality that has left millions struggling to afford basic necessities while corporate profits soar. This crisis predates any crisis related to the 'Trump Tariffs'.</p> The Missing Diagnosis: Inequality as Crisis <p>The mandate letter extensively discusses productivity, infrastructure, and global competitiveness, but barely acknowledges the defining issue of our time: the growing chasm between the wealthy and everyone else. Canada's challenges aren't primarily about being competitive enough for global markets, but about ensuring the wealth our economy generates reaches working families.</p><p>Consider what's absent from Carney's "generational challenge" framing. There's no mention that CEO compensation has grown exponentially while wages have stagnated. There's no acknowledgement that housing has become a financialised commodity rather than a human right. There is no recognition that our healthcare and social services are crumbling, not because of productivity problems but because of deliberate underfunding. At the same time, tax cuts flow to corporations and the wealthy.</p><p>The letter's focus on "weak productivity" as a root cause exemplifies this misdiagnosis. Canada's problem isn't that workers aren't productive enough; it's that the gains from increased productivity have been captured almost entirely by Canadian and foreign corporations, rather than being shared with labour. Workers are more productive than ever, but their share of the economic pie continues to shrink.</p> The Corporate Solutions Trap <p>Carney's seven priorities reveal an approach that sees private sector partnership as the solution to public challenges. This reflects a fundamental ideological blind spot that progressive critics have long identified: the assumption that what is good for business automatically translates into what is good for working people.</p><p>Take housing, presented as priority four. The mandate speaks of "unleashing the power of public-private cooperation" and "catalysing a modern housing industry." This language suggests more of the same market-based approaches that created the housing crisis in the first place. Why are we looking to private developers to solve a crisis they helped create through speculation and commodification?</p><p>Similarly, the emphasis on attracting "the best talent in the world" while reducing overall immigration rates reveals a troubling two-tier approach. This priority suggests Canada should welcome wealthy immigrants and skilled professionals while restricting opportunities for working-class migrants and refugees, precisely the kind of policy that exacerbates inequality by serving capital's need for cheap labour while restricting worker solidarity.</p><p>Further, this skilled immigration strategy represents a form of modern economic colonialism that extracts human capital from the Global South. When Canada recruits doctors, engineers, teachers, and skilled tradespeople from countries like Nigeria, India, or the Philippines, it effectively imports professionals whose education and training were paid for by much poorer nations. These countries invested scarce public resources in universities, medical schools, and technical training programs to develop their human capital, only to see their most skilled workers migrate to wealthy countries like Canada.</p> The Infrastructure Mirage <p>The letter's promise to "build an enormous amount of new infrastructure at speeds not seen in generations" sounds ambitious, but infrastructure for whom? The focus on diversifying trading relationships and becoming an "energy superpower" suggests infrastructure designed primarily to serve export markets and corporate interests rather than communities.</p><p>Real infrastructure investment that addresses inequality would prioritise public transit that connects working family neighbourhoods, social housing that removes profit from the equation, and green energy systems owned by communities rather than corporations. The mandate letter's infrastructure vision appears designed to facilitate capital flows rather than improve daily life for ordinary Canadians.</p> The Productivity Obsession <p>Perhaps most revealing is the letter's emphasis on government productivity through AI deployment and its focus on "results over spending." This technocratic language conceals austerity politics masquerading as innovation rhetoric. The problem with Canadian governance isn't inefficiency - it's inadequate funding for public services caused by decades of tax cuts for the wealthy and corporations. When the letter promises to spend "less on government operations," it promises less public capacity to address inequality at the moment when more robust public intervention is needed.</p><p>The emphasis on AI is particularly concerning. While technology can improve service delivery, the focus on AI as a solution to government "productivity" problems suggests an approach that prioritises automation over employment and efficiency over equity.</p> What's Missing: Power and Redistribution <p>The most glaring omission from Carney's mandate is any serious discussion of redistributing power and wealth. The letter treats inequality as a byproduct of other problems rather than recognising it as the central organising challenge of our time.</p><p>A progressive approach would centre policies like wealth taxes, stronger collective bargaining rights, public ownership of key industries, and robust social programs as rights rather than safety nets. Instead, the mandate offers market-friendly tweaks to a system that fundamentally concentrates power and wealth upward.</p><p>The letter's promise to work "in true partnership" with various stakeholders sounds collaborative, but readers should recognise this as false equivalency. Labour, business, and civil society don't have equal power in our current system - pretending they do while avoiding structural reforms maintains power imbalances.</p> The Climate Contradiction <p>While the letter mentions fighting climate change, it simultaneously promises to make Canada an "energy superpower in both clean and conventional energies." This contradiction reveals the fundamental tension in liberal approaches that try to address the climate crisis without challenging the growth-dependent capitalist system driving environmental destruction.</p><p>Meaningful climate action requires reducing overall resource consumption and energy use in wealthy nations like Canada, not simply adding renewable capacity while maintaining fossil fuel production. The "energy superpower" framing suggests Canada plans to export its way out of climate responsibility rather than leading the transformation to a sustainable economy.</p> Continuity Disguised as Change <p>Ultimately, Carney's mandate letter represents continuity with the neoliberal policy framework that created Canada's inequality crisis, dressed up in the language of transformation and renewal. This is a throwback to the Liberal budgets of the 1990s, which ‘solved’ the Federal deficit by shifting the burden to the provinces. While the rhetoric acknowledges serious challenges, the solutions remain firmly within the bounds of market-friendly approaches that have consistently failed to deliver broadly shared prosperity.</p><p>This mandate highlights the limitations of electoral politics when not accompanied by sustained social movements that demand structural change. Real solutions to inequality require challenging concentrated wealth and power, not partnering with it. They need robust public institutions and services, not AI-driven efficiency. They require treating housing, healthcare, and education as rights, not commodities.</p><p>Canadians facing the daily reality of unaffordable housing, precarious employment, and strained public services deserve better than another government that mistakes corporate competitiveness for social progress. The mandate letter's failure to address inequality meaningfully suggests that this government, like its predecessors, will leave the fundamental structures driving division and hardship intact, while offering modest reforms around the margins.</p><p>The crisis isn't that Canada isn't competitive enough - it's that too many Canadians can no longer compete for basic dignity in an economy rigged against them.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://www.webistemology.com/content/images/2025/06/MandateLetterFallsShort-Gemini_Generated_Image_wu8gb6wu8gb6wu8g.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:23 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 22. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/leaving-aws'>Leaving AWS</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  At the end of April, I wrapped up my time at AWS.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 <a href="https://substackcdn.com/image/fetch/$s_!rhoe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2a505ec-b40c-43a9-bbe3-ff9aa7e03f51_1536x1024.heic">          </a> <p>At the end of April, I wrapped up my time at AWS. I joined in September 2022, stepping into the world of AWS Identity, where I worked on authorization and related areas like Zero Trust. It was a deeply rewarding experience. I got a front-row seat to the sheer professionalism and operational excellence it takes to run a cloud service at that scale. The bar is high, and I came away with a renewed appreciation for what it means to build for resilience, security, and speed—at the same time, and without compromise.</p><p>For the past 20 months, we’ve been living in Virginia while I led a team of developers at HQ2, Amazon’s second headquarters in Arlington. That’s ultimately what made this decision necessary. As much as I loved the work and the people, we’ve long felt the pull of home. Utah is where our family is, and where we wanted to be. With AWS’s return-to-office mandates and no local office in Utah, something had to give. In the end, family won. No regrets there.</p><p>I’m especially grateful to Neha Rungta, who brought me into AWS. Neha and I go way back—I knew her when she was pursuing her PhD in computer science at BYU. She’s a remarkable leader, and AWS is fortunate to have her. I appreciate the trust she placed in me and the opportunity to be part of something as consequential as AWS Identity.</p><p>So, what’s next? I’m not retired—but for now, my time is my own. I’m working on a book for Manning about authorization, a topic that’s increasingly critical as digital systems become more interconnected and identity-aware. I’m also staying engaged with the identity community through the Internet Identity Workshop (IIW), which continues to be a wellspring of innovation and collaboration.</p><p>Recently, we launched the IIW Foundation, a 501(c)(3) nonprofit dedicated to advancing open, empowering approaches to digital identity. Our mission is to support not only the flagship IIW events but also IIW-Inspired™ regional gatherings around the world. There’s more to come on that front, and I’ll share details in future posts.</p><p>Stepping away from AWS wasn’t easy, but it was the right move. And as I turn the page, I’m excited about the work ahead—and grateful for the journey so far.</p>   <p>Photo Credit: <a>Leaving AWS</a> from DALL-E (public domain)</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!rhoe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2a505ec-b40c-43a9-bbe3-ff9aa7e03f51_1536x1024.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:30 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 22. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2025/05/extension-packaging-adventures/'>Adventures in Extension Packaging</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Narrative version of a PGConf.dev talk covering the many issues I stumbled
upon while designing a universal packaging format for PostgreSQL extensions,
maintaining pgt.dev packages, and experimenting with CloudNativePG
immutability.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>I gave a presentation at <a href="https://2025.pgconf.dev" title="PostgreSQL Development Conference 2025">PGConf.dev</a> last week, <a href="https://www.pgevents.ca/events/pgconfdev2025/schedule/session/331-adventures-in-extension-packaging/">Adventures in Extension
Packaging</a>. It summarizes stuff I learned in the past year in developing
the <a href="https://github.com/pgxn/rfcs/pull/3" title="RFC: Meta Spec v2">PGXN Meta v2 RFC</a>, re-packaging all of the extensions on
<a href="https://pgt.dev" title="Trunk: A Postgres Extension Registry">pgt.dev</a>, and experimenting with the <a href="https://cloudnative-pg.io" title="Run PostgreSQL. The Kubernetes way.">CloudNativePG</a> community’s
<a href="https://www.gabrielebartolini.it/articles/2025/03/the-immutable-future-of-postgresql-extensions-in-kubernetes-with-cloudnativepg/" title="The Immutable Future of PostgreSQL Extensions in Kubernetes with CloudNativePG">proposal</a> to mount extension OCI images in immutable PostgreSQL
containers.</p>
<p>Turns out a ton of work and experimentation remains to be done.</p>
 
 <a href="https://www.youtube.com/watch?v=Xfjq6yU0W2I" title="Adventures in Extension Packaging (PGConf.dev 2025)">Video</a> 
 <a href="https://justatheory.com/2025/05/extension-packaging-adventures/adventures-extension-packaging.pdf">Slides</a> 
 
<p>
</p>
<p>Previous work covers the first half of the talk, including:</p>
 
 A brief introduction to <a href="http://pgxn.org/" title="PostgreSQL Extension Network">PGXN</a>, borrowing from the <a href="https://justatheory.com/2025/03/mini-summit-one/#state-of-the-extensions-ecosystem">State of the
Extensions Ecosystem</a> 
 The metadata designed to enable automated packaging of extensions added to
the <a href="https://github.com/pgxn/rfcs/pull/3" title="RFC: Meta Spec v2">PGXN Meta v2 RFC</a> 
 The <a href="https://justatheory.com/2024/06/trunk-poc/" title="POC: PGXN Binary Distribution Format">Trunk Packaging Format</a>, a.k.a., <a href="https://github.com/pgxn/rfcs/pull/2">PGXN RFC 2</a> 
 <a href="https://justatheory.com/2024/06/trunk-oci-poc/" title="POC: Distributing Trunk Binaries via OCI">OCI distribution</a> of Trunk packages 
 
<p>The rest of the talk encompasses newer work. Read on for details.</p>
 Automated Packaging Challenges 
<p>Back in December I took over maintenance of the <a href="https://pgt.dev" title="Trunk: A Postgres Extension Registry">Trunk registry</a>,
a.k.a., <a href="https://pgt.dev" title="Trunk: A Postgres Extension Registry">pgt.dev</a>, refactoring and upgrading all 200+ extensions and adding
Postgres 17 builds. This experience opened my eyes to the wide variety of
extension build patterns and configurations, even when supporting a single OS
(Ubuntu 22.04 “Jammy”). Some examples:</p>
 
 <a href="https://github.com/tembo-io/trunk/blob/5f3de6d/contrib/pg_search/Dockerfile#L18">pglogical requires</a> an extra <code>make</code> param to build on PostgreSQL 17:
<code>make -C LDFLAGS_EX="-L/usr/lib/postgresql/17/lib"</code> 
 Some <a href="https://github.com/pgcentralfoundation/pgrx" title="pgrx: Build Postgres Extensions with Rust!">pgrx</a> extensions require additional params, for example:
 
 <a href="https://github.com/tembo-io/trunk/blob/5f3de6d/contrib/pg_search/Dockerfile#L18">pg_search needs</a> the <code>--features</code> flag to enable icu 
 <a href="https://github.com/tembo-io/trunk/blob/5f3de6d/contrib/vectorscale/Dockerfile#L25">vectorscale requires</a> the environment variable
<code>RUSTFLAGS="-C target-feature=+avx2,+fma"</code> 
 
 
 <a href="https://github.com/tembo-io/trunk/blob/5f3de6d/contrib/pljava/Dockerfile#L18">pljava needs</a> a pointer to <code>libjvm</code>:
<code>mvn clean install -Dpljava.libjvmdefault=/usr/lib/x86_64-linux-gnu/libjvm.so</code> 
 <a href="https://github.com/tembo-io/trunk/blob/5f3de6d/contrib/plrust/Dockerfile#L41-L46">plrust needs</a> files to be moved around, a shell script to be run, and to
be built from a subdirectory 
 <a href="https://github.com/tembo-io/trunk/blob/39b385d/contrib/postgresbson/Dockerfile#L15-L18">bson also needs</a> files to be moved around and a pointer to <code>libbson</code> 
 <a href="https://github.com/tembo-io/trunk/blob/39b385d/contrib/timescaledb/Dockerfile#L14">timescale requires</a> an environment variable and shell script to run
before building 
 Many extensions require patching to build for various configurations and
OSes, like <a href="https://github.com/tembo-io/trunk/blob/39b385d/contrib/pguri/Dockerfile#L15">this tweak</a> to build <a href="https://github.com/petere/pguri">pguri</a> on Postgres 17 and <a href="https://github.com/tembo-io/trunk/blob/39b385d/contrib/duckdb_fdw/Dockerfile#L18">this patch</a>
to get <a href="https://github.com/alitrack/duckdb_fdw/">duckdb_fdw</a> to build at all 
 
<p>Doubtless there’s much more. These sorts of challenges led the RPM and APT
packaging systems to support explicit scripting and patches for every package.
I don’t think it would be sensible to support build scripting in the <a href="https://github.com/pgxn/rfcs/pull/3" title="RFC: Meta Spec v2">meta
spec</a>.</p>
<p>However, the <a href="https://github.com/pgxn/meta">PGXN meta SDK</a> I developed last year supports the merging of
multiple <code>META.json</code> files, so that downstream packagers could maintain files
with additional configurations, including explicit build steps or lists of
packages, to support these use cases.</p>
<p>Furthermore, the plan to add reporting to PGXN v2 means that downstream
packages could report build failures, which would appear on PGXN, where they’d
encourage some maintainers, at least, to fix issues within their control.</p>
 Dependency Resolution 
<p>Dependencies present another challenge. The <a href="https://github.com/pgxn/rfcs/pull/3" title="RFC: Meta Spec v2">v2 spec</a> supports third
party dependencies — those not part of Postgres itself or the ecosystem of
extensions. Ideally, an extension like <a href="https://github.com/petere/pguri">pguri</a> would define its dependence on
the <a href="https://uriparser.github.io">uriparser</a> library like so:</p>
  <code>{
  "dependencies": {
    "postgres": { "version": "&gt;= 9.3" },
    "packages": {
      "build": {
        "requires": {
          "pkg:generic/uriparser": 0,
        }
      }
    }
  }
}
</code>  <p>An intelligent build client will parse the dependencies, provided as <a href="https://github.com/package-url/purl-spec" title="purl-spec: A minimal specification for purl a.k.a. a package “mostly universal” URL">purl</a>s,
to determine the appropriate OS packages to install to satisfy. For example,
building on a Debian-based system, it would know to install <code>liburiparser-dev</code>
to build the extension and require <code>liburiparser1</code> to run it.</p>
<p>With the aim to support multiple OSes and versions — not to mention Postgres
versions — the proposed PGXN binary registry would experience quite the
combinatorial explosion to support all possible dependencies on all possible
OSes and versions. While I propose to start simple (Linux and macOS, Postgres
14-18) and gradually grow, it could quickly get quite cumbersome.</p>
<p>So much so that I can practically hear <a href="https://www.df7cb.de" title="Christoph Berg">Christoph</a>’s and <a href="https://people.planetpostgresql.org/devrim/" title="Devrim Gündüz">Devrim</a>’s reactions
from here:</p>


	
			
		
			<p>Photo of Christoph, Devrim, and other long-time packagers laughing at me.</p>
		
	

<p>Or perhaps:</p>


	
			
		
			<p>Photo of Christoph and Devrim laughing at me.</p>
		
	

<p>I hardly blame them.</p>
 A CloudNativePG Side Quest 
<p><a href="https://www.gabrielebartolini.it">Gabriele Bartolini</a> blogged the <a href="https://www.gabrielebartolini.it/articles/2025/03/the-immutable-future-of-postgresql-extensions-in-kubernetes-with-cloudnativepg/" title="The Immutable Future of PostgreSQL Extensions in Kubernetes with CloudNativePG">proposal</a> to deploy extensions to
<a href="https://cloudnative-pg.io" title="Run PostgreSQL. The Kubernetes way.">CloudNativePG</a> containers without violating the immutability of the
container. The introduction of the <a href="https://github.com/postgres/postgres/commit/4f7f7b0"><code>extension_control_path</code></a> GUC in Postgres
18 and the <a href="https://kubernetes.io/docs/concepts/storage/volumes/#image">ImageVolume</a> feature in Kubernetes 1.33 enable the pattern, likely
to be introduced in CloudNativePG v1.27. Here’s a sample CloudNativePG cluster
manifest with the proposed extension configuration:</p>
  

 <code> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
</code> 

 <code>apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgresql-with-extensions
spec:
  instances: 1
  imageName: ghcr.io/cloudnative-pg/postgresql-trunk:18-devel
  postgresql:
    extensions:
      - name: vector
        image:
          reference: ghcr.io/cloudnative-pg/pgvector-18-testing
  storage:
    storageClass: standard
    size: 1Gi
</code> 
 
 <p>The <code>extensions</code> object at lines 9-12 configures <a href="https://pgxn.org/dist/vector/">pgvector</a> simply by
referencing an <a href="https://opencontainers.org" title="Open Container Initiative">OCI</a> image that contains nothing but the files for the
extension. To “install” the extension, the <a href="https://github.com/cloudnative-pg/cloudnative-pg/pull/6546" title="cloudnative-pg/cloudnative-pg#6546: feat: add support for configuring PostgreSQL extensions via Image Volume">proposed patch</a> triggers a rolling
update, replicas first. For each instance, it takes the following steps:</p>
 
 
<p>Mounts each extension as a read-only <a href="https://kubernetes.io/docs/concepts/storage/volumes/#image">ImageVolume</a> under <code>/extensions</code>; in
this example, <code>/extensions/vector</code> provides the complete contents of the
image</p>
 
 
<p>Updates <code>LD_LIBRARY_PATH</code> to include the path to the <code>lib</code> directory of
the each extension, e.g., <code>/extensions/vector/lib</code>.</p>
 
 
<p>Updates the <code>extension_control_path</code> and <a href="https://www.postgresql.org/docs/crrent/runtime-config-client.html#GUC-DYNAMIC-LIBRARY-PATH"><code>dynamic_library_path</code></a> GUCs to
point to the <code>share</code> and <code>lib</code> directories of each extension, in this
example:</p>
  <code>extension_control_path = '$system:/extensions/vector/share'
dynamic_library_path   = '$libdir:/extensions/vector/lib'
</code>   
 
<p>This works! Alas, the pod restart is absolutely necessary, whether or not any
extension requires it,<a>1</a>, because:</p>
 
 Kubernetes resolves volume mounts, including <a href="https://kubernetes.io/docs/concepts/storage/volumes/#image">ImageVolume</a>s, at pod
startup 
 The <code>dynamic_library_path</code> and <code>extension_control_path</code> GUCs require a
Postgres restart 
 Each extension requires another path to be appended to both of these GUCs,
as well as the <code>LD_LIBRARY_PATH</code> 
 
<p>Say we wanted to use five extensions. The <code>extensions</code> part of the manifest
would look something like this:</p>
  <code>extensions:
  - name: vector
    image:
      reference: ghcr.io/cloudnative-pg/pgvector-18-testing
  - name: semver
    image:
      reference: ghcr.io/example/semver:0.40.0
  - name: auto_explain
    image:
      reference: ghcr.io/example/auto_explain:18
  - name: bloom
    image:
      reference: ghcr.io/example/bloom:18
  - name: postgis
    image:
      reference: ghcr.io/example/postgis:18
</code>  <p>To support this configuration, CNPG must configure the GUCs like so:</p>
  <code>extension_control_path = '$system:/extensions/vector/share:/extensions/semver/share:/extensions/auto_explain/share:/extensions/bloom/share:/extensions/postgis/share'

dynamic_library_path   = '$libdir:/extensions/vector/lib:/extensions/semver/lib:/extensions/auto_explain/lib:/extensions/bloom/lib:/extensions/postgis/lib'
</code>  <p>And also <code>LD_LIBRARY_PATH</code>:</p>
  <code>LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/extensions/vector/lib:/extensions/semver/lib:/extensions/auto_explain/lib:/extensions/"
</code>  <p>In other words, every additional extension requires another prefix to be
appended to each of these configurations. Ideally we could use a single prefix
for all extensions, avoiding the need to update these configs and therefore to
restart Postgres. Setting aside the <a href="https://kubernetes.io/docs/concepts/storage/volumes/#image">ImageVolume</a> limitation<a>2</a> for the
moment, this pattern would require no rolling restarts and no GUC updates
unless a newly-added extension requires pre-loading via
<a href="https://www.postgresql.org/docs/17/runtime-config-client.html#GUC-SHARED-PRELOAD-LIBRARIES"><code>shared_preload_libraries</code></a>.</p>
<p>Getting there, however, requires a different extension file layout than
PostgreSQL currently uses.</p>
 RFC: Extension Packaging and Lookup 
<p>Imagine this:</p>
 
 A single extension search path GUC 
 Each extension in its own eponymous directory 
 Pre-defined subdirectory names used inside each extension directory 
 
<p>The search path might look something like:</p>
  <code>extension_search_path = '$system:/extensions:/usr/local/extensions'
</code>  <p>Looking at one of these directories, <code>/extensions</code>, its contents would be
extension directories:</p>
  <code>❯ ls -1 extensions
auto_explain
bloom
postgis
semver
vector
</code>  <p>And the contents of one these extension directories would be something like:</p>
  <code>❯ tree extensions/semver
extensions/semver
├── doc
│&nbsp;&nbsp; └── semver.md
├── lib
│&nbsp;&nbsp; └── semver.so
├── semver.control
└── sql
 &nbsp;&nbsp; ├── semver--0.31.0--0.31.1.sql
 &nbsp;&nbsp; ├── semver--0.31.1--0.31.2.sql
 &nbsp;&nbsp; ├── semver--0.31.2--0.32.0.sql
 &nbsp;&nbsp; └── semver--0.5.0--0.10.0.sql
</code>  <p>For this pattern, Postgres would look for the appropriately-named
directory with a control file in each of the paths. To find the <code>semver</code>
extension, for example, it would find <code>/extensions/semver/semver.control</code>.</p>
<p>All the other files for the extension would live in specifically-named
subdirectories: <code>doc</code> for documentation files, <code>lib</code> for shared libraries,
<code>sql</code> for SQL deployment files, plus <code>bin</code>, <code>man</code>, <code>html</code>, <code>include</code>,
<code>locale</code>, and any other likely resources.</p>
<p>With all of the files required for an extension bundled into well-defined
subdirectories of a single directory, it lends itself to the layout of the
proposed <a href="https://github.com/pgxn/rfcs/pull/2">binary distribution format</a>. Couple it with <a href="https://justatheory.com/2024/06/trunk-oci-poc/" title="POC: Distributing Trunk Binaries via OCI">OCI
distribution</a> and it becomes a natural fit for <a href="https://kubernetes.io/docs/concepts/storage/volumes/#image">ImageVolume</a> deployment:
simply map each extension OCI image to a subdirectory of the desired search
path and you’re done. The <code>extensions</code> object in the CNPG Cluster manifest
remains unchanged, and CNPG no longer needs to manipulate any GUCs.</p>
<p>Some might recognize this proposal from a <a href="https://justatheory.com/2024/11/rfc-extension-packaging-lookup/" title="RFC: Extension Packaging &amp; Lookup">previous RFC post</a>. It not only
simplifies the CloudNativePG use cases, but because it houses all of the files
for an extension in a single bundle, it also vastly simplifies installation
on any system:</p>
 
 Download the extension package 
 Validate its signature &amp; contents 
 Unpack its contents into a directory named for the extension in the
extension search path 
 
<p>Simple!</p>
 Fun With Dependencies 
<p>Many extensions depend on external libraries, and rely on the OS to find them.
OS packagers follow the dependency patterns of their packaging systems:
require the installation of other packages to satisfy the dependencies.</p>
<p>How could a pattern be generalized by the <a href="https://justatheory.com/2024/06/trunk-poc/" title="POC: PGXN Binary Distribution Format">Trunk Packaging Format</a> to work on
all OSes? I see two potential approaches:</p>
 
 List the dependencies as <a href="https://github.com/package-url/purl-spec" title="purl-spec: A minimal specification for purl a.k.a. a package “mostly universal” URL">purl</a>s that the installing client translates to
the appropriate OS packages it installs. 
 Bundle dependencies in the Trunk package itself 
 
<p>Option 1 will work well for most use cases, but not immutable systems like
<a href="https://cloudnative-pg.io" title="Run PostgreSQL. The Kubernetes way.">CloudNativePG</a>. Option 2 could work for such situations. But perhaps you
noticed the omission of <code>LD_LIBRARY_PATH</code> manipulation in the packaging and
lookup discussion above. Setting aside the multitude of reasons to avoid
<code>LD_LIBRARY_PATH</code><a>3</a>, how else could the OS find shared libraries needed by
an extension?</p>
<p>Typically, one installs shared libraries in one of a few directories known to
tools like <a href="https://www.man7.org/linux/man-pages/man8/ldconfig.8.html">ldconfig</a>, which must run after each install to cache their
locations. But one cannot rely on <code>ldconfig</code> in immutable environments,
because the cache of course cannot be mutated.</p>
<p>We could, potentially, rely on <a href="https://en.wikipedia.org/wiki/Rpath" title="Wikipedia: rpath">rpath</a>, a feature of modern dynamic linkers
that reads a list of known paths from the header of a binary file. In fact,
most modern OSes <a href="https://lekensteyn.nl/rpath.html" title="RPATH support">support</a> <code>$ORIGIN</code> as an <code>rpath</code> value<a>4</a> (or
<code>@loader_path</code> on Darwin/macOS), which refers to the same directory in which
the binary file appears. Imagine this pattern:</p>
 
 The Trunk package for an extension includes dependency libraries alongside
the extension module 
 The module is compiled with <code>rpath=$ORIGIN</code> 
 
<p>To test this pattern, let’s install the Postgres 18 beta and try the pattern
with the <a href="https://github.com/petere/pguri">pguri</a> extension. First, remove the <code>$libdir/</code> prefix (as <a href="https://justatheory.com/2025/04/update-control/" title="Update Your Control Files">discussed
previously</a>) and patch the extension for Postgres 17+:</p>
  <code>perl -i -pe 's{\$libdir/}{}' pguri/uri.control pguri/*.sql
perl -i -pe 's/^(PG_CPPFLAGS.+)/$1 -Wno-int-conversion/' pguri/Makefile
</code>  <p>Then compile it with <code>CFLAGS</code> to set <code>rpath</code> and install it with a <code>prefix</code>
parameter:</p>
  <code>make CFLAGS='-Wl,-rpath,\$$ORIGIN'
make install prefix=/usr/local/postgresql
</code>  <p>With the module installed, move the <code>liburiparser</code> shared library from OS
packaging to the <code>lib</code> directory under the prefix, resulting in these
contents:</p>
  <code>❯ ls -1 /usr/local/postgresql/lib
liburiparser.so.1
liburiparser.so.1.0.30
uri.so
</code>  <p>The <a href="https://linux.die.net/man/1/chrpath">chrpath</a> utility shows that the extension module, <code>uri.so</code>, has its
<code>RUNPATH</code> (the modern implementation of <code>rparth</code>) properly configured:</p>
  <code>❯ chrpath /usr/local/postgresql/lib/uri.so 
uri.so: RUNPATH=$ORIGIN
</code>  <p>Will the OS be able to find the dependency? Use <a href="https://linux.die.net/man/1/ldd">ldd</a> to find out:</p>
  <code>❯ ldd /usr/local/postgresql/lib/uri.so 
	linux-vdso.so.1
	liburiparser.so.1 =&gt; /usr/local/postgresql/lib/liburiparser.so.1
	libc.so.6 =&gt; /lib/aarch64-linux-gnu/libc.so.6
	/lib/ld-linux-aarch64.so.1
</code>  <p>The second line of output shows that it does in fact find <code>liburiparser.so.1</code>
where we put it. So far so good. Just need to tell the GUCs where to find them
and restart Postgres:</p>
  <code>extension_control_path = '$system:/usr/local/postgresql/share'
dynamic_library_path   = '$libdir:/usr/local/postgresql/lib'
</code>  <p>And then it works!</p>
  <code>❯ psql -c "CREATE EXTENSION uri"
CREATE EXTENSION
❯ psql -c "SELECT 'https://example.com/'::uri"
         uri          
----------------------
 https://example.com/
</code>  <p>Success! So we can adopt this pattern, yes?</p>
 A Wrinkle 
<p>Well, maybe. Try it with a second extension, <a href="https://github.com/pramsey/pgsql-http" title="HTTP client for PostgreSQL, retrieve a web page from inside the database.">http</a>, once again building it
with <code>rpath=$ORIGIN</code> and installing it in the custom lib directory:</p>
  <code>perl -i -pe 's{$libdir/}{}g' *.control
make CFLAGS='-Wl,-rpath,\$$ORIGIN'
make install prefix=/usr/local/postgresql
</code>  <p>Make sure it took:</p>
  <code>❯ chrpath /usr/local/postgresql/lib/http.so 
http.so: RUNPATH=$ORIGIN
</code>  <p>Now use <a href="https://linux.die.net/man/1/ldd">ldd</a> to see what shared libraries it needs:</p>
  <code>❯ ldd /usr/local/postgresql/lib/http.so
	linux-vdso.so.1 
	libcurl.so.4 =&gt; not found
	libc.so.6 =&gt; /lib/aarch64-linux-gnu/libc.so.6
</code>  <p>Naturally it needs <code>libcurl</code>; let’s copy it from another system and try again:</p>
  

 <code> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
</code> 

 <code>❯ scp dev:libcurl.so.4 /usr/local/postgresql/lib/
❯ ldd /usr/local/postgresql/lib/http.so
	linux-vdso.so.1
	libcurl.so.4 =&gt; /usr/local/postgresql/lib/libcurl.so.4
	libc.so.6 =&gt; /lib/aarch64-linux-gnu/libc.so.6
	/lib/ld-linux-aarch64.so.1
	libnghttp2.so.14 =&gt; not found
	libidn2.so.0 =&gt; /lib/aarch64-linux-gnu/libidn2.so.0
	librtmp.so.1 =&gt; not found
	libssh.so.4 =&gt; not found
	libpsl.so.5 =&gt; not found
	libssl.so.3 =&gt; /lib/aarch64-linux-gnu/libssl.so.3
	libcrypto.so.3 =&gt; /lib/aarch64-linux-gnu/libcrypto.so.3
	libgssapi_krb5.so.2 =&gt; /lib/aarch64-linux-gnu/libgssapi_krb5.so.2
	libldap.so.2 =&gt; not found
	liblber.so.2 =&gt; not found
	libzstd.so.1 =&gt; /lib/aarch64-linux-gnu/libzstd.so.1
	libbrotlidec.so.1 =&gt; not found
	libz.so.1 =&gt; /lib/aarch64-linux-gnu/libz.so.1
</code> 
 
 <p>Line 4 shows it found <code>libcurl.so.4</code> where we put it, but the rest of the
output lists a bunch of new dependencies that need to be satisfied. These did
not appear before because the <code>http.so</code> module doesn’t depend on them; the
<code>libcurl.so</code> library does. Let’s add <code>libnghttp2</code> and try again:</p>
  

 <code> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
</code> 

 <code>❯ scp dev:libnghttp2.so.14 /usr/local/postgresql/lib/
❯ ldd /usr/local/postgresql/lib/http.so
	linux-vdso.so.1
	libcurl.so.4 =&gt; /usr/local/postgresql/lib/libcurl.so.4
	libc.so.6 =&gt; /lib/aarch64-linux-gnu/libc.so.6
	/lib/ld-linux-aarch64.so.1
	libnghttp2.so.14 =&gt; not found
	libidn2.so.0 =&gt; /lib/aarch64-linux-gnu/libidn2.so.0
	librtmp.so.1 =&gt; not found
	libssh.so.4 =&gt; not found
	libpsl.so.5 =&gt; not found
	libssl.so.3 =&gt; /lib/aarch64-linux-gnu/libssl.so.3
	libcrypto.so.3 =&gt; /lib/aarch64-linux-gnu/libcrypto.so.3
	libgssapi_krb5.so.2 =&gt; /lib/aarch64-linux-gnu/libgssapi_krb5.so.2
	libldap.so.2 =&gt; not found
	liblber.so.2 =&gt; not found
	libzstd.so.1 =&gt; /lib/aarch64-linux-gnu/libzstd.so.1
	libbrotlidec.so.1 =&gt; not found
	libz.so.1 =&gt; /lib/aarch64-linux-gnu/libz.so.1
</code> 
 
 <p>Sadly, as line 7 shows, it still can’t find <code>libnghttp2.so</code>.</p>
<p>It turns out that <a href="https://en.wikipedia.org/wiki/Rpath" title="Wikipedia: rpath">rpath</a> works only for immediate dependencies. To solve this
problem, <code>liburl</code> and all other shared libraries must also be compiled with
<code>rpath=$ORIGIN</code> — which means we can’t simply copy those libraries from OS
packages<a>5</a>. In th meantime, only deirect dependencies could be
bundled with an extension.</p>
 Project Status 
<p>The vision of accessible, easy-install extensions everywhere remains intact.
I’m close to completing a first release of the PGXN v2 <a href="https://github.com/pgxn/build/">build SDK</a> with
support for meta spec v1 and v2, <a href="https://www.postgresql.org/docs/current/extend-pgxs.html" title="PostgreSQL Docs: Extension Building Infrastructure">PGXS</a>, and <a href="https://github.com/pgcentralfoundation/pgrx" title="pgrx: Build Postgres Extensions with Rust!">pgrx</a> extensions. I expect the
first deliverable to be a command-line client to complement and eventuallly
replace the <a href="https://pgxn.github.io/pgxnclient/">original CLI</a>. It will be put to work building all the extensions
currently distributed on <a href="http://pgxn.org/" title="PostgreSQL Extension Network">PGXN</a>, which will surface new issues and patterns
that inform the development and completion of the <a href="https://github.com/pgxn/rfcs/pull/2">v2 meta spec</a>.</p>
<p>In the future, I’d also like to:</p>
 
 Finish working out Trunk format and dependency patterns 
 Develop and submit the prroposed <code>extension_search_path</code> patch 
 Submit <a href="https://kubernetes.io/docs/concepts/storage/volumes/#image">ImageVolume</a> feedback to Kubernetes to allow runtime mounting 
 Start building and distributing OCI Trunk packages 
 Make the pattern available for distributed registries, so anyone can build
their own Trunk releases! 
 Hack fully-dynamic extension loading into <a href="https://cloudnative-pg.io" title="Run PostgreSQL. The Kubernetes way.">CloudNativePG</a> 
 
 Let’s Talk 
<p>I recognize the ambition here, but feel equal to it. Perhaps not every bit
will work out, but I firmly believe in setting a clear vision and executing
toward it while pragmatically revisiting and revising it as experience
warrants.</p>
<p>If you’d like to contribute to the project or employ me to continue working on
it, let’s talk! Hit me up via one of the services listed on the <a href="https://justatheory.com/about/" title="About Just a Theory">about page</a>.</p>
 
 
 
 
<p>The feature does not yet support pre-loading shared libraries.
Presumably a flag will be introduced to add the extension to
<a href="https://www.postgresql.org/docs/17/runtime-config-client.html#GUC-SHARED-PRELOAD-LIBRARIES"><code>shared_preload_libraries</code></a>.&nbsp;<a>↩︎</a></p>
 
 
<p>Though we should certainly <a href="https://github.com/kubernetes/enhancements/issues/4639#issuecomment-2898844498">request</a> the ability to add new
<code>ImageVolume</code> mounts without a restart. We can’t be the only ones thinking
about kind of feature, right?&nbsp;<a>↩︎</a></p>
 
 
<p>In general, one should avoid <code>LD_LIBRARY_PATH</code> for variety of
reasons, not least of which its <a href="https://blogs.oracle.com/solaris/post/ld_library_path-just-say-no" title="Oracle Solaris Blog: LD_LIBRARY_PATH - just say no">bluntness</a>. For various security reasons,
macOS ignores it unless <a href="https://support.apple.com/guide/security/system-integrity-protection-secb7ea06b49/web" title="Apple Platform Security: System Integrity Protection">sip</a> is disabled, and SELinux <a href="https://selinux.tycho.nsa.narkive.com/yQnAv3QF/policy-regarding-ld-library-path#" title="SELinux policy regarding LD_LIBRARY_PATH">prevents its
propagation</a> to new processes.&nbsp;<a>↩︎</a></p>
 
 
<p>Although not Windows, alas.&nbsp;<a>↩︎</a></p>
 
 
<p>Unless packagers could be pursuaded to build all libraries
with <code>rpath=$ORIGIN</code>, which seems like a tall order.&nbsp;<a>↩︎</a></p>
 
 
 

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/packaging/">Packaging</a> 
                 <a href="https://justatheory.com/tags/pgconf.dev/">PGConf.dev</a> 
                 <a href="https://justatheory.com/tags/cloudnativepg/">CloudNativePG</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/2025/05/extension-packaging-adventures/laughing-1.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:31 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 22. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://kyledenhartog.com/posts/'>Kyle Den Hartog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://kyledenhartog.com/why-crypto-on-web/'>Why crypto: An explanation for Improving Transactions On the Web</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Fundamentally transacting on the web is limited by credit card payment systems today
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I was recently talking with <a href="https://x.com/gnukeith">@gnukieth</a> over DM on Twitter, and he asked if he was missing something about why people perceive crypto and blockchains as bloated features in a browser. This made me realize it’s common feedback I see when working on Brave, so it’s worth detailing why I see value in Web3 extending the Web as it stands today.</p>

<p>Many users complain about the inclusion of Web3 features because they think it’s going to somehow degrade their overall browser experience. They often perceive it as an obstruction, despite being optional. To me, this is short-sighted, as features like the wallet, rewards, or .brave domains are opt-in. The one exception here is sponsored images in the background of new tabs, which primarily crypto-native companies have relied upon to gain exposure to Brave users and can be <a href="https://support.brave.com/hc/en-us/articles/360040912932-How-do-I-customize-my-New-Tab-Page">disabled</a>. However, it’s not only crypto companies who’ve used sponsored images. Ford is just one company, who has also seen the value in this top of funnel marketing ad unit. Thus, complaints about “crypto bloat” confuse me. This is akin to complaining about “accessibility bloat” due to a browser having screen reader and accessibility tools. Or labeling translation features as bloat because one only views websites in their native language. Or dismissing other features as not useful simply because one doesn’t use them, while others do. After all, this is the trade-off browser developers must assess when building software used daily by billions.</p>

<p>However, when I break down their feedback and engage with these users, I often find they are unwilling to learn why we’re using crypto in the browser to enhance the Web experience. Usually, this is because they’ve been burned by a shitcoin or know someone who has, leading them to discard the entire concept. This is a dangerous approach in my view, akin to avoiding all tech companies after a poor investment in one, like Airbnb stock. Or, worse, condemning all credit cards because their details were stolen from one site. It’s effectively throwing the baby out with the bath water.</p>

<p>Fundamentally, transacting on the web today is limited by credit card payment systems. Two examples are the Web Payments API and content creator monetization on platforms like Facebook and YouTube.</p>

<p>In the case of web payments, making a payment for a product or service on the web is often a bolted-on experience, not natively built on the <a href="https://www.w3.org/TR/payment-request/">Web Payments Request API</a>. This is because most fintech and tech companies have worked hard to route around the standard and capture payment flows to collect percentage-based fees, keeping payments as a sticky feature within their walled gardens. The ‘Apple tax’ is a prime example, where Apple mandates in-app purchases, and other e-commerce transaction methods, then charges app developers. In cases like in-app purchases, a 30% tax was charged per use. They’ve also chosen to focus Safari’s integration on Apple Pay which puts them squarely in the middle of transactions and allows them to capture purchase behaviors of their users. I’m not certain Apple is specifically using this data, but I’d guess Google does rely on this data to sell more Ads with Google Pay. Similarly, each checkout requires supplying credit card details, trusting every site not to share them and properly protect them to prevent fraud. Meanwhile, payment processors like Stripe, credit card networks like Visa and Mastercard, along with banks, collect percentage-based fees through credit card processing agreements, taxing users a small amount for every transaction. This results in a more cumbersome and expensive experience. Furthermore, credit card payment privacy is abysmal, with purchasing behavior profiles built behind the scenes to track users via their credit card number which acts as a globally unique identifier. This is a key reason tech companies like Google, Apple, and Samsung aimed to capture the client-side payment flow with services like Google Pay, Apple Pay, and Samsung Pay. Credit card purchase data is increasingly valuable to Adtech companies, data brokers, and large e-commerce sites like Amazon for promoting new products and encouraging further purchases. Knowing purchasing habits is incredibly valuable data. Therefore, the value of an alternative method to transact with crypto is to be cheaper, more convenient, more private, and more secure. Admittedly, it isn’t this today, but Rome wasn’t built in a day and there’s many of us still improving things.</p>

<p>Moreover, content creators have received unfair “deals” as platforms increasingly capture user attention. Spotify is a perfect example: musicians publish there for fan discovery. However, Spotify collects most of the ad or subscription revenue, passing little value to the musicians who sustain the platform. Platforms live off of these connections between users and creators and without them they’d be the next Myspace. The situation is similar on nearly every other content creator platform reliant on ads or subscriptions, such as YouTube, Twitter, TikTok, and even newer platforms like Substack and Patreon. The platform provides creators an audience through algorithmic marketing and users get algorithmic content discovery. They’re effectively a matching service to connect creators and viewers. In exchange, platforms capture the largest revenue share from the attention creators generate through engaging with their viewers. Additionally, creators are limited in monetizing their users. For example, a content creator currently cannot charge a one-time payment of $0.01 for content they generate. This is because credit card fees would exceed the product’s worth, causing the creator to lose money per transaction or lose users due to the cumbersome credit card checkout. This is why microtransactions haven’t materialized on the Web. Additionally, their user experience (UX) remains uncertain.</p>

<p>In summary, I see crypto’s value in transforming transactions to make interactions between buyers and sellers of content, products, or services more delightful. Even if crypto succeeds, I don’t expect it to solve all problems, nor do I expect credit card rails to disappear on the Web. However, I welcome its addition as it will force competition and innovation, rather than allowing existing networks and walled garden platforms to rest on their laurels, extracting data and fees. This is why I believe in the value of crypto on the Web. Essentially, I see crypto as the open payments network enabling this change that we didn’t get from Web Payments. Until it’s objectively a better system, however, I expect most users won’t opt-in yet, and that’s fine. Many of us see where we’re trying to take the Web to make it better. Therefore, I’m happy to continue working on crypto in the background to build a better Web, even when the grifters who scam people are making all the headlines. And I hope this helps people who don’t see the value in crypto yet to understand why it may be useful in the future to them.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   00:00 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 21. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.webistemology.com/privacy-prospectus-2025/'>Privacy Prospectus 2025</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I'm curious whether new ministers' mandates will genuinely champion Canadians' digital autonomy and privacy.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 New Ministers to Helm Privacy Legislation: Will Innovation and Data Monetisation Trump Canadians' Digital Rights? <p>As the 45th Canadian Parliament prepares to convene, I wonder who will be in charge of updating Canada's privacy legislation. This will likely fall to newly appointed ministers. I'm curious whether their mandates will genuinely champion Canadians' digital autonomy and privacy or if these fundamental rights will be subordinated to the government's commitment to fighting a trade war (including digital trade), building new trade partnerships, and supporting businesses seeking profit in the global surveillance capitalism markets for personal data.</p><p>Prime Minister Mark Carney's cabinet, revealed on May 13, 2025, has placed responsibility for the digital and industrial files, and by extension federal privacy legislation, with:</p>  The Honourable <a href="https://en.wikipedia.org/wiki/M%C3%A9lanie_Joly?ref=webistemology.com">Mélanie Joly</a>, Minister of Industry.  The Honourable <a href="https://en.wikipedia.org/wiki/Evan_Solomon?ref=webistemology.com">Evan Solomon</a>, Minister of Artificial Intelligence and Digital Innovation.  <p>While these appointments suggest a focus on modernising Canada's approach to the digital economy, there is room for scepticism about where their true priorities will lie. The emphasis on "Industry" and "Artificial Intelligence and Digital Innovation"—especially within a governmental context focused on aggressive trade strategies and fostering participation in "global surveillance capitalism markets"—raises concerns that the primary objective will be to streamline data flows for businesses and enhance Canada's competitive position in data-driven industries. Genuine privacy protections and digital autonomy for citizens could become secondary, framed perhaps as necessary but ultimately flexible conditions to enable broader economic and trade ambitions.</p><p>The division of responsibilities might create a dynamic where the push for economic leverage through data under one minister (or both) overshadows the more nuanced and rights-protective considerations. The crucial test will be in the substance of forthcoming legislation: will it prioritise empowering Canadians, or will it pave the way for more seamless data exploitation in the name of national economic interest and international trade competitiveness?</p><p>Formal oversight bodies will play their roles, but their capacity to meaningfully counter a strong governmental push towards data monetisation for trade and economic supremacy is questionable:</p>  The Office of the Privacy Commissioner of Canada (OPC): As an independent Officer of Parliament, the OPC is tasked with protecting privacy rights and overseeing federal privacy laws. However, its influence is largely through recommendations and reports. In an environment where digital trade and participation in global data markets are paramount, the OPC’s calls for robust, citizen-centric privacy safeguards could be easily downplayed or selectively implemented if perceived as impediments to the government's economic agenda.  Parliamentary Committees: Parliamentary committees will scrutinise privacy bills, though a cynic might point out that the (almost) majority of the Liberal government and the (likely) weak resistance to data monetisation that will come from the Conservatives will make that scrutiny performative rather than substantive.  A committee focused on industry and technology, likely a successor to the Standing Committee on Industry and Technology (INDU), will examine legislation. Given its traditional focus, its recommendations may heavily favour innovation and business facilitation, aligning with a strategy to boost Canada's role in digital trade and data markets, rather than championing stringent privacy rights that might complicate such goals.  The Standing Committee on Access to Information, Privacy and Ethics (ETHI) has a direct mandate over privacy. While ETHI could provide a platform for a more rights-oriented discussion, its ability to significantly alter legislation driven by strong economic and trade imperatives is limited. Its recommendations might be noted, but potentially overridden if they conflict with the overarching goal of positioning Canada favourably in a globalised, data-driven economy that often operates on principles of surveillance capitalism.    <p>Canadians have witnessed previous attempts to overhaul privacy laws, such as Bill C-27, falter. The concern now is that any new legislative initiatives from these ministries, under a government focused on trade wars and global data commerce, will be engineered primarily to serve Canada's economic and strategic trade interests. This could mean that Canadians' "digital autonomy" will be defined by what serves the state's economic strategy, rather than by genuine individual control over personal information. The upcoming throne speech and parliamentary session will reveal whether privacy will be a foundational right in Canada's digital and trade future, or a negotiable concession in pursuing broader economic and geopolitical objectives.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://www.webistemology.com/content/images/2025/05/AI-DigitalInnovation-Gemini_Generated_Image_i0f2phi0f2phi0f2.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:30 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 20. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2704'>Essential Moments in the OAuth and OpenID Timeline</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Duende Software just posted an insightful piece titled Essential Moments in the OAuth and OpenID Connect Timeline. It’s a trip down memory lane, recounting significant developments in the identity and security standards repertoire that we now take for granted. It reminds us that all of this has come about in the last 15 years. These […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://duendesoftware.com/">Duende Software</a> just posted an insightful piece titled <a href="https://blog.duendesoftware.com/posts/20250520-essential-moments-in-the-oauth-and-openid-connect-timeline/">Essential Moments in the OAuth and OpenID Connect Timeline</a>.  It’s a trip down memory lane, recounting significant developments in the identity and security standards repertoire that we now take for granted.</p>
<p>It reminds us that all of this has come about in the last 15 years.  These standards didn’t happen by accident.  They were all created to meet specific needs that we understood at the time.  Fortunately, they’ve also largely stood the test of time.  I’m proud to have been involved in creating many of them – of course, always in close collaboration with others.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/openid-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:43 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2025/05/release-on-pgxn/'>Auto-Release PostgreSQL Extensions on PGXN</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Step-by-step instructions to publish PostgreSQL extensions and utilities on
the PostgreSQL Extension Network (PGXN).
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>I last wrote about auto-releasing PostgreSQL extensions on PGXN <a href="https://justatheory.com/2020/10/release-postgres-extensions-with-github-actions/" title="Automate Postgres Extension Releases on GitHub and PGXN">back in
2020</a>, but I thought it worthwhile, following my <a href="https://pgext.day" title="Postgres Extensions Day Montréal 2025">Postgres
Extensions Day</a> talk last week, to return again to the basics. With the
goal to get as many extensions distributed on <a href="https://pgxn.org" title="PostgreSQL Extension Network">PGXN</a> as possible, this post
provides step-by-step instructions to help the author of any extension or
Postgres utility to quickly and easily publish every release.</p>
 TL;DR 
 
 Create a <a href="https://manager.pgxn.org/account/register" title="Request a PGXN Account">PGXN Manager</a> account 
 Add a <a href="https://rfcs.pgxn.org/0001-meta-spec-v1.html"><code>META.json</code></a> file to your project 
 Add a <a href="https://hub.docker.com/r/pgxn/pgxn-tools">pgxn-tools</a> powered CI/CD pipeline to publish on tag push 
 <a>Fully-document</a> your extensions 
 
 Release your extensions on PGXN 
<p><a href="https://pgxn.org" title="PostgreSQL Extension Network">PGXN</a> aims to become the defacto source for all open-source PostgreSQL
extensions and tools, in order to help users quickly find and learn how to use
extensions to meet their needs. Currently, PGXN distributes source releases
for around 400 extensions (stats on the <a href="https://pgxn.org/about/" title="About PGXN">about page</a>), a fraction of the ca.
<a href="https://gist.github.com/joelonsql/e5aa27f8cc9bd22b8999b7de8aee9d47" title="🗺🐘 1000+ PostgreSQL EXTENSIONs">1200 known extensions</a>. Anyone looking for an extension might exist to solve
some problem must rely on search engines to find potential solutions between
PGXN, GitHub, GitLab, blogs, social media posts, and more. Without a single
trusted source for extensions, and with the proliferation of <a href="https://en.wikipedia.org/wiki/AI_slop" title="Wikipedia: AI Slop">AI Slop</a> in
search engine results, finding extensions aside from a few well-known
solutions proves a challenge.</p>
<p>By publishing releases and full documentation — all fully indexed by its
search index — PGXN aims to be that trusted source. Extension authors
provide all the documentation, which PGXN formats for legibility and linking.
See, for example, the <a href="https://pgxn.org/dist/vector/README.html">pgvector docs</a>.</p>
<p>If you want to make it easier for users to find your extensions, to read your
documentation — not to mention provide sources for binary packaging systems
— publish every release on PGXN.</p>
<p>Here’s how.</p>
 Create an Account 
<p>Step one: create a <a href="https://manager.pgxn.org/account/register" title="Request a PGXN Account">PGXN Manager</a> account. The Email, Nickname, and Why
fields are required. The form asks “why” as a simple filter for bad actors.
Write a sentence describing what you’d like to release — ideally with a link
to the source repository — and submit. We’ll get the account approved
forthwith, which will send a confirmation email to your address. Follow the
link in the email and you’ll be good to go.</p>
 Anatomy of a Distribution 
<p>A PostgreSQL extension source tree generally looks something like this (taken
from the <a href="https://github.com/theory/kv-pair/">pair repository</a>):</p>
 <code>pair
├── Changes
├── doc
│&nbsp;&nbsp; └── pair.md
├── Makefile
├── META.json
├── pair.control
├── README.md
├── sql
│&nbsp;&nbsp; ├── pair--unpackaged--0.1.2.sql
│&nbsp;&nbsp; └── pair.sql
└── test
    ├── expected
    │&nbsp;&nbsp; └── base.out
    └── sql
        └── base.sql
</code> <p>Extension authors will recognize the standard <a href="https://www.postgresql.org/docs/current/extend-pgxs.html" title="PostgreSQL Docs: Extension Building Infrastructure">PGXS</a> (or <a href="https://github.com/pgcentralfoundation/pgrx" title="pgrx: Build Postgres Extensions with Rust!">pgrx</a>) source
distribution files; only <code>META.json</code> file needs explaining. The <code>META.json</code>
file is, frankly, the only file that PGXN requires in a release. It contains
the metadata to describe the release, following the <a href="https://rfcs.pgxn.org/0001-meta-spec-v1.html">PGXN Meta Spec</a>.
This example contains only the required fields:</p>
  <code>{
  "name": "pair",
  "version": "0.1.0",
  "abstract": "A key/value pair data type",
  "maintainer": "David E. Wheeler &lt;david@justatheory.com&gt;",
  "license": "postgresql",
  "provides": {
    "pair": {
      "file": "sql/pair.sql",
      "version": "0.1.0"
    }
  },
  "meta-spec": {
    "version": "1.0.0"
  }
}
</code>  <p>Presumably these fields contain no surprises, but a couple of details:</p>
 
 It starts with the name of the distribution, <code>pair</code>, and the release
version, <code>0.1.0</code>. 
 The <code>abstract</code> provides a brief description of the extension, while the
<code>maintainer</code> contains contact information. 
 The <code>license</code> stipulates the distribution license, of course, usually one
of a few known, but may be <a href="https://rfcs.pgxn.org/0001-meta-spec-v1.html#license">customized</a>. 
 The <code>provides</code> object lists the extensions or tools provided, each named
by an object key that points to details about the extension, including
main file, version, and potentially an abstract and documentation file. 
 The <code>meta-spec</code> object identifies the meta spec version used for the
<code>META.json</code> itself. 
 
 Release It! 
<p>This file with these fields is all you need to make a release. Assuming Git,
package up the extension source files like so (replacing your extension name
and version as appropriate).</p>
 <code>git archive --format zip --prefix=pair-0.1.0 -o pair-0.1.0.zip HEAD
</code> <p>Then navigate to the <a href="https://manager.pgxn.org/upload">release page</a>, authenticate, and upload the resulting
<code>.zip</code> file.</p>


	
			
	

<p>And that’s it! Your release will appear on <a href="https://pgxn.org" title="PostgreSQL Extension Network">pgxn.org</a> and on <a href="https://mastodon.social/@pgxn">Mastodon</a>
within five minutes.</p>
 Let’s Automate it! 
<p>All those steps would be a pain in the ass to follow for every release. Let’s
automate it using <a href="https://hub.docker.com/r/pgxn/pgxn-tools">pgxn-tools</a>! This OCI image contains the tools necessary to
package and upload an extension release to PGXN. Ideally, use a <a href="https://en.wikipedia.org/wiki/CI/CD" title="Wikipedia: CI/CD">CI/CD</a>
pipeline like a <a href="https://docs.github.com/en/actions/writing-workflows">GitHub Workflow</a> to publish a release on every version tag.</p>
 Set up Secrets 
<p><a href="https://hub.docker.com/r/pgxn/pgxn-tools">pgxn-tools</a> uses your PGXN credentials to publish releases. To keep them
safe, use the secrets feature of your preferred CI/CD tool. This figure shows
the “Secrets and variables” configuration for a GitHub repository, with two
repository secrets: <code>PGXN_USERNAME</code> and <code>PGXN_PASSWORD</code>:</p>


	
			
	

 Create a Pipeline 
<p>Use those secrets and <a href="https://hub.docker.com/r/pgxn/pgxn-tools">pgxn-tools</a> in CI/CD pipeline. Here, for example, is a
minimal GitHub workflow to publish a release for every <a href="https://semver.org" title="Semantic Versioning 2.0.0">SemVer</a> tag:</p>
  

 <code> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
</code> 

 <code>on:
  push:
    tags: ['v[0-9]+.[0-9]+.[0-9]+']
jobs:
  release:
    name: Release on PGXN
    runs-on: ubuntu-latest
    container: pgxn/pgxn-tools
    env:
      PGXN_USERNAME: ${{ secrets.PGXN_USERNAME }}
      PGXN_PASSWORD: ${{ secrets.PGXN_PASSWORD }}
    steps:
    - name: Check out the repo
      uses: actions/checkout@v4
    - name: Bundle the Release
      run: pgxn-bundle
    - name: Release on PGXN
      run: pgxn-release
</code> 
 
 <p>Details:</p>
 
 Line 3 configures the workflow to run on a <a href="https://semver.org" title="Semantic Versioning 2.0.0">SemVer</a> tag push, typically
used to denote a release. 
 Line 8 configures the workflow job to run inside a <a href="https://hub.docker.com/r/pgxn/pgxn-tools">pgxn-tools</a> container. 
 Lines 10-11 set environment variables with the credentials from the
secrets. 
 Line 16 bundles the release using either <code>git archive</code> or <code>zip</code>. 
 Line 18 publishes the release on PGXN. 
 
<p>Now publishing a new release is as simple as pushing a <a href="https://semver.org" title="Semantic Versioning 2.0.0">SemVer</a> tag, like so:</p>
  <code>git tag v0.1.0 -sm 'Tag v0.1.0'
git push --follow-tags
</code>  <p>That’s it! The workflow will automatically publish the extension for every
release, ensuring the latest and greatest always make it to PGXN where users
and packagers will find them.</p>
<p>The <a href="https://hub.docker.com/r/pgxn/pgxn-tools">pgxn-tools</a> image also provides tools to easily test a <a href="https://www.postgresql.org/docs/current/extend-pgxs.html" title="PostgreSQL Docs: Extension Building Infrastructure">PGXS</a> or <a href="https://github.com/pgcentralfoundation/pgrx" title="pgrx: Build Postgres Extensions with Rust!">pgrx</a>
extension on supported PostgreSQL versions (going back as far as 8.2), also
super useful in a CI/CD pipeline. See <a href="https://justatheory.com/2020/06/test-extensions-with-github-actions/">Test Postgres Extensions With GitHub
Actions</a> for instructions. Depending on your CI/CD tool of choice, you might
take additional steps, such as publishing a release on GitHub, as <a href="https://justatheory.com/2020/10/release-postgres-extensions-with-github-actions/" title="Automate Postgres Extension Releases on GitHub and PGXN">previously
described</a>.</p>
 Optimizing for PGXN 
<p>But let’s dig deeper into how to optimize extensions for maximum
discoverability and user visibility on <a href="https://pgxn.org" title="PostgreSQL Extension Network">PGXN</a>.</p>
 Add More Metadata 
<p>The <code>META.json</code> file supports many more fields that PGXN indexes and
references. These improve the chances users will find what they’re looking
for. This detailed example demonstrates how a <a href="https://postgis.net" title="PostGIS">PostGIS</a> <code>META.json</code> file might
start to provide additional metadata:</p>
  

 <code> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
</code> 

 <code>{
   "name": "postgis",
   "abstract": "Geographic Information Systems Extensions to PostgreSQL",
   "description": "This distribution contains a module which implements GIS simple features, ties the features to R-tree indexing, and provides many spatial functions for accessing and analyzing geographic data.",
   "version": "3.5.0",
   "maintainer": [
      "Paul Ramsey &lt;pramsey@example.com&gt;",
      "Sandro Santilli &lt;sandro@examle.net&gt;"
   ],
   "license": [ "gpl_2", "gpl_3" ],
   "provides": {
      "postgis": {
         "abstract": "PostGIS geography spatial types and functions",
         "file": "extensions/postgis/postgis.control",
         "docfile": "extensions/postgis/doc/postgis.md",
         "version": "3.5.0"
      },
      "address_standardizer": {
         "abstract": "Used to parse an address into constituent elements. Generally used to support geocoding address normalization step.",
         "file": "extensions/address_standardizer/address_standardizer.control",
         "docfile": "extensions/address_standardizer/README.address_standardizer",
         "version": "3.5.0"
      }
   },
   "prereqs": {
      "runtime": {
         "requires": {
            "PostgreSQL": "12.0.0",
            "plpgsql": 0
         }
      },
      "test": {
         "recommends": {
            "pgTAP": 0
         }
      }
   },
   "resources": {
      "bugtracker": {
         "web": "https://trac.osgeo.org/postgis/"
      },
      "repository": {
         "url": "https://git.osgeo.org/gitea/postgis/postgis.git",
         "web": "https://git.osgeo.org/gitea/postgis/postgis",
         "type": "git"
      }
   },
   "generated_by": "David E. Wheeler",
   "meta-spec": {
      "version": "1.0.0",
      "url": "https://pgxn.org/meta/spec.txt"
   },
   "tags": [
      "gis",
      "spatial",
      "geometry",
      "raster",
      "geography",
      "location"
   ]
}
</code> 
 
  
 Line 4 contains a longer description of the distribution. 
 Lines 6-9 show how to list multiple maintainers as an array. 
 Line 10 demonstrates support for an array of licenses. 
 Lines 11-24 list multiple extensions included in the distribution, with
abstracts and documentation files for each. 
 Lines 25-37 identify dependencies for various phases of the distribution
lifecycle, including configure, build, test, runtime, and develop. Each
contains an object identifying PostgreSQL or extension dependencies. 
 Lines 38-47 lists resources for the distribution, including issue
tracking and source code repository. 
 Lines 53-60 contains an array of tags, an arbitrary list of keywords for a
distribution used both in the search index and the <a href="https://pgxn.org/tags" title="PGXN Release Tags">PGXN tag cloud</a>. 
 
<p>Admittedly the <a href="https://rfcs.pgxn.org/0001-meta-spec-v1.html">PGXN Meta Spec</a> provides a great deal of information.
Perhaps the simplest way to manage it is to copy an existing <code>META.json</code> from
another project (or above) and edit it. In general, only the <code>version</code> fields
require updating for each release.</p>
 Write Killer Docs 
<p>The most successful extensions provide ample descriptive and reference
documentation, as well as examples. Most extensions feature a README, of
course, which contains basic information, build and install instructions, and
contact info. But as the <a>pair tree</a>, illustrates,
PGXN also supports extension-specific documentation in a variety of formats,
including:</p>
 
 <a href="https://asciidoc.org">Asciidoc</a> 
 <a href="https://www.bbcode.org">BBcode</a> 
 <a href="https://www.wikicreole.org">Creole</a> 
 <a href="https://whatwg.org/html">HTML</a> 
 <a href="https://daringfireball.net/projects/markdown/">Markdown</a> 
 <a href="https://en.wikipedia.org/wiki/Help:Contents/Editing_Wikipedia">MediaWiki</a> 
 <a href="https://fletcherpenney.net/multimarkdown/">MultiMarkdown</a> 
 <a href="https://metacpan.org/dist/perl/view/pod/perlpodspec.pod">Pod</a> 
 <a href="https://docutils.sourceforge.io/rst.html">reStructuredText</a> 
 <a href="https://textile-lang.com">Textile</a> 
 <a href="https://trac.edgewall.org/wiki/WikiFormatting">Trac</a> 
 
<p>Some examples:</p>
 
 <a href="https://github.com/theory/pg-jsonschema-boon/blob/main/doc/jsonschema.md">jsonschema</a> (Markdown) 
 <a href="https://github.com/theory/pg-semver/blob/main/doc/semver.mmd">semver</a>
(MultiMarkdown) 
 
<p>PGXN will also index and format additional documentation files in any of the
above formats. See, for example, all the files formatted for <a href="https://pgxn.org/dist/orafce/">orafce</a>.</p>
 Exclude Files from Release 
<p>Use <a href="https://git-scm.com/docs/gitattributes">gitattributes</a> to exclude files from the release. For example,
distributions don’t generally include <code>.gitignore</code> or the contents of the
<code>.github</code> directory. Exclude them from the archive created by <code>git archive</code> by
assigning <code>export-ignore</code> to each path to exclude in the <code>.gitattributes</code>
file, like so:</p>
 <code>.gitignore export-ignore
.gitattributes export-ignore
.github export-ignore
</code>  What’s It All For? 
<p><a href="https://pgxn.org" title="PostgreSQL Extension Network">PGXN</a> aims to be the trusted system of record for open-source PostgreSQL
extensions. Of course that requires that it contain all (or nearly all) of
said extensions. Hence this post.</p>
<p>Please help make it so by adding your extensions, both to help users find the
extensions they need, and to improve the discoverability of your extensions.
Over time, we aim to feed downstream extension distribution systems, such as
<a href="https://yum.postgresql.org" title="PostgreSQL Yum Repository">Yum</a>, <a href="https://wiki.postgresql.org/wiki/Apt" title="The PostgreSQL Wiki: “Apt”">APT</a>, <a href="https://cloudnative-pg.io" title="Run PostgreSQL. The Kubernetes way.">CloudNativePG</a>, <a href="https://justatheory.com/2024/06/trunk-oci-poc/" title="POC: Distributing Trunk Binaries via OCI">OCI</a>, and more.</p>
<p>Let’s make extensions available everywhere to everyone.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extension/">Extension</a> 
                 <a href="https://justatheory.com/tags/github/">GitHub</a> 
                 <a href="https://justatheory.com/tags/github-actions/">GitHub Actions</a> 
                 <a href="https://justatheory.com/tags/automation/">Automation</a> 
                 <a href="https://justatheory.com/tags/ci/cd/">CI/CD</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/2025/05/release-on-pgxn/upload-to-pgxn.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:49 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 19. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2025/05/19/using-multiple-external-identity-providers-from-asp-net-core-identity-and-duende-identityserver/'>Using multiple external identity providers from ASP.NET Core Identity and Duende IdentityServer</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This blog post shows how an ASP.NET Core Identity application can integrate and implement multiple external identity providers. An OIDC client UI uses the solution and is implemented using Duende IdentityServer. The same scheme is used for all the external providers and mapped to the identity for the client UI and the application. Using OpenID […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This blog post shows how an ASP.NET Core Identity application can integrate and implement multiple external identity providers. An OIDC client UI uses the solution and is implemented using Duende IdentityServer. The same scheme is used for all the external providers and mapped to the identity for the client UI and the application. Using OpenID Connect this is returned to the web application in tokens or the user profile API.</p>



<p>Code: <a href="https://github.com/damienbod/DuendeProfileServiceAspNetCoreIdentity">https://github.com/damienbod/DuendeProfileServiceAspNetCoreIdentity</a></p>



<p></p>



 Setup 



<p>The application is used as an identity provider. This can be used for local users or for external users using OpenID Connect federation. All applications using the application are separated from the further authentication systems. By using Duende, it is possible to use the high end OAuth an OpenID Connect authentication flows which are not supported by some of the other well known identity providers. It would also be possible to use OpenIddict in this setup. The users of the server authenticate using OpenID Connect. The claims need to be mapped as well as each of the external authentication providers. The Identity Callback UI is used to handle all of the external authentication flow results. The claims from each external authentication are different and need to be mapped to the claims used in the closed system.</p>







<p></p>



 External providers 



<p>When implementing external authentication providers in ASP.NET Core Identity, different strategies can be used. Each external provider uses a separate scheme for the OpenID Connect flow. On a successful result , the identity can be persisted to a common external identity session or each one can use a unique scheme. Both have advantages and disadvantages. If all use the same, the logout and callback scheme logic can be simple and the claims mapping are implemented on a per provider logic. If separate schemes are used for each provider, the callback and the logout require scheme logic and checks.</p>



<p>In this demo, we follow the recommendation from the Duende samples and use one scheme to persist the session for all external providers. Each external provider MUST use specific URLs for the authentication flow, otherwise the state and the flows will not work as the different providers break. </p>


  builder.Services.AddAuthentication(options =&gt;
{
   options.DefaultAuthenticateScheme = IdentityConstants.ApplicationScheme;
   options.DefaultChallengeScheme = IdentityConstants.ApplicationScheme;
   options.DefaultSignInScheme = IdentityConstants.ExternalScheme;
})
.AddOpenIdConnect("Auth0Scheme", "Auth0", options =&gt;
{
   // SignInScheme must match the scheme(s) used in the Identity callback
   options.SignInScheme = IdentityServerConstants.ExternalCookieAuthenticationScheme;
   options.SignOutScheme = IdentityConstants.ApplicationScheme;
   
   // paths must be different for each client
   options.CallbackPath = new PathString("/signin-oidc-auth0");
   options.RemoteSignOutPath = new PathString("/signout-callback-oidc-auth0");
   options.SignedOutCallbackPath = new PathString("/signout-oidc-auth0");

   // more oidc options ...
   };
})
.AddOpenIdConnect("EntraID", "EntraID", oidcOptions =&gt;
{
   builder.Configuration.Bind("AzureAd", oidcOptions);
   oidcOptions.SignInScheme = IdentityServerConstants.ExternalCookieAuthenticationScheme;
   oidcOptions.SignOutScheme = IdentityConstants.ApplicationScheme;

   oidcOptions.CallbackPath = new PathString("/signin-oidc-entraid");
   oidcOptions.RemoteSignOutPath = new PathString("/signout-callback-oidc-entraid");
   oidcOptions.SignedOutCallbackPath = new PathString("/signout-oidc-entraid");

   // more oidc options ...
});
  


<p></p>



 Using Microsoft.Identity.Web 



<p>If using the Microsoft.Identity.Web Nuget packages to implement the external provider logic, a new separate scheme is required for the handling of the callback and logout because the AddMicrosoftIdentityWebApp extension method creates it’s own scheme and cannot re-use the default scheme defined by Identity. The scheme would then require implementation logic in the callback UI and the logout logic.</p>



 Duende IProfileService 



<p>If using ASP.NET Core Identity together with an OpenID Connect provider like Duende IdentityServer or OpenIddict, the claims from the different external providers need to be mapped back to the claims used by the different UI applications. In Duende, the claims can be mapped using the IProfileService. See the Duende <a href="https://docs.duendesoftware.com/identityserver/reference/services/profile-service">documentation</a> for this. The GetProfileDataAsync is can be called multiple times for each successful authentication of a UI application, each time for a different claims type. What is used depends on the OpenID Connect client setup. You should avoid adding claims multiple times for the same value and avoid added too many claims to the identity token. The mapping should work in the same way for identity token mapping or when the client uses the user info endpoint. </p>



<p>When using many client applications, you should aim for standard claims and not use different claims depending on the multiple downstream external authentication providers.</p>


  public class ProfileService: IProfileService
{
	public async Task GetProfileDataAsync(ProfileDataRequestContext context)
	{
        // context.Subject is the user for whom the result is being made
        // context.Subject.Claims is the claims collection from the user's session cookie at login time
        // context.IssuedClaims is the collection of claims that your logic has decided to return in the response

        if (context.Caller == IdentityServerConstants.ProfileDataCallers.ClaimsProviderAccessToken)
        {
            // access_token
        }
        if (context.Caller == IdentityServerConstants.ProfileDataCallers.ClaimsProviderIdentityToken)
        {
            // id_token
            var oid = context.Subject.Claims.FirstOrDefault(t =&gt; t.Type == "oid");
            if(oid != null)
            {
                context.IssuedClaims.Add(new Claim("oid", oid.Value));
            }
        }
        if (context.Caller == IdentityServerConstants.ProfileDataCallers.UserInfoEndpoint)
        {
            // user_info endpoint 
        }

        // ALL
        context.IssuedClaims.Add(new Claim("test", "A"));
        return;
	}
  


<p></p>



 Mapping claims in Identity only solutions 



<p>If not using an OIDC server and only using ASP.NET Core Identity, a ClaimsTransformation can be implemented to map the claims. </p>



 Links 



<p><a href="https://docs.duendesoftware.com/identityserver/reference/services/profile-service">https://docs.duendesoftware.com/identityserver/reference/services/profile-service</a></p>



<p><a href="https://duendesoftware.com/products/identityserver">https://duendesoftware.com/products/identityserver</a></p>



<p><a href="https://learn.microsoft.com/en-us/aspnet/core/security/authentication/identity">https://learn.microsoft.com/en-us/aspnet/core/security/authentication/identity</a></p>



<p><a href="https://learn.microsoft.com/en-us/aspnet/core/security/authentication/claims">https://learn.microsoft.com/en-us/aspnet/core/security/authentication/claims</a></p>



<p><a href="https://github.com/damienbod/MulitipleClientClaimsMapping">https://github.com/damienbod/MulitipleClientClaimsMapping</a></p>



<p><a href="https://learn.microsoft.com/en-us/aspnet/core/security/authentication/social/">https://learn.microsoft.com/en-us/aspnet/core/security/authentication/social/</a></p>



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2025/05/overview.drawio-1.png?w=630'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   06:25 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 19. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2025/05/mini-summit-cnpg/'>Mini Summit 5 Transcript: Improving the PostgreSQL Extensions Experience in Kubernetes with CloudNativePG</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  At the final Mini-Summit of 2025, Gabriele Bartolini gave an overview of
PostgreSQL extension management in CloudNativePG.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	
			
	

         
<p>The final PostgresSQL Extension Mini-Summit took place on May 7. <a href="https://www.gabrielebartolini.it">Gabriele
Bartolini</a> gave an overview of PostgreSQL extension management in
<a href="https://cloudnative-pg.io" title="Run PostgreSQL. The Kubernetes way.">CloudNativePG</a> (CNPG). This talk brings together the topics of several
previous Mini-Summits — notably <a href="https://peter.eisentraut.org">Peter Eisentraut</a> on <a href="https://justatheory.com/2025/04/mini-summit-two/" title="2025 Postgres Extensions Mini Summit Two">implementing an
extension search path</a> — to look at the limitations of extension
support in CloudNativePG and the possibilities enabled by the extension search
path feature and the Kubernetes 1.33 <a href="https://kubernetes.io/docs/concepts/storage/volumes/#image">ImageVolume</a> feature. Check it out:</p>
 
 <a href="https://www.youtube.com/watch?v=LbNuUs59j2I">Video</a> 
 <a href="https://justatheory.com/2025/05/mini-summit-cnpg/postgres-operand-image-future.pdf">PDF Slides</a> 
 
<p>Or read on for the full transcript with thanks to <a href="https://dev.to/@floord">Floor Drees</a> for putting it
together.</p>
 Introduction 
<p>Floor Drees.</p>
<p>On May 7 we hosted the last of five (5) virtual Mini-Summits that lead up to
the big one at the Postgres Development Conference (PGConf.Dev), taking place
next week, in Montreal, Canada. <a href="https://www.gabrielebartolini.it">Gabriele Bartolini</a>, <a href="https://cloudnative-pg.io" title="Run PostgreSQL. The Kubernetes way.">CloudNativePG</a>
maintainer, PostgreSQL Contributor, and VP Cloud Native at <a href="https://enterprisedb.com" title="EnterpriseDB">EDB</a>, joined to
talk about improving the Postgres extensions experience in Kubernetes with
CloudNativePG.</p>
<p>The organizers:</p>
 
 <a href="https://justatheory.com/">David Wheeler</a>, Principal Architect at <a href="https://tembo.io/">Tembo</a>, maintainer of <a href="https://pgxn.org/">PGXN</a> 
 <a href="https://ca.linkedin.com/in/yrashk">Yurii Rashkovskii</a>, <a href="https://omnigres.com/">Omnigres</a> 
 <a href="https://pgxn.org/user/keithf4/">Keith Fiske</a>, <a href="https://www.crunchydata.com/">Crunchy Data</a> 
 <a href="https://dev.to/@floord">Floor Drees</a>, Principal Program Manager at <a href="https://enterprisedb.com" title="EnterpriseDB">EDB</a>, PostgreSQL CoCC member,
PGDay Lowlands organizer 
 
<p>The stream and the closed captions available for the recording are supported
by <a href="https://2025.pgconf.dev" title="PostgreSQL Development Conference 2025">PGConf.dev</a> and their gold level sponsors, Google, AWS, Huawei, Microsoft,
and EDB.</p>
 Improving the Postgres extensions experience in Kubernetes with CloudNativePG 
<p>Gabriele Bartolini.</p>
<p>Hi everyone. Thanks for this opportunity, and thank you Floor and David for
inviting me today.</p>
<p>I normally start every presentation with a question, and this is actually the
question that has been hitting me and the other maintainers of <a href="https://cloudnative-pg.io" title="Run PostgreSQL. The Kubernetes way.">CloudNativePG</a>
— and some are in this call — from the first day. We know that extensions
are important in Kubernetes, in Postgres, and we’ve always been asking how can
we deploy extensions, without breaking the immutability of the container.</p>
<p>So today I will be telling basically our story, and hopefully providing good
insights in the future about how with CloudNativePG we are trying to improve
the experience of Postgres extensions when running databases, including
issues.</p>
<p>I’ve been using Postgres for 25 years. I’m one of the co-founders of
2ndQuadrant, which was bought by a <a href="https://enterprisedb.com" title="EnterpriseDB">EDB</a> in 2020. And because of my
contributions, I’ve been recognized as a Postgres contributor and I’m really
grateful for that. And I’m also “Data on Kubernetes ambassador”; my role is to
promote the usage of stateful workloads in Kubernetes. I’m also DevOps
evangelist. I always say this: DevOps is the reason why I encountered
Kubernetes, and it will also be the reason why I move away one day from
Kubernetes. It’s about culture and I’ll explain this later.</p>
<p>In the past I’ve been working with <a href="https://pgbarman.org">Barman</a>; I’m one of the creators of
Barman. And since 2022, I’m one of the maintainers of <a href="https://cloudnative-pg.io" title="Run PostgreSQL. The Kubernetes way.">CloudNativePG</a>. I want
to thank my company, <a href="https://enterprisedb.com" title="EnterpriseDB">EDB</a>, for being the major contributor in Postgres
history in terms of source code. And right now we are also the creators of
CloudNativePG. And as we’ll see, the company donated the IP to the <a href="https://www.cncf.io">CNCF</a>. So
it’s something that is quite rare, and I’m really grateful for that.</p>
<p>What I plan to cover tonight is first, set the context and talk about
immutable application containers, which have been kind of a dogma for us from
day one. Then, how we are handling right now extensions in Kubernetes with
CNPG. This is quite similar to the way other operators deal with it. Then the
future and key takeaways.</p>
<p>First, we’re talking about Kubernetes. If you’re not familiar, it’s an
orchestration system for containers. It’s not just an executor of containers,
but it’s a complex system that also manages infrastructure. When it manages
infrastructure, it also manages cloud native applications that are also called
workloads. When we’re thinking about Postgres in Kubernetes, the database is a
workload like the others. That, I think, is the most important mind shift
among Postres users that I have faced myself, that I’ve always treated
Postgres differently from the rest. Here in Kubernetes is it’s just another
workload.</p>
<p>Then of course, it’s not like any other workload, and that’s where operators
come into play, and I think the work that we are doing even tonight is in the
direction to improve how databases is run in Kubernetes in general, and for
everyone.</p>
<p>It was open sourced in 2014, and, it’s owned by the <a href="https://www.cncf.io">CNCF</a>, and it’s actually
the first project that graduated, and graduated is the most advanced stage in
the graduation process of the CNCF, which starts with sandbox, then incubation
and then graduation.</p>
<p>CloudNativePG is an operator for Postgres. It’s production-ready — what we
say is level five. Level five is kind of an utopic, and unbounded level, the
highest one as defined by the operator development framework. It’s used by all
these players including Tembo, IBM Cloud Paks, Google Cloud, Azure, Akamai,
and so on. CNPG is a CNCF project since January. It’s distributed under Apache
License 2.0 and the IP — the Intellectual Property — is owned by the
community and protected by the CNCF. It therefore is a vendor neutral and
openly governed project. This is kind of a guarantee that it will always be
free. This is also, in my opinion, a differentiation between CloudNativePG and
the rest.</p>
<p>The project was originally created by EDB, but specifically at that time, by
2ndQuadrant. And, as I always like to recall, it was Simon Riggs that put me
in charge of the initiative. I’ll always be grateful to Simon, not only for
that, but for everything he has done for me and the team.</p>
<p>CNPG can be installed in several ways. As you can see, it’s very popular in
terms of stars. There’s more than 4,000 commits. And what’s impressive is the
number of downloads in three years, which is 78 million, which means that it’s
used the way we wanted it to be used: with CICD pipelines.</p>
<p>This is the <a href="https://landscape.cncf.io">CNCF landscape</a>; these are the CNCF projects. As you can see,
there are only five projects in the CNCF in the database area, and
CloudNativePG is the only one for Postgres. Our aim for 2025 and 2026 is to
become incubating. If you’re using CNPG and you want to help with the process,
get in touch with me and Floor.</p>
<p>I think to understand again, what, why we’ve done all this process, that led
to the patch that, you’ve seen in Postgres 18, it’s important to understand
what cloud native has meant to us since we started in 2019. We’ve got our own
definition, but I think it still applies. For us it’s three things, Cloud
native. It’s people that work following DevOps culture. For example, there are
some capabilities that come from DevOps that apply to the cloud native world.
I selected some of them like in user infrastructure, infrastructure
abstraction, version control. These three form the infrastructure-as-code
principle, together with the declarative configuration.</p>
<p>A shift left on security. You’ll see with CloudNativePG, we rarely mention
security because it’s pretty much everywhere. It’s part of the process. Then
continuous delivery.</p>
<p>The second item is immutable application containers, which kind of led the
immutable way of thinking about extensions. And then the third one is that
these application containers must be orchestrated via an
infrastructure-as-code by an orchestrator, and the standard right now is
Kubernetes.</p>
<p>For us it’s these three things, and without any of them, you cannot achieve
cloud native.</p>
<p>So what are these immutable application containers? To explain immutability
I’d like to talk about immutable infrastructure, which is probably what the
majority of people that have historically worked with Postgres are used to.
I’m primarily referring to traditional environments like VMs and bare metal
where the main ways we deploy Postgres is through packages, maybe even managed
by configuration managers, but still, packages are the main artifacts. The
infrastructure is seen as a long-term kind of project. Changes happen over
time and are incremental updates, updates on an existing infrastructure. So if
you want to know the history of the infrastructure over time, you need to
check all the changes that have applied. In case of failure of a system,
systems are healed. So that’s the <a href="https://cloudscaling.com/blog/cloud-computing/the-history-of-pets-vs-cattle/">pets</a> concept that comes from DevOps.</p>
<p>On the other hand, immutable infrastructure relies on <a href="https://opencontainers.org">OCI</a> container images.
OCI is a standard, the <a href="https://opencontainers.org">Open Container Initiative</a> and it’s part of the
Linux Foundation as well. Immutable infrastructure is founded on continuous
delivery, which is the foundation of <a href="https://www.gitops.tech">GitOps</a> practices. In an immutable
infrastructure, releasing a new version of an application is not updating the
system’s application, it is building a new image and publishing it on a public
registry and then deploying it. Changes in the system happen in an atomic way:
the new version of a container is pulled from the registry and the existing
image is almost instantaneously replaced by the new one. This is true for
stateless applications and we’ll see, in the case of stateful applications
like Postgres, is not that instantaneous because we need to perform a
switchover or restart — in any case, generate a downtime.</p>
<p>When it comes to Kubernetes, the choice was kind of obvious to go towards that
immutable infrastructure. So no incremental updates, and in the case of
stateful workloads where you cannot change the content of the container, you
can use data volumes or persistent volumes. These containers are not changed.
If you want to change even a single file or a binary in a container image, you
need to create a new one. This is very important for security and change
management policies in general.</p>
<p>But what I really like about this way of managing our infrastructure is that,
at any time, Kubernetes knows exactly what software is running in your
infrastructure. All of this is versioned in an <a href="https://en.wikipedia.org/wiki/Change_control">SCM</a>, like Git or whatever.
This is something that in the mutable world is less easy to obtain. Again, for
security, this is the foundational thing because this is how you can control
<a href="https://cve.mitre.org">CVEs</a>, the vulnerabilities in your system. This is a very basic
representation of how you build, contain — let’s say the lifecycle of a
container image. You create a <code>Dockerfile</code>, you put it in Git, for example,
then there’s an action or a pipeline that creates the container image, maybe
even run some tests and then pushes it to the container registry.</p>
<p>I walked you through the concepts of mutable and immutable containers, what
are, these immutable application containers? If you go back and read what we
were rising before CloudNativePG was famous or was even used, we were always
putting in immutable application containers as one of the principles we could
not lose.</p>
<p>For an immutable application container, it means that there’s only a single
application running; that’s why it’s called “application”. If you have been
using Docker, you are more familiar with system containers: you run a Debian
system, you just connect and then you start treating it like a VM. Application
containers are not like that. And then they are immutable — read-only — so
you cannot even make any change or perform updates of packages. But in
CloudNativePG, because we are managing databases, we need to put the database
files in separate persistent volumes. Persistent volumes are standard
resources provided by Kubernetes. This is where we put PGDATA and, if you
want, a separate volume for WAL files with different storage specifications
and even an optional number of table spaces.</p>
<p>CloudNativePG orchestrates what we call “operand images”. These are very
important to understand. They contain the Postgres binaries and they’re
orchestrated via what we call the “instance manager”. The instance manager is
just the process that runs and controlled Postgres; I’ss the PID 1 — or the
entry point — of the container.</p>
<p>There’s no other, like SSHD or other, other applications work. There’s just
the instance manager that then controls everything else. And this is the
project of the operating images. This is one open source project, and every
week we rebuild the Postgres containers. We recently made some changes to the
flavors of these images and I’ll talk about it shortly.</p>
<p>We mentioned the database, we mentioned the binaries, but what about
extensions? This is the problem. Postgres extensions in Kubernetes with
CloudNativePG is the next section, and it’s kind of a drama. I’m not hiding
this. The way we are managing extensions in Kubernetes right now, in my
opinion, is not enough. It works, but it’s got several limitations — mostly
limitations in terms of usage.</p>
<p>For example, we cannot place them in the data files or in persistent volumes
because these volumes are not read-only in any way. In any case, they cannot
be strictly immutable. So we discarded this option to have persistent volume
where you could kind of deploy extensions and maybe you can even download on
the fly or use the package manager to download them or these kind of
operations. We discarded this from the start and we embraced the operand image
solution. Essentially what we did was placing these extensions in the same
operand image that contains the Postgres binaries. This is a typical approach
of also the other operators. If you think about also <a href="https://github.com/zalando/postgres-operator">Zalando</a> we call it “the
Spilo way”. Spilo contained all the software that would run with the Zalando
operator.</p>
<p>Our approach was a bit different, in that we wanted lighter images, so we
created a few flavors of images, and also selected some extensions that we
placed in the images. But in general, we recommended to build custom images.
We provided instructions and we’ve also provided the requirements to build
container images. But as you can see, the complexity of the operational layer
is quite high, it’s not reasonable to ask any user or any customer to build
their own images.</p>
<p>This is how they look now, although this is changing as I was saying:</p>


	
			
	

<p>You’ve got a base image, for example, the Debian base image. You deploy the
Postgres binaries. Then — even right now though it’s changing —
CloudNativePG requires Barman Cloud to be installed. And then we install the
extensions that we think are needed. For example, I think we distribute
<a href="https://pgaudit.org/">pgAudit</a>, if I recall correctly, <a href="https://pgxn.org/dist/vector/">pgvector</a> and <a href="https://github.com/EnterpriseDB/pg_failover_slots">pg_failover_slots</a>. Every
layer you add, of course, the image is heavier and we still rely on packages
for most extensions.</p>
<p>The problem is, you’ve got a cluster that is already running and you want, for
example, to test an extension that’s just come out, or you want to deploy it
in production. If that extension is not part of the images that we build, you
have to build your own image. Because of the possible combinations of
extensions that exist, it’s impossible to build all of these combinations. You
could build, for example, a system that allows you to select what extensions
you want and then build the image, but in our way of thinking, this was not
the right approach. And then you’ve got system dependencies and, if an
extension brings a vulnerability that affects the whole image and requires
more updates — not just of the cluster, but also of the builds of the image.</p>
<p>We wanted to do something else, but we immediately faced some limitations of
the technologies. One  was on Postgres, the other one was on Kubernetes. In
Postgres, extensions need to be placed in a single folder. It’s not possible
to define multiple locations, but thanks to the work that Peter and this team
have done, <a href="https://justatheory.com/2025/04/mini-summit-two/" title="2025 Postgres Extensions Mini Summit Two">now we’ve got <code>extension_control_path</code> in version 18</a>.</p>
<p>Kubernetes could not allow until, 10 days ago, to mount OCI artifacts as
read-only volumes. There’s a new feature that is now part of Kubernetes 1.33
that allows us to do it.</p>
<p>This is <a href="https://github.com/postgres/postgres/commit/4f7f7b0">the patch</a> that I was talking about, by <a href="https://peter.eisentraut.org">Peter Eisentraut</a>. I’m
really happy that CloudNativePG is mentioned as one of the use cases.
And there’s also mentioned for the work that, me, David, and Marco and,
primarily Marco and Niccolò from CloudNativePG have done.</p>
<p>This is <a href="https://github.com/kubernetes/enhancements/issues/4639">the patch</a> that introduced VolumeSource in Kubernetes 1.33.</p>
<p>The idea is that with Postgres 18 now we can set in the configuration where we
can look up for extensions in the file system. And then, if there are
libraries, we can also use the existing <code>dynamic_library_path</code> GUC.</p>
<p>So, you remember, this is where we come from [image above]; the good thing is
we have the opportunity to build Postgres images that are minimal, that only
contain Postgres.</p>


	
			
	

<p>Instead of recreating them every week — because it’s very likely that
something has some dependency, has a CVE, and so recreate them for everyone,
forcing everyone to update their Postgres systems — we can now release them
maybe once a month, and pretty much follow the Postgres cadence patch
releases, and maybe if there are CVEs it’s released more frequently.</p>
<p>The other good thing is that now we are working to remove the dependency on
Barman Cloud for CloudNativePG. CloudNativePG has a new plugin interface and
with 1.26 with — which is expected in the next weeks — we are suggesting
people start moving new workloads to the Barman Cloud plugin solution. What
happens is that Barman Cloud will be in that sidecar image. So it will be
distributed separately, and so its lifecycle is independent from the rest. But
the biggest advantage is that any extension in Postgres can be distributed —
right now we’ve got packages — The idea is that they are distributed also as
images.</p>
<p>If we start thinking about this approach, if I write an extension for
Postgres, until now I’ve been building only packages for Debian or for RPM
systems. If I start thinking about also building container images, they could
be immediately used by the new way of CloudNativePG to manage extensions.
That’s my ultimate goal, let’s put it that way.</p>
<p>This is how things will change at run time without breaking immutability.</p>


	
			
	

<p>There will be no more need to think about all the possible combinations of
extensions. There will be the Postgres pod that runs, for example, a primary
or standby, that will have the container for Postgres. If you’re using Barman
Cloud, the sidecar container managed by the plugin with Barman Cloud. And
then, for every extension you have, you will have a different image volume
that is read-only, very light, only containing the files distributed in the
container image of the extension, and that’s all.</p>
<p>Once you’ve got these, we can then coordinate the settings for external
<code>extension_control_path</code> and <code>dynamic_library_path</code>. What we did was, starting
a fail fast pilot project within EDB to test the work that Peter was doing on
the <code>extension_control_path</code>. For that we used the <a href="https://github.com/cloudnative-pg/postgres-trunk-containers">Postgres Trunk Containers
project</a>, which is a very interesting project that we have at
CloudNativePG. Every day it rebuilds the latest snapshot of the master branch
of Postgres so that we are able to catch, at an early stage, problems with the
new version of Postgres in CloudNativePG. But there’s also an action that
builds container images for a specific, for example, <a href="https://commitfest.postgresql.org">Commitfest</a> patch. So we
use that.</p>
<p>Niccolò wrote a pilot patch, an exploratory patch, for the operator to define
the extensions stanza inside the cluster resource. He also built some bare
container images for a few extensions. We make sure to include a very simple
one and the most complex one, which is <a href="https://postgis.net">PostGIS</a>. This is the patch that —
it’s still a draft — and the idea is to have it in the next version, 1.27
for CloudNativePG. This is how it works:</p>
  <code>apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgresql-with-extensions
spec:
  instances: 1
  imageName: ghcr.io/cloudnative-pg/postgresql-trunk:18-devel
  postgresql:
    extensions:
      - name: pgvector
        image:
          reference: ghcr.io/cloudnative-pg/pgvector-18-testing:latest
  storage:
    storageClass: standard
    size: 1Gi
</code>  <p>We have the <code>extensions</code> section in the cluster definition. We name the
extension. Theoretically we could also define the version and we point to the
image. What’s missing in this pilot patch is support for image catalogs, but
that’s something else that we can worry about later.</p>
<p>What happens under the hood is that when you update, or when you add a new
extension in the cluster definition, a rolling update is initiated. So there’s
this short downtime, but the container image is loaded in the replicas first,
and then in the primary. n image volume is mounted for each extension in,
let’s say, <code>/extensions/$name_of_extension</code> folder and CNPG updates, these two
parameters. It’s quite clean, quite neat. It works, but most of the work needs
to happen here. So that’s been my call, I mean to call container images as a
first class artifacts. If these changes, we have a new way to distribute
images.</p>
<p>Just to approach the conclusion, if you want to know more about the whole
story, I wrote <a href="https://www.gabrielebartolini.it/articles/2025/03/the-immutable-future-of-postgresql-extensions-in-kubernetes-with-cloudnativepg/" title="The Immutable Future of PostgreSQL Extensions in Kubernetes with CloudNativePG">this blog article</a> that recaps everything, and the key
takeaway for me — and then we go more on the patch if you want to, and also
address the questions. But what is important for me? Being in the Postgres
community for a long time, I think this is a good way, a good moment for us to
challenge the status quo of the extension distribution ecosystem.</p>
<p>I think we have an opportunity now to define a standard, which, I just want to
be clear, I’m focusing myself primarily on CNPG, but this is in general, even
for other operators. I’m sure that this will benefit everyone and overall it
will reduce the waste that we collectively create when distributing these
extensions in Kubernetes. If this becomes a standard way to distribute
extensions, the benefits will be much better operational work for everyone,
primarily also easier testing and validation of extensions. I mean, right now,
if you see an extension, ideally that extension — and it’s very easy to
build — if you’re in GitHub, to build the container images. GitHub, for
example, already provides the whole infrastructure for you to easily build
container images.</p>
<p>So if we find a standard way to define a GitHub action to build Postgres
extensions, I think, if you’re a developer of an extension, you can just use
it and then you find a registry in your project directly that continuously
publishes or periodically publishes this extension. Any user can just
reference that image URL and then without having to build images, they’re just
one rolling update away from testing a patch, testing also the upgrade paths.</p>
<p>I think there are some unknown unknowns that kind of scare me, in general,
about upgrades, upgrades of extensions. This is, in my opinion, one of the
biggest issues. It’s not that they’re not solved, but they require more
attention and more testing if you’re using them in an immutable world. All of
these will, in my opinion, will be much, much better with the approach we’ve
proposed. Images will be lighter, and the lighter image is also safer and more
secure, so less prone to have CVEs,lLess prone to require frequent updates,
and also they reduce the usage of bandwidth, for an organization in general.
What I was saying before, any extension project can be fully independent,
have their own way to build images and publish them.</p>
<p>One last point. I keep hearing many signs, that all of the stuff that we are
proposing right now seem like a kind of a limitation of Kubernetes. The way I
see it, in my view, that it’s not actually a limitation, it’s that these
problems have never been addressed before. The biggest mistake we can do is
focus on the specific problem of managing extensions without analyzing the
benefits that the entire stack brings to an organization. Kubernetes brings a
lot of benefits in terms of security, velocity, change management and,
operations that any organization must consider right now. Any Postgres DBA,
any Postgres user, my advice is, if you haven’t done it yet, start taking
Kubernetes, seriously.</p>
 Discussion 
<p>Floor: I do think that David, you wanted to talk maybe a little bit about the
mutable volume pattern?</p>
<p>David: Well, if people are interested, in your early slide where you were
looking at alternatives, one you were thinking of was putting extensions on a
mutable volume and you decided not to do that. But at Tembo we did do that and
I did a bunch of work trying to improve it and try to minimize image size and
all that in the last couple months. Tembo Cloud is shutting down now, so I had
to stop before I finished it, but I made quite a bit of progress. I’m happy to
kind of talk through the ideas there. But I think that this approach is a
better long term solution, fundamentally.</p>
<p>Gabriele: I would like if Marco and Niccolò, if you want to talk about the
actual work you’ve done. Meanwhile, Peter asks, “why does an installation of
an extension require a small downtime?” The reason is that at the moment, the
image volume patch, if you add a new image volume, it requires the pod to
restart. Nico or Marco, Jonathan, if you want to correct me on that.</p>
<p>Nico or Marco or Jonathan: It provides a rolling update of the cluster right
now.</p>
<p>Gabriele: So that’s the reason. That’s the only drawback, but the benefits in
my opinion, are…</p>
<p>David: My understanding is that, to add a new extension, it’s mounted it in a
different place. And because every single extension is its own mount, you have
to add it to both those GUCs. And at least one of them requires a restart.</p>
<p>Gabriele: But then for example, we’ve had this conversation at EDB for
example, we’re planning to have flavors of predefined extensions. For
example, you can choose a flavor and we distribute those extensions. For
example, I dunno, for AI we place some AI kind of extensions in the same
image, so it would be different.</p>
<p>But otherwise I’m considering the most extreme case of one extension, one
container image, which in my opinion, for the open source world is the way
that hopefully will happen. Because this way, think about that – I haven’t
mentioned this — if I write an extension, I can then build the image and
then run automated tests using Kubernetes to assess my extension on GitHub. If
those tests fail, my commit will never be merged on main. This is trunk
development, continuous delivery. This is, in my opinion, a far better way of
delivering and developing software. This is, again, the reason why we ended up
in Kubernetes. It’s not because it’s a technology we like, it’s a toy or so,
it’s because it solves bigger problems than database problems.</p>
<p>Even when we talk about databases, there’s still work that needs to be done,
needs to be improved. I’m really happy that we have more people that know
Postgres nowadays that are joining CloudNativePG, and are elevating the
discussions more and more on the database level. Because before it was
primarily on Kubernetes level, but now we see people that know Postgres better
than me get in CloudNativePG and propose new ideas, which is great. Which is
the way it needs to be, in my opinion.</p>
<p>But I remember, Tembo approached us because we actually talked a lot with
them. Jonathan, Marco, I’m sure that you recall, when they were evaluating
different operators and they chose CloudNativePG. I remember we had these
discussions where they asked us to break immutability and we said, “no way”.
That’s why I think Tembo had to do the solution you described, because we
didn’t want to do it upstream.</p>
<p>I think, to be honest, and to be fair, if image volumes were not added, we
would’ve probably gone down that path, because this way of managing
extensions, as I was saying, is not scalable, the current one. Because we want
to always improve, I think that the approach we need to be critical on what we
do. So, I don’t know, Niccolò, Marco, I would like you to, if you want, explain
briefly.</p>
<p>[A bit of chatter, opened <a href="https://github.com/EnterpriseDB/pgvector/blob/dev/5645/Dockerfile.cnpg">this Dockerfile</a>.]</p>
  <code>FROM ghcr.io/cloudnative-pg/postgresql-trunk:18-devel AS builder

USER 0

COPY . /tmp/pgvector

RUN set -eux; \
	mkdir -p /opt/extension &amp;&amp; \
	apt-get update &amp;&amp; \
	apt-get install -y --no-install-recommends build-essential clang-16 llvm-16-dev &amp;&amp; \
	cd /tmp/pgvector &amp;&amp; \
	make clean &amp;&amp; \
	make OPTFLAGS="" &amp;&amp; \
	make install datadir=/opt/extension/share/ pkglibdir=/opt/extension/lib/

FROM scratch

COPY --from=builder /opt/extension/lib/* /lib/
COPY --from=builder /opt/extension/share/extension/* /share/
</code>  <p>Niccolò: I forked, for example, <a href="https://pgxn.org/dist/vector/">pgvector</a>, That’s what we can do basically
for every simple extensions that we can just build. This is a bit more
complicated because we have to build from a trunk version of Postgres 18. So
we have to compile pgvector from source, and then in a scratch layer we just
archive the libraries and every other content that was previously built. But
ideally whenever PG 18 comes out as a stable version of Postgres, we just need
to <code>apt install pgvector</code> and grab the files from the path. Where it gets a
bit more tricky is in the case of <a href="https://postgis.net">PostGIS</a>, or <a href="https://github.com/timescale/timescaledb">TimescaleDB</a>, or any
extension whose library requires third party libraries. For example, PostGIS
has a strong requirement on the geometric libraries, so you need to import
them as well inside the mount volume. I can link you an example of the
<a href="https://github.com/cloudnative-pg/postgres-trunk-containers/blob/dev/postgis/postgis/Dockerfile-postgis.cnpg">PostGIS one</a>.</p>
<p>Gabriele: I think it’s important, we’ve got, I think Peter here, David as
well, I mean, for example, if we could get standard ways in Postgres to
generate <code>Dockerfile</code>s for extensions, that could be great. And as I said,
these extensions can be used by any operator, not only CNPG.</p>
<p>David: That’s my <a href="https://justatheory.com/2024/06/trunk-oci-poc/" title="POC: Distributing Trunk Binaries via OCI">POC</a> does. It’s a patch against the PGXS that would build a
trunk image.</p>
<p>Gabriele: This is the work that Niccolò had to do to make PostGIS work in the
pilot project: he had to copy everything.</p>
<p>Niccolò: I think we can make it a little bit smoother and dynamically figure
out everything from the policies library, so we don’t have to code everything
<a href="https://github.com/cloudnative-pg/postgres-trunk-containers/blob/dev/postgis/postgis/Dockerfile-postgis.cnpg">like this</a>, but this is just a proof of concept that it can
work.</p>
<p>David: So you installed all those shared libraries that were from packages.</p>
<p>Niccolò: Yeah, they’re being copied in the same <code>MountVolume</code> where the actual
extensions are copied as well. And then the pilot patch is able to set up the
library path inside the pod so that it makes the libraries available to the
system because of course, these libraries are only part of the <code>MountVolume</code>.
They’re not injected inside the system libraries of the pod, so we have to set
up the library path to make them available to Postgres. That’s how we’re able
to use them.</p>
<p>David: So they end up in <code>PKGLIBDIR</code> but they still work.</p>
<p>Niccolò: Yeah.</p>
<p>Gabriele: I mean, there’s better ideas, better ways. As Niccolò also said, it
was a concept.</p>
<p>David: Probably a lot of these shared libraries could be shared with other
extensions. So you might actually want other OCI images that just have some of
the libraries that shared between.</p>
<p>Gabriele: Yeah, absolutely. So we could work on a special kind of, extensions
or even metadatas so that we can place, you know…</p>
<p>So, yeah, that’s it.</p>
<p>Jonathan: I think it’s important to invite everyone to try and test this,
especially the Postgres <a href="https://github.com/cloudnative-pg/postgres-trunk-containers">trunk</a> containers, when they want to try something
new stuff, new like this one, just because we always need people testing. When
more people review and test, it’s amazing. Because every time we release
something, probably we’ll miss something, some extension like PostGIS missing
one of the libraries that wasn’t included in the path. Even if we can try to
find a way to include it, it will not be there. So testing, please! Test all
the time!</p>
<p>Gabriele: Well, we’ve got this action now, they’re failing. I mean, it’s a bit
embarrassing. [Cross talk.] We already have patch to fix it.</p>
<p>But I mean, this is a great project as I mentioned before, because it allows
us to test the current version of Postgres, but also if you want to build from
a <a href="https://commitfest.postgresql.org">Commitfest</a> or if you’ve got your own Postgres repository with sources, you
can compile, you can get the images from using <a href="https://github.com/cloudnative-pg/postgres-trunk-containers">this project</a>.</p>
<p>Floor: Gabriele, did you want to talk about <a href="https://www.cisa.gov/sbom">SBOM</a>s?</p>
<p>Gabriele: I forgot to mention <a href="https://www.cisa.gov/sbom">Software Bill of Materials</a>. They’re very
important. It’s kind of now basic for any container image. There’s also the
possibility to add them to these container images too. This is very important.
Again, in a change manager for security and all of that — in general supply
chain. And signatures too. But we’ve got signature for packages as well.
There’s also a attestation of provenance.</p>
<p>Floor: Very good, thanks everyone!</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
                 <a href="https://justatheory.com/tags/cloudnativepg/">CloudNativePG</a> 
                 <a href="https://justatheory.com/tags/gabriele-bartolini/">Gabriele Bartolini</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/shared/extension-ecosystem-summit/cnpg-extensions-card.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   03:05 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 18. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2025/05/cbor-json-number-string/'>CBOR Tag for JSON Number Strings</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Use the new IANA-registered tag to preserve JSON numbers represented as strings when encoding in Concise Binary Object Representation.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>For a side project, I’m converting JSON inputs to <a href="https://cbor.io" title="CBOR — Concise Binary Object Representation">CBOR</a>, or Concise Binary
Object Representation, defined by <a href="https://www.rfc-editor.org/rfc/rfc8949.html" title="RFC 8949 Concise Binary Object Representation">RFC 8949</a>, in order to store a more compact
representation in the database. This go Go app uses <a href="https://pkg.go.dev/encoding/json">encoding/json</a> package’s
<a href="https://pkg.go.dev/encoding/json#Decoder.UseNumber"><code>UseNumber</code></a> decoding option to preserve numbers as strings, rather tha
<code>float64</code>s. Alas, CBOR has no support for such a feature, so such values
cannot survive a round-trip to CBOR and back, as demonstrating by this example
using the <a href="https://pkg.go.dev/github.com/fxamacker/cbor/v2">github.com/fxamacker/cbor</a> package (<a href="https://go.dev/play/p/a0ukEGoQFSG">playground</a>)</p>
  

 <code> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</code> 

 <code>// Decode JSON number using json.Number.
input := bytes.NewReader([]byte(`{"temp": 98.6}`))
dec := json.NewDecoder(input)
dec.UseNumber()
var val map[string]any
if err := dec.Decode(&amp;val); err != nil {
	log.Fatalf("Err: %v", err)
}

// Encode as CBOR.
data, err := cbor.Marshal(val)
if err != nil {
	log.Fatalf("Err: %v", err)
}

// Decode back into Go.
var newVal map[string]any
if err := cbor.Unmarshal(data, &amp;newVal); err != nil {
	log.Fatalf("Err: %v", err)
}

// Encode as JSON.
output, err := json.Marshal(newVal)
if err != nil {
	log.Fatalf("Err: %v", err)
}

fmt.Printf("%s\n", output)
</code> 
 
 <p>The output:</p>
  <code>{"temp":"98.6"}
</code>  <p>Note that the input on line 2 contains the number <code>98.6</code>, but once the value
has been transformed to CBOR and back it becomes the string <code>"98.6"</code>.</p>
<p>I wanted to preserve JSON numbers treated as strings. Fortunately, <a href="https://cbor.io" title="CBOR — Concise Binary Object Representation">CBOR</a> uses
numeric tags to identify data types, and includes a <a href="https://www.iana.org/assignments/cbor-tags/cbor-tags.xhtml" title="Concise Binary Object Representation (CBOR) Tags">registry</a> maintained by
<a href="https://www.iana.org" title="Internet Assigned Numbers Authority">IANA</a>. I <a href="https://mailarchive.ietf.org/arch/msg/cbor/BjA7Bc0CSubgIDGyzyiTJeLSGaQ/">proposed</a> a new tag for JSON numbers as strings and, through a few
iterations, the <a href="https://mailman3.ietf.org/mailman3/lists/cbor@ietf.org/">CBOR group</a> graciously accepted the <a href="https://gist.github.com/theory/ef667af1c725240e6e30d525786d58e6" title="JSON Number String Tag for CBOR">formal description of
semantics</a> and assigned tag <code>284</code> in the <a href="https://www.iana.org/assignments/cbor-tags/cbor-tags.xhtml" title="Concise Binary Object Representation (CBOR) Tags">registry</a>.</p>
<p>Now any system that handles JSON numbers as strings can use this tag to
preserve the numeric representation in JSON output.</p>
<p>Here’s how to use the tag customization features of
<a href="https://pkg.go.dev/github.com/fxamacker/cbor/v2">github.com/fxamacker/cbor</a> to transparently round-trip <code>json.Number</code> values
<a href="https://go.dev/play/p/o2-4a76fE_5">playground</a>:</p>
  

 <code> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
</code> 

 <code>// Create tag 284 for JSON Number as string.
tags := cbor.NewTagSet()
tags.Add(
    cbor.TagOptions{
        EncTag: cbor.EncTagRequired,
        DecTag: cbor.DecTagRequired,
    },
    reflect.TypeOf(json.Number("")),
    284,
)

// Create a custom CBOR encoder and decoder:
em, _ := cbor.EncOptions{}.EncModeWithTags(tags)
dm, _ := cbor.DecOptions{
    DefaultMapType: reflect.TypeOf(map[string]any(nil)),
}.DecModeWithTags(tags)

// Decode JSON number using json.Number.
input := bytes.NewReader([]byte(`{"temp": 98.6}`))
dec := json.NewDecoder(input)
dec.UseNumber()
var val map[string]any
if err := dec.Decode(&amp;val); err != nil {
    log.Fatalf("Err: %v", err)
}

// Encode as CBOR.
data, err := em.Marshal(val)
if err != nil {
    log.Fatalf("Err: %v", err)
}

// Decode back into Go.
var newVal map[string]any
if err := dm.Unmarshal(data, &amp;newVal); err != nil {
    log.Fatalf("Err: %v", err)
}

// Encode as JSON.
output, err := json.Marshal(newVal)
if err != nil {
    log.Fatalf("Err: %v", err)
}

fmt.Printf("%s\n", output)
</code> 
 
 <p>Lines 1-16 contain the main difference from the previous example. They create
a CBOR encoder (<code>em</code>) and decoder (<code>dm</code>) with tag <code>284</code> assigned to
<code>json.Number</code> values. The code then uses them rather than the <code>cbor</code> package
to <code>Marshal</code> and <code>Unmarshal</code> the values on lines 28 and 35. The result:</p>
 <code>{"temp":98.6}
</code> <p>Et voilà! <code>json.Number</code> values are once again preserved.</p>
<p>I believe these custom CBOR encoder and decoder configurations bring full
round-trip compatibility to any regular JSON value decoded by <a href="https://pkg.go.dev/encoding/json">encoding/json</a>.
The other important config for that compatibility is the <code>DefaultMapType</code>
decoding option on line 15, which ensures maps use <code>string</code> values for map
keys rather the CBOR-default <code>any</code> values.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/go/">Go</a> 
                 <a href="https://justatheory.com/tags/cbor/">CBOR</a> 
                 <a href="https://justatheory.com/tags/json/">JSON</a> 
                 <a href="https://justatheory.com/tags/ietf/">IETF</a> 
                 <a href="https://justatheory.com/tags/iana/">IANA</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:32 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://herestomwiththeweather.com'>Heres Tom with the Weather</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://herestomwiththeweather.com/2025/05/18/webfinger-reverse-discovery/'>Webfinger Reverse Discovery</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Activitypub addresses the problem of participating in a decentralized social network with a low barrier to entry. You participate through the server you have joined but often times the people you want to interact with reside on other servers. For instance, if you want to follow a friend, visiting that friend’s url does not provide a simple follow button.  That simple follow button is on your own s
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>Activitypub addresses the problem of participating in a decentralized social network with a low barrier to entry. You participate through the server you have joined but often times the people you want to interact with reside on other servers. For instance, if you want to follow a friend, visiting that friend’s url does not provide a simple follow button.  That simple follow button is on your own server but you need to navigate to your server’s profile page for your friend who is on a remote server.  An easy way to do this is to perform a search on your friend’s webfinger address which looks like an email address. Your server can make a <a href="https://www.w3.org/community/reports/socialcg/CG-FINAL-apwf-20240608/#forward-discovery">forward discovery</a> request to ask for the url of your friend’s actor document so that you can visit your server’s profile page for your friend.</p>

<p>Your server needs to do more than forward discovery to validate that the actor url actually belongs to the requested webfinger address in case the domain of the webfinger address is different than the domain of the actor url.  In this case, after fetching the actor url, your server needs to construct a 2nd webfinger address composed of the preferredUsername it found in the actor document followed by the domain of the actor url.  Your server can make a webfinger request to this 2nd address and use the response to verify that the subject matches the original webfinger address that you submitted in your search.  If they don’t match, your server can display the profile associated with the 2nd address and ignore the original webfinger address since the validation failed.</p>

<p>I wrote a <a href="https://github.com/herestomwiththeweather/irwin/commit/330765dc10054156cd26c397ed1973c93ed30760#diff-e66847e10576724ed355f062a794f1953374908bf08e248a62f3ccde6682de42R25-R34">should use the custom domain</a> example spec to make sure the server can accommodate a custom domain different than the domain in the actor url.</p>

<p>In the example spec, we are given bob@example.com whose webfinger points to an actor document at activitypub.test:</p>

  <code>let(:bob_webfinger_info) { {"subject" =&gt; "acct:bob@example.com", "links"=&gt;[{"rel"=&gt;"self", "type"=&gt;"application/activity+json", "href"=&gt;"https://activitypub.test/users/bob" }]} }
</code> 
 

<p>It is not enough to fetch the actor document and assume bob is at activitypub.test.  Instead, <a href="https://docs.joinmastodon.org/spec/webfinger/#mastodons-requirements-for-webfinger">as Mastodon does</a>, a reverse discovery should be performed by constructing a new WebFinger address by combining the preferredUsername from the actor document and the hostname of the id of the actor document.</p>

<p>In the example spec, this new WebFinger address would be bob@activitypub.test and, in this case, the test host activitypub.test returns a webfinger response that confirms that the subject is bob@example.com that was requested with forward discovery.</p>

<p>Another example spec <a href="https://github.com/herestomwiththeweather/irwin/commit/8f4b4f3abb8e47bd1c9d144439874f25ed6cf0c1#diff-e66847e10576724ed355f062a794f1953374908bf08e248a62f3ccde6682de42R61-R93">should not use the custom domain if subject returned by activitypub server is different than the original subject</a> tests when george@example.com is not recognized by the host activitypub.test who george points his webfinger address to:</p>

  <code>let(:george_webfinger_info) { {"subject" =&gt; "acct:george@example.com", "links"=&gt;[{"rel"=&gt;"self", "type"=&gt;"application/activity+json", "href"=&gt;"https://activitypub.test/users/george" }]} }
</code> 
 

<p>In this case, the validation fails because the host returns acct:george@activitypub.test in the 2nd webfinger request instead of acct:george@example.com so example.com is discarded and the domain of the account should fall back to activitypub.test.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:49 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 17. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2697'>OpenID Federation Interop Event at SUNET in Stockholm</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  At the end of April, I had the privilege of gathering in Stockholm with 30 participants to perform interoperability testing among 14 different OpenID Federation implementations. Leif Johansson and SUNET were fabulous hosts for the meeting at their offices in Stockholm. People from 15 countries participated, coming from as far as Australia and New Zealand! […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>At the end of April, I had the privilege of gathering in Stockholm with 30 participants to perform interoperability testing among 14 different <a href="https://openid.net/specs/openid-federation-1_0.html">OpenID Federation</a> implementations.  <a href="https://www.linkedin.com/in/leifjohansson/">Leif Johansson</a> and <a href="https://www.sunet.se/">SUNET</a> were fabulous hosts for the meeting at their offices in Stockholm.  People from 15 countries participated, coming from as far as Australia and New Zealand!  We performed eight different classes of tests between the implementations plus tested the <a href="https://openid.net/certification/federation_testing/">OpenID Certification tests being developed for OpenID Federation</a>.</p>
<p>It was great to have many of the core contributors to OpenID Federation come together and meet one another, most in-person, a few virtually, many for the first time.  The sense of community and shared mission in the room was palpable!  Besides testing, we also took time for architectural discussions, addressing open issues, and of course, socializing over drinks and dinners.</p>
<p>I must say that the OpenID Foundation staff who helped organize the meeting did a bang-up job!  <a href="https://www.linkedin.com/in/stephaniemeli/">Stephanie Meli</a> and <a href="https://www.linkedin.com/in/gareth-narinesingh-539b8325/">Gareth Narinesingh</a> both pitched in in numerous ways, resulting in a flawless and fun event!  I’d normally be the one blogging and posting to capture the essence of the event, but they already more than covered that base.  Their posts are full of facts, anecdotes, and photos.  Check them out…</p>
 
 <a href="https://www.linkedin.com/posts/openid-foundation_earlier-this-morning-30-delegates-from-different-activity-7322576545884102658-9Gi6/">Day 1</a> 
 <a href="https://www.linkedin.com/posts/openid-foundation_digitalid-activity-7322910436335480833-gBeZ/">Day 2</a> 
 <a href="https://www.linkedin.com/posts/openid-foundation_and-thats-yet-another-interop-wrap-day-activity-7323665348921106432-cEw8/">Day 3</a> 
 <a href="https://openid.net/the-openid-federation-interoperability-event/">Recap of the Interop Event</a> 
 
<p>I thought I’d add a few more photos and graphics to capture the spirit of the interop.</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
   
  
    
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/openid-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   02:21 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 16. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://idm-thoughtplace.blogspot.com/'>IdM Thoughtplace</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://idm-thoughtplace.blogspot.com/2025/05/identity-as-kitchens-and-driving-on-new.html'>Identity Management as Kitchens and driving on the New Jersey Turnpike</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Those of you who have been following me for years are aware
of my preference for Identity Management Programs over one-off Projects.&nbsp; The fact is, one might consider that a proper
program goes something like this:    Set up the Directory/IDP  Define Roles  Set up Access Management (SSO/MFA)  Set up LCM processes  Implement Fine-grained authorization  Implement Self-Sovereign Identity and digi
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Those of you who have been following me for years are aware
of my preference for Identity Management Programs over one-off Projects.&nbsp; The fact is, one might consider that a proper
program goes something like this:</p><p></p>  Set up the Directory/IDP  Define Roles  Set up Access Management (SSO/MFA)  Set up LCM processes  Implement Fine-grained authorization  Implement Self-Sovereign Identity and digital wallets  <p></p><p>









</p><p></p>











<p>Of course, this list and its order depend on the needs and
culture of the organization being served. In the long term, it is virtually
impossible to do just some of this. It’s like upgrading or updating your kitchen.
Now the Dining Room looks off, which makes the Den look dated, and then the
carpeting, and then, of course, the bedrooms. All because one part of the house
was improved. </p>

<p>My thinking has always been that you can’t really grant
access until you have some sort of Identity store in place, which is usually
the Directory Service for the Workforce and an IDP when it comes to CIAM. </p>

<p>Furthermore, steps two and three are somewhat
interchangeable, but if you need to organize your identities, it’s likely due
to an Access Management requirement, so you may want to complete this task
sooner rather than later.</p>

<p>LCM needs are required regardless of use case, but of course
take different forms. For the Workforce, this is more about how an employee
progresses through their corporate career. On the CIAM side, this might involve
subscriptions, optional services, and the ability to unsubscribe and be
forgotten.</p>

<p>Refining all these processes and connecting them to
additional applications will likely require some form of fine-grained
authorization to ensure that all users can access only what they are intended
to.</p>

<p>Once all of this is in place and working, we can begin to
think about utilizing this information for digital wallets and establishing the
foundations of Self-Sovereign identity using wallets. This will ensure that, in
any given Identity-based transaction, only the minimum required attributes are shared.&nbsp;&nbsp;&nbsp;&nbsp; </p>

<p>As far as the Identity Program goes, it’s like driving on
the New Jersey Turnpike; the construction and work never seem to end. As soon
as we finish one round of repairs and upgrades, it’s probably time to start
over again.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   00:07 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 15. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@justinsecurity'>Justin Richer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justinsecurity.medium.com/tangled-tokens-and-authorized-agents-331e4db02fb4?source=rss-ce3fbf1372f2------2'>Tangled Tokens and Authorized Agents</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Right now, many folks are excited about the prospect of agentic AI: intelligent computer systems that can access your stuff and do useful things for you, all without you having to program them to do it. But in order for that to happen, these bots need a way to actually go and do things. The Model Context Protocol (MCP) was recently proposed as a common interface for agents to get access to service
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*NR6ZjpeJq_3VL4iCvnp0ug.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Right now, many folks are excited about the prospect of agentic AI: intelligent computer systems that can access your stuff and do useful things for you, all without you having to program them to do it. But in order for that to happen, these bots need a way to actually go and do things. The Model Context Protocol (MCP) was recently proposed as a common interface for agents to get access to services through a&nbsp;proxy.</p>The MCP Proxy&nbsp;Pattern<p>The idea is pretty solid at its core: an MCP server provides a common API for agents to query services and data, and for services to advertise what actions are available to the agents. The MCP server sits in the middle to facilitate the whole shebang. For this to work, we need two distinct authorization contexts connected by the MCP server. Thankfully, OAuth gives us a great set of tools to address this, and Aaron Parecki wrote a fantastic piece about <a href="https://aaronparecki.com/2025/04/03/15/oauth-for-model-context-protocol">how OAuth can be applied to help solve this problem</a>, I consider that article required reading for anyone in this&nbsp;space.</p><p>As it turns out, though, the MCP deployment pattern defies some of the assumptions about how OAuth ought to&nbsp;work.</p> Two Worlds <p>The proxy setup splits the world into an MCP Protocol space, where the agent connects to an MCP server, and what we’ll call the upstream service space, where the MCP server connects to some other service that does the actual work. This is where OAuth starts to come into&nbsp;play.</p>The OAuth flavored parts of the MCP&nbsp;Server<p>If we look at MCP as an OAuth-protected API, we can pretty easily see how we can split out the AS and RS roles inside the MCP server space. In the general case, it’s easy to see how the AS portion can facilitate the user authorizing the agent. The agent gets an OAuth token to call the MCP server, which maps to some set of credentials upstream. We don’t want to just pass through the MCP client’s token, though — there’s no guarantee the upstream service even uses OAuth, let alone the same set of credentials. So while we will need a mapping between these sides, this pattern allows us tons of flexibility in how we roll this&nbsp;out.</p><p>In the enterprise case, we can use existing AS policy to authenticate users to the MCP server and map their agent’s access to whatever sets of services that user can use. The integration between the service and the MCP server can be handled by the enterprise, without users needing to do any extra work. We can even allowlist this connection so that users don’t have to see a consent screen, as long as all the right policy conditions are in play. If the integrated service uses OAuth itself, we could even apply token exchange between the MCP client’s access token and the service’s required access token, to limit exposure. The enterprise can even lock down the MCP server to use only pre-registered, pre-approved MCP clients, already a common pattern among corporate OAuth deployments.</p><p>On the other end of the deployment spectrum, we might have an agent, MCP server, and upstream service all deployed and owned by completely separate entities with no prior relationship. This might sound crazy, but it’s not really any different from how desktop email clients work with the IMAP protocol. For most IMAP use cases, the mail client stores the user’s credentials and impersonates them on the API. With OAuth, we can probably do better than that, but OAuth was built to connect websites together in a world where things are more predictable and&nbsp;stable.</p><p>So how can we do this in a wildly dynamic MCP&nbsp;space?</p> Intertwined Credentials <p>OAuth classically requires the user to log in to the AS and approve the client, which is registered with the AS, to act on their behalf to call a resource. While we can apply that pattern to the MCP proxy, and as we saw in the enterprise case it can make a lot of sense, I propose that we can learn more from the world of email&nbsp;clients.</p><p>An email server isn’t going to know anything about a particular instance of email software ahead of time, and the server probably isn’t even going to know anything about a class of email software. A user could, if they chose, implement IMAP from scratch and use it with the server — that’s the <a href="https://justinsecurity.medium.com/discovery-negotiation-and-configuration-93abbc8139e2">promise of interoperability</a> for protocols like IMAP. The server only cares if the protocol is implemented correctly and if the mail client can authenticate to the user account. If the authentication works, then the client is&nbsp;valid.</p><p>I argue that it’s much the same in the MCP proxy case. The identity of a particular instance of client software is less important because it should always be mapped to a particular set of access rights upstream. And where does the MCP server get those access rights? From the user authorizing the MCP server somehow. This could be yet another OAuth flow, it could be storing the user’s credentials, or it could be something very un-OAuth like accessing a local socket connection. If the MCP server can make a valid connection to the upstream service in the context of the user setting up the connect to their agent, then that’s all an average MCP server should really care about. The token that it issues to the agent will get mapped to the authenticated context used to call the upstream service. When the MCP server’s AS issues a token for the agent, the AS can store a mapping to the authentication needed for the upstream service. This is not dissimilar from mapping an OAuth access token to the user who was logged in to the AS at the time, and making that information available to the RS. In this case, our RS also needs to make a client call to something else, and that’s the information we make available.</p><p>I could be true that we <a href="https://www.ietf.org/archive/id/draft-richer-oauth-pushed-client-registration-00.html">might not even need client IDs</a> in the traditional OAuth sense. The only security artifact that matters is the access token and its mapping to the upstream credential set. If I need to re-connect my upstream API, my agent can get a new token as a result and just use that. The MCP server might not even care about who I am, so long as I can successfully connect to the upstream system. After all, this is exactly the argument used for the design of OAuth clients in general: if they can get to the resources they need, they don’t need to know who the user is in many cases. (And if they do, there’s OIDC for&nbsp;that.)</p> Weaving Webs <p>This work is bringing to light some of the limitations and assumptions of the OAuth protocol. Some of these are solved by things that <a href="https://www.rfc-editor.org/rfc/rfc9635.html#name-identifying-the-client-inst">we built into GNAP</a>, especially the notion of ephemeral clients, but even with GNAP it’s not a simple&nbsp;world.</p><p>For example, if the upstream service requires a static OAuth registration and then allowlists a client after a user authorizes it, does this leave open a door for attackers to exploit? Is there anything that the MCP server, sitting in the middle as a proxy, needs to do to help this? What about cases where the MCP server has no user interface at&nbsp;all?</p><p>There are many questions still left to be answered, but I, for one, am excited that they’re being asked and discussed right now. I look forward to being part of the conversation, and I hope you can join in. Maybe we’ll even invite the bots to&nbsp;help.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*NR6ZjpeJq_3VL4iCvnp0ug.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:29 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@rufftimo'>Timothy Ruff</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://rufftimo.medium.com/mocking-surveillance-a10b004c4cd8?source=rss-11906019170a------2'>Mocking Surveillance</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Sometimes a situation is so absurd, or even so dangerous, you gotta make fun of it to keep from getting too bothered.  Unfortunately, surveillance-enabled digital identity systems are springing up around the world in societies that have traditionally shunned such privacy-invasive things, like the U.S., EU, U.K., and Australia. The biggest problem is the “phone home” that’s a standard capability — 
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*h2MvaVdLn7mjXxyR-xU1OA.jpeg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Sometimes a situation is so absurd, or even so dangerous, you gotta make fun of it to keep from getting too bothered.</p><p>Unfortunately, surveillance-enabled digital identity systems are springing up around the world in societies that have traditionally shunned such privacy-invasive things, like the U.S., EU, U.K., and Australia. The biggest problem is the “phone home” that’s a standard capability — whether active or latent — in most new systems: when either the identity verifier or user’s application interacts with or “phones home” to the identity issuer or another third party, enabling centralized tracking and other abuses. This is the case with all mDL implementations that utilize ISO 18013, and all implementations that utilize OpenID Connect (OIDC), among&nbsp;others.</p><p>When phone home is active the alarming result is far more than surveillance, it is total control, where authorities can allow or disallows citizens’ digital activities. With phone home capability, each identity usage is approved or disapproved by some authority. Seriously. That’s just dandy in cultures that tolerate or even expect surveillance, but it’s a large and surreptitious loss of privacy and liberty in cultures that don’t. And it’s not an incremental hop, it’s a huge&nbsp;leap.</p><p>So if you care about digital identity, you love privacy and liberty, and you loathe tracking and surveillance, The Phone Home Song is for&nbsp;you…</p><p>Link to song: <a href="https://youtube.com/shorts/9XvsHoZjBHI?feature=share">https://youtube.com/shorts/9XvsHoZjBHI</a></p><p>Check it out, it’s 2 minutes that’ll give you a chuckle…&nbsp;:)</p><p>Lyrics:</p><p>Well I flashed my MDL at the grocery store,<br>Just buyin’ eggs, not startin’ war,<br>But somewhere in a server farm,<br>A red light blinks to sound the&nbsp;alarm.</p><p>“Verify!” the verifier cried,<br>And phoned the mothership worldwide.<br>Now Uncle Sam knows I bought jam — <br>And scoffs at how surveilled I&nbsp;am.</p><p>Don’t phone home, it’s a trap,<br>It’s surveillance with a friendly app.<br>They say it’s safe, just protocol,<br>But your ID’s got a silent&nbsp;call.</p><p>You think it’s off? It’s just on snooze — <br>Till panic strikes and pow! You lose.<br>So if your state says, “Scan that code,”<br>Yell, ‘Hey no way — that’s snitching mode.’</p><p>Now Europe’s got an ID game,<br>EUID — it sounds so tame.<br>But under EIDAS’ polite exterior,<br>Lurks a data vampire far superior.</p><p>ISO’s the blueprint fine,<br>18013’s the covert spine.<br>They say it’s good, just OID<br>Its users say: “Please surveil&nbsp;me!”</p><p>Don’t phone home, we ain’t fools,<br>Keep your data, you make the rules.<br>No lazy call to Daddy State,<br>Who tracks your swipe at Coffee&nbsp;Gate.</p><p>They say it’s off, but it’s not gone,<br>Just one “crisis” and it’s game on.<br>So build it right, or build it not — <br>’Cause once it’s on, you’re a moving&nbsp;dot.</p><p>Now bureaucrats in vests and ties,<br>Say “Phone home helps!” (surprise, surprise).<br>But basic rights aren’t things to tweak,<br>When politicians start to&nbsp;freak.</p><p>Digital ID’s got some flair,<br>But never phone home, if you care.<br>We need credentials, sure, that’s fair — <br>But not ones that stalk us everywhere.</p><p>Don’t phone home, make it clear:<br>Privacy’s not a souvenir.<br>We won’t trade our rights away,<br>For QR scans and&nbsp;TSA.</p><p>Tell your mayor, tell your mom — <br>This ain’t China or Pyongyong.<br>We like our IDs sleek and clean — <br>Not part of some trackin’&nbsp;machine.</p><p>So raise your voice and pass the law,<br>That cuts Phone Home’s spyware claw.<br>’Cause when they track your every roam…<br>You’re just a dot, and they’ve phoned&nbsp;home.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*h2MvaVdLn7mjXxyR-xU1OA.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   18:46 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2694'>W3C Verifiable Credentials 2.0 Specifications are Now Standards</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  As announced by the W3C, the Verifiable Credentials 2.0 family of specifications is now a W3C Recommendation. The new W3C Recommendations that I was an editor for are: Verifiable Credentials Data Model v2.0 Securing Verifiable Credentials using JOSE and COSE Controlled Identifiers v1.0 I joined the VC 2.0 journey in 2022 with the goal of […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="http://www.w3.org/"></a>As announced by the W3C, <a href="https://www.w3.org/news/2025/the-verifiable-credentials-2-0-family-of-specifications-is-now-a-w3c-recommendation/">the Verifiable Credentials 2.0 family of specifications is now a W3C Recommendation</a>.  The new <a href="https://www.w3.org/policies/process/#RecsW3C">W3C Recommendations</a> that I was an editor for are:</p>
 
 <a href="https://www.w3.org/TR/2025/REC-vc-data-model-2.0-20250515/">Verifiable Credentials Data Model v2.0</a> 
 <a href="https://www.w3.org/TR/2025/REC-vc-jose-cose-20250515/">Securing Verifiable Credentials using JOSE and COSE</a> 
 <a href="https://www.w3.org/TR/2025/REC-cid-1.0-20250515/">Controlled Identifiers v1.0</a> 
 
<p>I joined the VC 2.0 journey in 2022 with the goal of there being a simple, secure, standards-based way to sign W3C Verifiable Credentials.  The <a href="https://www.w3.org/TR/2025/REC-vc-jose-cose-20250515/">VC-JOSE-COSE specification</a> accomplishes that – defining how to secure VC Data Model payloads with <a href="https://www.rfc-editor.org/rfc/rfc7515.html">JOSE</a>, <a href="https://datatracker.ietf.org/doc/draft-ietf-oauth-selective-disclosure-jwt/">SD-JWT</a>, or <a href="https://www.rfc-editor.org/rfc/rfc9052.html">COSE</a> signatures.  <a href="https://self-issued.info/?p=2603">As I wrote</a> when the Proposed Recommendations were published, while I’m admittedly not a fan of JSON-LD, to the extent that Verifiable Credentials using the JSON-LD-based VC Data Model are in use, I was committed to there being a solid VC-JOSE-COSE specification so there is a simple, secure, standards-based way to secure these credentials.  That goal is now accomplished.</p>
<p>Particular thanks go to my co-editors of VC-JOSE-COSE <a href="https://www.linkedin.com/in/cohengabe/">Gabe Cohen</a> and <a href="https://www.linkedin.com/in/mprorock/">Mike Prorock</a>, former editor <a href="https://www.linkedin.com/in/or13b/">Orie Steele</a>, and working group chair <a href="https://www.linkedin.com/in/bzundel/">Brent Zundel</a> for the significant work they all both put in throughout the journey.  And of course, <a href="https://www.linkedin.com/in/manusporny/">Manu Sporny</a> and <a href="https://github.com/iherman">Ivan Herman</a> were always diligent about moving things along.</p>
<p>One of my personal mottos is “Finishing things matters”.  This is now finished.  As the song says, “What a long, strange trip it’s been”!</p>
   
  
    
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/w3c_home.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:17 &bull;
   2 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 14. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.talkingidentity.com'>Talking Identity</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.talkingidentity.com/2025/05/the-innovation-we-need-is-strategic-not-technical.html'>The Innovation We Need is Strategic, Not Technical</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  In my recap of RSAC 2025, I referenced the open letter that Patrick Opet, CISO of JPMorgan Chase, published in which he spoke about how essential security guardrails are being broken down by the lack of secure-by-design thinking in modern integration patterns within the SaaS world. His open letter challenged Cloud and SaaS providers to […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>In my <a href="https://blog.talkingidentity.com/2025/05/rsac-2025-ai-is-everywhere-trust-not-so-much.html">recap of RSAC 2025</a>, I referenced the <a href="https://www.jpmorgan.com/technology/technology-blog/open-letter-to-our-suppliers">open letter that Patrick Opet</a>, CISO of JPMorgan Chase, published in which he spoke about how essential security guardrails are being broken down by the lack of secure-by-design thinking in modern integration patterns within the SaaS world. His open letter challenged Cloud and SaaS providers to step up their security posture, and he called out the way in which modern identity protocols like OAuth are being used as contributing to the problem. The summary: industry is sacrificing foundational controls in exchange for business velocity, and creating a growing surface of identity and token-based vulnerabilities in the process.</p>



<p>Here’s an uncomfortable truth. Most of the risks being called out already have solutions – at least technically. The standards, protocols, and architectural patterns required to address them already exist. What’s lacking is the organizational will to implement them, the investment required to modernize infrastructure for security, and the discipline to prioritize long-term resilience over short-term delivery.</p>



<a href="https://blog.talkingidentity.com/wp-content/uploads/2025/05/parkour_crumblingfoundations-scaled.jpg"></a>



<p></p>



<p>And this isn’t just a failure of vendors. It’s also a consequence of enterprise buyers rewarding compliance over capability, and executives framing security as a function of audit readiness rather than operational integrity.</p>



 Standards ≠ Security 



<p>Any conversation about this situation must necessarily discuss this fundamental disconnect. More than once, I’ve heard a senior IT leader assure me their APIs are “secure” because they’ve implemented OAuth. However, while standards do provide a foundation for security, they are not guarantees for security in of themselves. Implementation matters. Configuration matters. Layering matters. After all, deploying OpenID Connect for authentication means little if the authentication method is still password-based and single-factor.</p>



<p>Let’s look at Opet’s concern about session token theft, an increasingly common attack vector. This risk wasn’t unforeseen by those working on modern identity standards. It is among the many reasons why the <a href="https://curity.io/resources/learn/what-is-financial-grade/">Financial-grade API (FAPI) security profile</a> was created by the OpenID Foundation. Originally designed for the financial sector, FAPI improves security for the OAuth and OpenID Connect standards by explicitly defining a security profile designed to ensure consistent and robust security measures across implementations of the standards with no room for misinterpretation. FAPI adds stricter constraints and requirements, like mandatory Strong Customer Authentication (SCA) and Mutual TLS, while also specifying additional controls like Demonstration of Proof-of-Possession (DPoP) and Proof Key for Code Exchange (PKCE). These aren’t experimental ideas. They’re proven, deployable measures that can be adopted by any organization (not just FIs), designed to eliminate many common attack vectors, including the ones highlighted in Opet’s letter.</p>



<p>Complementing this, the <a href="https://sharedsignals.guide/">Shared Signals Framework (SSF)</a> and <a href="https://sgnl.ai/2025/03/caep-cloud-security/">Continuous Access Evaluation Profile (CAEP)</a> provide a mechanism to continuously align session state with real-time risk posture. CAEP enables cloud providers and enterprises to stay ahead of evolving threats by providing an event-based mechanism to add much needed context into the continuous adaptation and enforcement of access policies beyond the initial session established.</p>



<p>But here’s the problem: few organizations implement these controls at scale (unless forced by regulations like Open Banking). Why? Because they require real effort: redesigning app architectures, updating legacy integrations, and investing in a deeper understanding of token and access lifecycle management. And don’t get me started on SaaS providers that charge extra for foundational security capabilities (a practice we lovingly refer to as the “SSO Tax”), essentially compounding the problem since cybersecurity is already viewed as a cost-center.</p>



<p>The technology is here. The risk is clear. The inertia is organizational.</p>



 Compliance ≠ Security 



<p>It’s become cliche, but the fact remains that too many organizations still optimize for compliance, not security. Security leaders are pushed to check boxes and meet deadlines, not to redesign systems around identity assurance. Dev teams are incentivized to ship features fast, not to integrate securely. Auditors audit for usage of standards and control frameworks, not for the strength and resilience of those controls. Executive teams often lack visibility into how quickly small risks compound into systemic exposures. All while CISOs lack budget to implement the programs that could help mitigate the unfolding nightmare.</p>



<a href="https://blog.talkingidentity.com/wp-content/uploads/2025/05/audit_files-scaled.jpg"></a>



<p></p>



<p>Don’t get me wrong. Compliance provides a crucial baseline of security controls that can be assessed and measured for existence. However, it doesn’t define how contextually strong or sufficient they are to meet an organizations security needs (if the organization can even define that). Put differently: compliance is a critical driver for putting better security in place, but it should be understood to be the starting point, not the end state.</p>



<p> Vulnerabilities like session hijacking and over-scoped permissions are therefore the entirely predictable outcomes of the fragile and hard-to-monitor architectures of systems that have been designed around minimum viable control.</p>



 We Don’t Need New Innovation. We Need New Priorities. 



<p>Cybersecurity threats are evolving at a rapid pace, and emerging technologies like AI are adding fuel to the fire. Compliance mandates will not be able to keep up (if they ever were), leaving organizations open to emerging threats unless they are proactive and innovative. To that end, many of the the tools are already there. Deterministic controls like phishing-resistant authentication, verifiable credentials, and more can help organizations dramatically reduce their attack surface. Comprehensive identity security posture management can help organizations gain visibility and remediate weaknesses in their identity infrastructure, automate controls, and proactively reduce potential risks to their digital ecosystem. FAPI, SSF, and CAEP are amongst a set of standards-based, security-focused architectural patterns that can be implemented today. And as overused as the term may be, the principles of Zero Trust are key: trust no session, verify continuously, and assume compromise as a starting condition.</p>



<p>The response to Patrick Opet’s letter shouldn’t be a scramble to invent something new. It should be a serious look at why we aren’t doing what we already know works. It’s time we recognize that the gap isn’t technical; it’s cultural and strategic. The change we need is a function of priorities incentives. As long as security is viewed as a cost center, where the cost of not being secure-by-design can be passed onto individuals and society, the fundamental calculus and therefore the current state of affairs won’t change. But that’s a discussion for a follow-up post.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://blog.talkingidentity.com/wp-content/uploads/2025/05/parkour_crumblingfoundations-1024x390.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   03:58 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 13. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2025/05/2025-gsoc/'>2025 GSOC: Mankirat Singh — ABI Compliance Reporting</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Please welcome 2025 Google Summer of Code contributor Mankirat Singh, who will
be developing an ABI compliance checker for the PostgreSQL maintenance
branches.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>I’m pleased to welcome <a href="https://blog.mankiratsingh.com">Mankirat Singh</a> to the Postgres community as a
2025 <a href="https://summerofcode.withgoogle.com">Google Summer of Code</a> contributor. Mankirat will be developing an ABI
compliance checker and reporting system to help identify and prevent
unintentional ABI changes in future minor Postgres releases. This follows on
the heels of the addition of <a href="https://justatheory.com/2024/06/abi-api-guidance/" title="Patch: Postgres ABI and API Guidance">ABI and API guidance</a> in Postgres 18, as well as
the <a href="https://www.enterprisedb.com/blog/explaining-abi-breakage-postgresql-171">ABI-breaking Postgres 17.1 release</a>. What timing!</p>
<p>Please follow Mankirat’s <a href="https://blog.mankiratsingh.com">blog</a> as he develops the project this summer, under
the mentorship of myself and <a href="https://pashagolub.github.io/blog/">Pavlo Golub</a>. It should also soon be on <a href="https://planet.postgresql.org">Planet
PostgreSQL</a>. We’ve also set up the <code>#gsoc2025-abi-compliance-checker</code> channel
on the <a href="https://pgtreats.info/slack-invite">community Slack</a> for ad-hoc discussion. Join us!</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/summer-of-code/">Summer of Code</a> 
                 <a href="https://justatheory.com/tags/abi-compliance/">ABI Compliance</a> 
                 <a href="https://justatheory.com/tags/mankirat-singh/">Mankirat Singh</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   18:25 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2688'>Fully-Specified Algorithms are now the Law of the Land</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I’m thrilled to be able to report that, from now on, only fully-specified algorithms will be registered for JOSE and COSE. Furthermore, fully-specified signature algorithms are now registered to replace the previously registered polymorphic algorithms, which are now deprecated. For example, you can now use Ed25519 and Ed448 instead of the ambiguous EdDSA. The new […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I’m thrilled to be able to report that, from now on, only fully-specified algorithms will be registered for JOSE and COSE.  Furthermore, fully-specified signature algorithms are now registered to replace the previously registered polymorphic algorithms, which are now deprecated.  For example, you can now use <code>Ed25519</code> and <code>Ed448</code> instead of the ambiguous <code>EdDSA</code>.</p>
<p>The new <a href="https://www.iana.org/assignments/jose/">IANA JOSE registrations</a> and <a href="https://www.iana.org/assignments/cose/">IANA COSE registrations</a> are now in place, as are the deprecations of the polymorphic signing algorithms.  And perhaps most significantly for the long term, the instructions to the designated experts for both registries have been updated so that only fully-specified algorithms will be registered going forward.</p>
<p>Lots of people deserve credit for this significant improvement to both ecosystems.  <a href="https://www.linkedin.com/in/filipskokan/">Filip Skokan</a> was the canary in the coal mine, alerting the<a href="https://openid.net/wg/connect/"> OpenID Connect working group</a> to the problems with trying to sign with <code>Ed25519</code> and <code>Ed448</code> when there were no algorithm identifiers that could be used to specify their use.  Similarly, <a href="https://www.linkedin.com/in/ve7jtb/">John Bradley</a> alerted the <a href="https://www.w3.org/groups/wg/webauthn/">WebAuthn working group</a> to the same problems for <a href="https://www.w3.org/TR/2021/REC-webauthn-2-20210408/">WebAuthn</a> and <a href="https://fidoalliance.org/specs/fido-v2.2-ps-20250228/fido-client-to-authenticator-protocol-v2.2-ps-20250228.html">FIDO2</a>, devising the <a href="https://www.w3.org/TR/2021/REC-webauthn-2-20210408/#sctn-public-key-easy">clever and awful workaround</a> that, when used by those specs, <code>EdDSA</code> is to be interpreted as meaning <code>Ed25519</code>.  John also supported this work as a <a href="https://datatracker.ietf.org/wg/jose/about/">JOSE working group</a> chair.  <a href="https://www.linkedin.com/in/roman-danyliw-9675741/">Roman Danyliw</a> supported including the ability to specify the use of fully-specified algorithms in the <a href="https://datatracker.ietf.org/doc/charter-ietf-jose/04/">JOSE charter</a> as the Security Area Director then responsible for JOSE.  <a href="https://www.linkedin.com/in/kodonog/">Karen O’Donoghue</a> created the <a href="https://datatracker.ietf.org/doc/draft-ietf-jose-fully-specified-algorithms/shepherdwriteup/">shepherd write-up</a> as JOSE co-chair.  Deb Cooley thoroughly reviewed and facilitated advancement of the specification as the Security Area Director currently responsible for JOSE.  And of course, <a href="https://www.linkedin.com/in/or13b/">Orie Steele</a>, the co-inventor of the fully-specified algorithms idea, and my co-author since <a href="https://datatracker.ietf.org/meeting/117/materials/slides-117-jose-fully-specified-algorithms-for-jose-and-cose-00">our audacious proposal to fix the polymorphic algorithms problem</a> at IETF 117 in July 2023 deserves huge credit for making the proposal a reality!</p>
<p>The specification is now in the <a>RFC Editor Queue</a>.  I can’t wait until it pops out the other side as an RFC!</p>
<p>The specification is available at:</p>
 
 <a href="https://datatracker.ietf.org/doc/draft-ietf-jose-fully-specified-algorithms/">https://datatracker.ietf.org/doc/draft-ietf-jose-fully-specified-algorithms/</a> 
 
<p>Thanks to all who helped make fully-specified algorithms the law of the land!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:23 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://aaronparecki.com'>Aaron Parecki</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://aaronparecki.com/2025/05/12/27/enterprise-ready-mcp'>Enterprise-Ready MCP</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I've seen a lot of complaints about how MCP isn't ready for the enterprise.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I've seen a lot of complaints about how MCP isn't ready for the enterprise.</p>

<p>I agree, although maybe not for the reasons you think. But don't worry, this isn't just a rant! I believe we can fix it!</p>

<p>The good news is the <a href="https://modelcontextprotocol.io/specification/draft/basic/authorization">recent updates to the MCP authorization spec</a> that separate out the role of the authorization server from the MCP server have now put the building blocks in place to make this a lot easier.</p>

<p>But let's back up and talk about what enterprise buyers expect when they are evaluating AI tools to bring into their companies.</p>

 Single Sign-On 

<p>At a minimum, an enterprise admin expects to be able to put an application under their single sign-on system. This enables the company to manage which users are allowed to use which applications, and prevents their users from needing to have their own passwords at the applications. The goal is to get every application managed under their single sign-on (SSO) system. Many large companies have more than 200 applications, so having them all managed through their SSO solution is a lot better than employees having to manage 200 passwords for each application!</p>

<p>There's a lot more than SSO too, like lifecycle management, entitlements, and logout. We're tackling these in the <a href="https://openid.net/wg/ipsie/">IPSIE</a> working group in the OpenID Foundation. But for the purposes of this discussion, let's stick to the basics of SSO.</p>

<p>So what does this have to do with MCP?</p>

<p>An AI agent using MCP is just another application enterprises expect to be able to integrate into their single-sign-on (SSO) system. Let's take the example of <a href="https://claude.ai">Claude</a>. When rolled out at a company, ideally every employee would log in to their company Claude account using the company identity provider (IdP). This lets the enterprise admin decide how many Claude licenses to purchase and who should be able to use it.</p>

 Connecting to External Apps 

<p>The next thing that should happen after a user logs in to Claude via SSO is they need to connect Claude to their other enterprise apps. This includes the built-in integrations in Claude like Google Calendar and Google Drive, as well as any MCP servers exposed by other apps in use within the enterprise. That could cover other SaaS apps like Zoom, Atlassian, and Slack, as well as home-grown internal apps.</p>

<p>Today, this process involves a somewhat cumbersome series of steps each individual employee must take. Here's an example of what the user needs to do to connect their AI agent to external apps:</p>

<p>First, the user logs in to Claude using SSO. This involves a redirect from Claude to the enterprise IdP where they authenticate with one or more factors, and then are redirected back.</p>

<p></p>

<p>Next, they need to connect the external app from within Claude. Claude provides a button to initiate the connection. This takes the user to that app (in this example, Google), which redirects them to the IdP to authenticate again, eventually getting redirected back to the app where an OAuth consent prompt is displayed asking the user to approve access, and finally the user is redirected back to Claude and the connection is established.</p>

<p></p>

<p>The user has to repeat these steps for every MCP server that they want to connect to Claude. There are two main problems with this:</p>

 
 This user experience is not great. That's a lot of clicking that the user has to do. 
 The enterprise admin has no visibility or control over the connection established between the two applications. 
 

<p>Both of these are significant problems. If you have even just 10 MCP servers rolled out in the enterprise, you're asking users to click through 10 SSO and OAuth prompts to establish the connections, and it will only get worse as MCP is more widely adopted within apps. But also, should we really be asking the user if it's okay for Claude to access their data in Google Drive? In a company context, that's not actually the user's decision. That decision should be made by the enterprise IT admin.</p>

<p>In "<a href="https://www.jpmorgan.com/technology/technology-blog/open-letter-to-our-suppliers">An Open Letter to Third-party Suppliers</a>", Patrick Opet, Chief Information Security Officer of JPMorgan Chase writes:</p>

<blockquote>"Modern integration patterns, however, dismantle these essential boundaries, relying heavily on modern identity protocols (e.g., OAuth) to create direct, often unchecked interactions between third-party services and firms' sensitive internal resources."</blockquote>

<p>Right now, these app-to-app connections are happening behind the back of the IdP. What we need is a way to move the connections between the applications into the IdP where they can be managed by the enterprise admin.</p>

<p>Let's see how this works if we leverage a new (in-progress) OAuth extension called "Identity and Authorization Chaining Across Domains", which I'll refer to as "Cross-App Access" for short, enabling the enterprise IdP to sit in the middle of the OAuth exchange between the two apps.</p>

 A Brief Intro to Cross-App Access 

<p>In this example, we'll use Claude as the application that is trying to connect to Slack's (hypothetical) MCP server. We'll start with a high-level overview of the flow, and later go over the detailed protocol.</p>

<p>First, the user logs in to Claude through the IdP as normal. This results in Claude getting either an ID token or SAML assertion from the IdP, which tells Claude who the user is. (This works the same for SAML assertions or ID tokens, so I'll use ID tokens in the example from here out.) This is no different than what the user would do today when signing in to Claude.</p>

<p></p>

<p>Then, instead of prompting the user to connect Slack, Claude takes the ID token back to the IdP in a request that says "Claude is requesting access to this user's Slack account."</p>

<p>The IdP validates the ID token, sees it was issued to Claude, and verifies that the admin has allowed Claude to access Slack on behalf of the given user. Assuming everything checks out, the IdP issues a new token back to Claude.</p>

<p></p>

<p>Claude takes the intermediate token from the IdP to Slack saying "hi, I would like an access token for the Slack MCP server. The IdP gave me this token with the details of the user to issue the access token for." Slack validates the token the same way it would have validated an ID token. (Remember, Slack is already configured for SSO to the IdP for this customer as well, so it already has a way to validate these tokens.) Slack is able to issue an access token giving Claude access to this user's resources in its MCP server.</p>

<p></p>

<p>This solves the two big problems:</p>

 
 The exchange happens entirely without any user interaction, so the user never sees any prompts or any OAuth consent screens. 
 Since the IdP sits in between the exchange, this gives the enterprise admin a chance to configure the policies around which applications are allowed this direct connection. 
 

<p>The other nice side effect of this is since there is no user interaction required, the first time a new user logs in to Claude, all their enterprise apps will be automatically connected without them having to click any buttons!</p>

 Cross-App Access Protocol 

<p>Now let's look at what this looks like in the actual protocol. This is based on the adopted in-progress OAuth specification "<a href="https://datatracker.ietf.org/doc/draft-ietf-oauth-identity-chaining/">Identity and Authorization Chaining Across Domains</a>". This spec is actually a combination of two RFCs: <a href="https://www.rfc-editor.org/rfc/rfc8693">Token Exchange</a> (RFC 8693), and <a href="https://www.rfc-editor.org/rfc/rfc7523">JWT Profile for Authorization Grants</a> (RFC 7523). Both RFCs as well as the "Identity and Authorization Chaining Across Domains" spec are very flexible. While this means it is possible to apply this to many different use cases, it does mean we need to be a bit more specific in how to use it for this use case. For that purpose, I've written a profile of the Identity Chaining draft called "<a href="https://datatracker.ietf.org/doc/draft-parecki-oauth-identity-assertion-authz-grant/">Identity Assertion Authorization Grant</a>" to fill in the missing pieces for the specific use case detailed here.</p>

<p>Let's go through it step by step. For this example we'll use the following entities:</p>

 
 Claude - the "Requesting Application", which is attempting to access Slack 
 Slack - the "Resource Application", which has the resources being accessed through MCP 
 Okta - the enterprise identity provider which users at the example company can use to sign in to both apps 
 

<p></p>

 Single Sign-On 

<p>First, Claude gets the user to sign in using a standard OpenID Connect (or SAML) flow in order to obtain an ID token. There isn't anything unique to this spec regarding this first stage, so I will skip the details of the OpenID Connect flow and we'll start with the ID token as the input to the next step.</p>

 Token Exchange 

<p>Claude, the requesting application, then makes a Token Exchange request (RFC 8693) to the IdP's token endpoint with the following parameters:</p>

 
 <code>requested_token_type</code>: The value <code>urn:ietf:params:oauth:token-type:id-jag</code> indicates that an ID Assertion JWT is being requested. 
 <code>audience</code>: The Issuer URL of the Resource Application's authorization server. 
 <code>subject_token</code>: The identity assertion (e.g. the OpenID Connect ID Token or SAML assertion) for the target end-user. 
 <code>subject_token_type</code>: Either <code>urn:ietf:params:oauth:token-type:id_token</code> or <code>urn:ietf:params:oauth:token-type:saml2</code> as defined by RFC 8693. 
 

<p>This request will also include the client credentials that Claude would use in a traditional OAuth token request, which could be a client secret or a JWT Bearer Assertion.</p>

 <code>POST /oauth2/token HTTP/1.1
Host: acme.okta.com
Content-Type: application/x-www-form-urlencoded

grant_type=urn:ietf:params:oauth:grant-type:token-exchange
&amp;requested_token_type=urn:ietf:params:oauth:token-type:id-jag
&amp;audience=https://auth.slack.com/
&amp;subject_token=eyJraWQiOiJzMTZ0cVNtODhwREo4VGZCXzdrSEtQ...
&amp;subject_token_type=urn:ietf:params:oauth:token-type:id_token
&amp;client_assertion_type=urn:ietf:params:oauth:client-assertion-type:jwt-bearer
&amp;client_assertion=eyJhbGciOiJSUzI1NiIsImtpZCI6IjIyIn0...
</code> 

 ID Assertion Validation and Policy Evaluation 

<p>At this point, the IdP evaluates the request and decides whether to issue the requested "ID Assertion JWT". The request will be evaluated based on the validity of the arguments, as well as the configured policy by the customer.</p>

<p>For example, the IdP validates that the ID token in this request was issued to the same client that matches the provided client authentication. It evaluates that the user still exists and is active, and that the user is assigned the Resource Application. Other policies can be evaluated at the discretion of the IdP, just like it can during a single sign-on flow.</p>

<p>If the IdP agrees that the requesting app should be authorized to access the given user's data in the resource app's MCP server, it will respond with a Token Exchange response to issue the token:</p>

 <code>HTTP/1.1 200 OK
Content-Type: application/json
Cache-Control: no-store

{
  "issued_token_type": "urn:ietf:params:oauth:token-type:id-jag",
  "access_token": "eyJhbGciOiJIUzI1NiIsI...",
  "token_type": "N_A",
  "expires_in": 300
}
</code> 

<p>The claims in the issued JWT are defined in "<a href="https://datatracker.ietf.org/doc/draft-parecki-oauth-identity-assertion-authz-grant/">Identity Assertion Authorization Grant</a>". The JWT is signed using the same key that the IdP signs ID tokens with. This is a critical aspect that makes this work, since again we assumed that both apps would already be configured for SSO to the IdP so would already be aware of the signing key for that purpose.</p>

<p>At this point, Claude is ready to request a token for the Resource App's MCP server</p>

 Access Token Request 

<p>The JWT received in the previous request can now be used as a "JWT Authorization Grant" as described by RFC 7523. To do this, Claude makes a request to the MCP authorization server's token endpoint with the following parameters:</p>

 
 <code>grant_type</code>: <code>urn:ietf:params:oauth:grant-type:jwt-bearer</code> 
 <code>assertion</code>: The Identity Assertion Authorization Grant JWT obtained in the previous token exchange step 
 

<p>For example:</p>

 <code>POST /oauth2/token HTTP/1.1
Host: auth.slack.com
Authorization: Basic yZS1yYW5kb20tc2VjcmV0v3JOkF0XG5Qx2

grant_type=urn:ietf:params:oauth:grant-type:jwt-bearer
assertion=eyJhbGciOiJIUzI1NiIsI...
</code> 

<p>Slack's authorization server can now evaluate this request to determine whether to issue an access token. The authorization server can validate the JWT by checking the issuer (<code>iss</code>) in the JWT to determine which enterprise IdP the token is from, and then check the signature using the public key discovered at that server. There are other claims to be validated as well, described in <a href="https://www.ietf.org/archive/id/draft-parecki-oauth-identity-assertion-authz-grant-03.html#section-6.1">Section 6.1</a> of the Identity Assertion Authorization Grant.</p>

<p>Assuming all the validations pass, Slack is ready to issue an access token to Claude in the token response:</p>

 <code>HTTP/1.1 200 OK
Content-Type: application/json
Cache-Control: no-store

{
  "token_type": "Bearer",
  "access_token": "2YotnFZFEjr1zCsicMWpAA",
  "expires_in": 86400
}
</code> 

<p>This token response is the same format that Slack's authorization server would be responding to a traditional OAuth flow. That's another key aspect of this design that makes it scalable. We don't need the resource app to use any particular access token format, since only that server is responsible for validating those tokens.</p>

<p>Now that Claude has the access token, it can make a request to the (hypothetical) Slack MCP server using the bearer token the same way it would have if it got the token using the traditional redirect-based OAuth flow.</p>

<p>Note: Eventually we'll need to define the specific behavior of when to return a refresh token in this token response. The goal is to ensure the client goes through the IdP often enough for the IdP to enforce its access policies. A refresh token could potentially undermine that if the refresh token lifetime is too long. It follows that ultimately the IdP should enforce the refresh token lifetime, so we will need to define a way for the IdP to communicate to the authorization server whether and how long to issue refresh tokens. This would enable the authorization server to make its own decision on access token lifetime, while still respecting the enterprise IdP policy.</p>

 Cross-App Access Sequence Diagram 

<p>Here's the flow again, this time as a sequence diagram.</p>

<p></p>

 
 The client initiates a login request 
 The user's browser is redirected to the IdP 
 The user logs in at the IdP 
 The IdP returns an OAuth authorization code to the user's browser 
 The user's browser delivers the authorization code to the client 
 The client exchanges the authorization code for an ID token at the IdP 
 The IdP returns an ID token to the client 
 

<p>At this point, the user is logged in to the MCP client. Everything up until this point has been a standard OpenID Connect flow.</p>

 
 The client makes a direct Token Exchange request to the IdP to exchange the ID token for a cross-domain "ID Assertion JWT" 
 The IdP validates the request and checks the internal policy 
 The IdP returns the ID-JAG to the client 
 The client makes a token request using the ID-JAG to the MCP authorization server 
 The authorization server validates the token using the signing key it also uses for its OpenID Connect flow with the IdP 
 The authorization server returns an access token 
 The client makes a request with the access token to the MCP server 
 The MCP server returns the response 
 

<p>For a more detailed step by step of the flow, see <a href="https://www.ietf.org/archive/id/draft-parecki-oauth-identity-assertion-authz-grant-03.html#appendix-A.3">Appendix A.3</a> of the Identity Assertion Authorization Grant.</p>

 Next Steps 

<p>If this is something you're interested in, we'd love your help! The in-progress spec is <a href="https://www.ietf.org/archive/id/draft-parecki-oauth-identity-assertion-authz-grant-03.html">publicly available</a>, and we're looking for people interested in helping prototype it. If you're building an MCP server and you want to make it enterprise-ready, I'd be happy to help you build this!</p>

<p>You can find me at a few related events coming up:</p>

 
 <a href="https://lu.ma/quvg7kzs">MCP Night</a> on May 14 
 <a href="https://lu.ma/mcpdevsummit2025">MCP Developers Summit</a> on May 23 
 <a href="https://lu.ma/mcphack">AWS MCP Agents Hackathon</a> on May 30 
 <a href="https://identiverse.com/idv25">Identiverse 2025</a> on June 3-6 
 

<p>And of course you can always find me <a href="https://linkedin.com/in/aaronparecki">on LinkedIn</a> or email me at <code>aaron.parecki@okta.com</code>.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://aaronparecki.com/2025/05/12/27/SSO-to-Claude.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   05:01 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 12. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2025/05/12/implement-client-assertions-with-client-credentials-flow-using-oauth-dpop/'>Implement client assertions with client credentials flow using OAuth DPoP</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This blog looks at implementing client assertions for the client credentials flow using OAuth 2.0 Demonstration of Proof-of-Possession (DPoP). The client credentials flow is an OAuth 2.0 authorization grant type used for machine-to-machine authentication. DPoP further strengthens the security by ensuring that the client possesses a specific key at the time of the request, forcing […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This blog looks at implementing client assertions for the client credentials flow using OAuth 2.0 Demonstration of Proof-of-Possession (DPoP). The client credentials flow is an OAuth 2.0 authorization grant type used for machine-to-machine authentication. DPoP further strengthens the security by ensuring that the client possesses a specific key at the time of the request, forcing token binding. </p>



<p>Code: <a href="https://github.com/damienbod/OAuthClientAssertions">https://github.com/damienbod/OAuthClientAssertions</a></p>



<p>Blogs in this series</p>



 
 Implement client assertions with client credentials flow using OAuth DPoP  



 <a href="https://damienbod.com/2025/04/21/implement-client-assertions-for-oauth-client-credential-flows-in-asp-net-core/">Implement client assertions for OAuth client credential flows in ASP.NET Core</a> 



 <a href="https://damienbod.com/2025/02/24/use-client-assertions-in-openid-connect-and-asp-net-core/">Using client assertions in OpenID Connect and ASP.NET Core</a> 
 



<p>NOTE: The code in the blog and the linked repository was created using the samples from IdentityServer.</p>



<p></p>



 Setup 



<p>Three different applications are used in this setup, an API which uses the DPoP access token, an OAuth client application implemented as a console app and an OAuth server, implemented using ASP.NET Core and Duende IdentityServer. The OAuth client credentials flow is used to acquire the access token and the signed JWT is used to authenticate the client request. A second RSA Key is used for the DPoP implementation and created on the fly for the token requests.</p>







<p></p>



 OAuth Server using Duende 



<p>Duende IdentityServer supports DPoP really good. The Enterprise license is required to use the DPoP feature. The client credentials flow just needs the RequireDPoP property set to true and DPoP is supported.</p>


  new Client
{
    ClientId = "mobile-dpop-client",
    ClientName = "Mobile dpop client",
    RequireDPoP = true,

    AllowedGrantTypes = GrantTypes.ClientCredentials,
    ClientSecrets =
    [
        new Secret
        {
            // X509 cert base64-encoded
            Type = IdentityServerConstants.SecretTypes.X509CertificateBase64,
            Value = Convert.ToBase64String(rsaCertificate.GetRawCertData())
        }
    ],

    AllowedScopes = { "scope-dpop" }
}
  


<p></p>



<p>Client assertions required middleware which is not added in the default setup.</p>


  idsvrBuilder.AddJwtBearerClientAuthentication();
  


<p></p>



 OAuth client credentials client requesting DPoP AT  



<p>(Note: code taken from the Duende samples.)</p>



<p>The Duende.AccessTokenManagement Nuget package is used to support client assertions and DPoP token usage in the client application. This is integrated into a named HttpClient factory. The support for client assertions and DPoP used this HttpClient is added using the AddClientCredentialsHttpClient extension.</p>


  services.AddDistributedMemoryCache();

services.AddScoped&lt;IClientAssertionService, ClientAssertionService&gt;();
// https://docs.duendesoftware.com/foss/accesstokenmanagement/advanced/client_assertions/

services.AddClientCredentialsTokenManagement()
 .AddClient("mobile-dpop-client", client =&gt;
 {
	 client.TokenEndpoint = "https://localhost:5001/connect/token";

	 client.ClientId = "mobile-dpop-client";
	 // Using client assertion
	 //client.ClientSecret = "905e4892-7610-44cb-a122-6209b38c882f";

	 client.Scope = "scope-dpop";
	 client.DPoPJsonWebKey = CreateDPoPKey();
 });

services.AddClientCredentialsHttpClient("mobile-dpop-client", "mobile-dpop-client", client =&gt;
{
 client.BaseAddress = new Uri("https://localhost:5005/");
});
  


<p></p>



<p>The DPoP is created for each instance.</p>


     private static string CreateDPoPKey()
   {
       var key = new RsaSecurityKey(RSA.Create(2048));
       var jwk = JsonWebKeyConverter.ConvertFromRSASecurityKey(key);
       jwk.Alg = "PS256";
       var jwkJson = JsonSerializer.Serialize(jwk);
       return jwkJson;
   }
  


<p></p>



<p>The IClientAssertionService interface is used to add the client assertion to the client credentials client using the Duende client Nuget package.  This works the same as in the previous blog.</p>


  public class ClientAssertionService : IClientAssertionService
{
    private readonly IOptionsSnapshot&lt;ClientCredentialsClient&gt; _options;

    public ClientAssertionService(IOptionsSnapshot&lt;ClientCredentialsClient&gt; options)
    {
        _options = options;
    }

    public Task&lt;ClientAssertion?&gt; GetClientAssertionAsync(
      string? clientName = null, TokenRequestParameters? parameters = null)
    {
        if (clientName == "mobile-dpop-client")
        {
            // client assertion
            var privatePem = File.ReadAllText(Path.Combine("", "rsa256-private.pem"));
            var publicPem = File.ReadAllText(Path.Combine("", "rsa256-public.pem"));
            var rsaCertificate = X509Certificate2.CreateFromPem(publicPem, privatePem);
            var signingCredentials = new SigningCredentials(new X509SecurityKey(rsaCertificate), "RS256");

            var options = _options.Get(clientName);

            var descriptor = new SecurityTokenDescriptor
            {
                Issuer = options.ClientId,
                Audience = options.TokenEndpoint,
                Expires = DateTime.UtcNow.AddMinutes(1),
                SigningCredentials = signingCredentials,

                Claims = new Dictionary&lt;string, object&gt;
                {
                    { JwtClaimTypes.JwtId, Guid.NewGuid().ToString() },
                    { JwtClaimTypes.Subject, options.ClientId! },
                    { JwtClaimTypes.IssuedAt, DateTime.UtcNow.ToEpochTime() }
                }
            };

            var handler = new JsonWebTokenHandler();
            var jwt = handler.CreateToken(descriptor);

            return Task.FromResult&lt;ClientAssertion?&gt;(new ClientAssertion
            {
                Type = OidcConstants.ClientAssertionTypes.JwtBearer,
                Value = jwt
            });
        }

        return Task.FromResult&lt;ClientAssertion?&gt;(null);
    }
}
  


<p></p>



<p>The services can be used like any other  HttpClient named client.</p>


  var client = _clientFactory.CreateClient("mobile-dpop-client");
var response = await client.GetAsync("api/values", stoppingToken);
  


<p></p>



 Notes 



<p>Using DPoP and client assertions work well together in this setup and different keys are used for the different OAuth flows. A lot of logic is solved using the Duende Nuget packages. Using DPoP and token binding for the API increases the security and should be used whenever possible. If using a web application with a user, a delegated OpenID Connect flow would be the better solution.</p>



 Links 



<p><a href="https://datatracker.ietf.org/doc/html/rfc9449">https://datatracker.ietf.org/doc/html/rfc9449</a></p>



<p><a href="https://docs.duendesoftware.com/identityserver/v7/tokens/authentication/jwt/">https://docs.duendesoftware.com/identityserver/v7/tokens/authentication/jwt/</a></p>



<p><a href="https://docs.duendesoftware.com/identityserver/v7/reference/validators/custom_token_request_validator/">https://docs.duendesoftware.com/identityserver/v7/reference/validators/custom_token_request_validator/</a></p>



<p><a href="https://docs.duendesoftware.com/identityserver/v7/tokens/authentication/jwt/">https://docs.duendesoftware.com/identityserver/v7/tokens/authentication/jwt/</a></p>



<p><a href="https://docs.duendesoftware.com/foss/accesstokenmanagement/advanced/client_assertions/">https://docs.duendesoftware.com/foss/accesstokenmanagement/advanced/client_assertions/</a></p>



<p><a href="https://www.scottbrady.io/oauth/removing-shared-secrets-for-oauth-client-authentication">https://www.scottbrady.io/oauth/removing-shared-secrets-for-oauth-client-authentication</a></p>



<p><a href="https://github.com/DuendeSoftware/products/tree/main/aspnetcore-authentication-jwtbearer">https://github.com/DuendeSoftware/products/tree/main/aspnetcore-authentication-jwtbearer</a></p>



 
<blockquote><a href="https://damienbod.com/2023/08/14/securing-apis-using-asp-net-core-and-oauth-2-0-dpop/">Securing APIs using ASP.NET Core and OAuth 2.0&nbsp;DPoP</a></blockquote>
 



 
<blockquote><a href="https://damienbod.com/2025/02/24/use-client-assertions-in-openid-connect-and-asp-net-core/">Use client assertions in OpenID Connect and ASP.NET&nbsp;Core</a></blockquote>
 



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2025/03/oauthccsignedjwtassertion-dpop.png?w=580'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   05:42 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 09. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2679'>So you want to use Digital Credentials? You’re now facing a myriad of choices!</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I gave the keynote talk So you want to use Digital Credentials? You’re now facing a myriad of choices! at EIC 2025. I opened by describing engineering choices – credential formats (W3C VCs, ISO mDOCs, SD-JWTs, SD-CWTs, JWPs, X.509 Certificates), issuance and presentation mechanisms (bespoke and standards-based, in-person and remote), mechanisms for choosing them (query […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I gave the keynote talk <a href="https://www.kuppingercole.com/sessions/5868">So you want to use Digital Credentials? You’re now facing a myriad of choices!</a> at <a href="https://www.kuppingercole.com/events/eic2025">EIC 2025</a>.  I opened by describing engineering choices – credential formats (W3C VCs, ISO mDOCs, SD-JWTs, SD-CWTs, JWPs, X.509 Certificates), issuance and presentation mechanisms (bespoke and standards-based, in-person and remote), mechanisms for choosing them (query languages, user interfaces), and trust establishment mechanisms (trust lists, certificates, and federation).</p>
<p>I then upped the ante by talking about the criticality of usability, the challenges of building ecosystems (something <a href="https://www.linkedin.com/in/andrewknash/">Andrew Nash</a> first explained to me most of two decades ago!), and how digital credentials are not an end in and of themselves; they’re a tool to help us solve real-world problems.  And of course, I closed by coming back to my theme <a href="https://self-issued.info/?p=2535">Standards are About Making Choices</a>, urging us to come together and make the right choices to enable interoperable use of digital credentials in ways that benefit people worldwide.</p>
<p>View my slides as <a href="https://self-issued.info/presentations/EIC_2025_So_you_want_to_use_Digital_Credentials.pptx">PowerPoint</a> or <a href="https://self-issued.info/presentations/EIC_2025_So_you_want_to_use_Digital_Credentials.pdf">PDF</a>.  I’ll also post a link to the video of the presentation here once Kuppinger Cole posts it.</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/EIC_2025_Logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:20 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 08. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.jonudell.net'>Jon Udell</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.jonudell.net/2025/05/07/the-musk-massacre/'>The Musk Massacre</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The great adventure of my birth family was the fifteen months we lived in New Delhi, from June of 1961, on a USAID-sponsored educational mission. So the destruction of USAID feels personal. I’m only now realizing that we were there at the very beginning of USAID, during what Jackie Kennedy later mythologized as the Camelot … Continue reading The Musk Massacre
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p></p>
<p>
The great adventure of my birth family was the fifteen months we lived in New Delhi, from June of 1961, on a USAID-sponsored educational mission. So the destruction of USAID feels personal. I’m only now realizing that we were there at the very beginning of USAID, during what Jackie Kennedy later mythologized as the Camelot era. On a tour of India, at a meet-and-greet in New Delhi, she appears in this family photo.</p>
<p>
We must have been at the embassy, she’s surrounded by Americans. You can see a few South Asian faces in the background. The young boy at the center of the photo, gazing up at the queen of Camelot, is five-year-old me.</p>
<p>
It could have been a Life Magazine cover: “A vision in white, Jackie represents America’s commitment to be of service to the world.” As corny as that sounds, though, the commitment was real. Our nation upheld it for sixty years and then, a few months ago, fed it to the wood chipper and set in motion a Holocaust-scale massacre.</p>
<p></p>
<blockquote><p>
We suggest the number of lives saved per year may range between 2.3 to 5.6 million with our preferred number resting on gross estimates of 3.3 million.
</p></blockquote>
<p>
The shutdown likely won’t kill 3.3 million people annually, say its “only” a million. Per year. For six years. It adds up.
</p>
<p>
Atul Gawande was leader of global public health for USAID. On a <a href="https://open.spotify.com/episode/3TYyseykH4FkDL4uNC1aPd">recent podcast</a> he runs some more numbers.</p>
<p>
On USAID “waste”:</p>
<p>
“It’s 0.35% of the federal budget, but that doesn’t help you, right? Try this. The average American paid $14,600 in taxes in 2024. The amount that went to USAID is under $50. For that we got control of an HIV epidemic that is at minuscule levels compared to what it was before. We had control of measles and TB. And it goes beyond public health. You also have agricultural programs that helped move India from being chronically food-aid-dependent to being an agricultural exporter. Many of our top trading partners once received USAID assistance that helped them achieve economic development.”</p>
<p>
On USAID “fraud”:</p>
<p>
“When Russia invaded Ukraine they cut off its access to medicine, bombed the factories that made oxygen, ran cyberattacks. The global health team moved the entire country’s electronic health record system to the cloud, and got a supply chain up and running for every HIV and TB patient in the country.”</p>
<p>
On USAID “abuse”:</p>
<p>
“The countries where we worked had at least 1.2 million lives saved.  In addition, there was a vaccine campaign for measles and for HPV. For every 70 girls in low income countries who are vaccinated against cervical cancer from HPV, one life is saved. It’s one of the most life-saving things in our portfolio. Our vaccine programs would have saved an additional 8 million lives over the next five years.”</p>
<p>
America has never been a shining city on the hill but USAID represented our best aspirations. In the throes of the Maoist cultural revolution that tore it down there are many other horrors to confront, but for me this one hits hardest. </p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/jonudell.info/images/jon-and-jackie.jpg?w=656&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   05:52 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 06. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.moxytongue.com/'>Moxy Tongue</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='http://www.moxytongue.com/2025/05/sovereignty-v-journalism.html'>Sovereignty v Journalism</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

       "Literature makes perfect what life makes real."&nbsp;     Some things repeat, without end... functional Sovereignty beats paid/fake Journalism daily.          Leaving absolutely no doubt... Sovereignty defeats Journalism.. again and again..         Static fiction and dynamic functional reality spar in the minds of aspiring minds daily... and academic intelligence (AI) has a hard time keeping 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p></p><blockquote><blockquote><blockquote>"Literature makes perfect what life makes real."&nbsp;</blockquote></blockquote></blockquote><p></p><p>Some things repeat, without end... functional Sovereignty beats paid/fake Journalism daily.</p>   <br>  <br>  Leaving absolutely no doubt... Sovereignty defeats Journalism.. again and again..  <br>  <br> <br><p>Static fiction and dynamic functional reality spar in the minds of aspiring minds daily... and academic intelligence (AI) has a hard time keeping up with direct functional experience. One door, infinite outcomes....</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:51 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2671'>Fully-Specified Algorithms Specification Addressing IESG Feedback</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Orie Steele and I have updated the “Fully-Specified Algorithms for JOSE and COSE” specification to address feedback received through directorate reviews and from Internet Engineering Steering Group (IESG) members. This prepares us for consideration of the specification by the IESG during its “telechat” on Thursday. This is an important milestone towards progressing the specification to […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p><a href="https://www.linkedin.com/in/or13b/">Orie Steele</a> and I have updated the “Fully-Specified Algorithms for JOSE and COSE” specification to address feedback received through directorate reviews and from <a href="https://www.ietf.org/about/groups/iesg/">Internet Engineering Steering Group (IESG)</a> members.  This prepares us for consideration of the specification by the IESG during its “telechat” on Thursday.  This is an important milestone towards progressing the specification to become an RFC.</p>
<p>Changes made since <a href="https://self-issued.info/?p=2627">I last wrote about the spec</a>, as summarized in the <a href="https://www.ietf.org/archive/id/draft-ietf-jose-fully-specified-algorithms-11.html#name-document-history">history entries</a>, are:</p>
<p>-11</p>
 
 Stated in the abstract that the specification deprecates some polymorphic algorithm identifiers, as suggested by Éric Vyncke. 
 
<p>-10</p>
 
 Provided a complete list of the Recommended column terms for COSE registrations, as suggested by Mohamed Boucadair. 
 Applied suggestions to improve the exposition received during IESG review. 
 
<p>-09</p>
 
 Addressed comments from secdir review by Kathleen Moriarty. 
 
<p>-08</p>
 
 Updated requested Brainpool algorithm numbers to match those chosen by Sean Turner. 
 Incorporated wording suggestions by Vijay Gurbani. 
 
<p>The specification is available at:</p>
 
 <a href="https://www.ietf.org/archive/id/draft-ietf-jose-fully-specified-algorithms-11.html">https://www.ietf.org/archive/id/draft-ietf-jose-fully-specified-algorithms-11.html</a> 
 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:00 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 05. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306551747/'>Mini Summit 5: Extension Management in CNPG</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The last Mini-Summit of the year features Gabriele Bartolini talking about
extension management in CloudNativePG.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	<a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306551747/">
			
		</a>
	

         
<p>The last Extension Ecosystem Mini-Summit is upon us. How did that happen?</p>
<p>Join us for a virtual conference session featuring <a href="https://www.gabrielebartolini.it">Gabriele Bartolini</a>, who
will be discussing <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306551747/">Extension Management in CNPG</a>. I’m psyched for this one,
as the PostgresSQL community has contributed quite a lot to improving
extensions management in <a href="https://cloudnative-pg.io" title="Run PostgreSQL. The Kubernetes way.">CloudNativePG</a> in the past year, some of which we
covered in <a href="https://justatheory.com/2025/04/mini-summit-two/" title="2025 Postgres Extensions Mini Summit Two: “Implementing an Extension Search Path”">previously</a>. If you miss it, the video, slides, and transcript
will appear here soon.</p>
<p>Though it may be a week or two to get the transcripts done, considering that
<a href="https://2025.pgconf.dev" title="PostgreSQL Development Conference 2025">PGConf.dev</a> is next week, and featuring the <a href="https://www.pgevents.ca/events/pgconfdev2025/schedule/session/241">Extension Ecosystem Summit</a> on
Tuesday, 13 May in Montreál, CA. Hope to see you there; be sure to say “hi!”</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
                 <a href="https://justatheory.com/tags/cloudnativepg/">CloudNativePG</a> 
                 <a href="https://justatheory.com/tags/gabriele-bartolini/">Gabriele Bartolini</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/shared/extension-ecosystem-summit/cnpg-extensions-card.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:51 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.talkingidentity.com'>Talking Identity</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.talkingidentity.com/2025/05/rsac-2025-ai-is-everywhere-trust-not-so-much.html'>RSAC 2025: AI is Everywhere. Trust? Not So Much.</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Just wrapped up a packed, somewhat frenetic, but mostly enjoyable RSAC 2025 Conference. And if I had to sum it up in a sentence: AI is everywhere, but trust and control are still catching up. The conference opened with a shot fired across the bow of the security and identity industry. Patrick Opet, CISO of […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Just wrapped up a packed, somewhat frenetic, but mostly enjoyable RSAC 2025 Conference. And if I had to sum it up in a sentence: AI is everywhere, but trust and control are still catching up.</p>


 
<a href="https://blog.talkingidentity.com/wp-content/uploads/2025/05/IMG_3063-scaled.jpg"></a> 


<p>The conference opened with a shot fired across the bow of the security and identity industry. Patrick Opet, CISO of JPMorganChase, published <a href="https://www.jpmorgan.com/technology/technology-blog/open-letter-to-our-suppliers">an open letter</a> challenging the lack of secure-by-design thinking in modern integration patterns within the SaaS world, which is breaking down essential security guardrails. He specifically called out how the success of modern identity protocols (like OAuth) in enabling these integrations is actually leading to complacency in the pursuit of speed and convenience, and that improper setup of permission scopes and controls is creating vulnerabilities across a far broader and distributed attack surface.</p>



<p>In a way, his call-to-action both complements and contradicts what I saw at RSAC 2025. AI is speeding its way into the IT and SOC infrastructure for every organization – not as a theoretical capability, but as a triage partner, copilot, and even autonomous investigator (with decision maker just around the corner). When it comes to the SOC, however, it’s becoming clear that flashy dashboards aren’t enough any more. CISOs and practitioners are looking for proof that these tools drive outcomes at scale and don’t just shift complexity elsewhere. You can’t just sprinkle some AI on your existing offerings and call it innovation.</p>



<p>Beyond tooling, a deeper theme emerged: AI is the new operational surface and the new attack surface. From agent memory manipulation to prompt injection, organizations are discovering vulnerabilities that didn’t exist a year ago. And with AI wrappers, SaaS sprawl, and loosely governed agent identities, enterprise risk is evolving faster than our control models.</p>



<p>Here’s what stuck with me most:</p>



 
 Identity is fragmenting fast: Humans, bots, APIs, and AI agents now live in parallel – each with its own lifecycle, permissions, and risks. Traditional IAM isn’t cutting it. Identity Security was one of the main themes, but few people outside of the identity bubble can properly define it. NHIs are taking over the world (hopefully not in the literal sense). Folks by and large understand that identity is key, and are paying increased attention, especially to delegated authorization, agent-specific policy enforcement, and fine-grained data access controls. 



 We’re not there yet on passkeys: While the news on passkey adoption and rollout continues to be encouraging, discussions at the conference show that it isn’t quite breaking through yet. Usability is still a major concern, manageability is viewed as a challenge, and people not immersed in identity still have fundamental questions. World Passwordkey Day happened to fall during the conference, but it is clear that there is still a lot of (hard) work to do to overcome the hesitation many stakeholders still have. 



 Memory is now a risk vector: Persistent agents that “learn” from interactions can also be misled by them. Attackers will go after context the way they once went after credentials. We need to secure not just the data agents access, but the logic and memory they build from it. 



 Identity and security must enable, not obstruct: Too many orgs still see security as something to bolt on – or worse, slow things down. But the smart ones are building it into the architecture from day one, recognizing that sacrificing a little bit of speed for control and resilience is more efficient in the long run. Identity Security has a critical role to play in this area, in making security and enablement scale together. 



 The cost curve needs a reality check: Using LLMs and AI isn’t cheap. A thousand alerts a day could cost millions in processing alone. AI-powered doesn’t always mean ROI-powered, and I was surprised that few of the customers (prospective or existing) of security tools were asking how their costs or subscription would increase due to the addition of AI powered features. We need to get sharper about value, not just capability. 



 CISOs are tired. And worried: Patrick Opet isn’t alone. Everyone’s chasing context and control. While many tools are claiming to combat alert fatigue, that is viewed largely as a temporary fix. Beyond visibility, they want clarity, resilience, and the ability to scale good decisions fast. Their overarching need is to shift the focus from detect-and-respond to prevent-and-prove. 
 



<p>RSAC confirmed what many of us already felt: despite how we may feel about its current state, we have to accept that AI is becoming a business mandate. But without smart controls, agent-level governance, and identity models that match this new reality, we’re flying blind. We’re not just securing users anymore – we’re securing decision-making. It’s time to stop asking whether AI can do it, and start asking whether we’re building the trust infrastructure to let it.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://blog.talkingidentity.com/wp-content/uploads/2025/05/IMG_3063-1024x575.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:42 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 01. May 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2025/05/2025-mini-summit-four/'>Mini Summit 4 Transcript: The User POV</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Last week Floor Drees moderated a panel on “The User POV” at the fourth
Extension Mini-Summit. Read on for the transcript and link to the video.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	<a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306682786/">
			
		</a>
	

         
<p>On April 23, we hosted the fourth of five (5) virtual Mini-Summits that lead
up to the big one at the Postgres Development Conference (<a href="https://2025.pgconf.dev" title="PostgreSQL Development Conference 2025">PGConf.dev</a>),
taking place May 13-16, in Montreál, Canada. <a href="https://www.linkedin.com/in/celeste-horgan-b65b5a1a/">Celeste Horgan</a>, Developer
Educator at Aiven, <a href="https://www.linkedin.com/in/sonia-valeja-69517a140/">Sonia Valeja</a>, PostgreSQL DBA at Percona, and <a href="https://www.linkedin.com/in/alexeypalazhchenko/overlay/about-this-profile/">Alexey
Palazhchenko</a>, CTO FerretDB, joined for a panel discussion moderated by <a href="https://dev.to/@floord">Floor
Drees</a>.</p>
 
 <a href="https://www.youtube.com/watch?v=d6XjsNAUvIw">Video</a> 
 
<p>And now, the transcripts of “The User POV” panel, by <a href="https://dev.to/@floord">Floor Drees</a></p>
 Introduction 
<p>My name is Floor, I’m one of the organizers of these Extension Ecosystem
Mini-Summits. Other organizers are also here:</p>
 
 <a href="https://justatheory.com/">David Wheeler</a>, Principal Architect at <a href="https://tembo.io/">Tembo</a>, maintainer of <a href="https://pgxn.org/">PGXN</a> 
 <a href="https://ca.linkedin.com/in/yrashk">Yurii Rashkovskii</a>, <a href="https://omnigres.com/">Omnigres</a> 
 <a href="https://pgxn.org/user/keithf4/">Keith Fiske</a>, <a href="https://www.crunchydata.com/">Crunchy Data</a> 
 <a href="https://dev.to/@floord">Floor Drees</a>, Principal Program Manager at <a href="https://enterprisedb.com" title="EnterpriseDB">EDB</a>, PostgreSQL CoCC member,
PGDay Lowlands organizer 
 
<p>The stream and the closed captions available for the recording are supported
by PGConf.Dev and their gold level <a href="https://2025.pgconf.dev/sponsors.html">sponsors</a>, Google, AWS, Huawei, Microsoft,
and EDB.</p>
<p>Next, and last in this series, on May 7 we’re gonna have <a href="https://www.gabrielebartolini.it">Gabriele Bartolini</a>
talk to us about <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306551747/">Extension Management in CloudNativePG</a>. Definitely make
sure you head over to the <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/" title="Postgres Extension Ecosystem Mini-Summit on Meetup">Meetup</a> page, if you haven’t already, and RSVP for
that one!</p>
 The User POV 
<p>Floor: For the penultimate edition of this series, we’re inviting a couple
of Postgres extension and tooling users to talk about how they pick and choose
projects that they want to use, how they do their due diligence and, their
experience with running extensions.</p>
<p>But I just wanted to set the context for the meeting today. We thought that
being in the depth of it all, if you’re an extension developer, you kind of
lose the perspective of what it’s like to use extensions and other auxiliary
tooling. You lose that user’s point of view. But users, maybe they’re coming
from other ecosystems are used to, maybe a different, probably smoother
experience. I’m coming from the Rails and Ruby community, so RubyGems are my
one stop shop for extending functionality.</p>
<p>That’s definitely a completely different experience from when I started using
Postgres extensions. That’s not to say that those ecosystems and NPM and PIP
and WordPress don’t have their own issues, ut we can certainly learn from some
of the differences between the ecosystems. Ultimately, what we want to cover
today is the experience of using extensions in 2025, and what are our users'
wishes for the future?</p>
<p>Celeste: Hello my name is Celeste, I am on the developer relations team at
Aiven. I only really started using Postgres as a part of my job here at Aiven,
but have been a much longer contributor to similar-sized ecosystems. I was
really heavily involved in the Kubernetes ecosystem for quite a while.
Kubernetes is an extensible-by-design piece of software, but it’s many, many
generations of software development later than some of the concepts that
Postgres pioneered. Thank you for having me, Floor!</p>
<p>Sonia: Hello everybody! I started working with PostgreSQL in the year
2012, and since then it has been a quite a journey. Postgres has been my
primary database, and along with learning PostgreSQL, I learned the other
database alongside. I learned Oracle, I learned SQLServer, but only from the
perspective — which is important — to migrate from X database to
PostgresSQL, as in Oracle to PostgreSQL migration, SQLServer to PostgreSQL
migration. I learned about the other databases and I’m fortunate to work as a
PostgreSQL developer, PL/pgSQL Developer, PostgreSQL DBA, onsite coordinator,
offsite coordinator, sometimes a trainer. So, in and out, it has been like I’m
breathing PostgreSQL since then.</p>
<p>Alexey: Thanks for having me! I first worked with Postgres in 2005. Fast
forward to today and I am doing FerretDB, which is the open source MongoDB
replacement built on top of PostgreSQL and also on top of the DocumentDB
extension recently open-sourced by Microsoft. We provide this extension to our
users, but also we consume this extension as users of that extension.
Somewhere in between, between 2005 and now, I also worked at Percona. At
Percona I worked on monitoring software and worked with pg_stat_statements and
pg_stat_monitor, which is made by Percona and I have pretty much a lot of
experience with Postgres extensions.</p>
<p>Floor: And you’re cheating a little on this panel, seeing as you are not
only a user but also a provider. I definitely have some questions for you!</p>
<p>And y’all talked a little about your sort of experience with extensibility of
other software or technology, and comparing that to the Postgres experience.
Can you all talk about what the main differences are that you have observed
with other ecosystems?</p>
<p>Celeste: I think as somebody who’s a bit of a newer Postgres user and I
guess comes from a different community, the biggest thing that weirded me out,
when I started working with Postgres, is that there’s no way to install an
extension except to install it against your live database.</p>
<p>If you compare that to something like Kubernetes, which again has a rather
robust extensibility ecosystem, both on the networking side of things, but
also other aspects of it, the inherent software architecture makes it so that
you have to plan out what you’re going to do, and then you apply a plan. In
theory you can’t apply a plan or add extensions to Kubernetes that won’t work
or will somehow break the system. Again, in theory, in practice things are
more interesting.</p>
<p>But with Postgres and with databases in general, you’re always working with
the live dataset, or at some point you have to work with the live dataset. So
there’s no real way to test.</p>
<p>Sonia: Most of the other databases — apart from PostgreSQL, which I have
worked with — most of them are licensed. So Oracle and SQLServer. When it
comes to PostgreSQL, it’s an open source, so you do your own thing: you do the
installation, do the checkout everything, which is open source, you can see
the code, and things like that. But when it comes to other databases, I since
it’s licensed, it is managed by the specific vendor, so you do not have rights
to do anything else. The things which will be common, like you do the POC in
both the databases before you actually implement it in the production
environment.</p>
<p>Alexey: Floor, you mentioned RubyGems, and I was thinking that actually
there is something similar between PostgreSQL extensions and RubyGems in a
sense that RubyGems quite often extend built-in Ruby classes, and Postgres
extensions could do the same. There is no separation between public and
private inside PostgreSQL, it’s all just C symbols, no special mark, don’t
touch the CPI, we are going to change it at central detail. Nothing like that.
They try not to break compatibility needlessly, but on the other hand, you
have to check all versions of your extensions with all separate versions of
PostgreSQL. In that sense it’s quite similar, unlike some other languages
where’s there’s better separation between internal private, if not on the
compiler level, at least on like documentation level or something like that.</p>
<p>Celeste: That’s not necessarily a criticism of Postgres. I think it’s just
that’s those were the tools available to Postgres as a community when Postgres
was being developed. There are some advantages to that too, because, for lack
of a better word, the lack of checks and balances let some Postgres extensions
do very, very interesting things that would maybe not be possible under a more
restricted framework.</p>
<p>Floor: The main difference I see between those two is that I know to go to
RubyGems as my place to get my plugins — or my gems, in that case. Whereas
with Postgres, they can live pretty much anywhere, right? There’s different
directories and there’s different places where you can get your stuff and
maybe there’s something that is in a private repo somewhere because that’s
what another team at your company is working on. It’s a bit of a mess, you
know? It’s really difficult to navigate, where maybe other things are lot less
difficult to navigate because there’s just the single place.</p>
<p>I wanna talk a little bit about when you’re looking for an extension to do a
certain thing for you. What do you consider when you’re looking for an
extension or when you’re comparing some of its tooling? I wrote down a couple
of things that you might be looking at, or what I might be looking at: maybe
it’s docs and tutorials, maybe it’s “has it seen a recent release?” Has it
seen frequent releases? Is there only one company that is offering this
extension? Or is it multiple companies supporting this extension? Is it a
community-built tool? Is it already in use by other teams in your company? So
it’s something that has been tested out with your system, with your stack, and
you feel like it’s something that you can easily adopt.</p>
<p>So what are some of the things for you that you definitely look at when you’re
looking to adopt new tooling?</p>
<p>Celeste: I think the main thing you wanna look for when you’re looking at
really any open source project, whether it’s an extension or not, is both
proof points within the project, but also social proof. Proof points within
the project are things that you mentioned, like is there documentation? Does
this seem to be actively maintained? Is the commit log in GitHub moving? How
many open issues are there? Are those open issues being closed over time?
Those are project health indicators. For example, if you look at the <a href="https://chaoss.community">CHAOSS
Project</a>, Dawn Foster has done a ton of work around monitoring project health
there.</p>
<p>But I think the other half of this — and this was actually something we
worked on a lot at the Cloud Native Computing Foundation when I was there, and
that work continues  — is — and this makes a bit more sense in some cases
than others — is social proof. So, are there other companies using it? Can
you point to case studies? Can you point to case studies of something being in
production? Can you point to people giving conference talks where they mention
something being in use?</p>
<p>This becomes really important when you start thinking about things being
enterprise-grade, an when you start thinking about the idea of
enterprise-grade open source. Everybody who’s on this panel works for a
company that does enterprise-grade open source database software, and you have
to ask yourself what that means. A lot of what that means is that other
enterprises are using it ,because that’s means that something comes to a
certain level of reliability.</p>
<p>Sonia: I would like to add some things. What I look at is how difficult or
how easy it is to install, configure, and upgrade the extension, and, whether
it needs restart of the database service or not. Why do I look at the restart
aspect? Because when I install it or configure or upgrade or whatever activity
I perform with it, if it requires the restart, that means it is not configured
online, so I need to involve other folks to do the database restart, as in an
application is connecting to it. When I restart, it goes for a maintenance
window for a very small time — whatever duration it goes offline, the
database service. So whether it requires restart or not, that is also very
important for me to understand.</p>
<p>Apart from the documentation, which should be of course easy to understand.
That is one of the aspects while you install and configure. It should not be
that difficult that I need to refer every time, everything, and do it, and
then maybe, I might need to create another script to use it. It should not be
the case. I look to those aspects, as well.</p>
<p>Apart from that, I also see how do I monitor the activities of this extension,
like whether it is available in the logs — what that extension is doing. So
it should not break my existing things basically. So how stable and how
durable it is, and I should be able to monitor the activities, whatever that
extension is doing.</p>
<p>From the durability perspective, even if I’m not able to monitor via logs, it
should be durable enough to that it should not break anything else, which is
up and running.</p>
<p>One more thing. I will definitely perform the POC, before putting it into the
production, into some lower environment or in my test environment somewhere
else.</p>
<p>Floor: How do you figure out though, how easy something is to sort of set
up and configure? Are you looking for that information from a README or
some documentation? Because I’ve definitely seen some very poorly documented
stuff out there…</p>
<p>Sonia: Yeah, documentation is one aspect. Apart from that, when you do the
POC, you will actually using you’ll be actually using that. So with that POC
itself, you’ll be able to understand how easy it is to install, configure, and
use it.</p>
<p>Alexey: For me as a user, I would say the most important thing is whatever
extension is packaged and easy to install. And if it’s not packaged in the
same way as PostgreSQL is packaged. For example, if I get PostgreSQL from my
Ubuntu distribution, if extension is not in the same Ubuntu target, it might
as well not exist for me because there is no way I’m going to compile it
myself. It’s like hundreds of flags and that being C, and okay, I can make it
1% faster, but then it’ll be insecure and will bring PostgreSQL down, or
worse. So there are a lot of problems like that.</p>
<p>If it’s not a package, then I would just probably just do something which is
not as good, not as stable, but I will do it myself and will be able to
support them using some third party extensions that is not packaged properly.
And properly for me, is the high bar. So if it’s some third party network of
extensions, that might be okay, I will take a look. But then of course, if
it’s in the Ubuntu repository or Debian repository, that would be of course,
much better.</p>
<p>Floor: I think that’s the build versus buy — or not necessarily buy if
it’s open source. Not to say that open source is free. But that’s the
discussion, right? When do you decide to spend the time to build something
over adopting something? And so for you, that’s mainly down to packaging?</p>
<p>Alexey: For me that’s the most important one because for features we
generally need to use in the current job and previous jobs, there are enough
hooks on the PostgreSQL itself to make what we want to do ourselves. Like if
sometimes we need to parse logs, sometimes we need to parse some low level
counters, but that’s doable and we could do it in a different language and in
the way we can maintain it ourselves. If you talk about PostgreSQL, I
typically recommend C and if there’s some problem, we will have a bigger
problem finding someone to maintain it, to fix it fast.</p>
<p>Floor: Alright When you build it yourself, would you then also open-source
it yourself and take on the burden of maintenance?</p>
<p>Alexey: I mean that really depends on the job. Like at Percona we open
sourced pg_stat_monitor. But that was like, implicit goal of making this
extension open source to make it like a superset of pg_stat_statement. In
FerretDB of course, DocumentDB is open source — we contribute to it, but I
couldn’t say that’s easier. Of course if it was written like in our perfect
language, Go, it would be much, much easier. Unfortunately, it’s not. So we
have to deal with it with packaging and what not.</p>
<p>Floor: I guess it’s also like build versus buy versus fork because there’s
definitely different forks available for a similar tooling that is just
optimized for a little bit of a different use case. But again, that’s then
another project out there that needs to be maintained.</p>
<p>Alexey: But at the same time, if you fork something, and don’t want to
contribute back, you just don’t have this problem of maintaining it for
someone else. You just maintain it for yourself. Of course, like if someone
else in upstream wants to pull your changes, they will be able to. And then
when they look at you like you’re a bad part of the community because you
don’t contribute back, but that depends on the size of the company, whatever
you have the sources and all that.</p>
<p>Celeste: But now you’re touching on something that I feel very strongly
about when it comes to open source. Why open source anything to begin with? If
we can all just maintain close forks of everything that we need, why is
Postgres open source to begin with and why does it continue to be open source
and why are we having this discussion 30 or 40 years into the lifespan of
Postgres at this point?</p>
<p>The fact of the matter is that Postgres being open source is the reason that
we’re still here today. Postgres is a 30 plus year old database at this point.
Yes, it’s extremely well architected because it continues to be applicable to
modern use cases when it comes to data. But really the fundamental of the
matter is that it is free, and being free means that two things can happen.
One, it’s a very smart move for businesses to build a business on top of a
particular piece of software. But two — and I would argue that this is
actually the more important point when it comes to open source and the long
term viability of open source — is that because it is free, that means it is
A) proliferative, it has proliferated across the software industry and B) it
is extremely valuable for professionals to learn Postgres or to learn
Kubernetes or to learn Linux because they know that they’re gonna encounter
that sometime in their career.</p>
<p>So when it comes to extensions, why open source an extension? You could simply
close source an extension. It’s the same reason: if you use open source
extensions, you can then hire for people who have potentially encountered
those extensions before.</p>
<p>I work for a managed service provider that deploys quite a few Postgreses for
quite a few clients. I obviously have a bit of a stake in the build versus buy
versus fork debate that is entirely financial and entirely linked to my
wellbeing. Regardless, it still makes sense for a company like Aiven to invest
in open source technologies, but it makes a lot more sense for us to hire
Postgres experts who can then manage those extensions and manage the
installation of those extensions and manage whether your database works or not
against certain extensions, than it is for literally every company out there
on the planet to hire a Postgres professional. There’s still a use case for
open-sourcing these things. That is a much larger discussion though, and I
don’t wanna derail this panel. [Laughs.]</p>
<p>Floor: I mean, if Alexey is game, you got yourself a conversation.</p>
<p>Alexey: First of all, I completely agree with you and I of course built my
whole carrier on open source. But there’s also the other side. So let’s say
you build an open source extension which is very specific, very niche, solves
your particular problem. And there are like 20 other people who are like, you
have the same problem, and then all 20 come to your GitHub and ask questions
about it. And they do it for free. You just waste your time supporting them
essentially. And you are a small company, you are just three people and you
open-source this extension just for fun. And they are three people and two of
them work full time and support that.</p>
<p>Celeste: Oh yeah, no, I didn’t say the economics of this worked out for
the people doing the open-sourcing, just to be perfectly clear. I think a much
larger question around the sustainability of open source communities in
general. Postgres, the overall project, and say, for example, the main
Kubernetes project, are outliers in terms of the amount of support and the
amount of manpower and people and the energy they get. Whereas most things
that get open-sourced are — I think Tidelift had <a href="https://tidelift.com/open-source-maintainer-survey-2024" title="The 2024 Tidelift state of the open source maintainer report">a survey</a>: the average
maintainer size for any given open source project is one. That is a much
larger debate though. Realistically it makes a lot of sense, particularly for
larger companies, to use open source software, Postgres included, because it
accelerates their time to innovation. They don’t need to worry about
developing a database, for example. And if they’re using Postgres and they
decide they want time series data, they don’t need to worry about migrating to
a time series database when they can just use Timescale.</p>
<p>However, “are they contributing back to those projects?” becomes a really big
question. I think the next questions that Floor would like to lead us to, amd
I’m just going to take the reins here, Floor —</p>
<p>Floor: Are you taking my job??</p>
<p>Celeste: Hardly, hardly, I could never! My understanding of why we’re
having this series of conversations that’s around the sustainability of the
Postgres extensions ecosystem,is that there’s a governance question there as
well. For the end user, the ideal state for any Postgres extension is that
they’re blessed and vetted by the central project. But as soon as you start
doing that, you start realizing how limited the resources in even a massive
project like Postgres are. And then you start asking: Where should those
people be coming from? And then you start thinking: There are companies like
Microsoft out there in the world that are hiring a lot of open source
contributors, and that’s great, but… What about the governments? What about
the universities? What about the smaller companies? The real issue is the
manpower and there’s only so far you can go, as a result of that. There’s
always sustainability issues around all open source, including Postgres
extensions, that come down to the sustainability of open source as a whole and
whether or not this is a reasonable way of developing software. Sorry to get
deep. [Laughs.]</p>
<p>Floor: Yeah, I think these are discussions that we’re definitely having a
lot in the open source community, and in the hallway at a lot of conferences.</p>
<p>We’re gonna open it up to  audience questions too in a minute. So if people
want to continue talking about the drama that is open source and sustainable
open source, we can definitely continue this discussion.</p>
<p>Maybe going back a little bit, Alexey, can we talk a little bit about —
because you’re also a provider — what your definition of “done” is or what
you wanna offer your users at minimum when you do decide to open-source some
of your stuff or make available some of some of your stuff.</p>
<p>Alexey: As an open source company, what we do, we just publish our code on
GitHub and that’s it. It’s open source, that’s done. Knock yourself out and if
you want some support, you just pay us, and then we will. That’s how we make
money. Well, of course not. That’s more complicated than that, and I wish it
was like to some degree, sometimes. Now there still a lot of users who just
come and ask for questions for free, and you want to support them because you
want to increase adoption and all that.</p>
<p>The same with extensions. So as I just described the situation, of course,
that was a bit like, not to provoke a discussion, but, let’s say you built a
PostgreSQL extension, you need to have some hooks in the core that ideally
would be stable, don’t change between versions as we discussed. That’s a bit
of a problem. PostgreSQL, no separation between private and public API. Then
how do you install? You need to package it some way that is the same way as
your current PostgreSQL version is packaged. There is no easy way, for
example, to extend a version of PostgreSQL, which is a part of Docker, you
just build your own container.</p>
<p>Celeste: I’ll segway into the point that I think I was supposed to make
when we were talking about extensions ecosystem, as opposed to a rant about
the sustainability of open source, which I am unfortunately always down to
give. Here’s the thing with extensions ecosystems. For the end user, it is
significantly more beneficial if those extensions are somehow
centrally-controlled. If you think about something like RubyGems or the Python
package installer or even Docker to a certain extent, those are all ways of
centralizing. Though with some of the exploits that have gone on with NPM
recently, there are obviously still problems there.</p>
<p>I mentioned, there’s always staffing problems when it comes to open source.
Assigning somebody to approve every single extension under the sun isn’t
really sustainable from a human perspective. The way that we handle this in
the Kubernetes community — particularly the container network interfaces, of
which there are many, many, many — is we effectively manage it with
governance. We have a page on the documentation in the website that says: here
are all the container network interfaces that have chosen to list themselves
with us. The listings are alphabetical, so there is no order of precedence.</p>
<p>The community does not take responsibility for this code because we simply
cannot. In being a container network interface, it means that they implement
certain functionalities, like an interface in the programming sense. We just
left it at that. That was the solution that the Kubernetes community came to.
I don’t know if that’s the solution that the Postgres community will
eventually come to, but community governance is a huge part of the solution to
that problem, in my opinion.</p>
<p>Alexey: I think one big difference between NPM and NodeJS ecosystem in
general, and, for example, Postgres extensions, is that NPM was so popular and
there are so many packages mostly because NodeJS by itself is quite small. The
core of NodeJS is really, really small. There is now standard library and a
lot of functionality is external. So I would say as long as your core, like
PostgreSQL or Ruby or Kubernetes is large enough, the amount of extensions
will be limited just by that. Because many people will not use any extensions,
they will just use the core. That could solve a problem of waiting and
name-squatting, but just by itself. I would say PostgreSQL more or less solves
this problem to some degree.</p>
<p>Floor: Before we open up for some questions from participants, Sonia, in a
previous call, shared a little bit of a horror story with us, with wanting to
use a certain extension and not being able to. I think this is something that
other people can resonate with, having been through a similar thing. Let’s
hear that story, And then, of course, Celeste, Alexey, if you have similar
stories, do share before we open up for questions from the rest of the peeps
joining here.</p>
<p>Sonia: So there was this requirement to transfer data from one database to
another database, specifically with respect to PostgreSQL. I wanted to
transfer the data from the production environment to some other environment,
or internally within the non-production environments. I created this extension
called <a href="https://www.postgresql.org/docs/current/contrib-dblink-function.html">dblink</a>. I’m talking about way back, 2012, 2013, somewhere, when I
started working with PostgreSQL, I used that extension. When you configure
that extension, we need to give the credentials in a human readable format.
And then, at times it also gets stored in the logs or somewhere.</p>
<p>I mean, even if it is not storing the logs, what the security team or the
audit team mentioned was that since it is using the credentials in a human
readable format, this is not good. And if somebody has has access to X
database, they also get the access to the Y database or the Y cluster. And
what if it goes to the production environment and then somebody can just steal
the data, without us even knowing it. It’ll not get logged inside the logs,
that somebody has accessed my production database via non-production database.
So that’s not good, and was not acceptable by the auditors.</p>
<p>I love that extension today also, because without doing any scripting or
anything, you just access one database from another database and then get
whatever you want. But then as a developer, it might be very easy for me to
use that thing. But then as an other person who is trying to snoop into your
production database or the other data of anything, it’s easy for them. So we
were asked not to use that extension specifically, at least not to connect to
the production environment.</p>
<p>I was working for a taxation project. It was a financial critical data, and
they did not want it to have any risk of anybody reaching to that data because
it was the numbers, the financial figures, and was critical. So that’s the
reason we were refrained from using it for that particular project. But then
other projects, which were not that critical, I somehow managed to convince
them to use it. [Laughs.]</p>
<p>Floor: So it’s sometimes you will choose it for convenience and it’s
acceptable risk, and then there might be restrictions from other teams as
well. Thanks for sharing that. If anyone wants to un-mute and ask questions or
share their own horror stories, you’re now very welcome to.</p>
<p>Yurii: There was a really good point about extensions being available as
part of your operating system environment, for example Ubuntu packages or Red
Hat packages. This is where we still have a lot of difficulty in general, in
this ecosystem. Obviously PGDG is doing an amazing job capturing a fraction of
those extensions. But because it is a complicated job, oftentimes unpaid,
people are trying to make the best out of it. On the one hand, it does serve
as a filter, as in only the best of the best extensions that people really use
get through that filter and become part of PGDG distribution. But it also
creates an impediment. For example, PGDG is not always able to update them as
the releases come out. Oftentimes people do need the latest, the best releases
available, and not when the packagers have time.</p>
<p>The other problem is how do extensions become popular if they’re not there in
the first place? It creates that kind of problem where you’re stuck with what
you have. And there’s a problem with a discovery: how do I find them? And how
do I trust this build? Or can I even get those builds for my operating system?</p>
<p>Obviously there are some efforts that try to mitigate that by building a
docker container and you run them with just copies of those files. But
obviously there’s a demand for a native deployment method. That is, if I
deploy my Postgres this way — say using RPM in my Red Hat-based distro, or
Debian based — I want everything else to fall into that. I don’t want a new
system.</p>
<p>I think we, we still have a lot of work to do on that end. I’ve been putting
some effort on our end to try and find how can we save a packager’s time that
has basically decreased the amount of work that that needs to be done. Can we
go essentially from, here’s the URL for the extension, figure it out. Like 80%
of them can, we just figure them out and package them automatically, and
repackage them when new versions come out, an only assign people on them for
the remaining 20% that are not building according to a certain convention. So
they need some attention.</p>
<p>This way we can get more extensions out and extract more value out of these
extensions. By using them, we’re helping the authors gain a wider audience and
effectively create value for everybody in the community. Otherwise, they would
feel like, “I can’t really promote this as well as I would’ve loved to, like
another ecosystems — RubyGems were mentioned today, and NPM, etc. It’s easy
to get your stuff out there. Whereas in the Postgres community, it is not easy
to get your stuff out there. Because there are so many risks associated with
that, we are oftentimes working with production data, right?</p>
<p>We need to make sure there is less friction on any other side. We need to get
these extensions to get considered. That’s at least one of the points that I
wanted to mention. I think there’s a lot to be done and I really hope that the
conference next month in Montréal will actually be a great place to get the
best minds together again and hash out some of the ideas that we’ve been
discussing in the past number of months.</p>
<p>Floor: David, do you wanna ask your question of where people go to learn
more about extensions and find their extensions?</p>
<p>David: This is something that I tried to solve a while ago with a modicum
of success — a bit. My question is, where do you all go to learn more about
extensions? To find out what extensions are available or, is there an
extension that does X, Y, Z? How do you find out if there is and, then
evaluate it? Where do you go?</p>
<p>Alexey: I generally just search, I guess. I don’t go to anything. The last
place I generally research and quite often I learned on some blog post on
sometimes on GitHub itself.</p>
<p>Celeste: If you think about that project-level activity proof, and then
the social proof, I think that Postgres actually has a really unique advantage
compared to a lot of other open source projects because it’s been going for so
long and because there is a very entrenched community. It’s very easy to find
social proof for basically anything Postgres-related that you might want.</p>
<p>If you do a search for, like, “I want a Postgres extension that does X”,
you’re going to get comparatively better Google search results because there’s
years and years and years of search results in some cases. However, that does
come with the equal and opposite problem of when you have maintenance issues,
because things have been going for years and years, and you don’t know whether
things have been maintained or not.</p>
<p>I’m thinking about this from an open source management perspective, and as
somebody who is not necessarily involved in the open source development of
Postgres. I think there is a case that you could make for some amount of
community vetting of some extensions and publicizing that community-vetting,
and having a small subset of — this has some sort of seal of approval, it’s
not gonna like nuke your database. To a certain extent, I think Postgres
already does that, because it does ship with a set of extensions by default.
In shipping with those extensions, it’s effectively saying the upstream
Postgres community blesses these, such that we will ship Postgres with them
because we are pretty confident that these are note going to nuke your
database.</p>
<p>When I was at the CNCF, I supported a whole bunch of different open source
projects. I was everybody’s documentation girl. So I’m trying to throw things
at them and then hopefully you can talk about them in Montréal and maybe
something useful will come of it. Another thing that you can use is almost
like an alpha beta experimental sort of feature where you define some set of
criteria for something being alpha or experimental, you define some set of
criteria that if met, they can call themselves beta, you define some set of
criteria of something being “production ready” for an extensions ecosystem.
Then you can have people submit applications and then it’s less of a mad rush.</p>
<p>I guess if I had any advice — not that Postgres needs my Charlton advice —
it would be to think about how you wanna manage this from a community
governance perspective, or else you will find yourself in utter mayhem.
There’s a reason that the Kubernetes container network interface page
specifies that things have to be listed in alphabetical order. It’s because
there was mayhem until we decided to list things in alphabetical order. It
seems completely silly, but it is real. [Laughs.]</p>
<p>Alexey: So my next project is going to start with “aa”.</p>
<p>Sonia: Yeah, what Celeste said. I will research about it online, normally,
and I will find something and, if I get lots of options for doing X thing, a
lot of extensions, I will go and search the documentation on postgresql.org
and then try to figure out which one is the one to start with my POC.</p>
<p>Celeste: Let me flip the question for you, Sonia. In an ideal world. If
you were to try and find an extension to use for a particular task, how would
you find that extension?</p>
<p>Sonia: Normally I will research it, Google it most of the times, and then try
to find out —</p>
<p>Celeste: But pretend you don’t have to Google it. Pretend that maybe
there’s a website or a resource. What would your ideal way of doing that be?
If you had some way that would give you more of a guarantee that it was
trustworthy, or would make it easier to find, or something. Would it be a tool
like RubyGems? Would it be a page on the Postgres website’s documentation?</p>
<p>Sonia: Page! The PostgreSQL website documentation. The Postgres
documentation is like a Bible for me, so I keep researching on that. In fact,
previously when you used to Google out anything, you used to get the initial
link as the postgresql.org, the website. Nowadays you don’t get the link as a
first link, but then I will scroll down to the page. I will try to figure out
where it is postgresql.org and then go there. That’s the first thing. Now
since I’ve been into the field, since a very long time, then I know, okay,
this website is authentic, I can go and check out the blogs, like who else has
used it or what is their experience or things like that.</p>
<p>Jay Miller: I have to ask this only because I am new to thinking about
Postgres outside of how I interact with it from a web developer’s perspective.
Usually I use some ORM, I use some module. I’m a Python developer, so I use
Python, and then from there, I don’t think about my database ever again.</p>
<p>Now I want to think about it more. I want to have a very strong relationship
with it. And we live in a world where you have to say that one of the answers
is going to be AI. One of the answers is I search for something, I get some
AI response, and, and here’s like the…</p>
  
    <blockquote>
      <p>David in comments: SLOP.</p>
    </blockquote>
  
<p>Jay: Exactly, this is the problem. If I don’t know what I should do and I
get a response, when the response could have just been, “use this extension,
it does everything you need to do and it makes your life so much easier.”
Instead, I wind up spending days, if not weeks, going in and fighting against
the system itself. Sonia, you mentioned having that experience. The idea or
the ability to discern when to go with some very kludgey PostgreSQL function
that makes your life miserable, to, “oh, there’s an extension for this
already! I’m just going to use that.” How do you expose that to people who are
not dumb, they’re not vibe coding, they just finally have a reason to actively
think about what their database is doing behind the scenes.</p>
<p>Sonia: If I understood your question correctly, you wanted to explore what
kind of activities a specific extension is doing.</p>
<p>Jay: I would just love the like, “hey, you’re trying to do a thing, this
has already been solved in this extension over here, so you don’t have to
think about it.” Or “you’re trying to do something brand new, no one’s thought
about this before, or people have thought about it before and talked about how
much of a pain it is. Maybe you should create an extension that does this. And
here’s the steps to do that.” Where is the proper documentation around coming
to that decision, or the community support for it?</p>
<p>Sonia: That’s a great question to discuss inside the community, to be
honest. Like, how do we go about that?</p>
<p>David: Come to Montréal and help us figure it out.</p>
<p>Jay: I was afraid of that answer. I’ll see you in New York, or hopefully
Chicago on Friday.</p>
<p>Floor: Fair enough, but definitely a wonderful question that we should
note down for the discussion.</p>
<p>Sonia: One thing which I want to add, this just reminded me of. There was
<a href="https://talkingpostgres.com/episodes/why-mentor-postgres-developers-with-robert-haas">one podcast which I was listening with Robert Haas</a>. The podcast is organized
by one of the Microsoft folks. The podcast was revolving around how to commit
inside the PostgreSQL, or how to read what is written inside the PostgreSQL
and the ecosystem around that. The questions were related to that. That could
also help. And of course, definitely when you go to a conference, which we are
discussing at the moment, there you’ll find a good answer. But listening to
that podcast will help you give the answers to an extent.</p>
<p>Floor: I think that’s <a href="https://talkingpostgres.com">Talking Postgres</a> with Claire Giordano, or if it
was the previous version, it was the “Path to Citus Con”, because that was
what it was called before.</p>
<p>David: The summit that’s in Montréal on May 13th is an unconference
session. We have a limited amount of time, so we want to collect topic ideas
and ad hoc votes for ideas of things to discuss. Last year I used a website
with Post-Its. This year I’m just trying a spreadsheet. I posted a link to the
Google Sheet, which anybody in the world can access and pollute — I mean,
put in great ideas — and star the ideas they’re really interested in talking
about. And I’d really appreciate, people contributing to that. Good topics
came up today! Thank you.</p>
<p>Floor: Thanks everyone for joining us. Thank you for our panelists
specifically, for sharing their experiences.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
                 <a href="https://justatheory.com/tags/celeste-horgan/">Celeste Horgan</a> 
                 <a href="https://justatheory.com/tags/sonia-valeja/">Sonia Valeja</a> 
                 <a href="https://justatheory.com/tags/alexey-palazhchenko/">Alexey Palazhchenko</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/shared/extension-ecosystem-summit/user-pov.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:02 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2666'>Five Million Italian Digital Wallet Users</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  My friend Giuseppe De Marco shared the article “Documenti su IO: 5 milioni di attivazioni per IT-Wallet” with me about how five million people are now using the Italian digital wallet. It adds the information that 4.3 million health cards, 4 million driver’s licenses and 100,000 European Disability Cards have been issued to those wallets. […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>My friend <a href="https://www.linkedin.com/in/giuseppe-de-marco-bb054245/">Giuseppe De Marco</a> shared the article “<a href="https://www.punto-informatico.it/documenti-su-io-5-milioni-attivazioni-itwallet/">Documenti su IO: 5 milioni di attivazioni per IT-Wallet</a>” with me about how five million people are now using the Italian digital wallet.  It adds the information that 4.3 million health cards, 4 million driver’s licenses and 100,000 European Disability Cards have been issued to those wallets.  These are significant accomplishments!</p>
<p>(Yes, the article is in Italian. ;-)  I read it with the assistance of machine translation.)</p>
<p>These accomplishments are made possible through use of standards.  Having just been at an <a href="https://openid.net/openid-federation-interop-apr-28-30-2025/">OpenID Federation interop event</a> in Stockholm, Sweden, I find it particularly timely that this is an example of five million people productively using <a href="https://openid.net/specs/openid-federation-1_0.html">OpenID Federation</a> in their daily lives.</p>
<p>This article about the <a href="https://innovazione.gov.it/progetti/sistema-it-wallet/">Italian Digital Wallet System</a> is a good companion piece, providing insights into the goals of the Italian Digital Wallet project.  I recommend them both!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/openid-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   10:37 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 30. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@rufftimo'>Timothy Ruff</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://rufftimo.medium.com/when-robots-lie-unethical-ai-study-reveals-dangerous-new-threats-to-digital-discourse-4c9fee096444?source=rss-11906019170a------2'>‘Unethical’ AI Study Reveals Terrifying New Threat to All Digital Discourse</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Ethical Hot&nbsp;Water  Researchers at the University of Zurich have been formally reprimanded by the university after not disclosing their use of AI in a fascinating and scarily conclusive study:  AI can change people’s minds 6X more effectively than humans&nbsp;can.  The ethical dilemma: the best way to accurately determine some AI threats may be for study subjects to not know they’re interactin
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Ethical Hot&nbsp;Water <p>Researchers at the University of Zurich <a href="https://retractionwatch.com/2025/04/29/ethics-committee-ai-llm-reddit-changemyview-university-zurich/">have been formally reprimanded</a> by the university after not disclosing their use of AI in a fascinating and <a href="https://www.reddit.com/r/changemyview/comments/1k8b2hj/meta_unauthorized_experiment_on_cmv_involving/">scarily conclusive study</a>:</p><p>AI can change people’s minds 6X more effectively than humans&nbsp;can.</p><p>The ethical dilemma: the best way to accurately determine some AI threats may be for study subjects to not know they’re interacting with AI, or even that they’re in a study. There’s no possibility for informed consent in that situation, but that’s what makes this particular effort authentic. The researchers say they couldn’t have gotten authentic results any other way, a claim that’s hard to&nbsp;refute.</p> The Incredible Danger of AI Persuasion <p>Ethics of their methods aside, the results are downright terrifying: if AI can persuade people 6X more effectively than real people can persuade people, what might bad actors do with this capability?</p>  Convince a teen they’re worthless;  Convince an employee to go&nbsp;rogue;  Break up a marriage;  Undermine confidence in an organization or&nbsp;market;  Convince someone to buy or sell anything;  Convince real people to separately make real comments, legitimizing a false narrative;  Foment hatred toward a community or&nbsp;person;  Convince a voter of almost anything.  <p>And on and&nbsp;on.</p><p>Bots now make up <a href="https://www.thalesgroup.com/en/worldwide/security/press_release/bots-now-make-nearly-half-all-internet-traffic-globally">nearly half of all internet traffic globally</a>. Half! That doesn’t tell us how much of social media is bots, however, but it’s likely close to that. Actual information may be hard to ever get from social media platforms apparently all too touchy about the subject, at least according to <a href="https://www.reddit.com/r/statistics/comments/1f6kxs0/comment/ll2xvtq/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button">one Reddit&nbsp;user</a>:</p><blockquote>“There are no currently available tools to accurately assess bot-like activity on social media… Chatgpt has made it trivial to generate convincing bots. It has even got to the point where there is an internet meme on the subject (Google “ignore all previous instructions meme”). The social media companies do not want anyone, but especially investors, to know how many active accounts are actually bots. Social media companies (twitter, meta, and Google) have become hostile to anyone doing any research on the topic and have shut down the apis that would allow you to identify bot activity…” (bold emphasis&nbsp;added)</blockquote><p>And that comment may have been written by a bot. The logic is persuasive… how would I know whether user “<a href="https://www.reddit.com/user/pjgreer/">pigreer</a>” is a real human? It’s become almost impossible to tell just by reading the output, AI is simply too good to spot the difference now.</p><p>So what can be done? Blockchain to the rescue? Um, no, we’re past&nbsp;that.</p> Blockchain Won’t Solve&nbsp;This <p>Blockchain proponents mean well, but if in 2025 they still think blockchain can solve this problem, they haven’t been paying close attention. Having been in the blockchain identity space for a decade now, decentralized identity technologies seem to have a half-life of about five years. Blockchain had its run in our space from about 2015 to 2020, maybe 2022. In that timeframe most serious blockchain projects — the ones focused on solving problems, not pitching tokens — that I’m aware of failed, despite huge amounts of funding and support from the world’s largest brands. IBM once touted blockchain as one of their three main pillars, now it’s a ghost town. Those in the SSI/decentralized identity space hardly even mention blockchain any&nbsp;more.</p><p>The main reason for these failures? “Ledger lock.” Blockchains are platforms, not protocols, and they eventually fail to get everyone to choose their chain over others — ironically proving their centralizing and competitive nature — and last only until their tokens run out of&nbsp;value.</p><p>Blockchains also struggle with security, cost, privacy, complexity, compliance, governance, interoperability and other issues — a subject of a separate piece I hope to publish soon. Blockchains are not the right tool for this problem (or, in my opinion, anything other than cryptocurrency).</p> The Only Correct Way Forward: Digital Signatures <p>The way forward — the only way I can see — is real people (and legitimate bots) must digitally sign their content, and platforms must verify these signatures and display results accordingly. If a piece of content isn’t digitally signed, the platform has a choice: display the content as “unsigned”, or block and discard. If displayed as “unsigned,” users can take that into&nbsp;account.</p><p>Signed content can display “Not a bot” or “Jill Jones” or “ACME Inc.” or whatever the platform allows and the signer&nbsp;prefers.</p><p>To be credible, a digital signature that verifies a user’s humanity — or their identity, or the organization they represent — must come from somewhere other than the user. Today, a signature would need to come from the platform itself, after vetting the user, because users don’t yet bring their own digital credentials. That’s an enormous burden for platforms having many millions or even billions of users. But I see no other way to prove the authenticity of digital content at scale, in real-time.</p><p>This unfortunate situation is both a huge threat and a huge opportunity for social media or other platforms that support third-party digital discourse: do nothing and your users could be subject to an unlimited number of surreptitious false and dangerous threats coming from digital adversaries globally, leading to lost fortunes and lost lives. Or go through the Herculean effort of vetting each real user and become a source of verifiable, authentically sourced&nbsp;content.</p><p>Doing nothing is always an option, but — as unscrupulous reserarch just showed — that just became far more dangerous.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   03:00 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 29. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://idm-thoughtplace.blogspot.com/'>IdM Thoughtplace</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://idm-thoughtplace.blogspot.com/2025/04/must-it-always-be-virtual.html'>Must it always be Virtual?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   &nbsp;  
  The
only constant in life is change
  
 
	-Heraclitus.&nbsp;
 
    

 

 
 
One of the things that most people in the Identity field know about
me is that I am a huge fan of Virtual Directory Services (VDS). But
it’s possible this is starting to
change. It’s also entirely
possible that working with the technologies at Ping Identity every
day has something to do with this. 1    
 
What I
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>&nbsp;</p><p>
</p><p>The
only constant in life is change
</p><p></p>
<p>
	-Heraclitus.&nbsp;
</p>
<p></p> <br> 

<br>

<p></p>
<p>
One of the things that most people in the Identity field know about
me is that I am a huge fan of Virtual Directory Services (VDS). But
it’s possible this is starting to
change. It’s also entirely
possible that working with the technologies at Ping Identity every
day has something to do with this. <a>1</a></p><p><br></p>
<p>
What I have always loved about a true Virtual Directory is its immediacy. Access the VDS, have it do the lookup, and then do something with the value. It doesn’t matter what the back end is—an LDAP directory, a database view, or even a CSV file. (Not that I ever wanted to go there.) Do the search, get the result, and move on with your life.</p><p><br></p>
<p>
But do we really need this when other, less complicated tools exist?
I’m starting to think that this is exactly what is happening. Let’s
face it: a Virtual Directory is a real pain to set up in the posterior&nbsp;(although once it’s running, you tend to forget it’s there).
Setting up the DIT, configuring joins of back-end sources, properly translating non-directory data into something resembling the DIT that was configured back in step 1, it's tedious and is about as error-prone a&nbsp;process as exists in the Identity field.</p>
<p><br></p><p>
What if I told you that there were solutions that just work better?</p><p><br></p>
<p>
I mean, if you just need to do some basic representations of an existing Directory and some simple transformations to handle things like mergers and acquisitions, a basic LDAP Proxy will handle this exceptionally well. There is no need to have anything else going on. A proxy also handles essential use cases such as Pass-Through Authentication, which can be helpful during “lazy migration”
scenarios.</p>
<p><br></p><p>
If you need to access different types of data, we need to think about
what are we doing with it. Does it really need to be referenced in
some sort of LDAP schema? Does inetOrgPerson (or other LDAP Object
classes) necessarily give any true advantages? Most of the time when
we need this information it’s to choose a course of action during
an identity related process. 
</p><p><br></p>
<p>What are we doing with it? Does it need to be referenced in some LDAP schema? Does inetOrgPerson (or other LDAP Object classes) necessarily give any actual advantages? Most of the time, we need this information to choose a course of action during an identity-related</p><p>
So, instead of the virtual attribute, why not consider fine-grained
authentication tools? The whole point here is that we are
looking at specific identity attributes to determine access or those involved in an orchestration flow, where
both data and policies are subject to change at a moment’s notice.
Being able to look up and evaluate that data with the same
tool seems to make the most sense to me.</p>
<p><br></p><p>
To me, the biggest value here is more efficient access to data and understanding how said data will be used. In an age where we are increasingly concerned with governance, compliance, and regulation, maybe this is the way we need to think about identity data and how it is represented for use in identity-related operations.</p>
<p>
<br>
<br>

</p>
<p>
<br>
<br>

</p>
 <p>1 My opinions remain my own, and nothing said here represents any
	official positions or statements from Ping Identity or any
	organization I might be associated with unless otherwise specified.</p> 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   00:52 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 28. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2025/04/update-control/'>Update Your Control Files</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Suggestions to PostgreSQL extension maintainers to make some tweaks to your
.control files and Makefiles.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Reviews of the extension search path patch, now <a href="https://github.com/postgres/postgres/commit/4f7f7b0">committed</a> and slated
for PostgreSQL 18, revealed a few issues with extension configuration. Based
on the ensuing discussion, and even though PostgreSQL 18 will include
workarounds, it’s best to make adjustments to the extensions you maintain, the
better to serve existing PostgreSQL versions and to hew closer to best
practices.</p>
<p>Thus, a couple of recommendations for extension maintainers.</p>
 
 
<p>Remove the <code>$libdir/</code> prefix from the <code>module_pathname</code> directive in the
<a href="https://www.postgresql.org/docs/current/extend-extensions.html#EXTEND-EXTENSIONS-FILES">control file</a>. The <code>$libdir/</code> requires extension modules to live in
<code>pkglibdir</code> (see <a href="https://www.postgresql.org/docs/17/app-pgconfig.html">pg_config</a>), and no other directories included in
<code>dynamic_library_path</code>, which limits where users can install it. Although
PostgreSQL 18 will ignore the prefix, the docs will also no longer
recommend it.</p>
 
 
<p>Remove the <code>directory</code> parameter from the <a href="https://www.postgresql.org/docs/current/extend-extensions.html#EXTEND-EXTENSIONS-FILES">control file</a> and the
<code>MODULEDIR</code> directive from the <code>Makefile</code>. Honestly, few people used these
directives, which installed extension files in subdirectories or even
completely different absolute directories. In some cases they may have
been useful for testing or extension organization, but the introduction of
the <a href="https://github.com/postgres/postgres/commit/4f7f7b0">extension search path</a> alleviates its use cases.</p>
 
 
<p>These changes will future-proof your extensions and make them better ecosystem
citizens. Plus, they clean out some otherwise funky configurations that just
aren’t necessary. Make the changes today — and while you’re at it, test your
extensions with PostgreSQL 18 pre-releases!</p>
<p>Look, <a href="https://github.com/theory/pg-semver/pull/76">I’ll go first</a>.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgxs/">PGXS</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   20:08 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.patrick-breyer.de/soll-ich-der-elektronischen-patientenakte-widersprechen-und-wie-geht-das/'>Soll ich der elektronischen Patientenakte widersprechen und wie geht das?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Wenn du gesetzlich krankenversichert bist, stellt deine Krankenkasse ab 29.04.2025 eine elektronische Patientenakte für dich bundesweit bereit – es sei denn, du widersprichst. Durch einen Widerspruch dürfen dir keine Nachteile entstehen. … 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Wenn du gesetzlich krankenversichert bist, stellt deine Krankenkasse ab 29.04.2025 eine elektronische Patientenakte für dich bundesweit bereit – es sei denn, du widersprichst. Durch einen Widerspruch dürfen dir keine Nachteile entstehen. Die Anschreiben der Krankenkassen informieren <a href="https://www.vzbv.de/pressemitteilungen/elektronische-patientenakte-krankenkassen-informieren-unzureichend">unzureichend</a>. Bedenke bei deiner Entscheidung: 



<p></p>



 
 
<p>1. Inhalt und Datensicherheit: Ohne deinen Widerspruch wird jeder Arztbesuch, jede Therapie und jede Medikation in einer zentral abrufbaren elektronischen Patientenakte gespeichert, darunter Arztbriefe, Entlassungsberichte und Diagnosekennungen. Hierunter fallen unter anderem auch Informationen zu deinen Suchtkrankheiten, Psychotherapien und psychischen Erkrankungen, Schwangerschaftsabbrüchen, Geschlechtskrankheiten und Reproduktionsstörungen (z.B. Impotenz), die gespeichert und zusammengetragen werden.&nbsp;Durch die zentrale Speicherung aller Behandlungen in einer elektronischen Patientenakte erhältst du eine einheitliche Zugriffsmöglichkeit auf deine Patientendaten. Andererseits waren in der Vergangenheit bereits mehrere zentralisierte, europäischen Gesundheitssysteme von umfangreichen Datenpannen betroffen. Bei der elektronischen Patientenakte in Deutschland ist noch nichts passiert. Sicherheitsforscher <a href="https://www.ccc.de/de/updates/2024/ende-der-epa-experimente">warnen</a> jedoch, Kriminelle könnten mit wenig Aufwand auf beliebige Patientenakten zugreifen. Falls es zu Hacking, Datendiebstahl oder Datenverlust kommen sollte, können viel mehr Informationen verloren gehen als ohne elektronische Patientenakte.</p>
 



 

			
		
 
 



<p>2. Direkter Zugriff: Mit der elektronischen Patientenakte können behandelnde Ärzte, deine Apotheker, deine Physiotherapeuten, dein Pflegeheim, usw., sowohl in Deutschland als auch künftig im EU-Ausland deine elektronische Patientenakte direkt einsehen. Sie können sie auch in ihrer eigenen Praxissoftware speichern. Die Vorteile: Der Datenzugriff kann deine Behandlungen und Medikation wesentlich beschleunigen und verbessern, z. B. auch fehlerhafte Medikamentationen vermeiden. Unterlagen müssen nicht erst angefordert werden und liegen auch im Notfall unmittelbar vor. Doppeluntersuchungen entfallen. Ein möglicher Nachteil könnte sich daraus ergeben, dass für&nbsp;andere medizinische Stellen nicht relevante Behandlungen, Medikamentationen, etc. abgerufen und somit eingesehen werden können (z.B. dein Zahnarzt oder Apotheker könnte von deiner Psychotherapie oder Geschlechtskrankheit erfahren).</p>



<p>Ohne elektronische Patientenakte wirst du, so wie du es bisher kennst und möglicherweise erlebt hast, auf einem Anamnesebogen nach Vorbehandlungen, Medikamentationen, Unverträglichkeiten, Allergien, usw. gefragt. Gegebenenfalls werden weitere Unterlagen angefordert wie z. B. MRT-Aufnahmen, usw. Es gibt auch einen papierbasierten, bundeseinheitlichen Medikationsplan, den du dir ausdrucken lassen kannst. Dieser ist aber auch in der elektronischen Patientenakte speicherbar. Es ist Patienten nicht möglich, aus der elektronischen Medikationsliste einzelne (sensible) Medikamente zu entfernen.</p>



<p>Mit elektronischer Patientenakte kannst du mündlich, über eine App oder über deine Krankenkasse einzelne (ärztliche) Leistungserbringer komplett vom Zugriff auf deine elektronische Patientenakte ausschließen. Dies ist mit gewissen Herausforderungen verbunden. Du hast auch das Recht, gegenüber Leistungserbringern der Übertragung von einzelnen Informationen in die elektronische Patientenakte zu widersprechen. Du kannst einzelne Dokumente auch nachträglich wieder aus ihrer elektronischen Patientenakte löschen (lassen) – leider wieder recht kompliziert. Bedenke, dass Löschungen Nachteile für deine Behandlung nach sich ziehen können. Nicht möglich ist es, einzelne Dokumente nur für bestimmte Stellen (z.B. Ärzte) freizugeben oder zu sperren.</p>



<p>3. Weitergabe an Dritte: Deine elektronische Patientenakte kann für bestimmte Zwecke auch an Politik und Behörden, Forschende und Wirtschaftsunternehmen (z.B. Technologieunternehmen und Pharmaindustrie) weitergegeben werden. Die Chancen: Anonymisierte Gesundheitsdatenanalysen ermöglichen bedeutende Fortschritte in der Pandemie- und Endemieeindämmung, z. B. durch Früherkennung, sowie in der Forschung, z.B. bei der Entwicklung sicherer Medikamente. Die Risiken: Nur teilweise werden Daten vor der Weitergabe anonymisiert. In anderen Fällen werden dein Name und deine Versichertennummer zwar entfernt, aber über andere Kennzeichen bleibst du eindeutig identifizierbar (sog. Pseudonymisierung). So kann es Dritten gelingen, deine Krankheitsgeschichte wieder dir zuzuordnen.</p>



<p>Hinweis: Auch wenn du die elektronische Patientenakte nutzt, kannst du ihrer Weitergabe an Dritte frühestens ab dem 15.01.2025 separat widersprechen (über eine App deiner Krankenkasse oder durch <a href="https://widerspruch-epa.de/widerspruch-gegen-uebernahme-forschungsdaten/">Brief an ihre Ombudsstelle</a>).</p>



 Ergebnis: Es ist letztendlich deine Entscheidung, ob du&nbsp;eine elektronische Patientenakte haben möchtest oder nicht. 



<p></p>



<p>Wenn du eine elektronische Patientenakte geführt haben möchtest, brauchst du nichts zu tun – sie wurde zum 15. Januar 2025 automatisch eingerichtet.</p>



<p>Wenn du dich gegen eine elektronische Patientenakte entscheidest, hilft dir der <a href="https://widerspruch-epa.de/widerspruchs-generator/">Widerspruchs-Generator</a> ein Widerspruchsschreiben zu erstellen. Du kannst auch per Krankenkassen-App oder – bei einigen Krankenkassen – auch <a href="https://www.heise.de/ratgeber/Widerspruch-bei-der-ePA-fuer-alle-Was-bisher-bekannt-ist-9710212.html">über deren Website widersprechen</a>.</p>



<p>Du kannst jederzeit widersprechen; dann werden alle bereits gespeicherte Daten wieder aus der elektronischen Patientenakte gelöscht. Solltest du später einmal deine Entscheidung ändern, kannst du deinen Widerspruch jederzeit wieder zurücknehmen und für die Zukunft eine elektronische Patientenakte bekommen.</p>



<p><a href="https://www.gkv-spitzenverband.de/media/dokumente/service_1/epa/InfoM_343_ePA_final_Stand_2024-06-03_Webfassung.pdf">Weitere Informationen des GKV-Spitzenverbands (mit der Bundesdatenschutzbeauftragten abgestimmt)</a></p>



<p><a href="https://www.patrick-breyer.de/beitraege/europaeischer-raum-fuer-gesundheitsdaten/">Wie die Piraten einen Zwang zur elektronischen Patientenakte verhindert haben</a><br></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://www.patrick-breyer.de/wp-content/uploads/2023/12/231213_Breyer-sensibleDaten-deutsch-1024x1024.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   09:32 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 26. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2657'>Hybrid Public Key Encryption (HPKE) for JOSE incorporating feedback from IETF 122</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The “Use of Hybrid Public-Key Encryption (HPKE) with JSON Object Signing and Encryption (JOSE)” specification has updated to incorporate feedback from IETF 122 in Bangkok. Per the History entries, the changes were: Use "enc":"int" for integrated encryption. Described the reasons for excluding authenticated HPKE. Stated that mutually known private information MAY be used as the […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The “<a href="https://www.ietf.org/archive/id/draft-ietf-jose-hpke-encrypt-08.html">Use of Hybrid Public-Key Encryption (HPKE) with JSON Object Signing and Encryption (JOSE)</a>” specification has updated to incorporate feedback from <a href="https://datatracker.ietf.org/meeting/122/proceedings">IETF 122 in Bangkok</a>.</p>
<p>Per the History entries, the changes were:</p>
 
 Use <code>"enc":"int"</code> for integrated encryption. 
 Described the reasons for excluding authenticated HPKE. 
 Stated that mutually known private information MAY be used as the HPKE info value. 
 
<p>At this point, the authors have closed all the <a href="https://github.com/ietf-wg-jose/draft-ietf-jose-hpke-encrypt">issues and PRs</a> that we believe there’s consensus to address.  I would normally suggest that we’re ready for working group last call at this point, but I’d like us to take the extra step to verify that the spec is aligned with the <a href="https://datatracker.ietf.org/doc/draft-ietf-cose-hpke/">COSE HPKE</a> spec first.  Both as an author of the JOSE HPKE spec and as a COSE chair interested in the COSE HPKE spec, I’d request that members of both working groups review the specs together and send their feedback.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   00:27 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 23. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/what-iiw-participants-value-most'>What IIW Participants Value Most</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Last week, I posted a report on IIW XL, our fortieth event.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 <a href="https://substackcdn.com/image/fetch/$s_!F341!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa61097be-5c42-46ab-81cb-e44444a067e0_8001x4501.heic">          </a> <p>Last week, I posted a <a href="https://windley.com/archives/2025/04/internet_identity_workshop_xl_report.shtml">report on IIW XL</a>, our fortieth event. When participants register, one of the questions we ask them is what they value most about IIW. Over 100 people answered that question. Rather than bore you with the raw data, I asked ChatGPT to summarize the responses. Here's what it said:</p><p>Attendees of the Internet Identity Workshop (IIW) overwhelmingly value the event for its strong sense of community, collaborative spirit, and the opportunity to connect in person with peers, innovators, and industry leaders. Many describe the environment as one of mutual respect and openness, where "creative, open discussions" thrive and "everyone is there" to engage deeply on current and emerging identity challenges. The unconference format stands out as a major strength, allowing participants to shape the agenda, dive into interactive workshops, and experience "productive conversations with other attendees" in a way that feels dynamic and inclusive.</p><p>Another consistent theme is access to cutting-edge knowledge and thought leadership in digital identity. Attendees appreciate being "in the room where the future of identity is being designed," hearing about "the latest developments in enterprise IAM," and learning directly from experts in topics like decentralized identity, verifiable credentials, OAuth, and OpenID Connect. The opportunity to "catch up on standards," "inform product roadmaps," and "gain knowledge about key trends" makes IIW not just informative but strategically valuable.</p><p>Crucially, IIW is also seen as a place where real progress happens. Participants value the ability to test ideas, gain feedback, and move forward on shared goals in a collaborative setting. As one attendee put it, it's a rare opportunity "to explore problem spaces and solution spaces together," while another highlighted the value of "making progress on standards or other collaborative efforts." The event's unique mix of expertise, spontaneity, and shared purpose creates the conditions for meaningful breakthroughs that extend well beyond the workshop itself.</p><p>Beyond the sessions, many emphasized the personal and professional relationships formed over time—"the relationships that have been developed over many years" and the chance to "collaborate in person with colleagues around the world." Several first-time attendees expressed excitement about joining a space described as "unlike any other" and "highly recommended" by peers. Whether returning veterans or newcomers, participants consistently frame IIW as a place of learning, contribution, and genuine connection.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!F341!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa61097be-5c42-46ab-81cb-e44444a067e0_8001x4501.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:52 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2653'>OAuth 2.0 Protected Resource Metadata is now RFC 9728</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The OAuth 2.0 Protected Resource Metadata specification has been published as RFC 9728! This is certainly the longest that any RFC that I have worked on has taken from initial individual draft to RFC – August 2016 to April 2025 – 8 years and 8 months. As we discussed at the 2025 OAuth Security Workshop […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The OAuth 2.0 Protected Resource Metadata specification has been published as <a href="https://www.rfc-editor.org/rfc/rfc9728.html">RFC 9728</a>!  This is certainly the longest that any RFC that I have worked on has taken from <a href="https://datatracker.ietf.org/doc/draft-jones-oauth-resource-metadata/00/">initial individual draft</a> to RFC – August 2016 to April 2025 – 8 years and 8 months.  <a href="https://self-issued.info/?p=2615">As we discussed</a> at the <a href="https://oauth.secworkshop.events/osw2025">2025 OAuth Security Workshop</a> in Reykjavík:</p>
<blockquote><p>
Timing can be fickle.  What may not be useful at one time can turn out to be useful later.
</p></blockquote>
<p>Per the abstract, here’s what it adds to the OAuth 2.0 family of specifications:</p>
<blockquote><p>
This specification defines a metadata format that an OAuth 2.0 client or authorization server can use to obtain the information needed to interact with an OAuth 2.0 protected resource.
</p></blockquote>
<p>It joins the OAuth 2.0 Dynamic Client Registration Protocol <a href="https://www.rfc-editor.org/rfc/rfc7591.html">[RFC 7591</a>] and OAuth 2.0 Authorization Server Metadata [<a href="https://www.rfc-editor.org/rfc/rfc8414.html">RFC 8414</a>] specifications, completing the set of metadata specifications for all three OAuth 2.0 roles.</p>
<p>I’m glad to have co-authored this one with long-time collaborator <a href="https://www.linkedin.com/in/phunt/">Phil Hunt</a> and new collaborator <a href="https://www.linkedin.com/in/aaronparecki/">Aaron Parecki</a>.  And I’m proud of the fact that all of <a href="https://datatracker.ietf.org/person/michael_b_jones@hotmail.com#rfcs-1">my last five RFCs</a> had a co-author for which it was their first RFC; in this case, it’s Aaron’s first RFC.</p>
<p>Congratulations, Aaron!  It was a pleasure working on this with you.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/oauth_logo_120x120.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:27 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 21. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306682918/'>Mini Summit 4: The User POV</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Join our fine panel on Wednesday at Extension Mini Summit #4 to hear all about
“The User POV” — for better and for worse!
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	<a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306682786/">
			
		</a>
	

         
<p>And we’re back.</p>
<p>This Wednesday, April 9 at noon America/New_York (16:00 UTC) for <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306682918/" title="Postgres Extensions Ecosystem Mini-Summit #4">Extension
Mini Summit #4</a>, where our panel consisting of <a href="https://www.linkedin.com/in/celeste-horgan-b65b5a1a/">Celeste Horgan</a> (Aiven),
<a href="https://www.linkedin.com/in/sonia-valeja-69517a140/">Sonia Valeja</a> (Percona), and <a href="https://www.linkedin.com/in/alexeypalazhchenko/overlay/about-this-profile/">Alexey Palazhchenko</a> (FerretDB) will discuss
“The User POV”. This session will be a terrific opportunity for those of us
who develop extensions to get an earful from the people who use them, in both
anger and joy. Bang on the <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/" title="Postgres Extension Ecosystem Mini-Summit on Meetup">Meetup</a> to register for this live video session.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
                 <a href="https://justatheory.com/tags/celeste-horgan/">Celeste Horgan</a> 
                 <a href="https://justatheory.com/tags/sonia-valeja/">Sonia Valeja</a> 
                 <a href="https://justatheory.com/tags/alexey-palazhchenko/">Alexey Palazhchenko</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/shared/extension-ecosystem-summit/user-pov.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   17:26 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/internet-identity-workshop-xl-report'>Internet Identity Workshop XL Report</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This past week, we held the 40th Internet Identity Workshop—a milestone event that brought together a global community of builders, dreamers, and implementers focused on the future of digital identity.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 <a href="https://substackcdn.com/image/fetch/$s_!cHin!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96923700-d120-4151-b51c-be9602ca23f9_2072x1248.heic">          </a> <p>This past week, we held the 40th Internet Identity Workshop—a milestone event that brought together a global community of builders, dreamers, and implementers focused on the future of digital identity. And what a gathering it was.</p> <a href="https://substackcdn.com/image/fetch/$s_!HZ6f!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29a39278-5a21-4bd0-83c2-544a81dacc6f_2047x739.heic">          </a> <p>If there's any lingering doubt about IIW's reach or relevance, just take a look at the map of attendees. People came from all over the world to shape the conversation on the importance of digital identity in the modern age.</p><p>As expected, the United States made up the bulk of attendees, with 223 participants from across 20+ states, but what stood out this time was the breadth of international participation:</p>  <p>🇰🇷 South Korea: 12 attendees</p>  <p>🇨🇦 Canada: 11 attendees</p>  <p>🇯🇵 Japan: 10 attendees</p>  <p>🇩🇪 Germany: 7 attendees</p>  <p>🇬🇧 United Kingdom: 5 attendees</p>  <p>🇮🇳 India: 4 attendees</p>  <p>🇮🇹 Italy, 🇩🇰 Denmark, 🇦🇹 Austria, 🇦🇺 Australia: 3 each</p>  <p>🇨🇷 Costa Rica, 🇨🇴 Colombia: 2 each</p>  <p>🇺🇦 Ukraine, 🇹🇭 Thailand, 🇹🇼 Taiwan, 🇨🇭 Switzerland, 🇸🇪 Sweden, 🇪🇸 Spain, 🇿🇦 South Africa, 🇵🇹 Portugal, 🇳🇿 New Zealand, 🇳🇱 Netherlands, 🇮🇪 Ireland, 🇫🇷 France, 🇪🇬 Egypt, 🇨🇱 Chile, 🇦🇷 Argentina: 1 each</p>  <p>That's 28 countries represented—more than we've ever had before. We still need more participation from Africa. We have a scholarship program if that would help you come! </p> <a href="https://substackcdn.com/image/fetch/$s_!Y8ph!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97220616-1269-4398-ab62-bab8c3983f91_2041x1275.heic">          </a>  California: The Identity Capital (Again) <p>Of course, California led the way in states with a whopping 117 attendees, and cities like San Francisco (24), San Jose (19), Oakland, and Mountain View formed a familiar cluster of identity wonks. Other strong showings came from Washington (18 attendees), Utah (12), and the tech corridors of Massachusetts and New York.</p><p>One surprise highlight? Seocho-gu, South Korea, which sent 10 participants—a remarkable show of commitment from a single district in Seoul. We're seeing more and more investment from Asia in building open, interoperable identity layers, and it's a welcome sign.</p> <a href="https://substackcdn.com/image/fetch/$s_!vOv0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b95308c-e74c-4399-8233-f15faa7f15d1_2048x1114.heic">          </a>  What We Talked About <p>While I'll save the detailed session notes for the <a href="https://internetidentityworkshop.com/past-workshops/">Book of Proceedings</a> (still to come), a few themes emerged repeatedly:</p>  <p>Agent-based architecture is gaining traction, and the discussions around personal digital agents (and their governance) were some of the most animated of the week.</p>  <p>Interoperability wasn't just a buzzword—there were concrete efforts to align schemas, protocols, and credential formats across communities.</p>  <p>Authenticity and trust were explored beyond technology—touching on human governance, decentralized reputation, and context-aware interactions.</p>  <p>And yes, AI made its appearance—both as a tool for agent enhancement and a source of identity risk.</p>   <a href="https://substackcdn.com/image/fetch/$s_!JW0C!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F875da224-2b9d-4e7c-9032-ff6db3534fdf_2048x1342.heic">          </a> <p>It's worth noting that the sessions weren't just technical deep dives. Some of the most impactful conversations happened in hallway chats, whiteboard scribbles, and shared coffee lines.</p> IIW Still Feels Like a Meetup (and That's a Good Thing) <p>Despite this being the 40th edition, IIW retains its uniquely informal, self-organized flavor. There's no main stage, no keynotes, and no vendors hawking wares. Just a grid of ideas and a crowd of people who care enough to show up, share, and build.</p><p>That's what makes IIW magical.</p><p>To everyone who attended—whether from Sunnyvale or São Paulo, Tokyo or Toronto—thank you for being part of this milestone. Let's keep making identity better.</p> <a href="https://substackcdn.com/image/fetch/$s_!YRhO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8cf0c2f-8606-430f-a4bc-59da3977075f_2048x1365.heic">          </a> <p>You can access all <a href="https://www.flickr.com/photos/docsearls/albums/72177720325192369/">Doc's wonderful pictures of IIW XL</a> on Flickr.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!cHin!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96923700-d120-4151-b51c-be9602ca23f9_2072x1248.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:10 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2025/04/21/implement-client-assertions-for-oauth-client-credential-flows-in-asp-net-core/'>Implement client assertions for OAuth client credential flows in ASP.NET Core</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This blog implements client assertions using an OAuth client credential flow in ASP.NET Core. Client assertions provide a secure way for client authentication without sharing a secret, enhancing the security the OAuth client credentials flow. By using JSON Web Tokens (JWTs) client assertions, this approach ensures strong client identity (application) verification and mitigates risks associated […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This blog implements client assertions using an OAuth client credential flow in ASP.NET Core. Client assertions provide a secure way for client authentication without sharing a secret, enhancing the security the OAuth client credentials flow. By using JSON Web Tokens (JWTs) client assertions, this approach ensures strong client identity (application) verification and mitigates risks associated with traditional shared client secrets.</p>



<p>Code: <a href="https://github.com/damienbod/OAuthClientAssertions">https://github.com/damienbod/OAuthClientAssertions</a></p>



<p>Blogs in this series</p>



 
 Implement client assertions for OAuth client credential flows in ASP.NET Core 



 <a href="https://damienbod.com/2025/02/24/use-client-assertions-in-openid-connect-and-asp-net-core/">Using client assertions in OpenID Connect and ASP.NET Core</a> 



 <a href="https://damienbod.com/2025/05/12/implement-client-assertions-with-client-credentials-flow-using-oauth-dpop/">Implement client assertions with client credentials flow using OAuth DPoP</a> 
 



<p>NOTE: The code in the blog and the linked repository was created using the samples from IdentityServer.</p>



 Setup 



<p>Three applications are used in this setup, an API which uses the access token, an OAuth client application implemented as a console app and an OAuth server, implemented using ASP.NET Core and Duende IdentityServer. OAuth client credentials is used to acquire the access token and the signed JWT is used to authenticate the client request.</p>







<p></p>



 Console client 



<p>The console application is implemented using the Nuget packages from Duende. The Duende.IdentityModel and Duende.AccessTokenManagement.OpenIdConnect Nuget packages are added to the project. </p>



<p>The signing key is created from PEM files but this can imported from any source. Only the private key is required to sign the JWT client assertion.</p>


  var privatePem = File.ReadAllText(Path.Combine("", "rsa256-private.pem"));
var publicPem = File.ReadAllText(Path.Combine("", "rsa256-public.pem"));
var rsaCertificate = X509Certificate2.CreateFromPem(publicPem, privatePem);
var signingCredentials = new SigningCredentials(new X509SecurityKey(rsaCertificate), "RS256");
  


<p></p>



<p>The JWT is created from the Duende sample code. The JWT is specified in an OAuth specification and all OAuth, OpenID Connect servers can implement this or have implemented this.</p>


  // Code from the Duende samples.
static string CreateClientToken(SigningCredentials credential, string clientId, string audience)
{
    var now = DateTime.UtcNow;

    var token = new JwtSecurityToken(
        clientId,
        audience,
        new List&lt;Claim&gt;()
        {
            new Claim(JwtClaimTypes.JwtId, Guid.NewGuid().ToString()),
            new Claim(JwtClaimTypes.Subject, clientId),
            new Claim(JwtClaimTypes.IssuedAt, now.ToEpochTime().ToString(), ClaimValueTypes.Integer64)
        },
        now,
        now.AddMinutes(1),
        credential
    );

    var tokenHandler = new JwtSecurityTokenHandler();
    var clientToken = tokenHandler.WriteToken(token);
    "\n\nClient Authentication Token:".ConsoleGreen();
    Console.WriteLine(token);
    return clientToken;
}
  


<p></p>



<p>The token is requested using the private key. The ClientAssertion parameter is used to add the JWT to the token request. </p>


  static async Task&lt;TokenResponse&gt; RequestTokenAsync(SigningCredentials signingCredentials)
{
    var client = new HttpClient();

    var disco = await client.GetDiscoveryDocumentAsync("https://localhost:5001");
    if (disco.IsError) throw new Exception(disco.Error);

    var clientToken = CreateClientToken(signingCredentials, "mobile-client", disco.Issuer);
    var response = await client.RequestClientCredentialsTokenAsync(new ClientCredentialsTokenRequest
    {
        Address = disco.TokenEndpoint,

        ClientAssertion =
        {
            Type = OidcConstants.ClientAssertionTypes.JwtBearer,
            Value = clientToken
        },

        Scope = "mobile",
    });

    if (response.IsError) throw new Exception(response.Error);
    return response;
}
  


<p></p>



 OAuth server using Duende IdentityServer 



 Client configuration 



<p>The Duende client configuration allows Secret definitions for the public key of the client assertion.</p>


  new Client
{
    ClientId = "mobile-client",
    ClientName = "Mobile client",

    AllowedGrantTypes = GrantTypes.ClientCredentials,
    ClientSecrets =
    [
        new Secret
        {
            // X509 cert base64-encoded
            Type = IdentityServerConstants.SecretTypes.X509CertificateBase64,
            Value = Convert.ToBase64String(rsaCertificate.GetRawCertData())
        }
    ],

    AllowedScopes = { "mobile" }
},
  


<p></p>



 Services setup 



<p>The AddJwtBearerClientAuthentication extension method is used to add the support for the client assertions.</p>


  var idsvrBuilder = builder.Services
 .AddIdentityServer(options =&gt;
 {
	 options.Events.RaiseErrorEvents = true;
	 options.Events.RaiseInformationEvents = true;
	 options.Events.RaiseFailureEvents = true;
	 options.Events.RaiseSuccessEvents = true;

	 options.EmitStaticAudienceClaim = true;
 })
 .AddInMemoryIdentityResources(Config.IdentityResources)
 .AddInMemoryApiScopes(Config.ApiScopes)
 .AddInMemoryClients(Config.Clients(builder.Environment))
 .AddAspNetIdentity&lt;ApplicationUser&gt;();

idsvrBuilder.AddJwtBearerClientAuthentication();
  


<p></p>



 Notes 



<p>The client assertion was created using an RSA key but other key types can sizes can be used. Refer to the NIST specifications for the actual recommendations. Client assertions offer a way to avoid shared secrets between the client and the OAuth server. If implementing both client and server applications and sharing the secret in an Azure Key Vault, this client assertion has no real security improvement. Clients can also import the key from the host environment. </p>



 Links 



<p><a href="https://docs.duendesoftware.com/identityserver/v7/tokens/authentication/jwt/">https://docs.duendesoftware.com/identityserver/v7/tokens/authentication/jwt/</a></p>



<p><a href="https://docs.duendesoftware.com/identityserver/v7/reference/validators/custom_token_request_validator/">https://docs.duendesoftware.com/identityserver/v7/reference/validators/custom_token_request_validator/</a></p>



<p><a href="https://docs.duendesoftware.com/identityserver/v7/tokens/authentication/jwt/">https://docs.duendesoftware.com/identityserver/v7/tokens/authentication/jwt/</a></p>



<p><a href="https://docs.duendesoftware.com/foss/accesstokenmanagement/advanced/client_assertions/">https://docs.duendesoftware.com/foss/accesstokenmanagement/advanced/client_assertions/</a></p>



<p><a href="https://www.scottbrady.io/oauth/removing-shared-secrets-for-oauth-client-authentication">https://www.scottbrady.io/oauth/removing-shared-secrets-for-oauth-client-authentication</a></p>



 
<blockquote><a href="https://damienbod.com/2025/02/24/use-client-assertions-in-openid-connect-and-asp-net-core/">Use client assertions in OpenID Connect and ASP.NET&nbsp;Core</a></blockquote>
 



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2025/03/oauthccsignedjwtassertion.png?w=580'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   08:24 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 20. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://theblockstalk.medium.com/'>Jack the Blockstalk</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://theblockstalk.medium.com/elf-a-social-card-game-of-collaboration-deception-and-suspense-730c107bccf3?source=rss-d8f296833e37------2'>Elf — A Social Card Game of Collaboration, Deception, and Suspense</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Elf — A Social Card Game of Collaboration, Deception, and&nbsp;Suspense  In 2015, deep in Patagonia, I was taught a unique and little-known card game called by a group of Israeli travelers they called Elf. Since then, while teaching it to friends, family and complete strangers it has captivated the hearts of all players! To this day, it remains undocumented — until&nbsp;now.  Elf is a hybrid of te
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/491/1*zdz_QU68oKJVP2rbUFAtJw.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Elf — A Social Card Game of Collaboration, Deception, and&nbsp;Suspense <p>In 2015, deep in Patagonia, I was taught a unique and little-known card game called by a group of Israeli travelers they called Elf. Since then, while teaching it to friends, family and complete strangers it has captivated the hearts of all players! To this day, it remains undocumented — until&nbsp;now.</p><p>Elf is a hybrid of teamwork and treachery. It thrives in social settings, encouraging conversation, suspicion, and strategy. The beauty of Elf lies in its simplicity: everyone collaborates to win — except one player, who must lie and manipulate to avoid&nbsp;losing.</p> 🃏 Overview <p>Players: 4–13</p><p>Goal: Avoid ending the game with the&nbsp;Joker.</p><p>Core Dynamic: All players work together to help one person get four-of-a-kind to end the game— except the player with the Joker, who tries to discreetly pass it&nbsp;on.</p> 🛠 Setup   Choose cards based on the number of players. For example, with 6 players, use four copies each of Ace through&nbsp;6.  Add 1 Joker if there are 6 or fewer players. Use 2 Jokers if there are more than 6 players. In that case, remove one card from the deck (e.g., one 7) to keep it balanced.  Shuffle the cards well and deal them all out. One player will end up with 5 cards, everyone else will have&nbsp;4.  Sit in a circle so play can move clockwise.   ▶️ How to&nbsp;Play  Turn Structure <p>1. The player with five cards begins by passing one card to the player on their left. It is normal for the all players to tell each other what cards they have and what they want from the other players to try to colaborate for one player to get the four-of-a-kind.</p><p>2. The exchange follows a three-step protocol:</p>  First Offer: The player with five cards slides one card face down and says what it is. The receiving player may take it or ask for a second&nbsp;card.  Second Offer: A second card is slid face down next to the first, and the giver states what it is. The receiver may choose the first or second card or request a&nbsp;third.  Third Offer: If the receiver requests the third card then they are no longer able to pick up the first two cards. The giver selects any card to give — without saying what it is — and the receiver must take&nbsp;it.  <p>3. The receiving player now has five cards and becomes the next to take a turn. Play continues clockwise around the&nbsp;circle.</p> 🎯 Ending the&nbsp;Game <p>A player who forms four of a kind (e.g., four Kings)&nbsp;must:</p>  Hold the set while still holding 5&nbsp;cards.  Successfully pass their fifth card&nbsp;away.  Wait for play to return to them with only four cards in&nbsp;hand.  <p>Once that happens, the game ends immediately.</p><p>Everyone wins — except the player holding the&nbsp;Joker.</p> 🤥 Deception Rules <p>Players may say anything about their&nbsp;cards.</p><p>Players are allowed (and expected) to lie, especially the Joker-holder.</p><p>No one may show cards to others — trust and suspicion must be&nbsp;verbal.</p> 🔁 Game Variations <p>Adding additional Jokers for big&nbsp;groups:</p>  When playing with more than 6 players you can add additional Jokers to the pack to have more than one loser at the&nbsp;end.  In this case, also remove one card from the deck to maintain balance (so that only one player has 5 cards) and let everyone know this card as it is no longer possible to collect four of a kind of that&nbsp;number.  <p>Skipping players&nbsp;rule:</p>  Players may skip passing to the next person and instead choose someone further around the&nbsp;circle.  This is only allowed with full consent from all skipped&nbsp;players.  This rule opens up strategic depth, especially for avoiding or targeting the&nbsp;Joker.  I suggest introducing this rule after one round has already been played and everyone is familiar with the rest of the game&nbsp;setup.   💡 Tips for a Better Experience   Switch seats between rounds to prevent predictable alliances or&nbsp;grudges.  Great for groups who enjoy bluffing games like Werewolf or&nbsp;Coup.  Best played in a relaxed but alert social atmosphere.  <p>Elf is a game that grows richer with each playthrough, revealing layers of psychology, teamwork, and trickery. Whether you’re surrounded by friends, family, or curious strangers, it’s guaranteed to bring laughter, suspicion — and probably a bit of healthy frustration.</p><p>Ready to pass the&nbsp;Joker?</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/491/1*zdz_QU68oKJVP2rbUFAtJw.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:26 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 18. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://self-issued.info'>Mike Jones: self-issued</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://self-issued.info/?p=2650'>SPICEy Developments</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This week saw several useful developments in the IETF Secure Patterns for Internet CrEdentials (SPICE) working group. Two new drafts were adopted and an individual draft was published also intended for later adoption by the working group. Here’s the tour… GLobal Unique Enterprise (GLUE) Identifiers was adopted. The specification’s abstract is: This specification establishes an […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This week saw several useful developments in the IETF <a href="https://datatracker.ietf.org/wg/spice/about/">Secure Patterns for Internet CrEdentials (SPICE)</a> working group.  Two new drafts were adopted and an individual draft was published also intended for later adoption by the working group.  Here’s the tour…</p>
 
 <a href="https://www.ietf.org/archive/id/draft-ietf-spice-glue-id-00.html">GLobal Unique Enterprise (GLUE) Identifiers</a> was adopted.  The specification’s abstract is:<br>
<blockquote><p>
This specification establishes an IETF URN namespace for GLobal Unique Enterprise (GLUE) Identifiers. It also establishes an IETF URN namespace for identifiers defined by the IETF Secure Patterns for Internet CrEdentials (SPICE) working group. The GLUE URN namespace is within the SPICE URN namespace.
</p></blockquote>
<p>I worked closely with <a href="https://www.linkedin.com/in/bzundel/">Brent Zundel</a> on this one, primarily defining and using the IETF SPICE URN namespace, in which the GLUE namespace now resides.
</p> 
 <a href="https://www.ietf.org/archive/id/draft-maldant-spice-oidc-cwt-02.html">OpenID Connect standard claims registration for CBOR Web Tokens</a> was adopted.  The specification’s abstract is:<br>
<blockquote><p>
This document registers OpenID Connect standards claims already used in JSON Web Tokens for CBOR Web Tokens.
</p></blockquote>
<p>While I didn’t work on this specification directly, I did suggest changes to the initial version to its author, Beltram Maldant, intended to make the spec ready for working group adoption, in my role as a Designated Expert for the IANA <a href="https://www.iana.org/assignments/cwt/cwt.xhtml">CBOR Web Token (CWT) Claims</a> registry.  I’m glad this is happening!
</p> 
 <a href="https://www.ietf.org/archive/id/draft-prorock-spice-cwt-traceability-claims-01.html">Traceability Claims</a> was updated with an eye towards future working group adoption.  The specification’s abstract is:<br>
<blockquote><p>
This document defines claims to support traceability of physical goods across supply chains, focusing on items such as bills of lading, transport modes, and container manifests. These claims standardize the encoding of essential logistics and transport metadata, facilitating enhanced transparency and accountability in global supply chains. These claims are registered for use in both CBOR Web Tokens (CWTs) and JSON Web Tokens (JWTs).
</p></blockquote>
<p>I worked closely with <a href="https://www.linkedin.com/in/mprorock/">Mike Prorock</a> on this one, primarily motivating and refining the claim definitions and registering JWT claims in addition to the corresponding CWT claims.
</p> 
 
<p>SPICEy indeed!</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://self-issued.info/images/ietf-logo.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   23:19 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 17. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://realestatecafe.com/'>Bill Wendels Real Estate Cafe</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://realestatecafe.com/last-call-cosmetic-changes-in-mlspin-delivering-consumer-savings/'>Last Call: “Cosmetic” changes in MLSpin delivering consumer savings?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   The clock is ticking on the Nosalek vs MLSpin lawsuit, one of an estimated 20 against the real estate cartel (aka #RECartel), but arguably the… 
The post Last Call: “Cosmetic” changes in MLSpin delivering consumer savings? first appeared on Real Estate Cafe.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>The clock is ticking on the Nosalek vs MLSpin lawsuit, one of an estimated 20 against the real estate cartel (aka #RECartel), but arguably the…</p>
The post <a href="https://realestatecafe.com/last-call-cosmetic-changes-in-mlspin-delivering-consumer-savings/">Last Call: “Cosmetic” changes in MLSpin delivering consumer savings?</a> first appeared on <a href="https://realestatecafe.com">Real Estate Cafe</a>.
   
  
  
  

</div>
</div>

<div class='item-footer'>
   14:59 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 16. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2025/04/fix-postgres-strchrnul/'>Fix Postgres strchrnul Compile Error on macOS 15.4</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A fix for the error: 'strchrnul' is only available on macOS 15.4 or newer
Postgres compile error.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Just a quick note to users of <a href="https://github.com/theory/pgenv" title="PostgreSQL binary manager">pgenv</a> and anyone else who compiles Postgres on
macOS. In macOS 15.4, Apple introduced a new API, <code>strchrnul</code>, which is common
from other platforms. As a result attempting to compile Postgres on 15.4 and
later will lead to this error:</p>
 <code>snprintf.c:414:27: error: 'strchrnul' is only available on macOS 15.4 or newer [-Werror,-Wunguarded-availability-new]
  414 |                         const char *next_pct = strchrnul(format + 1, '%');
      |                                                ^~~~~~~~~
snprintf.c:366:14: note: 'strchrnul' has been marked as being introduced in macOS 15.4 here, but the deployment target is macOS 15.0.0
  366 | extern char *strchrnul(const char *s, int c);
      |              ^
snprintf.c:414:27: note: enclose 'strchrnul' in a __builtin_available check to silence this warning
</code> <p>Tom Lane <a href="https://postgr.es/m/385134.1743523038@sss.pgh.pa.us">chased down and committed the fix</a>, which will be in the next
releases of Postgres 13-17. It should also go away once macOS 16.0 comes out.
But in the meantime, set <code>MACOSX_DEPLOYMENT_TARGET</code> to the current OS release
to avoid the error:</p>
  <code>export MACOSX_DEPLOYMENT_TARGET="$(sw_vers -productVersion)"
</code>  <p>If you use <a href="https://github.com/theory/pgenv" title="PostgreSQL binary manager">pgenv</a>, you can <a href="https://github.com/theory/pgenv/issues/93" title="theory/pgenv#93">add it to your configuration</a>. It will need to be
added to all the version configs, too, unless they don’t exist and you also set:</p>
  <code>PGENV_WRITE_CONFIGURATION_FILE_AUTOMATICALLY=no
</code>  
         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/macos/">macOS</a> 
                 <a href="https://justatheory.com/tags/pgenv/">pgenv</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   19:03 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 15. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/why-do-policy-languages-feel-so-hard'>Why Do Policy Languages Feel So Hard?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Permit.io just published the results of a survey they conducted with over 200 developers on access control. There's lots of good data there, but one thing that struck me is that over 50% of developers said that they've never used any of the popular policy languages. I was wondering why that could be and came up with a few reasons why policy languages often feel foreign and frustrating:
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 <a href="https://substackcdn.com/image/fetch/$s_!zG4K!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6251b108-7ec5-4b92-8c9e-dab228486c7f_1792x1024.heic">          </a> <p>Permit.io just published the <a href="https://www.permit.io/blog/state-of-authorization-2025?utm_source=Permit.io&amp;utm_campaign=934f605fa7-EMAIL_CAMPAIGN_2025_04_15_11_36&amp;utm_medium=email&amp;utm_term=0_-934f605fa7-478209030">results of a survey they conducted with over 200 developers on access control</a>. There's lots of good data there, but one thing that struck me is that over 50% of developers said that they've never used any of the popular policy languages. I was wondering why that could be and came up with a few reasons why policy languages often feel foreign and frustrating:</p>  <p>It's Not Code as You Know It—Policy languages like Cedar or Rego are declarative. You're not writing a script to execute; you're defining rules that the engine interprets. That's a big mental shift if you're used to imperative programming.</p>  <p>Jargon Overload—Terms like "principal," "resource," and "condition" may make sense to security architects but leave developers guessing. Without real-world examples, they feel like word salad.</p>  <p>You Can't Just Run It—Want to know if your policy works? You might have to build a whole request context and simulate access decisions. That lack of immediate feedback makes learning feel slow and brittle.</p>  <p>Where's My IDE?—Most policy tools lack the rich developer environments we expect from modern programming — no autocomplete, minimal syntax checks, and few ways to step through logic.</p>  <p>Complexity Scales Fast—It's easy to write "Alice can read financial documents." But throw in multi-tenancy, delegated authority, and time-based rules, and suddenly your policy reads like a legal contract.</p>  <p>The Stakes Are High—Unlike a misaligned UI, broken policies result in security breaches or outages. That makes experimentation feel risky instead of fun.</p>  <p>These are real challenges, but many can be overcome with better tooling. Policy languages simplify app logic and make access control more reliable and maintainable—especially when treated like regular code with testing and version control. I'm betting that their benefits will help developers overcome their lack of familiarity with the programming style and come to rely on them more and more.</p>   <p>Photo Credit: <a>Developer contemplating PBAC</a> from DALL-E (public domain)</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!zG4K!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6251b108-7ec5-4b92-8c9e-dab228486c7f_1792x1024.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:04 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 14. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2025/04/mini-summit-three/'>Mini Summit 3 Transcript: Apt Extension Packaging</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Last week Christoph Berg, who maintains PostgreSQL’s APT packaging system,
gave a very nice talk on that system. Herein lie the transcript and links to
the slides and video.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	
			
	

         
<p>Last week <a href="https://www.df7cb.de">Christoph Berg</a>, who maintains PostgreSQL’s APT packaging system,
gave a very nice talk on that system at the third PostgreSQL <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/" title="Postgres Extension Ecosystem Mini-Summit on Meetup">Extension
Mini-Summit</a>. We’re hosting five of these virtual sessions in the
lead-up to the main <a href="https://www.pgevents.ca/events/pgconfdev2025/schedule/session/241/" title="PGConf.dev: Extensions Ecosystem Summit">Extension Summit</a> at <a href="https://2025.pgconf.dev" title="PostgreSQL Development Conference 2025">PGConf.dev</a> on May 13 in Montréal,
Canada. Check out Christoph’s session on April 9:</p>
 
 <a href="https://www.youtube.com/watch?v=35a7YCEfaRY">Video</a> 
 <a href="https://justatheory.com/shared/extension-ecosystem-summit/apt-extension-packaging.pdf">Slides</a> 
 
<p>There are two more Mini-Summits coming up:</p>
 
 April 23: <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306682918/">The User POV</a>. Join our panelist of extension users for a
lively discussion on tool choice, due diligence, and their experience
running extensions. 
 May 7: <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306551747/">Extension Management in CloudNativePG"</a>. <a href="https://cloudnative-pg.io" title="Run PostgreSQL. The Kubernetes way.">CNPG</a> maintainer
<a href="https://www.gabrielebartolini.it">Gabriele Bartolini</a> will talk about recent developments in extension
management in this official <a href="https://www.cncf.io/projects/cloudnativepg/">CNCF project</a>. 
 
<p>Join <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/" title="Postgres Extension Ecosystem Mini-Summit on Meetup">the Meetup</a> to attend!</p>
<p>And now, without further ado, thanks to the efforts of <a href="https://dev.to/@floord">Floor Drees</a>, the
thing you’ve all been waiting for: the transcript!</p>
 Introduction 
<p>David Wheeler introduced the organizers:</p>
 
 <a href="https://justatheory.com/">David Wheeler</a>, Principal Architect at <a href="https://tembo.io/">Tembo</a>, maintainer of <a href="https://pgxn.org/">PGXN</a> 
 <a href="https://ca.linkedin.com/in/yrashk">Yurii Rashkovskii</a>, <a href="https://omnigres.com/">Omnigres</a> 
 <a href="https://pgxn.org/user/keithf4/">Keith Fiske</a>, <a href="https://www.crunchydata.com/">Crunchy Data</a> 
 <a href="https://dev.to/@floord">Floor Drees</a>, Principal Program Manager at <a href="https://enterprisedb.com" title="EnterpriseDB">EDB</a>, PostgreSQL CoCC member,
PGDay Lowlands organizer 
 
<p><a href="https://www.df7cb.de">Christoph Berg</a>, <a href="https://wiki.postgresql.org/wiki/Apt">PostgreSQL APT</a> developer and maintainer par excellence,
talked through the technical underpinnings of developing and maintaining
PostgresSQL and extension packages.</p>
<p>The stream and the closed captions available for the recording are supported
by <a href="https://2025.pgconf.dev" title="PostgreSQL Development Conference 2025">PGConf.dev</a> and its gold level <a href="https://2025.pgconf.dev/sponsors.html">sponsors</a>: Google, AWS, Huawei, Microsoft,
and EDB.</p>
 APT Extension Packaging 
<p>Speaker: <a href="https://www.df7cb.de">Christoph Berg</a></p>
<p>Hello everyone. So what is this about? It’s about packaging things for
PostgresSQL for Debian distributions. We have PostgreSQL server packages,
extension packages, application packages and other things. The general
workflow is that we are uploading packages to Debian unstable first. This is
sort of the master copy, and from there things eventually get to Debian
testing. Once they’re being released, they end up in Debian stable.</p>
<p>Perhaps more importantly for the view today is that the same package is then
also rebuilt for <a href="https://apt.postgresql.org">apt.postgresql.org</a> for greater coverage of Postgres major
versions. And eventually the package will also end up in an Ubuntu release
because, Ubuntu is copying Debian unstable, or Debian testing, every six
months and then doing their release from there. But I don’t have any stakes in
that.</p>
<p>For an overview of what we are doing in this Postgres team, I can just briefly
show you <a href="https://qa.debian.org/developer.php?email=team%2bpostgresql%40tracker.debian.org">this overview page</a>. That’s basically the view of packages we are
maintaining. Currently it’s 138, mostly Postgres extensions, a few other
applications, and whatever comes up in the Postgres ecosystem.</p>
<p>To get a bit more technical let’s look at how the Debian packages look from
the inside.</p>
<p>We have two sorts of packages. We have source packages, which are the source
of things that are built. The way it works is that we have a directory inside
that source tree called Debian, which has the configuration bits about how the
packages created should look like. And from this the actual binary packages,
the <code>.deb</code> files are built.</p>
<p>Over the past years, I’ve got a few questions about, “how do I get my
application, my extension, and so on packaged?” And I wrote that down as <a href="https://salsa.debian.org/postgresql/postgresql-common/blob/master/doc/postgresql-debian-packaging.md">a
document</a>. Hopefully to answer most of the questions. And I kind of think that
since I wrote this down last year, the questions somehow stopped. If you use
that document and like it, please tell me because no one has ever given me any
feedback about that. The talk today is kind of loosely based on this document.</p>
<p>I’m not going to assume that you know a whole lot of Debian packaging, but I
can’t cover all the details here, so I’ll keep the generic bits a bit
superficial and dive a bit more into the Postgres-specific parts.</p>
<p>Generally, the most important file in the Debian package is this Debian
control file, which describes the source and the binary packages. This is
where the dependencies are declared. This is where the package description
goes, and so on. In the Postgres context, we have the first problem that, we
don’t want to encode any specific PG major versions inside that control file,
so we don’t have to change it each year once a new Postgres version comes out.</p>
<p>This is why, instead of a Debian control file, we actually have a
<code>debian/control.in</code> file, and then there’s a tool called <code>pg_buildext</code>,
originally written by <a href="https://tapoueh.org/about/">Dimitri Fontaine</a>, one or two decades ago, and then
maintained by me and the other Postgres maintainers since then. That tool is,
among other things, responsible for rewriting that <code>control.in</code> file to the
actual <code>control</code> file.</p>
<p>I just picked one random extension that I happen to have on the system here.
This postgresql-semver extension, the upstream author is actually David here.
In <a href="https://salsa.debian.org/debian/postgresql-semver/-/blob/debian/master/debian/control.in?ref_type=heads">this control file</a> we say the name of the package, the name of the
Debian maintainer — in this case the group — there’s a few uploaders, there’s
build dependencies and other things that are omitted here because, the slide
was already full. And then we have, next to this source section, we have a
package section and here we have this placeholder:
<code>postgresql-PGVERSION-semver</code>.</p>
<p>Once we feed this <code>control.in</code> file through this <code>pg_buildext</code> tool, it’ll
generate the control file, which expands this <code>PGVERSION</code> placeholder to
actually a list of packages. This is just a mechanical translation; we have
<code>postgresql-15-semver</code>, 16, 17 and whatever other version is supported at that
point.</p>
<p>Once a new PostgreSQL version is released, PostgreSQL 18 comes out, we don’t
have to touch anything in this <code>control.in</code> file. We just rerun this
<code>pg_buildext</code> update control command, and it’ll automatically add the new
package.</p>
<p>There’s about half a dozen layers talking to each other when building a
package On the lowest level, no one actually touches it at at that level. But
Debian packages are actually <code>ar</code> archives, the one from library fame, was yet
another, archive inside control called <code>control.tar.xz</code> or something. But. No
one actually touches it at that level anymore.</p>
<p>We have <code>dpkg</code> on top of that, which provides some building blocks for
creating actual Debian packages. So you would call <code>dpkg-builddeb</code> and other
<code>dpkg</code> helpers to actually create a package from that. But because this is
complicated, there’s yet another level on top of that, called <code>debhelper</code>.
This is the actual standard for building Debian package nowadays. So instead
of invoking all the <code>dpkg</code> tools directly, everyone uses the step helper tools
which provide some wrappers for the most common build steps that are executed.
I will show an example in a second.</p>
<p>Next to these wrappers for calling “create me a package”, “copy all files”,
and so on, there’s also this program called <code>dh</code>, it’s called a sequencer
because it’ll invoke all the other tools in the correct order. So let me show
you an example before it gets too confusing. The top level command to actually
build a Debian package — to create the binary packages from the source
package — is called <code>dpkg-buildpackage</code>. It will invoke this <code>debian/rules</code>
file. The <code>debian/rules</code> file is where all the commands go that are used to
build a package. For historical reasons it’s a Makefile. In the shortest
incantation it just says, “for anything that is called invoke this <code>dh</code>
sequencer with some arguments.”</p>
<p>Let me skip ahead one more slide and if we’re actually running it like that,
it kind of looks like this. I’m invoking <code>dpkg-buildpackage</code>,
<code>dpkg-buildpackage</code> invokes <code>debian/rules</code> with target name <code>debian/rules</code>,
invokes <code>dh</code> and <code>dh</code> then calls all the helper steps that are required for
getting the package to run. The first one would be
<code>dh_update_autotools_config</code>, so if any ancient auto conf things are used,
it’ll be updated. The package will be reconfigured, and then it would it will
be built and so on.</p>
<p>This was the generic Debian part. Postgres actually adds more automation on
top of that. This is this “<code>dh</code> with <code>pgxs</code> step.” Let me go back two slides.
We have this <code>pgxs</code> plugin for <code>debhelper</code> which adds more build steps that
actually call out this tool called <code>pg_buildext</code>, which interfaces with the
<code>pgxs</code> build system in your extension package. Basically <code>debhelper</code> calls
this <code>pgxs</code> plugin, and this <code>pgxs</code> plugin called <code>pg_buildext</code>, and this one
finally invokes the <code>make</code> command, including any <code>PG_CONFIG</code> or whatever
settings that are required for compiling this extension.</p>
<p>If we go back to the output here, we can see that one of the steps here is
actually invoking this <code>pg_buildext</code> tool and <code>pg_buildext</code> will then continue to
actually compile this extension.</p>
<p>This means in the normal case for extensions that don’t do anything special,
you will actually get away with a very short <code>debian/rules</code> file. Most of the
time it’s just a few lines. In this case I added more configuration for two of
the helpers. In this step, I told <code>dh_installchangelogs</code> that, in this
package, the changelog has a file name that <code>dh_installchangelogs</code> doesn’t
automatically recognize. Usually if you have a file called <code>changelog</code>, it
will be automatically picked up. But in this case I told it to use this file.
Then I’m telling it that some documentation file should be included in all
packages. Everything else is standard and will be picked up by the default
Debian tool chain.</p>
<p>Another thing specific for the Postgres bits is that we like to run the
package tests at build time. One of the build steps that gets executed is this
<code>dh_pgxs</code> test wrapper, which in turn invokes <code>pg_buildext install check</code>.
That will create a new Postgres cluster and proceed to invoke <code>pg_regress</code> on
that package. This is actually the place where this patch that <a href="https://justatheory.com/2025/04/mini-summit-two/" title="2025 Extension Mini Summit 2: Implementing an extension search path">Peter was
talking about two weeks ago</a> is coming into play.</p>
<p>The actual call chain of events is that <code>dh_pgxs</code> starts
<code>pg_buildext installcheck</code>, <code>pg_buildext</code> starts <code>pg_virtualenv</code>, which is a
small wrapper shipped with Debian — but not very specific to Debian — that
just creates a new Postgres environment and then executes any command in that
environment. This is actually very handy to create test instances. I’m using
that all day. So if anyone is asking me, “can you try this on Postgres 15?” or
something, I’m using <code>pg_virtualenv -v 15</code> to fire up a temporary Postgres
instance. I can then play with it, break it or something, and, as soon as I
exit the shell that <code>pg_virtualenv</code> opens, the cluster will be deleted again.</p>
<p>In the context of <code>pg_buildext</code>, what <code>pg_virtualenv</code> is doing here is that
it’s calling <code>pg_createcluster</code> to actually fire up that instance and it’s
passing an option to set this <code>extension_control_path</code> to the temporary
directory that the extension was installed to during the build process. While
we are compiling the package, the actual install command is invoked, but it
does not write to <code>/usr/share/postgresql</code> or something, but it writes to a
subdirectory of the package build directory. So it’s writing to
<code>debian/$PACKAGE/$THE_ORIGINAL_PATH</code>.</p>
<p>And that’s why before we had this in Postgres 18, the Debian packages had a
patch that does the same thing as this <code>extension_control_path</code> setting. It
was called <code>extension_destdir</code>. It was basically doing the same thing except
that it was always assuming that you had this structure of some prefix and
then the original path. The new patch is more flexible that: it can be an
arbitrary directory. The old <code>extension_destdir</code> patch assumes that it’s
always <code>/$something/usr/share/postgres/$something</code>. I’m glad that that patch
finally went in and we can still run the test at build time.</p>
<p>So far we’ve only seen how to build things for one Postgres version. The
reason why this <code>pg_buildext</code> layer is there is that this tool is the one that
does the building for each version in turn. So <code>pg_buildext</code> will execute any
command pass to it for all the versions that are currently supported by that
package. What’s happening here is that we have one source package for
extension covered. And that one source package then builds a separate binary
for each of the major versions covered. But it does this from a single build
run.</p>
<p>In contrast to what <a href="https://github.com/devrimgunduz" title="Devrim Gündüz">Devrim</a> is doing with the <a href="https://yum.postgresql.org">RPM packages</a>, he’s actually
in invoking the builds several times separately for each version. We could
also have done this, it’s just a design choice that, we’ve done it one way
round and he’s doing it the other way round.</p>
<p>To tell <code>pg_buildext</code> which versions are supported by the package, there’s a
file called <code>debian/pgversions</code> which usually just contains a single line
where you can either say, “all versions are supported”, or you can say that
“anything, starting 9.1” or “starting PostgreSQL 15 and later” is supported.
In this example here, 9.1+ is actually copied from the semver package because
the requirement there was that it needs to support extensions and that’s when
9.1 was introduced. We don’t care about these old versions anymore, but the
file was never changed since it was written.</p>
<p>We know how to build several Postgres major versions from a source package.
Now the next axis is supporting multiple architectures. The build is invoked
separately for each architecture. This single source package is compiled
several times for each architecture. On <a href="https://apt.postgresql.org">apt.postgresql.org</a>, we’re currently
supporting amd64, arm64 and ppc64el. We used to have s390x support, but I
killed that recently because IBM is not supporting any build machine anymore
that actually works. Inside Debian there are a lot more architecture
supported.</p>
<p>There’s also something called Debian ports, which are not official
architectures, but either new architectures that are being introduced like
this loong64 thing, or it’s sometimes it’s old architectures that are not
official anymore, but are still being kept around like the Sparc one. There’s
also some experimental things like hurd-amd64, hurd-i386. Isn’t even Linux.
This is a hurd kernel, but still running everything Debian on top of it, and
some time ago it even started to support Postgres. The packages are even
passing the tests there, which is kind of surprising for something that hasn’t
ever seen any production.</p>
<p>For Postgres 17, <a href="https://buildd.debian.org/status/package.php?p=postgresql-17">it looks like this</a>. The architectures in the upper half of
that table are the official ones, and the gray area on the bottom are the
unofficial ones that are, let’s say, less supported. If anything breaks in the
upper half, maintainers are supposed to fix it. If anything breaks in the
lower half, people might care or might not care.</p>
<p>I like to keep it working because if Postgres breaks, all the other software
that needs it — like <code>libpq</code>, so it’s not even extensions, but any software
that depends on <code>libpq</code> — wouldn’t work anymore if that’s not being built
anymore. So I try to keep everything updated, but some architectures are very
weird and just don’t work. But at the moment it looks quite good. We even got
Postgres 18 running recently. There were some problems with that until last
week, but I actually got that fixed on the <a href="http://archives.postgresql.org/pgsql-hackers/" title="pgsql-hackers Archives">pg-hackers list</a>.</p>
<p>So, we have several Postgres major versions. We have several architectures.
But we also have multiple distribution releases. For Debian this is currently
sid (or unstable), trixie, (currently testing), bookworm, bullseye, Ubuntu
plucky, oracular, noble, jammy, focal — I get to know one funny adjective
each year, once Ubuntu releases something new. We’re compiling things for each
of those and because compiling things yields a different result on each of
these distributions, we want things to have different version numbers so
people can actually tell apart where the package is coming from.</p>
<p>Also, if you are upgrading — let’s say from Debian bullseye to Debian
bookworm — you want new Postgres packages compiled for bookworm. So things
in bookworm need to have higher version numbers than things in bullseye so you
actually get an upgrade if you are upgrading the operating system. This means
that packages have slightly different version numbers, and what I said before
— that it’s just one source package — it’s kind of not true because, once
we have new version numbers, we also get new source packages.</p>
<p>But these just differ in a new change log entry. It’s basically the same
thing, they just get a new change log entry added, which is automatically
created. That includes this, plus version number part. Wwhat we’re doing is
that the original version number gets uploaded to Debian, but packages that
show up on <a href="https://apt.postgresql.org">apt.postgresql.org</a> have a marker inside the version number that
says “PGDG plus the distribution release number”. So for the Ubuntu version,
it says <code>PGDG-24.0.4</code> or something and then Debian is, it’s plus
120-something.</p>
<p>The original source package is tweaked a bit using <a href="https://salsa.debian.org/postgresql/apt.postgresql.org/-/blob/master/jenkins/generate-pgdg-source">this shell script</a>. I’m
not going to show it now because it’s quite long, but, you can look it up
there. This is mostly about creating these extra version numbers for these
special distributions. It applies a few other tweaks to get packages working
in older releases. Usually we can just take the original source or source
package and recompile it on the older Debians and older Ubuntus. But sometimes
build dependencies are not there, or have different names, or some feature
doesn’t work. In that case, this <code>generate-pgdg-source</code> has some tweaks, which
basically invokes <code>set</code> commands on the source package to change some minor
bits. We try to keep that to minimum, but sometimes, things don’t work out.</p>
<p>For example, when <code>set compression</code> support was new in Postgre, compiling the
newer Postgres versions for the older releases required some tweaks to disable
that on the older releases, because they didn’t have the required libraries
yet.</p>
<p>If you’re putting it all together, you get this combinatorial explosion. From
one project, <code>postgresql-semver</code>, we get this many builds and each of those
builds — I can actually show you <a href="https://jengus.postgresql.org/job/postgresql-semver-binaries/">the actual page</a> — each of those builds
is actually several packages. If you look at the list of artifacts there, it’s
creating one package for PostgreSQL 10, 11, 12, and so on. At the moment it’s
still building for PostgreSQL 10 because I never disabled it. I’m not going to
complain if the support for the older versions is broken at some point. It’s
just being done at the moment because it doesn’t cost much.</p>
<p>And that means that, from one source package quite a lot of artifacts are
being produced. The current statistics are this:</p>
 
 63355 .deb files 
 2452 distinct package names 
 2928 source packages 
 210 distinct source package names 
 47 GB repository size 
 
<p>We have 63,000 <code>.deb</code> files. That’s 2,400 distinct package names — so
<code>package-$PGVERSION</code> mostly built from that many source packages. The actual
number of distinct source packages is 210. Let’s say half of that is
extensions. Then there’s of course separate source packages for Postgres 10,
11, 12, and so on, and there’s a few application packages. Yeah, in total the
repository is 47 gigabytes at the moment.</p>
<p>This is current stuff. All the old distributions are moved to
<a href="https://apt-archive.postgresql.org">apt-archive.postgresql.org</a>. We are only keeping the latest built inside the
repository. So if you’re looking for the second-latest version of something,
you can go to <a href="https://apt-archive.postgresql.org">apt-archive.postgresql.org</a>. I don’t have statistics for that,
but that is much larger. If I had to guess, I would say probably something
like 400 gigabytes/ I could also be off by with guessing.</p>
<p>That was how to get from the source to the actual packages. What we’re doing
on top of that is doing more testing. Next to the tests that we are running at
build time, we are also running tests at installation time, or once the
package is installed we can run tests. For many packages, that’s actually the
same tests, just rerun on the actual binaries as installed, as opposed to
<code>debian/something</code>. Sometimes it’s also different tests For some tests it’s
just simple smoke tests. id everything get installed to the correct location
and does the service actually start, sometimes it’s more complex things.</p>
<p>Many test suites are meant to be run at compilation time, but we want to run
them at install time. This is kind of <code>make check</code>, <code>make installcheck</code>, but
some projects are not really prepared to do that. They really want, before you
can run the test suite, you have to basically compile everything. I try to
avoid that because things that work at compilation time might not mean that
it’s running at install time because we forgot to install some parts of the
build.</p>
<p>I try to get the test suite running with as few compilation steps as possible,
but sometimes it just doesn’t work. Sometimes the <code>Makefile</code> assumes that
<code>configure</code> was run and that certain variables got substituted somewhere.
Sometimes you can get it running by calling <code>make</code> with more parameters, but
it tends to break easily if something changes upstream. If you’re an extension
author, please think of someone not compiling your software but still wanting
to run the tests.</p>
<p>What we’re doing there is to run these tests each month. On each day, each
month, a random set of tests is scheduled — that’s three or four per day or
something. It’s not running everything each day because if something breaks, I
can’t fix 50 things in parallel. You can see <a href="https://jengus.postgresql.org/view/Testsuite/">test suite tab</a> there. At the
moment, actually everything worked. For example, we could check something…</p>
<p>With <a href="https://jengus.postgresql.org/view/Testsuite/job/bgw-replstatus-autopkgtest/">this background worker rapid status</a> thing, that’s an extension that
<a href="https://www.hagander.net" title="Magnus Hagander">Magnus</a> wrote sometime ago. Everything is running fine,  but something was
broken in January. Ah, there, the S390 machine was acting up. That was
probably a pretty boring failure. Probably something with network broken. Not
too interesting. This is actually why I shut down this architecture, because
the built machine was always having weird problems. This is how we keep the
system actually healthy and running.</p>
<p>One thing that’s also catching problems is called <a href="https://jengus.postgresql.org/view/Testsuite/job/debcheck/">debcheck</a>. This is a static
installability analysis tool by Debian. You feed it a set of packages and it
will tell you if everything is installable. In this case, something was not
installable on Debian testing. And — if we scroll down there — it would
say that <code>postgresql-10-icu-ext</code> was not installable because this <code>lib-icu-72</code>
package was missing. What happened there is that project or library change
so-name, from time to time, and in this case, in Debian, ICU was moving from
72 to 76 and I just had to recompile this module to make it work.</p>
<p>Usually if something breaks, it’s usually on the development suites — sid,
trixie, unstable, and testing — the others usually don’t break. If the
others break, then I messed something up.</p>
<p>That was a short tour of how the packaging there works. For open issues or
pain pain points that there might be, there are packages that don’t have any
tests. If we are looking at, what was the number, 63,000 packages, I’m not
going to test them by hand, so we really rely on everything being tested
automatically. Extensions are usually very well covered, so there’s usually
not a problem.</p>
<p>Sometimes there’s extensions that don’t have tests, but they are kind of hard
to test. For example, modules that don’t produce any SQL outputs like
<a href="https://www.postgresql.org/docs/current/auto-explain.html">auto_explain</a> are kind of hard to test because the output goes somewhere
else. I mean, in the concrete case, auto_explain probably has tests, but it’s
sometimes it’s things that are not as easily testable as new data types.</p>
<p>Things that usually don’t have tests by nature is GUI applications; any
program that opens a window is hard to test. But anything that produces text
output is usually something I like to cover. Problems with software that we
are shipping and that actually breaks in production is usually in the area
where the tests were not existing before.</p>
<p>One problem is that some upstream extensions only start supporting Postgres 18
after the release. People should really start doing that before, so we can
create the packages before the 18.0 release. Not sure when the actual best
point to start would be; maybe today because yesterday was feature freeze. But
sometime during the summer would be awesome. Otherwise <a href="https://github.com/devrimgunduz" title="Devrim Gündüz">Devrim</a> and I will go
chasing people and telling them, “please fix that.”</p>
<p>We have of course packages for Postgres 18, but we don’t have extension
packages for Postgres 18 yet. I will start building that perhaps now, after
feature freeze. Let’s see how, how much works and not. Usually more than half
of the packages just work. Some have trivial problems and some have hard
problems, and I don’t know yet if Postgres 18 will be a release with more hard
problems or more trivial problems.</p>
<p>Another problem that we’re running into sometimes is that upstream only cares
about 64bit Intel and nothing else. We recently stopped caring about 32 bits
for extensions completely. So Debian at postgresql.org is not building any
extension packages for any 32-bit architectures anymore. We killed i386, but
we also killed arm, and so on, on the Debian side.</p>
<p>The reason is that there are too many weird bugs that I have to fix, or at at
least find, and then chase upstreams about fixing their 32-bit problems. They
usually tell me “I don’t have any 32-bit environment to test,” and they don’t
really care. In the end, there are no users of most extensions on 32-bit
anyway. So we decided that it just doesn’t make sense to fix that. In order to
prevent the problems from appearing in the first place, we just disabled
everything 32-bit for the extensions.</p>
<p>The server is still being built. It behaves nicely. I did find a 32-bit
problem in Postgres 18 last week, but that was easy to fix and not that much
of a problem. But my life got a lot better once I started not caring about
32-bit anymore. Now the only problem left is big-endian s390x in Debian, but
that doesn’t cause that many problems.</p>
<p>One thing where we are only covering a bit of stuff is if projects have
multiple active branches. There are some projects that do separate releases
per Postgres major version. For example, <a href="https://pgxn.org/dist/pgaudit/">pgaudit</a> has separate branches for
each of the Postgres versions, so we are tracking those separately, just to
make pgaudit available. <a href="https://pgxn.org/search?q=pg_hint_plan&amp;in=dists">pg-hint-plan</a> is the same, and this Postgres graph
extension thing (<a href="https://pgxn.org/dist/apacheage/">Apache Age</a>) is also the same. This is just to support all
the Postgres major versions. We have separate source packages for each of the
major versions, which is kind of a pain, but doesn’t work otherwise.</p>
<p>Where we are not supporting several branches is if upstream is maintaining
several branches in parallel. For example, <a href="https://postgis.net" title="PostGIS">PostGIS</a> is maintaining 3.5, 3.4,
3.3 and so on, and we are always only packaging the latest one. Same for
<a href="https://www.pgpool.net/">Pgpool</a>, and there’s probably other projects that do that. We just don’t do
that because it would be even more packages we have to take care of. So we are
just packaging the latest one, ad so far there were not that many complaints
about it.</p>
<p>Possibly next on the roadmap is looking at what to do with <a href="https://www.rust-lang.org">Rust</a> extensions.
We don’t have anything Rust yet, but that will probably be coming. It’s
probably not very hard; the question is just how much of the build
dependencies of the average extension is already covered in Debian packages
and how much would we have to build or do we just go and render all the
dependencies or what’s the best way forward?</p>
<p>There’s actually a very small number of packages that are shipped on
<a href="https://apt.postgresql.org">apt.postgresql.org</a> that are not in Debian for this reason. For example, the
<a href="https://tada.github.io/pljava/">PL/Java</a> extension is not in Debian because too many of the build
dependencies are not packaged in Debian. I have not enough free time to
actually care about those Java things, and I can’t talk Java anyway, so it
wouldn’t make much sense anyway.</p>
<p>I hope that was not too much, in the too short time.</p>
 Questions and comments 
 
 
<p>Pavlo Golub: When you show the <code>pg_virtualenv</code>, usage, do you use pre-built
binaries or do you rebuild every time? Like for every new version you are
using?</p>
 
 
<p>Christoph: No, no, that’s using the prebuilt binaries. The way it works
is, I have many Postgres versions installed on that machine, and then I
can just go and say, <code>pg_virtualenv</code>, and I want, let’s say, an 8.2
server. It’s calling <code>initdb</code> on the newer version, it’s actually telling
it to skip the <code>fsync</code> — that’s why 8.3 was taking a bit longer, because
it doesn’t have that option yet. And there it’s setting <code>PGPORT</code>, <code>PGHOST</code>
and so on, variables. So I can just connect and then play with this old
server. The problem is that <code>psql</code> pro-compatibility at some point, but
it’s still working for sending normal commands to modern <code>psql</code>.</p>
 
 
<p>Pavlo: For modern <code>psql</code>, yeah. That’s cool! Can you add not only vanilla
Postgres, but any other flavors like by EDB or Cybertec or, …?</p>
 
 
<p>Christoph: I’ve thought about supporting that; the problem there is that
there’s conflicting requirements. What we’ve done on the Cybertec side is
that if the other Postgres distribution wants to be compatible to this
one, it really has to place things in the same directories. So it’s
installing to exactly this location and if it’s actually behaving like the
original, it’ll just work. If it’s installing to <code>/opt/edb/something</code>, its
not supported at the moment, but that’s something we could easily add.
What it’s really doing is just invoking the existing tools with enough
parameters to put the data directory into some temporary location.</p>
 
 
<p>Pavlo: And one more question. You had <a href="https://go.dev">Go</a> extensions mentioned on your last
slide, but you didn’t tell anything about those.</p>
 
 
<p>Christoph: Yeah, the story is the same as with Rust. We have not done
anything with it yet and we need to explore it.</p>
 
 
<p>David Wheeler: Yurii was saying a bit about that in the chat. It seems
like the problem is that, both of them expect to download most of their
dependencies. And vendoring them swells up the size of the download and
since they’re not runtime dependencies, but compile-time dependencies, it
seems kind of silly to make packages.</p>
 
 
<p>Christoph: Yeah. For Debian, the answer is that Debian wants to be
self-contained, so downloading things from the internet at build time is
prohibited. The ideal solution is to package everything; if it’s things
that are really used only by one package, then vendoring the modules might
be an option. But people will look funny at you if you try to do that.</p>
 
 
<p>Yurii: I think part of the problem here is that in the Rust ecosystem in
particular, it’s very common to have a lot of dependencies, as in
hundreds. When you start having one dependency and that dependency brings
another dependency. The other part of the problem is that you might depend
on a particular range of versions of particular dependencies and others
depend on others. Packaging all of that as individual dependencies is
becoming something that is really difficult to accomplish. So vendorizing
and putting that as part of the source is something that we could do to
avoid the problem.</p>
 
 
<p>Christoph: Yeah, of course, it’s the easy solution. Some of the
programming language ecosystems fit better into Debian than others. So I
don’t know how well Rust fits or not.</p>
<p>What I know from the Java world is that they also like to version
everything and put version restrictions on their dependencies. But what
Debian Java packaging helpers are doing is just to nuke all those
restrictions away and just use the latest version and usually that just
works. So you’re reducing the problem by one axis by having everything at
the latest version. No idea how reasonable the Rust version ranges there
are. So if you can just ignore them and things
still work, or…</p>
 
 
<p>Yurii: Realistically, this is impossible. They do require particular
versions and they will not compile oftentimes. The whole toolchain expects
particular versions. This is not only dependency systems themselves, it’s
also Rust. A package or extension can have a particular demand
for minimum supported Rust version. If that version is not available in
particular distro, you just can’t compile.</p>
 
 
<p>Christoph: Then the answer is we don’t compile and you don’t get it. I
mean, Rust is possibly still very new and people depend on the latest
features and then are possibly just out of luck if they want something on
Debian bullseye. But at some point that problem should resolve itself and
Rust get more stable so that problem is not as common anymore.</p>
 
 
<p>Yurii: It’s an interesting take actually because if you think about, the
languages that have been around for much longer should have solved this
problem. But if you look at, I don’t know, C, C++, so GCC and Clang,
right? They keep evolving and changing all the time too. So there’s a lot
of code say in C++ that would not compile with a compiler that is older
than say, three years. So yeah, but we see that in old languages.</p>
 
 
<p>Christoph: Yea, but Postgres knows about that problem and just doesn’t use
any features that are not available in all compilers. Postgres has
solved the problem.</p>
 
 
<p>Yurii: Others not so much. Others can do whatever they
want.</p>
 
 
<p>Christoph: If upstream doesn’t care about their users, that’s upstream’s
problem.</p>
 
 
<p>David: I think if there’s there’s a centralized place where the discussion
of how to manage stuff, like Go and Rust do, on packaging systems is
happening, I think it’s reaching a point where there’s so much stuff that
we’ve gotta figure out how to work up a solution.</p>
 
 
<p>Christoph: We can do back ports of certain things in the repository and
make certain toolchain bits available on the older distributions. But you
have to stop at some point. I’m certainly not going to introduce GCC back
ports, because I just can’t manage that. So far we haven’t done much of
that. I think <a href="https://github.com/devrimgunduz" title="Devrim Gündüz">Devrim</a> is actually backporting parts of the GIST tool
chain, like GL and libproj or something. I’ve always been using what is
available in the base distribution for that. There is some room for making
it work, but it’s always the question of how much extra work we want to
put in, how much do we want to deviate from the base distribution, and
ultimately also, support the security bits of that.</p>
 
 
<p>[David makes a pitch for the next two sessions and thanks everyone for coming].</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
                 <a href="https://justatheory.com/tags/debian/">Debian</a> 
                 <a href="https://justatheory.com/tags/apt/">APT</a> 
                 <a href="https://justatheory.com/tags/christoph-berg/">Christoph Berg</a> 
                 <a href="https://justatheory.com/tags/transcript/">Transcript</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/shared/extension-ecosystem-summit/apt-packaging-card.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   22:48 &bull;
   3 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 12. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.jonudell.net'>Jon Udell</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.jonudell.net/2025/04/12/who-will-take-care-of-you-in-your-time-of-need/'>Who will take care of you in your time of need?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This Fresh Air interview with Hanif Kureishi had me riveted from the beginning, for one reason, and then at the end for a different reason. Kureishi is best known as the author of the 1985 British rom-com My Beautiful Laundrette. During an illness in 2022 he fainted, fell on his face, broke his neck, and … Continue reading Who will take care of you in your time of need?
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>
        This Fresh Air <a href="https://www.npr.org/2025/03/29/1241613815/best-of-a-writer-grapples-with-a-life-changing-accident-the-post-wwii-red-scare">interview with Hanif Kureishi</a> had me riveted from the beginning, for one reason, and then at the end for a different reason. Kureishi is best known as the author of the 1985 British rom-com My Beautiful Laundrette. During an illness in 2022 he fainted, fell on his face, broke his neck, and woke up paraplegic. His account of what that’s like resonated deeply.</p>
<p>
        Soon after we moved to Santa Rosa a decade ago I became close friends with someone who had suffered the same fate. Until the age of 30 Stan Gow was a rodeo rider, mountain climber, and ski patrol hotshot.</p>
 
        <p></p>
<p>        
        </p> 
<p>        Then he dove into a shallow pool, broke his neck, and spent the next 40 years in a motorized wheelchair.</p>
<p>        </p>
<p>        Before an accident like that you’re an autonomous person, then suddenly and forever after you’re as helpless as an infant, wholly dependent on others who feed you, clean you, dress you, hoist you into the chair in the morning, put you to bed at night, and turn you over in bed during the night.</p>
<p>
        “You feel like a helpless baby,” Kureishi says, “and a tyrant too.” I saw this happen with Stan. When you have to ask caregivers for everything it feels shameful and embarrassing. Those feelings can convert polite requests into angry demands.</p>
<p>
        The only escape from that condition, for those lucky enough to be able to own and use one, is the motorized wheelchair. Kureishi has just enough use of an arm to be able to drive himself around the neighborhood. Stan did too, and over the years we walked just about everywhere his wheels could go. Tagging along I gained a deep appreciation for that miracle of mobility, and for the consequences when it’s thwarted by stairs that lack ramps and curbs that lack cuts.</p>
<p>
        The interview brought back powerful memories of my time with Stan, who died a few years ago after outliving expectations for an injury like his by decades. And then it took a turn when Terri Gross asked about the ethnicity of Kureishi’s caregivers. He was in Italy when the accident happened, and nearly everyone in the hospital was white. When he returned to England it was a different story.</p>
<p>
        The whole of our huge NHS is run by people from all over the world, and it’s just incredible to lie in bed to be changed and washed by someone and you have these incredible conversations with somebody from Africa, from the Philippines, from India or Pakistan. One of the things you become aware of in these British hospitals is our dependence on immigration.
        </p>
<p>
        It’s not quite like that in the US, but much more so than in Italy. During my mother’s final illness one of her caretakers was a Haitian nurse. Mom was a linguist who spoke and taught French, Spanish, and Italian. She’d  been unresponsive for a few days, but when the nurse spoke to her in French she perked up like one of the patients in Awakenings.</p>
<p>
        Paraplegia is rare but helplessness is universal. We all begin that way, we all end that way. Demonizing immigrants is wrong for so many reasons. Among them: who else will take care of you in your time of ultimate need?</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/jonudell.info/images/stan-rodeo.jpg?w=656&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:55 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 10. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/establishing-first-person-digital'>Establishing First Person Digital Trust</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I've spent the better part of the week thinking about the idea of first-person identity and verifiable relationship credentials after Drummond Reed spoke about them on Monday at VRM day.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 <a href="https://substackcdn.com/image/fetch/$s_!8fZH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fc199ed-1f71-4ca3-bca3-82a9840691d8_1792x1024.heic">          </a> <p>I've spent the better part of the week thinking about the idea of <a>first-person identity</a> and verifiable relationship credentials after Drummond Reed spoke about them on Monday at VRM day. I decided to write about it to force myself to understand it better.</p><p>One of the hard parts of first-person identity is knowing who to trust online. This isn't a new problem. Back in the day, people trying to use Pretty Good Privacy (PGP) faced the same issue when dealing with public keys. Their solution? Key signing parties.</p><p>Never heard of a key signing party? Imagine Alice and Bob are at the O'Reilly Open Source conference in 2007, tucked into a side room labeled "PGP Key Signing Party." About a dozen people mill about, each holding a printed sheet of paper covered in strange-looking hexadecimal strings. Alice approaches Bob, both a little unsure of how to proceed.</p><p>"Hi, I'm Alice," she says, holding up her badge and offering her driver's license. Bob does the same. They each squint at the other's ID, then down at the printouts, comparing fingerprints. Neither really knows what they're supposed to be verifying beyond the digits matching. Satisfied enough, they nod awkwardly and move on.</p><p>Later, back at her laptop, Alice uses the terminal to sign Bob's key and upload the signature to a public key server. It's a little thrilling, in a nerdy kind of way—but the truth is, she's not sure if she'll ever need Bob's key again.</p><p>This ritual—half security theater, half social ceremony—was the heart of early attempts at decentralized identity verification. It was a noble effort to build trust without relying on central authorities. But as creative and community-driven as key signing parties were, they never really worked at scale.</p><p>Let's talk about why—and how decentralized identifiers and verifiable credentials might offer a better path to first-person trust in the digital world.</p> Why They Didn't Work <p>After the conference, Alice doesn't think much more about Bob's key. Sure, she signed it and uploaded the signature to a key server, but that was more out of politeness than practical necessity. Weeks later, when she sees Bob's name in her inbox, she vaguely remembers meeting him—but she has no idea whether she should trust the key attached to his email.</p><p>Bob, meanwhile, has been trying to get more people to sign his key. He's collected half a dozen signatures, but they're from people he met once, briefly. The "web of trust" he's supposed to be building still feels like a pile of disconnected threads.</p><p>This is where things fell apart:</p>  <p>It wasn't user-friendly and was far too manual—Every step was an opportunity for confusion, mistakes, or simply giving up. And once the key was signed, there was no easy way to use that trust meaningfully in everyday communication. Nothing about the process felt intuitive. Fingerprints were long strings of hexadecimal gibberish. The tools were cryptic and unforgiving. Even for technical folks like Alice and Bob, the experience was brittle. For most people, it was impossible.</p>  <p>The web of trust never reached critical mass—The key idea behind the web of trust was that if Alice trusted Bob, and Bob trusted Carol, then Alice might come to trust Carol, too. But that only works if:</p>  <p>A lot of people are participating</p>  <p>They're actively managing their trust relationships</p>  <p>The connections form a dense, navigable graph</p>  <p>Instead, what Alice and Bob ended up with were isolated clusters—tiny pockets of trust with no meaningful way to bridge between them.</p>  <p>No immediate payoff—The effort required didn't translate into practical value. Alice never encrypted an email to Bob. Bob never used his signed key to unlock any kind of access or reputation. Signing a key became a kind of ceremonial gesture—well-meaning, but ultimately inconsequential.</p>  <p>Trust was binary and shallow—In theory, key signing meant "I've verified this person's identity." In practice, it often meant "I met this person at a conference and glanced at their ID." The depth of trust was thin, and the binary nature of key signatures (signed or not) didn't reflect the nuanced reality of human relationships.</p>  <p>The core idea was right: identity verification shouldn't require a central authority. But the implementation relied on people doing too much, too manually, and for too little benefit. The trust infrastructure never got far enough to be usable in real life—and so, even though Alice and Bob meant well, their efforts ended up as little more than cryptographic footnotes.</p> What Can We Learn from the Experience? <p>Let's rewind and replay that moment between Alice and Bob—only this time, they're operating in a modern, <a href="https://windley.com/archives/2024/06/what_is_decentralized_identity.shtml">decentralized identity system</a>. No key servers. No GPG. No fingerprints printed on paper.</p><p>At another tech conference, Alice scans a QR code on Bob's badge or uses her device's NFC reader to create a connection with Bob. Her <a href="https://www.windley.com/archives/2022/12/wallets_and_agents.shtml">personal agent</a> (not necessarily AI-powered) resolves the <a href="https://www.windley.com/archives/2021/06/the_self-sovereign_internet.shtml">self-certifying, autonomic decentralized identifier (DID)</a> that Bob provided, pulling Bob's DID document—not from a central directory, but from a peer-to-peer interaction.</p><p>Bob's agent reciprocates, requesting a DID from Alice. This isn't just identity exchange—it's mutual authentication. Each party cryptographically proves control over their identifier. No centralized certificate authority is involved; trust is rooted in the interaction itself, supported by verifiable credentials issued by organizations and communities both recognize.</p><p>But here's where it gets really interesting: by exchanging DIDs, Alice and Bob have created an actionable connection. Their exchange creates a secure, private <a href="https://windley.com/archives/2020/11/didcomm_and_the_self-sovereign_internet.shtml">DIDComm messaging channel</a>. This isn't just for encrypted chat—though it could be. It's a foundation for ongoing interaction: credential presentations, access control, consent requests, proofs of presence, or even contract negotiation. The connection is both trusted and usable.</p><p>Later, Alice could send Bob a verifiable credential confirming they met. Bob could follow up by sharing a credential that gives Alice access to a community space. Their agents handle the details behind the scenes, using DIDComm protocols to maintain privacy and ensure integrity.</p><p>There are a number of important changes in this new model:</p>  <p>Trust is peer-to-peer—No key servers. No middlemen. Just Alice and Bob exchanging self-certifying identifiers directly and building trust based on verifiable claims and mutual context.</p>  <p>Mutual authentication is built-in—Both parties authenticate each other through cryptographic proof of control and credentials. It's not a one-way lookup; it's a handshake.</p>  <p>DIDs enable ongoing, secure interaction—Unlike traditional key signing, which ended after the ceremony, exchanging DIDs gives Alice and Bob a secure channel for ongoing communication. DIDComm messaging transforms identity exchange into a <a href="https://windley.com/archives/2020/08/authentic_digital_relationships.shtml">persistent, actionable relationship</a>.</p>  <p>Trust has become usable—What began as an in-person meeting becomes a functional connection: a secure link over which credentials, messages, and permissions can flow. Trust becomes a bridge, not just a checkmark.</p>  <p>There are no key servers, no command line—Everything happens in the background: the agents manage key material, update DIDs, and maintain the messaging link. Alice and Bob stay focused on their goals—not cryptography.</p>  <p>Key signing parties were built on a noble idea: decentralized, user-driven trust. But they stopped at verification. In the world of DIDs, DIDComm, and Verifiable Credentials, trust becomes a living channel, not a static record. Alice and Bob didn't just verify each other. They connected. And that is a huge difference.</p> Improving the UX of Trust: Verifiable Relationship Credentials <p>After Alice and Bob exchange DIDs and establish a secure DIDComm channel, they have the foundation of a relationship. But what if they want to do more than just message each other? What if they want to capture, express, and eventually use the fact that they met—on their own terms? That's where the verifiable relationship credential (VRC) comes in.</p><p>Let's say Alice decides to issue a VRC to Bob. She does this through her personal agent, which creates a standard verifiable credential with self-asserted attributes describing her side of the relationship. The credential could include:</p>  <p>Her name and other contact information</p>  <p>A claim that Alice met Bob in person at "IIW XL"</p>  <p>An optional role or label she assigns ("professional contact," "trusted peer," "collaborator")</p>  <p>A brief note about context ("Talked about SSI, aligned on agent interoperability")</p>  <p>A timestamp and a validity window, if she wants the credential to expire</p>  <p>Her DID as the issuer and Bob's DID as the subject</p>  <p>Importantly, her identifier within a shared community context (e.g., her IIW working group handle or project-specific DID)</p>  <p>The VRC is signed by Alice as the issuer. Bob can now store that credential in his wallet—not just as a keepsake, but as evidence of his connection to Alice. He can selectively present this credential to others who might trust Alice, using it to bootstrap his reputation or prove participation in a network. Crucially, this credential is voluntary, signed, and contextual. Alice isn't vouching for Bob's entire identity—just the fact that she knows him, in a specific capacity, at a specific time.</p><p>Bob, in turn, can issue a VRC to Alice, reflecting his view of the relationship. These credentials don't have to match. They don't have to be symmetrical. But together, they form a mutual web of attestations—a decentralized, trust-enhancing social layer. Over time, as Bob collects similar credentials from others, he builds a mosaic of relationships that's both verifiable and portable. It's like LinkedIn endorsements, but cryptographically signed and under the subject's control—not platform-owned.</p><p>This works better than key signing parties for several reasons:</p>  <p>Trust becomes tangible—Instead of an abstract handshake, Alice gives Bob something concrete: a verifiable statement of trust. It's not absolute—it's scoped to their interaction—but it's actionable.</p>  <p>Portable reputation—Bob can present Alice's credential in other contexts where Alice is known or trusted. It's a decentralized version of "you can use my name."</p>  <p>Contextual and subjective—The VRC reflects Alice's view of Bob. It's self-scoped and doesn't pretend to be a universal truth. That makes it both useful and safe—especially when combined with selective disclosure.</p>  <p>Built for agents—Bob's agent can surface VRCs when interacting with third parties: "Alice has attested to this relationship." This creates a fabric of lightweight, useful credentials that can augment decision-making.</p>  <p>The verifiable relationship credential is simple, but it captures something that key signing never could: the social, situational texture of trust. It turns a peer-to-peer interaction into a reusable proof of connection—issued by people, not platforms. For Alice and Bob, it's no longer just "we exchanged keys." It's "we created a relationship—and here's what it meant."</p> From Relationships to Reputation: Trust as a Graph <p>Alice and Bob meet at <a href="https://internetidentityworkshop.com/">Internet Identity Workshop (IIW)</a>—a place where decentralized identity isn't just theory, it's hallway conversations, whiteboard sessions, and rapid prototyping in the lounge. After exchanging DIDs and establishing a DIDComm channel, they each issued the other a verifiable relationship credential (VRC). Alice's credential says she met Bob at IIW, discussed personal agents and DIDComm, and found him a thoughtful collaborator. Bob issues a similar credential to Alice, reflecting his side of the relationship.</p><p>Fast forward a few months: Bob keeps showing up in conversations, contributing to working groups, and collaborating on new specs. Each new interaction leads to more VRCs—credentials from others in the community who are attesting, in their own words and context, to their relationship with him. These VRCs, taken individually, are simple statements of relationship. But collectively, they form a decentralized, living trust graph—a network of attestations that agents can navigate.</p><p>Now imagine Carol, another participant in the identity community, is deciding whether to bring Bob into a working group on credential portability. She doesn't know Bob personally, but she sees that he has a VRC from Alice—a name she recognizes and trusts from prior collaboration. Her agent reviews the credential and spots something important: the community identifier in the VRC Bob presents from Alice is the same one that appears in the VRC Carol received directly from Alice months earlier.</p><p>That shared identifier becomes a verifiable thread—linking two private relationships into a meaningful chain of trust. Carol's agent now has high confidence that the Alice in Bob's credential is the same Alice who endorsed Carol. Bob doesn't need to present Alice's global identity—just the portion she's chosen to make consistent in this context. Carol's agent reviews Bob's broader trust graph and finds:</p>  <p>Multiple VRCs from known IIW regulars</p>  <p>Overlapping context (working on agents, involved in open standards)</p>  <p>A consistent pattern of positive, scoped endorsements</p>  <p>Crucially, a link back to someone she already knows and trusts, via Alice's community identifier</p>  <p>Carol doesn't have to "trust Bob" in the abstract. She can trust that Bob is part of her extended network, with specific, verifiable relationships that support the decision she needs to make.</p><p>This is reputation without centralization:</p>  <p>Peer-to-peer, not platform-owned</p>  <p>Contextual, not generic</p>  <p>Verifiable, but privacy-preserving</p>  <p>There's no algorithm deciding who's "influential." There's no reputation score being gamed. Each relationship credential is a piece of a mosaic, curated and held by the people who made them.</p><p>Personal agents that are augmented with AI could traverse these graphs on our behalf, weighting relationships based on factors like recency and frequency of interactions, the trustworthiness of issuers (based on our past experience), and relevance to the current task or decision. The agent doesn't just tally up VRCs—it reasons about them. It can say, "Bob is trusted by people you've worked with, in contexts that matter, and here's what they said." That's real, usable trust—not a badge, but a story.</p><p>This system isn't just more private—it's more resilient. There's no single point of failure. No platform to de-platform you. Just people, agents, and credentials, all stitched together into a flexible, interpretable web of trust. It's the old dream of the PGP web of trust—but with context, usability, and actionability baked in. From one simple moment at IIW, Alice and Bob built not just a connection, but a durable credentialed relationship. And from many such connections, a rich, decentralized reputation emerges—one that's earned, not claimed.</p> Relationships Are the Root of First-Person Identity <p>When Alice and Bob met at IIW, they didn't rely on a platform to create their connection. They didn't upload keys to a server or wait for some central authority to vouch for them. They exchanged DIDs, authenticated each other directly, and established a secure, private communication channel.</p><p>That moment wasn't just a technical handshake—it was a statement of first-person identity. Alice told Bob, "This is who I am, on my terms." Bob responded in kind. And when they each issued a verifiable relationship credential, they gave that relationship form: a mutual, portable, cryptographically signed artifact of trust. This is the essence of first-person identity—not something granted by an institution, but something expressed and constructed in the context of relationships. It's identity as narrative, not authority; as connection, not classification.</p><p>And because these credentials are issued peer-to-peer, scoped to real interactions, and managed by personal agents, they resist commodification and exploitation. They are not profile pages or social graphs owned by a company to be monetized. They are artifacts of human connection, held and controlled by the people who made them. In this world, Alice and Bob aren't just users—they're participants. They don't ask permission to establish trust. They build it themselves, one relationship at a time, with tools that respect their agency, privacy, and context.</p><p>In the end, relationships are the root of first-person identity, based on the people we meet, the trust we earn, and the stories we're willing to share. If we want identity systems that serve people, not platforms, we should start where trust always begins: with relationships.</p>   <p>Photo Credit: <a>Alice and Bob Exchange VRCs</a> from DALL-E (public domain)</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!8fZH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fc199ed-1f71-4ca3-bca3-82a9840691d8_1792x1024.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:16 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 09. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://realestatecafe.com/'>Bill Wendels Real Estate Cafe</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://realestatecafe.com/fool-in-white-house-creating-greater-fools-housing-market/'>Fool in White House creating Greater Fool’s Housing Market?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Trying to forecast interest rates can make a fool of all of us, but Peter Schiff argues: “If Trump’s secret agenda is to crash the… 
The post Fool in White House creating Greater Fool’s Housing Market? first appeared on Real Estate Cafe.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>Trying to forecast interest rates can make a fool of all of us, but Peter Schiff argues: “If Trump’s secret agenda is to crash the…</p>
The post <a href="https://realestatecafe.com/fool-in-white-house-creating-greater-fools-housing-market/">Fool in White House creating Greater Fool’s Housing Market?</a> first appeared on <a href="https://realestatecafe.com">Real Estate Cafe</a>.
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:13 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 08. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://idmlab.eidentity.jp/'>IdM Laboratory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://idmlab.eidentity.jp/2025/04/openid-foundation-workshop.html'>OpenID Foundation Workshopクィックレビュー</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  こんにちは、富士榮です。    今年もInternet Identity Workshop（IIW）に参加するためにMountainViewに来ています。     今日は前日ということで例年通りOpenID FoundationのWorkshopとDCP Working Groupの対面会議がありました。     ということで書ける範囲でクィックレビューを。（主にOIDF Workshopについて）       今回の会場はGoogleのオフィスでした。いつものことながらチャリが可愛い。乗って帰ろうかと思いました。      ということで中身に。     OIDF Milestones in the last 6 Months: Gail  まずはOpenID FoundationのExecutive DirectorのGailからここ半年のOpenID Foundationのアクティビティ
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://blogger.googleusercontent.com/img/a/AVvXsEgH6XOyxjKPftHmId7Ed0DiUlC_l_dGmDY7KdaB9VA98sU6j5gLoNi1twetcbS1Er5M64Xup8LfVoDGaDD2_CTlgoVOWj1QxBY7SnrGMZUILUJAXcc_XBhJYzzz-kKCfIKSNKb6jxGz_gzMS9wlT_HniSQXr47VfZh0gawpcm2icftXc2bkMkevkG5AHrw_=w640-h480'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

こんにちは、富士榮です。 <br>  今年もInternet Identity Workshop（IIW）に参加するためにMountainViewに来ています。  <br>  今日は前日ということで例年通りOpenID FoundationのWorkshopとDCP Working Groupの対面会議がありました。  <br>  ということで書ける範囲でクィックレビューを。（主にOIDF Workshopについて）  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgH6XOyxjKPftHmId7Ed0DiUlC_l_dGmDY7KdaB9VA98sU6j5gLoNi1twetcbS1Er5M64Xup8LfVoDGaDD2_CTlgoVOWj1QxBY7SnrGMZUILUJAXcc_XBhJYzzz-kKCfIKSNKb6jxGz_gzMS9wlT_HniSQXr47VfZh0gawpcm2icftXc2bkMkevkG5AHrw_"></a> 今回の会場はGoogleのオフィスでした。いつものことながらチャリが可愛い。乗って帰ろうかと思いました。  <br>  <br>ということで中身に。  <br>  OIDF Milestones in the last 6 Months: Gail  まずはOpenID FoundationのExecutive DirectorのGailからここ半年のOpenID Foundationのアクティビティのサマリーを。しかし活動量が激増しているので超ボリューミーです。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEilQixf6_jXuuZY5sDSvbCG6R0RhD9ZlHCoTwt4Fg3hiXJkYXXym1V6FSPUAPZIXzvRKM5LMQ1AlAcUdbz7Mjk7uKD6S9MZMrQxmFbmlHqEuwHUXkKKtt9XFGtpzsVQLe8H8Y41XUGKKnHdWgrf5pnu54Y65oWxTKtFLPtK3iKfyTC6WcrUtyYX2Ar77ZnM"></a> <br> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgS1VypahB6VLZCMT8Iptq-nh6JaVc5cJquhAk-6RqFMeryTcTZ62h9N80V34tBXek_AGQlcr6woDsOW-sVaXSGIJ5Pvx_iPOhVPNb-K7Q3ZK3xsNdApe3N36OMeWDH9BOOO-SJnVuKKuUA8-AsGh0tolUyly98JIkWqdEkEK3yaEGehPreJ0NLrrbYxOEW"></a> <br>なんか炎上しているように見えますが、ホットトピックスってことだと思います。  FAPI、DCP、eKYC&amp;IDA、AuthZENなど最新仕様がどんどんリリースされていますし、Interopイベントもたくさん実施されています。  また、面白いトピックスとしては最近活動を停止したOpen Identity Exchange（OIX）の持っていたドキュメントへのアクセスがOpenID Foundationのメンバーに公開されたっていうのは良い話ですね。Trust Frameworkの設計をする人にとっては非常によいドキュメントが揃っています。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgyG89GBhUmV1QnFlIV1La9ZwxvPFFSLngB9Cu3cH4_hJ1Fgbks2A7Cw8YxFXEDKVfR6I9i44VcZoEq26hoflTJid-od6cJWRvqUJP0XoBfdGSlBiBw9FPXzdmVMCicw9AeK1sLFZb3BTwAFn5qaDEL6PRT1pcDdgICQVhHLfan_i69c7YF49wrUJ7C0S1M"></a> <br>  メディアへの露出も色々と。日本国内でもこの辺りは意識していきたいところです。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgeMYS0vrXJ2Gu48R09Ymp5ixeRYQafgbHR8AZFsRqyVyP9rCT7ZfAvJeGWaPn-no96YbsD8fDx5NEFUfFgMe_pOwuPL24BJrV71um-4xH_HOwHJEr8Ew4NDGVGwzs8HLZG6Hkjsvj_Zi8oxGvTPZwhZKtENUB4uHJs6lihLlOd0CvhZiVcWmrwxo1OimKI"></a> <br>先日<a href="https://idmlab.eidentity.jp/2025/03/okta-ventures25the-identity-25.html">こちら</a>のBlogでも書いたOkta VenturesのIdentity 25にOIDF関係者が数多く選出されているのは素晴らしいことですね。  <br>  Automation Tooling Roadmap: Mark  次に仕様のドキュメントをHTML化するあたりを自動化するツールの開発についてMarkから、と思ったらMarkが体調不良でスキップです。来週、共同議長向けに説明会があるそうなので聞いておこうと思います。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEi9muVBTJOgP6S0j26I2W_bQ5fApUpk1CJ7qolS4tkUNBrJ89Ygmg9XbDG1Nbr0nYwlPIu9gSCWItD38yGx6ll6CicJXSpGNxRqGtN6I8ubvW_R7HO_OJbrygMs4XWD7iseZxjxRxOG2yMKkZnS7YOq5BDOQB-yd-DJPr1hue7H2GcQeYer-HCNOwSP-spm"></a> <br><br>  eKYC &amp; IDA: Hodari  次は我らがeKYC &amp; IDAワーキンググループです。先日共同議長に就任したHodariから説明がありました。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEiGTNiEemptD_snQCe3H7KzX0-x_S-HjN9Lnzbod8b5FkZB9oZeipeGSDTi4Qzjo5adbT3RQz760Q4rYN025taNBQTAyb2DEs3uk-gznZ2NUPjSdh8IJ40Rqe0FXni_3ddVz3jX1l3HEPW0SkbD7A_iicZ25rSrH5UN_I-2jC-ULrm2O5xhX8vASUlEuyJq"></a> <br><br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgUgHo8XzSaGBPesNFE6OaeORzBM-3EegWTV2Z9i17jWHYnYmc3O52giubq5usnApMS8Cq_0mLLRR6uFaLd5oDvACfLMPrSc2sb8Qch1Vsxes5Yw1PhOigwTvdEjnJxz9GnU3I0KCnJJ0eTfdfU5-zled4Y6rgczzax5TOovICV3egyZwEJECco_Buy05eE"></a> <br>こちらもネタ満載です。  ISOのPASにIDA coreとSchemaがサブミットされている話とか、APAC（というかオーストラリアと日本）にフレンドリーな時間帯でのコールを実験的に開始した話がありました。  とはいえ、逆に日本時間だと通常のお仕事で埋まっていることが多く、結局夜中のスロットに出る方が出やすいというジレンマを抱えていますが・・・  スペックのFinalizeに合わせてコンフォーマンステストもFinalizeに向けて進んでいたり、次のチャレンジとして年齢確認のシナリオについて検討が進んでいたり、とにかく色々とアクティビティがあります。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEjB-2mZ7lmSzjMicK_YwEySQa9ON5qKjGubPEDklRpAm9iKAvwKN7zL0kk0hwDfJDOkmyp_LASsN-506x-JcMy1XS5p9VW4MJIn23siPfuFJof_3hEXXIDfFIuTQumtMU08ixiKo6VSWNKjvXT3sswU8P4daXPQEn4bJE0krFkvdoF51dKVngf-Y6ST5ZOS"></a> <br>今後のロードマップとしてはQ1（もう終わってるけど）にAttachments、Q2にAuthority ExtensionのFinalizeをしていきます、という話です。  <br>  <br>  DADE CG: Dean  次はDeanからDADEの話です。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEiUX4uBemc0LOuXW2Z6BAHgSvyvSL4HA6CutY3VT4Q3jo51ED3twPBGxfVIukiWX2T7yjI-0ue8xt2aHTvyMZDzcwDhctkR2U8TmbIMw3W70bKCIj_fgT0IiYhhiwhNu7UUHIP_zPgUVm6us_QNMLxd3IeZcJggpylnpxDjRk5qLSroqBkwhqNElsVaVEP7"></a> <br><br>  ちょうど先日アイスランドで開かれたOAuth Security Workshop（OSW）でも話をしたんですが、DADEのように死後にデジタルリソースをどうやって引き継ぐか、っていう話は突き詰めるとリソースへの代理アクセスの話にも繋がるのでeKYC &amp; IDAやDCPのクレデンシャルの委譲など、色々なスペックに共通したユースケースになるんですよね。うまくPluggableな仕様に練り上げられると汎用性が上がって良いと思います。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEjCVLKZx7Y760Klye3wc3fU5ARzgYBBzXeIdPcwCUsFJcQo5t7ZUw_X3G52l_4bb9dor5Hvpqu4PX3YrclMqLHbOZgqOjF-DsUr8z-6l2cqPSbb8jnAFJd-r5V-9X926NG7H8d9TeBnCpyINW8-u9otpg0kzpULpj_ofGob9zs079_eRzBEe5ihuSgTBNv9"></a> <br><br>  このCG（Community Group）では定期的にミーティングを開催し、ユースケースについて議論を進めています。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEiO7ESs8ycuHiC7XyxwN7uoy3XMKYHOABdp7FREDJ1b5ETe04g7T-Amp_KkW59w0OP-QseZry75QaTvnAJ8KEWI5mMzAupCsCmP8u_P7aTUUyXUzAMOA63ZDpeGjDOujjJJZfdA2BFzQYf22-Mpz1m6q9RBoqhrUlZ1_5-6qb58k88mg5B-NTc7TeSmZfLf"></a> <br><br>  次のマイルストーンはホワイトペーパーとして議論の結果を取りまとめて発出する、ということです。今年の10月がターゲットになっているので活発に議論が進んでいくことになるでしょう。  <br>  <br>  AI Whitepaper / Panel: Tobin, Dean, George, Aaron, Atul  次はスペシャルセッションということでAI文脈の話です。スタンフォードでAIの研究をしているTobinを中心としてOIDFの主要なメンバがパネリストとして参加しました。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgczS3AS6lEEzQbRineMWHy_TT3ltGuZKJgGJjhAPSz92dEg1UdGvZ9nfNTZnF_Wr1mjCTYB73EgBZljDLLw5Tcs2VtmgtazXmi8fRs4wqve9ZpuKZ46IPosoxTxVJCRZRsVlX4h27djk6bwGvSd5WZjgt-ocDmuxR8n-jyL-aANMOS7KgoOGM8eTUbeE4Z"></a> <br><br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhDvCC1hd9WgGDWMZ5n10OV4rAfR0mq9VXjLCF3G-uoiP6UApSXH-7pcZiX4fRoX1g4Y8oChKlnKhdyvd-CODoVWUGo0fLnKvh0rTtuUtxDSdiQQRf5k_PjN6a7XwijFiLSz2nCbq000tXsqcDXQzHx07SogUY4CY9SrGS90r8_dkADuxd2Tm9-l1QBQk9h"></a> <br>書いてある通り、チャットbotやAIエージェントが流行るなか、色々なスタートアップが認証や認可、アクセスコントロールの話を置き去りにしてとりあえずサービスをリリースする、なんていうカオスになっているので、ちゃんと考えようよ、っていう話ですね。おっしゃる通り。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgGsJnV23H1eAWjG7BNnENN0IXLSs1OZOrIW5W_MWgyyRjR9En_Yihummqwg-7HB00_Mtc7864NWkMcpRUv4O1fqKXp8OteD40dckM7D8I975FVDKw-M63B3VBlmY2KZp9xdbRJ1lio7dzvL4QSC63ya-h3jlxYUBgu5FVU64oCnbWPEMfwVCJAR7DTQn1n"></a> <br><br>  そういうことなので、こちらでもホワイトペーパーを書いているよ、と。  Aaronが最近投稿した<a href="https://aaronparecki.com/2025/04/03/15/oauth-for-model-context-protocol">記事</a>にもありますが、MCP（Model Context Protocol）にはちゃんとOAuthを組み込みましょう、って話です。  <br>  この辺の議論が盛り上がった結果？かどうかは分かりませんがMCPの最新の<a href="https://spec.modelcontextprotocol.io/specification/2025-03-26/basic/authorization/">仕様</a>を見るとOAuth2.1の利用が必須、ということになっています。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgTOc0HAyfYLuzcBusItS3_PB-LE8tv9J1pydpDOjA3CjlQxNMKFozXMBh7Rv49vV8DXmU6PM2NZx8MdO3i0N-tAU_Qt1SficGm73l4N7VVPtXsNP4PE_Ba6F8Iy29Czw72Vv9mnRB20uYpdr6PWDQ71UXK3hpiS49P8bkq_2gUPqiMHCVobgBBjVWwHR6s"></a> <br>難しいのは、事前にAIエージェントがMCPからデータを取得する際の認可を事前に与えるのか、コンテキストによって都度リソースオーナーの同意を得るのか、この辺りのユーザ体験を考えながら実装しないといけないあたりでしょうか。  <br>  あとは、権限の範囲をscopeを使って表現仕切れるのか？というのも個人的には課題だと思っています。AIエージェントとMCPサーバの間はそれでいいのかもしれませんが、AIエージェントに対して問い合わせをしてくるクライアント（人かもしれないし別のエージェントかもしれない）とAIエージェント（もしくはAIエージェントに権限を委譲している人）の間のコンテキストをAIエージェントとMCPサーバの間のコンテキストに反映しようとすると単純にscopeだけで表現できるのかしら？？？というところはこれからの議論の対象になるんだろうなぁ、と朧げながらに思ったりしています。  <br>  AB/Connect: Mike  次はAB/Connectです。最近はOpenID Federationが中心になってる感じですね。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEjZWgZHyI6ULNVbuwZctg1roK2RPk63NVEGT3osDsLTWXD6VE2PornjD2dF8z8sSXpc3Ti2QJYwOLcFIGUeNWcje0avUuwhTRTO_ZO-rd4-mOkt9mWZY1VafjfIzHzoHqA1EbTKZuT34fszoJkAZftjhSw60hehECMImr2QPyYnRpSdbV-vW4ANQOyeHntY"></a> <br> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgb_0ll2SG3Q0fdLjiefaIwMJJGQ8ifsoQUPlmiYuOsT9ozcnphDwIfAADtQpNPacIKZ-UtKbR1uQA9mYrJOlz_hiyR8rgg91ghIu3Nc3xR8tQmAHRQzX5qU9JgTvZhQYvz7Jby5Po0uHVE7iW7tLtlL_fQ1KuGb-TXdMohV7fzNU1rkaa17gupyR-fsA7O"></a> <br>やはりOpenID Federationにフォーカスが当たっていますが、結構重要な話としてOpenID Federationのセキュリティ分析の中で見つかったJWTのaudienceに関する脆弱性が他の仕様にも影響があった、というのがトピックスでしょうか。  <br>  2月にOpenID Foundationの<a href="https://openid.net/notice-of-a-security-vulnerability/">ページ</a>でも情報公開がされていますね。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgUmVrC1uoKs6KQ63dOHkHEdGL0dICe4VwJp6rflDghQzFNn5bSQkmBgiZ-iSG4QNdFZGVJOKfZ5e6xlJBY555YcIWTcNVfBGlSM1eBgQbcxD21rnRutfuu949_dToR_uUBQo-Afnyxzf1bZ7KoassyhxMyBF7xzy_UQqy8Z-5YOM8X4YyaCpyvwnxvonEQ"></a> <br>OpenID Federation以外にもOpenID Connect CoreやFAPIなどそれなりに影響があり仕様の改修を進めてきました。   <br>  <a href="https://blogger.googleusercontent.com/img/a/AVvXsEj-Extw4Bw68GcW545AvpzzDGtJTgSKSqAA1aj5qjFAZJ_TlRz1IkALlbUN1tTRNh6zVTrs0OtF6lhL4zX2ERCtyq4NP9mSPd22QonkVqQsjzQjmsgss1KgW0FW5mDoQnsnLboqUQuYXq2ocHHJZzu7wP86wIz43Jh7GvVkpCq6YqJjO3BAQ5XWjmU-noKx"></a> <br><br>  OpenID Federationに関するInteropイベントも開催され多くの参加者により接続テストが行われました。新しい仕様が普及するためにはこのように色々な実装がちゃんと繋がるか？というのは非常に重要な観点だと思います。  <br>  OpenID Provider Commands: Dick  個人的にはこれも非常に興味深い取り組みです。特に後述するIPSIEなどエンタープライズでOpenID Connectなどを使う場合には非常に重要な話だと思います。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEh3G-c7RG1QziVO3YlEh6bCtEIvExPA55j3SlIzTgOTT2tTY_ggIin5Fty8ujmsL0Udvrt1Xoee8dgdQ6g1Pw73Sv0STPLbyKOd7RQRni6oXjJa5OB7XCX9JQcJ0CstVCPQjsiNWzmeVUotf8K78iHmzoWy21e1i85HmLmSKOVrsp87LV8ktmam05NlpzNa"></a> <br> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhBYvJ4fcTTOyvCW7f0XhWqo7OufwdzrItL6biDPzFUvY6QzO3CN81rjCqaWobVy0j1Crs5V-iyN_Nw0yvTfnxx6w954dKcvO3slCX1uXHK-KMXwqGYx0MVhFJHYnmI3alLTgIhTGvfL81LOsi0pz_LYtINuy8tZfXsj_Qi-UwM4is4e-_F5331oZx0GATb"></a> <br><br>  めちゃくちゃ簡略化して話すとOpenID ProviderがRelying Partyにコマンドを投げ込む、って話で、主にアカウントやセッションなどのライフサイクル管理を念頭に置いて設計されています。（よくある、Identity Providerへのプロビジョニングは人事システムから直接連携されているけど、アプリケーションへのプロビジョニングはCSVを別途作ってバッチで取り込んでます、的な話をAPIでやっちゃいましょう、という話です）  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEid81yLLx9zQ9u8VDNUOWvZhzrdcdOmwNjBgKTIyyggWKcUHj_1AUOzmf8kbcLgeRTPdxGl-YSmjRMwvTfuo8aNu6RQldqQm1leUlsiuV33bmG6qddtYIa0raQ4g2V2QJGvB8QrJkaK_Qdy-0U3u20pqIYleHyPaUoukKJWWETBLag_HhSrzk0fEIWfkPpv"></a> <br><br>  ほんとこの辺りはIPSIEやSSFとも関係してきますが、アカウントやセッションライフサイクル管理には非常に重要なコマンド群を整備していくことになりそうです。なお、こちらでもMCPへの適用についても触れられていますね。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEjNgdS0OjRExqDY3gIRSiMpEqihHBWR4Ltrrogx9lfIP_v67k5dd8IgRylbDttzavrRQFGpYgz_qlX3Zz20o-8OIbY3fxYTbBbhd047n2F__aoLj1zStV9orucCSsJZgpcfJPfaOBRxjJxYkCu73c_Y0EWXWzRd2N1SKgLExBkKFY6B3G2TP9HEma2xv2Xa"></a> <br>認可取り消しは結構難しい問題でしたが、OPからのコマンドが出せれば便利ですね。  <br>  <br>  AuthZEN: Omri  次はAuthZENです。こちらもエンタープライズをはじめとして利用シーンはたくさんありそうです。これまで鬼門だった認可・アクセス制御に踏み込んだ面白い仕様ですね。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhDH3-aA8MP67faSFXwxp-fbtchZw-sWMNjG7m-cb2ikfZ-HqTaF0HNUa2SMJUBT2KYa9OPVVhpYuuLSGp1jK5GzLDi7rcZkEsHEC1s7D-z6hIzvYZuFZdlPn1H-B0U-9oj5FRgFK2zYBddncNnckOObbB3oPw-u6PU9JliPOBBoPdvlSBhXdt2xrWeWVRJ"></a> <br> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhMFq3l7TtQTbc0oWDXNG8yb573QTdyEkOnZsyEgUrtRlC-1xqKKeksNfLR3sm-UgRINLYX-haLVNLHWpTsI1-tEiouvrCuOv-TIUDrpVVcS5OzBXSDNOg2P-KJV9q7EpJ4RBryCtjnWBVQ1c68lOmnDx_Heg6gn4ga2cFsVADOB03xe83AJ-rqVER0UQsr"></a> <br>Authorization APIも徐々にアップデートが進んでいます。  こちらもInteropイベントをやっていますね。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEil3WQ9o8kwB-unQVwhRgDiH5H3PZqfZHpCGzDDFZgVtfYuZi0AoLueLLUrDidTgbtvY13ChS9rw6zIWpjr4d4_6d8AewKlOKn4w4c-8_ZMhcH5vC_JYNC_Ax3TKUb_hn5fpCgdxwYjWx1WyIeCWhmognc2WpIlycJodbp3YKYPELUpnvwSs7AavUZkxhKt"></a> <br><br>  こんなアーキテクチャで実装する感じです。（Interopイベントでの構成）  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEjnEfvGHyYkiSbH6qLDTb2ioedcvU8A_ntOMTlYZEbWzvWm8GYgLgtCtfY9wSKgGpjaBhKgvEEqZ5uDwk2ThZAKJuWVRzvX05Z5TTmTc100IqPyhZMlJ6xFuiaezUGiVWcZqewzUzSOnnp6LstTKV9LLpz7IYHTDDZv19irTxy3v72CQK1s_CT9RVxAVRmm"></a> <br><br>  Interopイベントに参加している企業もこんなに増えました。2024年末は14社だったのが2025年3月には倍増しています。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEifCupJPl92xyUOm8ock1M3gh2x0J-z1c5aeT9zSZ3Z9XBFaunfL99L4zcPC6hOk8U8_Hlu1VK77JO2E5l5xkSqVTU8wKmHDNH2zuH4K19RIrntDLVFUQVzVdbo2yrrs_JOra6K8Tv9zdBd6neDl7pwAHk-VdHboGrUmz8B5FHx_oUQ4elxqVkGre7kEWbA"></a> <br>今後のロードマップも発表されましたが2025年の夏〜秋にかけてcoreに加えてAPI Agewayなどに向けたプロファイルの策定も予定されています。  <br>  IPSIE: Aaron, Dean  次はIPSIEです。特にエンタープライズでID基盤を運用する上で必要なことを全部まとめて仕様にしちゃおう、という野心的な取り組みです。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEiQu3Ha7abF0MkUBL84vUw9Q1arthyyFgx0F7k4QtsQNxzXjcnEQ5saIUVGkRbPjNGhYz4XJCmwIoS-rBRH1ddj5AQ79vQJtquyYPFy1gaMkc4GJCqT2sMI5WPtiwcBSPfK-3RXjoW5s4worQEtuJ0Jotpfnd6yjlXfDFt84aYZslZ5xul_92aL5yghKoB0"></a> <br> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhENvr2j9ZCb135bmMH8zh_c-2LGrZZgzQcGSHeog5cPsPNp8oztGMhE-iP2_0WTA5bgY56FtvoflrgdF0UUVdNHLN-Q9CXY8VkcyJ-FaAM42wu5g360o7fYz5vtFddga367UvLC_V2EJNfF1dvtk7oS585n6D9B5IlnbLqFPlOOivDpOJSDzGa7Sj6-0QH"></a> <br>SSOから権限管理、セッションやユーザやトークン管理、リスクシグナルの共有など主に6つのスコープでIPSIEは構成されます。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEh2mEeNDRBihXI1EweNx23W7rApNB8faoxqe8cPIYIqNsjpcuKhsfBbvX9-Ovgl_bckWP8xg-uL5-TQhVQ4J0aFZBHDvGRYk2hiCbOJ7cMcykZAAoh3EJnOSPRmaQukV8MTyTnYRmrJl8sH2tDguUtnbbkCYogcMrmHSF0yWqwSn86v6hxZPtIJCp_nemFM"></a> <br>昨年秋にスタートしましたが、すでにセッションライフサイクルとアイデンティティライフサイクルに関する管理レベルの定義（SL、IL）を定義しています。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhW7cyH2S3GRm80W7BSXaPhS2k9pqmO3qB_qwaf4jk4EOZqankiuApoa1A0ALTTgYjxUsou6NXQBVSZNmjOvlADy0zItcKd2EGlCzSwO2K47Hqngdt368FRyUz1Dx48XnJkyEwXYnUImCJ9oEKG94OrNGgK0u_cpUNwdc1o4uAiSpX3Qpb7OTb2aysqmqKP"></a> <br>いわゆるトラストフレームワークに該当する形でレベルを定義、それぞれのレベルに応じてやるべきことと実装を決めていく、という方法を取ります。このことで各企業がどこまでやればいいの？という疑問に対して答えを出すことを目標にしています。  <br>  Shared Signals: Atul  続いてShared Signalsです。この仕様も汎用的なフレームワークなのでIPSIEやDADEなどいろんなところで登場しますね。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhmejmZYVYARThC6GEMo7gEgRX0bCEyh-KOx6lo4ZAifenn1qqRvfop2elbJ14rbNtrWt5zk_sF6VqR_e5sU-F5GjX1KMq2sETv9hJ2LInEGPBBTU0DGA7-1Zc--wq0jEfRCTwG4hPqB33kGtJaCgpY5NDzeKJKHNTQaoHQWjkwwfYVUOivv6cs2gJBesov"></a> <br> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhmQSKdhEQ1UmNTjN_UCdusxFMnKTugZB965RauT-5YhX7FEAypVlRARVd54wHRmS2r_WQ4BcnYFSwfY_JhjXGpiCZGCoZwOySg1qEyK-83wlIQ049xTA3XVtkUYy6sjNyoZ_gktEcUQL9SfHKMZs5t5eblCxRL_XkXoqigLz_9iOE5q_c-JhB34M25N224"></a> <br>従来のリスクイベントの伝搬、継続的なアクセス評価のシナリオに加えてSCIMイベント、つまりアイデンティティライフサイクルに関するところも柱の一つになっています。この辺りはOpenID Provider CommandsやIPSIEとの連携が期待される部分かと思います。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhbXV-j7F_T6MqV1j1Fdhh2HMA_LMPDhzu2R9QneHVtwJbvTQ3MGmhu6BBJRqOroxgfeXFcF_hTlphGe-Ai6Z6zRIdu52urwJ_V9zDsJtxHgprRevSVryK_3kXEStYFgWYZYKwIfIhruM3ruY3qE_8yyYQkDf151NIMVisC7h3lKDLMOR5uYd2xMD1NBi57"></a> <br><br>  全体的なイメージですね。TransmitterとReceiverを実装してその間でイベントに応じてメッセージの交換がされる、という仕組みです。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgwCvXtRvjLK_wDzP_F58cFR45zazGALCJGE0RWGh63Kk3H3qvoPTnE-ZqBbUGfCLEoZJfQ6sqmbWYROPj2zEIuFa-H0u0ODDpRWF6U7uEeFi-FCAWMGByQ3wMp62RJgBt9EmpLu_q3mqxK9HVBlx2fWmMRRB2qM8WJdQ27rv7oONP5c-NWjQCZ4ZxmPFTn"></a> <br><br>  こちらもInteropが非常に重要なプロトコルなのでInteropイベントが積極的に実施されています。多くの企業が参加していますね。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgXi8IpawHcJdE9q-SI8_SlkbLI_X5RnDk8Fl-POZLn7TZFPJcfjRf0Q9OvhGu6QudKueNn7MFqxzLP06_ww48SAFRzJ_Ik4rwNDcMATU8KpBs6IaJFGIyN2WMMtxoH_VMWm1XxBU3FKqBs91UhLdD6NGFT9cgDIYcUgtnQAzRc51N9pc08VHdg_AtI-R6O"></a> <br><br>  すでにプロダクションで実装されているところも出てきているのは良いニュースです。特にLogin.govなどちゃんと政府機関がサポートしているのも大きいですし、MicrosoftのEntra IDでもCAEという名前で結構前から部分的にこの仕様をサポートしています。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEjk0x5MsgmeQhHzErtJTTunR4IIm2MLL3Ha7xTvdpUiuLYlvDASHgzf0r6Zj5o2qdEJfeP25u5QnXZ6fi4dK31JUqteDGXWfVy_SSRADubfDue1OUSF-qjfyomdyR-R_nYSWu3brP047A3ojRvM-KPwezQd-Pg08b1TxIEb1dRp3yDOJuWc-oSdHWsKaZRA"></a> <br>2025年は仕様の最終化やホワイトペーパーの発出、非営利のシンクタンクのAspen Instituteとの情報交換なども進めていきます。  <br>  MODRNA: Bjorn  次はBjornからMODRNAです。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEizsOc8PDSZhO5Omo4t53fTAkV5HR2qdQbhDLiE8NRnRZwtIBVm1jAh9SiJMNLTQefZY-R31awzW0VuwVEz0l1YFycDyEcfEIyHlD0p1TMhHlWMyEMkNtEHT53n0piPXY9ofdVZjWb808qk-s0C-TlO-vkCswWAfZSwfDfbYtxc6SFOkbFb63z4jk6nDmOj"></a> <br> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgUVTZHKgQ2nPF9pFmcMNroXIUVgOrlwMpaWfXJcP2kcyOmkx3zDwF8tXcJF300WBiPyggBh6zL5Ai_hWaNp3i7xv82jxHnm9A1dlVmTq7UVJHSYziyodvx7k7SmFRjNdGWipY-4T1FCcsmOpVfrbkwlq394Ku3ZzheyyQbGJ5FJPsAkEAFz_4XCd0HQ3sy"></a> <br><br>  トピックスとしてはCIBA Core Errata setのリリースですかね。  他にも昨年から続けているCAMARA Projectとの協業なども進んでいるようです。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgxJuX0D6LZcZDRMhX98-aNag9zCjV5q4xYYZ0WDtHFac42HSyUkFq_x3TZYCb9gJGfShmzE57Hg5FaHdA5NwO5xsm32kcnQwu7ZJERRk5xa-t3maa4MuNPpWi72T1Zq5lSdYiI72-KS2dfwvrqgEP33PnYL-ADNWlN7QRaT4D-OIA9Wod1QFZy-Ebv0F38"></a> <br><br>  今後のロードマップも色々と盛りだくさん。  <br>  ITU-T Submission Update: Bjorn  引き続きBjornからITU-Tの話です。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhPurmEZjpTdY9UJw_1jvEgfrbcs4oV-CGE1Wc7h8f3cVHpjOBFFqcmjmzO-sjw-3dPnZtDo9Othrh9LW64ZCqg5tKw7_4X94YUrHNds5mydrV0gCqdZCb-MSIDve6Qht0tmMGkUz16_5KWmbmI6NAbETILKO8VndtfqoLgpP0OEY1s_3ixvga1ULj2mc37"></a> <br>ISOのPASもそうですが、どうしてもOIDFはフォーラム標準の団体なので政府機関などデジュールを要求する人たちへの対応を考えるとISOやITU-Tとの連携が重要になってきます。  こちらも継続して連携していきますよ、という話でした。  <br>  SIDI Hub: Elizabeth  続いてElizabethからSIDI Hubの話です。今年も頑張りますとのこと。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhIF6cOAzbvlaa8nrLnxJ0N8qxuAVKJUKhZjbdP-7k58aszU_ufZzIvwbFXoSXCd1whPuZbajasRmeK4Z8F_1qb7JnCGVaPGf02or-9CxESfVYl-r1xyKrqGg_JB1jElqY8YYUW_EvQvbJ2JnAXRhKKuITmVciNPA984PImDd8kS5X5wvCf-QG2AHjjoTnh"></a> <br> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEjjowKGiUIq2AA4VNZZHhV4o4eDljDalGYrQUveW7tejUy1IOcrk9WhGpycCBBkfm5xr5k-JovFwFMC5wPVkUunrF4pmEQgTn3bUcQYYDlMT0SakBrE3sMAaP9Z8_t0NxYk8ieELUG-XVqQKe4-9t8FjHpvkh7zxJg7wqteCSkcsoL4BV7pvZOG2B1AJOG4"></a> <br>2024年は多くの参加者たちに支えられてグローバルでイベントをやってきました。（東京を含む）   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhEsbHPbZ2hRVpgdjFIb_nWLaprIU4Eb-xSpaP62P7dhigwctKMIc3EHaE7O3iDUk712CPxgI83mFI5JTQVRxinrv5mr8EyKzH80wtnf1LGgrw71UNNKSodftBHG88Av436nVY4QZWy25I5fMlcsKJyu5anHkjOFdFDBx_Ofx0yO7f6hUniEODoKpdIQgUV"></a>  <br> 2025年の１回目はID4Africaに合わせてケープタウンで実施ですかね。  6月末にノルウェーで開催される国連のIGFへのセッション提案もしているので通ればそちらもいい機会になる、という話です。  <br>  FAPI: Joseph  次はJosephからFAPIについてです。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEiGTEBxwD_U6hD_j8SuOaJ1iOOB0RCl9hyc9SpC_yb2aGPh7JUqViH9Jg6yIBzhluqnqJvzKR4K_CNrw8HTAEA8dbezOzBz7fS0ENytW6NHgxn9ANxzIruxarv9r5R2v1DJi-SE1asYvBUVwUDOEv9gjuJdQuV4bOyYZ1mcUj0FeIWduwqvWKRxbh1iu7Fm"></a> <br> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEiYK9megt23CRmVWqV4DPYadqjuFArgQ8l0V9PCDM7JRqvy89c1g-VrbEOJumoE_O7QH85awnkWcZKbBSKJSKlvnh3MnW34nw7NKRrQDa9inYm33D0IWS_G4hn5_c9qK1PP3LTiKd4fe65_WSb0BZcY9RiKkwJzXi-su8WkSVdrKv1NufyKFRCziKFqxYwg"></a> <br>仕様もFinalizeしましたし、エコシステムの拡大がトピックスでしょうね。  UKのSelectIDはIDAもサポートしていますし、良いユースケースだと思います。  ここに書いてないところだとFDXとも連携して進めてるっていう補足もありました。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEiDmnj0o4rFteXf-KHnXQypEOlU-yIQD_DeE82FzADHjMcHop1KBkIxaYJNsANfXqUWyaP6waCVsgZBgQ7xAmT-4H-yWuXNg4J4mHXlUvTGLo7sbfxcZmH-PNZP2T0gCt-WMj7ItjFCX0l3lb_BeBdwu1bdfDXBBSOd6iYYTuFtMKeBI04dEpvdh51X4XrI"></a> <br>FAPI2.0がFinalということで、それまでのImplementers Draft2からの更新部分についてまとめてブログで公開しています。エコシステムがそれなりに広がっているのでID2で実装していたところも多かったんでしょうね。  <br>  Digital Credentials Protocols: Joseph  引き続きJosephからDCPです。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhIhFEmXduRzrk4Ox97KHD3kXIvi2woJgzMUha3ZcqJNph8wwNIgeF8sBXCqvGuz7Nm5mL9FoqFcAh5VFcjeSu3KamQ0rtEfn5vo7-2amxPcv_n0a48b5xamH6_OY72dKSFfaRYU3a5_7SGxopm5w2Pkc19_zU9s06BqtfRWG-iis52LqV4fHYOrw0vrKC-"></a> <br> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhxwJkT0chqcm8Yj-8S9JdpBm1hljFPNS17k0qtJ_lpCRWldyH6qPCliASMc4INl7WcUad_oM181usDyh8itbNtPYG3y0PklzSFTJix4eZB4udXiOPcjkO4Jall9UIll9AWXJvAIAnIhwVLCj6MG7bvBOnVU9_Ltc2zlQ96xoqnxVgFMcVi64hXcarlwKBB"></a> <br>いよいよ仕様の最終化が秒読みになってきていますので、重要な変更などについてまとめが発表されてきています。特に先日のOID4VPのID3HAIPのID1（小岩井さんご指摘ありがとうございます。VPのID3ではまだ両方残ってました）ではPresentation Exchangeが廃止されてDCQLのみのサポートになったので、VerifierやWalletの実装者は対応が必要ですね。  また、ID3が出ていますがmdocを使う場合はdraft24を使うように、という注意喚起もありました。  うーん、まだ結構色々ありそうですがFinalizeは間に合うのだろうか・・・  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhb8Sf6fWUTg3CtSQU-SwH6s7GFyA74Sxv_fR-N6kUCzhkLdaXGDr-TOctbktrI0sSvv4caQnqK1UcBR2tUMn8EVoQhqydPvXupRP4bBr9zdafA7NxudzmOCvzHKQE0KS-C7CqNqvHcHIq5kARrVlIOquwffZDDqxceE23FZoCmvNbFszyEkOTA0dJd3PtY"></a> <br><br>  といっても主に対応しなきゃいけないのはこのくらい、ということです。  ゴールは見えてきているようですね。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhxN7Ix8jGqYMo1gX5WA19yHCI_GphZdj4kzNVTr2DBVigx3B58EB_H19OTUhmCXqTGKDD9qJ37bz8EnrfvednEQ_ek9qN0D5g9UwqAzPZmVgml56t8T1bucLmiMP-J9e8c4x_GcH4f_BpsRm-geKdpo_3_JAxW6IPsTGbOEypbUGSyMXTCfkWGqJpRDwrW"></a> <br><br>コンフォーマンステストも対応して開発が進められていますし、Interopイベントも進んでいます。  <br>  OI4VC Initial Interop Results: Juliana, Gail  ということでOID4VC関係のプロトコルのInteropイベントの状況についてJulianaとGailからUpdateがありました。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEjQiuuGDJ7bJ1Ahtfu2gUm5sVfsSIaphn6yAHz1HonTqZk8E3Cn9Nt8NUj7K4A9w8RhZydJKMok3ejyyhkfdJl0WB5lx8o7nIbNnidobieG4IgQ512yrb6C2kEDSZS1niMYW_M52jrMAC8lrR8T0JlMSWjCexwySATIKJPJ4wAdSdw_bpkx7nq8BbLQTMvZ"></a> <br> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhFKOICbetAHxV7xJJXtih44J7l95APUpSrWoJvAUy_Oqkzp6xjSSCtPPcT9wbR7YPsowslrAGvZ19j9J0PFIDj0VY15pca45N9PTSRN5ieqtLgd8IIbVLBSeivpJ-cFufeDefioCbKiA0W4VymTmPi9prQzZbR6W6cDOC-0PTahQIuC_ByQgrlYjRJkTmf"></a> <br>NIST NCCoE（National Cybersecurity Center of Excellence）のInteropイベントの結果が発表されました。まだ数は少ないですがちゃんとテストしてますね。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEiJuOVyJeYKRCT0Fjky0Zb6J-evXfySKtmyu2kBDGIKgy_UxFhMgHsZl30WonP6L9a4Tv1OyLIXzh76-mESK4nnl3lOZB6wwXQpbr4-lg9AYC1zCEI1pIPmDQyYjTAox2VEWsnH3QZ8cn1kx42oCinYqjHoCMaUnmZFjJZa_FIvuk7qTcZ4RfP6eDk4MMle"></a> <br>今月・来月を含め直近でもInteropイベントが予定されています。5月のEICの前にもイベントがあるので、楽しみにしています。（私も参加予定です）  <br>  Conformance &amp; Certification: Joseph  それぞれの仕様のところでも触れましたが、コンフォーマンステストと認定プログラムに関してJosephから改めてまとめです。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEjeM_p1c0oSLNDyxwaGzpL2xT_s4dSG10hy66-65FDtqUla6HY73hysoa2EV_w6MwWg2MYRu_1FXBhU8LHmNwMKhVmGk3Xg_fLmnt5P2fgzL-HlpjLNMeTAIqNBF-Nj1Z1zH2TVZ2gI9BG5oTmlI6euaUb00z1zs3KAiNC-urIeDnk7Odxgzmf22quR_0sU"></a> <br> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgs3S3UK4Zeue5wvEec1wTpqgWowQ9k0R7z2jNskA4uPnWiyTRejvu4AfSVxJytmLy9VX7ZKh10CP7FTyNuVybcwng4z9QYFzucg6aZsh5t4OMeos6UVjhVKRgDSu9YyL-y8NWpndQetTYV4Q0uZlZl3sqLvZ2OGLFo7kEcW4aNjm9WXD4V0sgvDe57Y0-8"></a> <br><br>  FAPI、Federation、IDA、SSF、OID4VCI/VPと色々と並行して開発が進んでいます。  相互運用に向けて非常に重要な取り組みですね。  <br>  <br>  <br>  ということでIIW前日のOIDF Workshopをクィックに振り返ってみました。  明日からはIIW本番です。  <br>  <br>  <br> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://blogger.googleusercontent.com/img/a/AVvXsEgH6XOyxjKPftHmId7Ed0DiUlC_l_dGmDY7KdaB9VA98sU6j5gLoNi1twetcbS1Er5M64Xup8LfVoDGaDD2_CTlgoVOWj1QxBY7SnrGMZUILUJAXcc_XBhJYzzz-kKCfIKSNKb6jxGz_gzMS9wlT_HniSQXr47VfZh0gawpcm2icftXc2bkMkevkG5AHrw_=w640-h480'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   01:06 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 07. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/first-person-identity'>First Person Identity</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  I'm sitting in VRM day listening to Drummond Reed talk about his First Person Project.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>I'm sitting in VRM day listening to Drummond Reed talk about his First Person Project. If you know Drummond or me, you know we've been interested in this idea since 2011 when we were both working on something called a <a href="https://windley.com/archives/2011/11/the_second_pc_revolution_personal_clouds.shtml">personal cloud</a>. I've written about this idea extensively on this blog, arguing that people have no place to stand on the internet and thus our <a href="https://windley.com/archives/2020/08/authentic_digital_relationships.shtml">digital relationships are, as a result, anemic</a>.</p><p>As I listened to Drummond, I realized that "first person" is a more powerful descriptor than "self-sovereign". First person describes the idea in words that most people understand and doesn't have the baggage of sovereignty. First person is "I," "me," "my," and "mine." First person describes precisely the kind of thinking that will allow people to create their own oneline relationships without an intermediating administrator like a social network. Drummond's vision is, as you'd expect from someone who's been working on this for 15 years or more, much more extensive than a simple change to branding, but still, I think it's powerful.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   23:56 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306682786/'>Mini Summit 3: APT Extension Packaging</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Join us at PostgresSQL Extension Mini Summit #3 this week, where PostgreSQL
Debian packaging maintainer Christoph Berg will takes on a tour of APT
extension packaging.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	<a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306682786/">
			
		</a>
	

         
<p>This Wednesday, April 9 at noon America/New_York (16:00 UTC) for <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306682786/" title="Postgres Extensions Ecosystem Mini-Summit #3">Extension
Mini Summit #3</a>, where <a href="https://www.df7cb.de">Christoph Berg</a> will take us on a tour of the
<a href="https://wiki.postgresql.org/wiki/Apt" title="PostgreSQL Wiki: Apt">PostgreSQL Global Development Group’s APT repository</a> with a focus on
packaging extensions. For those of us foolish enough to consider building our
own binary packaging systems for extensions, this will be an essential
session. For everyone else, come be amazed by the sheer volume of extensions
readily available from the repository. Browse on over to the <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/" title="Postgres Extension Ecosystem Mini-Summit on Meetup">Meetup</a> to
register for this live video conference.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
                 <a href="https://justatheory.com/tags/debian/">Debian</a> 
                 <a href="https://justatheory.com/tags/apt/">APT</a> 
                 <a href="https://justatheory.com/tags/christoph-berg/">Christoph Berg</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/shared/extension-ecosystem-summit/apt-packaging-card.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   18:33 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 05. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://benwerd.medium.com/doctor-who-is-the-best-show-ever-made-heres-why-f2619aed7a46?source=rss-3b16402f5b9d------2'>Doctor Who is the best show ever made. Here’s why.</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

      62 years of trying to be nice, always being kind.  Continue reading on Medium »  
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/0*EhJNdkI68I0DmqEe.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   <p><a href="https://benwerd.medium.com/doctor-who-is-the-best-show-ever-made-heres-why-f2619aed7a46?source=rss-3b16402f5b9d------2"></a></p><p>62 years of trying to be nice, always being kind.</p><p><a href="https://benwerd.medium.com/doctor-who-is-the-best-show-ever-made-heres-why-f2619aed7a46?source=rss-3b16402f5b9d------2">Continue reading on Medium »</a></p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/0*EhJNdkI68I0DmqEe.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:27 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 03. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://aaronparecki.com'>Aaron Parecki</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://aaronparecki.com/2025/04/03/15/oauth-for-model-context-protocol'>Let's fix OAuth in MCP</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Update: The changes described in this blog post have been incorporated into the 2025-06-18 version of the MCP spec!
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Update: The changes described in this blog post have been incorporated into the 2025-06-18 version of the MCP spec! 

<p>Let's not overthink auth in MCP.</p>

<p>Yes, the MCP server is going to need its own auth server. But it's not as bad as it sounds. Let me explain.</p>

<p>First let's get a few pieces of terminology straight.</p>

<p>The confusion that's happening in the discussions I've seen so far is because the spec and diagrams show that the MCP server itself is handing authorization. That's not necessary.</p>

<p></p>

<p>In OAuth, we talk about the "authorization server" and "resource server" as distinct roles. I like to think of the authorization server as the "token factory", that's the thing that makes the access tokens. The resource server (usually an API) needs to be able to validate the tokens created by the authorization server.</p>

<p></p>

<p>It's possible to build a single server that is both a resource server and authorization server, and in fact many OAuth systems are built that way, especially large consumer services.</p>

<p></p>

<p>But nothing about the spec requires that the two roles are combined, it's also possible to run these as two totally unrelated services.</p>

<p>This flexibility that's been baked into OAuth for over a decade is what has led to the rapid adoption, as well the proliferation of open source and commercial products that provide an OAuth authorization server as a service.</p>

<p>So how does this relate to MCP?</p>

<p>I can annotate the flow from the <a href="https://spec.modelcontextprotocol.io/">Model Context Protocol</a> spec to show the parts where the client talks to the MCP Resource Server separately from where the client talks to the MCP Authorization Server.</p>

<p><a></a></p>

<p>Here is the updated sequence diagram showing communication with each role separately.</p>

<p><a></a></p>

<p>Why is it important to call out this change?</p>

<p>I've seen a few conversations in <a href="https://github.com/modelcontextprotocol/specification/issues/205">various</a> <a href="https://github.com/modelcontextprotocol/specification/issues/195">places</a> about how requiring the MCP Server to be both an authorization server and resource server is too much of a burden. But actually, very little needs to change about the spec to enable this separation of concerns that OAuth already provides.</p>

<p>I've also seen various suggestions of other ways to separate the authorization server from the MCP server, like delegating to an enterprise IdP and having the MCP server validate access tokens issued by the IdP. These other options also conflate the OAuth roles in an awkward way and would result in some undesirable properties or relationships between the various parties involved.</p>

<p>So what needs to change in the MCP spec to enable this?</p>

 Discovery 

<p>The main thing currently forcing the MCP Server to be both the authorization server and resource server is how the client does discovery.</p>

<p>One design goal of MCP is to enable a client to bootstrap everything it needs based on only the server URL provided. I think this is a great design goal, and luckily is something that can be achieved even when separating the roles in the way I've described.</p>

<p>The MCP spec currently says that clients are expected to fetch the <a href="https://oauth.net/2/authorization-server-metadata/">OAuth Server Metadata</a> (RFC8414) file from the MCP Server base URL, resulting in a URL such as:</p>

<p><code>https://example.com/.well-known/oauth-authorization-server</code></p>

<p>This ends up meaning the MCP Resource Server must also be an Authorization Server, which leads to the complications the community has encountered so far. The good news is there is an OAuth spec we can apply here instead: <a href="https://datatracker.ietf.org/doc/draft-ietf-oauth-resource-metadata/">Protected Resource Metadata</a>.</p>

 Protected Resource Metadata 

<p>The <a href="https://datatracker.ietf.org/doc/draft-ietf-oauth-resource-metadata/">Protected Resource Metadata</a> spec is used by a Resource Server to advertise metadata about itself, including which Authorization Server can be used with it. This spec is both new and old. It was started in 2016, but was never adopted by the OAuth working group until 2023, after I had presented at an IETF meeting about the need for clients to be able to bootstrap OAuth flows given an OAuth resource server. The spec is now awaiting publication as an RFC, and should get its RFC number in a couple months. (Update: This became <a href="https://datatracker.ietf.org/doc/html/rfc9728">RFC 9728</a> on April 23, 2025!)</p>

<p>Applying this to the MCP server would result in a sequence like the following:</p>

<p><a></a></p>

 
 The MCP Client fetches the Resource Server Metadata file by appending <code>/.well-known/oauth-protected-resource</code> to the MCP Server base URL. 
 The MCP Client finds the <code>authorization_servers</code> property in the JSON response, and builds the Authorization Server Metadata URL by appending <code>/.well-known/oauth-authorization-server</code> 
 The MCP Client fetches the Authorization Server Metadata to find the endpoints it needs for the OAuth flow, the authorization endpoint and token endpoint 
 The MCP Client initiates an OAuth flow and continues as normal 
 

<p><br>
Note: The Protected Resource Metadata spec also supports the Resource Server returning <code>WWW-Authenticate</code> with a link to the resource metadata URL if you want to avoid the requirement that MCP Servers host their metadata URLs at the <code>.well-known</code> endpoint, it just requires an extra HTTP request to support this.</p>

 Access Token Validation 

<p>Two things to keep in mind about how the MCP Server validates access tokens with this new separation of concerns.</p>

<p>If you do build the MCP Authorization Server and Resource Server as part of the same system, you don't need to do anything special to validate the access tokens the Authorization Server issues. You probably already have some sort of infrastructure in place for your normal API to validate tokens issued by your Authorization Server, so nothing changes there.</p>

<p>If you are using an external Authorization Server, whether that's an open source product or a commercial hosted service, that product will have its own docs for how you can validate the tokens it creates. There's a good chance it already supports the standardized <a href="https://oauth.net/2/jwt-access-tokens/">JWT Access Tokens</a> described in RFC 9068, in which case you can use off-the-shelf <a href="https://oauth.net/code/">JWT validation middleware</a> for common frameworks.</p>

<p>In either case, the critical design goal here is that the MCP Authorization Server issues access tokens that only ever need to be validated by the MCP Resource Server. This is in line with the security recommendations in <a href="https://www.rfc-editor.org/rfc/rfc9700.html#section-2.3">Section 2.3 of RFC 9700</a>, in particular that "access tokens SHOULD be audience-restricted to a specific resource server". In other words, it would be a bad idea for the MCP Client to be issued an access token that works with both the MCP Resource Server and the service's REST API.</p>

 Why Require the MCP Server to have an Authorization Server in the first place? 

<p>Another argument I've seen is that MCP Server developers shouldn't have to build any OAuth infrastructure at all, instead they should be able to delegate all the OAuth bits to an external service.</p>

<p>In principle, I agree. Getting API access and authorization right is tricky, that's why there are entire companies dedicated to solving the problem.</p>

<p>The architecture laid out above enables this exact separation of concerns. The difference between this architecture and some of the other proposals I've seen is that this cleanly separates the security boundaries so that there are minimal dependencies among the parties involved.</p>

<p>But, one thing I haven't seen mentioned in the discussions is that there actually is no requirement than an OAuth Authorization Server provide any UI itself.</p>

 An Authorization Server with no UI? 

<p>While it is desirable from a security perspective that the MCP Resource Server has a corresponding Authorization Server that issues access tokens for it, that Authorization Server doesn't actually need to have any UI or even any concept of user login or accounts. You can actually build an Authorization Server that delegates all user account management to an external service. You can see an example of this in <a href="https://developer.paypal.com/limited-release/agents/">PayPal's MCP server</a> they recently launched.</p>

<p>PayPal's traditional API already supports OAuth, the authorization and token endpoints are:</p>

 
 <code>https://www.paypal.com/signin/authorize</code> 
 <code>https://api-m.paypal.com/v1/oauth2/token</code> 
 

<p>When PayPal built their MCP server, they launched it at <code>https://mcp.paypal.com</code>. If you fetch the metadata for the MCP Server, you'll find the two OAuth endpoints for the MCP Authorization Server:</p>

 
 <code>https://mcp.paypal.com/authorize</code> 
 <code>https://mcp.paypal.com/token</code> 
 

<p>When the MCP Client redirects the user to the authorization endpoint, the MCP server itself doesn't provide any UI. Instead, it immediately redirects the user to the real PayPal authorization endpoint which then prompts the user to log in and authorize the client.</p>

<p></p>

<p>This points to yet another benefit of architecting the MCP Authorization Server and Resource Server this way. It enables implementers to delegate the actual user management to their existing OAuth server with no changes needed to the MCP Client. The MCP Client isn't even aware that this extra redirect step was inserted in the middle. As far as the MCP Client is concerned, it has been talking to only the MCP Authorization Server. It just so happens that the MCP Authorization Server has sent the user elsewhere to actually log in.</p>

 Dynamic Client Registration 

<p>There's one more point I want to make about why having a dedicated MCP Authorization Server is helpful architecturally.</p>

<p>The MCP spec strongly recommends that MCP Servers (authorization servers) support <a href="https://oauth.net/2/dynamic-client-registration/">Dynamic Client Registration</a>. If MCP is successful, there will be a large number of MCP Clients talking to a large number of MCP Servers, and the user is the one deciding which combinations of clients and servers to use. This means it is not scalable to require that every MCP Client developer register their client with every MCP Server.</p>

<p>This is similar to the idea of using an email client with the user's chosen email server. Obviously Mozilla can't register Thunderbird with every email server out there. Instead, there needs to be a way to dynamically establish a client's identity with the OAuth server at runtime. Dynamic Client Registration is one option for how to do that.</p>

<p>The problem is most commercial APIs are not going to enable Dynamic Client Registration on their production servers. For example, in order to get client credentials to use the Google APIs, you need to register as a developer and then register an OAuth client after logging in. Dynamic Client Registration would allow a client to register itself without the link to the developer's account. That would mean there is no paper trail for who the client was developed by. The Dynamic Client Registration endpoint can't require authentication by definition, so is a public endpoint that can create clients, which as you can imagine opens up some potential security issues.</p>

<p>I do, however, think it would be reasonable to expect production services to enable Dynamic Client Registration only on the MCP's Authorization Server. This way the dynamically-registered clients wouldn't be able to use the regular REST API, but would only be able to interact with the MCP API.</p>

<p>Mastodon and BlueSky also have a similar problem of needing clients to show up at arbitrary authorization servers without prior coordination between the client developer and authorization server operator. I call this the "<a href="https://aaronparecki.com/2018/07/07/7/oauth-for-the-open-web">OAuth for the Open Web</a>" problem. Mastodon used Dynamic Client Registration as their solution, and has since documented some of the issues that this creates, linked <a href="https://github.com/mastodon/mastodon/issues/21991">here</a> and <a href="https://github.com/mastodon/mastodon/issues/27740">here</a>.</p>

<p>BlueSky decided to take a different approach and instead uses <a href="https://datatracker.ietf.org/doc/draft-parecki-oauth-client-id-metadata-document/">an https URL as a client identifier</a>, bypassing the need for a client registration step entirely. This has the added bonus of having at least some level of confidence of the client identity because the client identity is hosted at a domain. It would be a perfectly viable approach to use this method for MCP as well. There is a discussion on that within MCP <a href="https://github.com/modelcontextprotocol/specification/discussions/202">here</a>. This is an ongoing topic within the OAuth working group, I have a couple of drafts in progress to formalize this pattern, <a href="https://datatracker.ietf.org/doc/draft-parecki-oauth-client-id-metadata-document/">Client ID Metadata Document</a> and <a href="https://datatracker.ietf.org/doc/draft-parecki-oauth-client-id-scheme/">Client ID Scheme</a>.</p>

 Enterprise IdP Integration 

<p>Lastly, I want to touch on the idea of enabling users to log in to MCP Servers with their enterprise IdP.</p>

<p>When an enterprise company purchases software, they expect to be able to tie it in to their single-sign-on solution. For example, when I log in to work Slack, I enter my work email and Slack redirects me to my work IdP where I log in. This way employees don't need to have passwords with every app they use in the enterprise, they can log in to everything with the same enterprise account, and all the apps can be protected with multi-factor authentication through the IdP. This also gives the company control over which users can access which apps, as well as a way to revoke a user's access at any time.</p>

<p>So how does this relate to MCP?</p>

<p>Well, plenty of people are already trying to figure out how to let their employees safely use AI tools within the enterprise. So we need a way to let employees use their enterprise IdP to log in and authorize MCP Clients to access MCP Servers.</p>

<p>If you're building an MCP Server in front of an existing application that already supports enterprise Single Sign-On, then you don't need to do anything differently in the MCP Client or Server and you already have support for this. When the MCP Client redirects to the MCP Authorization Server, the MCP Authorization Server redirects to the main Authorization Server, which would then prompt the user for their company email/domain and redirect to the enterprise IdP to log in.</p>

<p>This brings me to yet another thing I've been seeing conflated in the discussions: user login and user authorization.</p>

<p>OAuth is an authorization delegation protocol. OAuth doesn't actually say anything about how users authenticate at the OAuth server, it only talks about how the user can authorize access to an application. This is actually a really great thing, because it means we can get super creative with how users authenticate.</p>

<p></p>

<p>Remember the yellow box "User logs in and authorizes" from the original sequence diagram? These are actually two totally distinct steps. The OAuth authorization server is responsible for getting the user to log in somehow, but there's no requirement that how the user logs in is with a username/password. This is where we can insert a single-sign-on flow to an enterprise IdP, or really anything you can imagine.</p>

<p>So think of this as two separate boxes: "user logs in", and "user authorizes". Then, we can replace the "user logs in" box with an entirely new OpenID Connect flow out to the enterprise IdP to log the user in, and after they are logged in they can authorize the client.</p>

<p></p>

<p>I'll spare you the complete expanded sequence diagram, since it looks a lot more complicated than it actually is. But I again want to stress that this is nothing new, this is already how things are commonly done today.</p>

<p>This all just becomes cleaner to understand when you separate the MCP Authorization Server from the MCP Resource Server.</p>

<p>We can push all the complexity of user login, token minting, and more onto the MCP Authorization Server, keeping the MCP Resource Server free to do the much simpler task of validating access tokens and serving resources.</p>

 Future Improvements of Enterprise IdP Integration 

<p>There are two things I want to call out about how enterprise IdP integration could be improved. Both of these are entire topics on their own, so I will only touch on the problems and link out to other places where work is happening to solve them.</p>

<p>There are two points of friction with the current state of enterprise login for SaaS apps.</p>

 
 IdP discovery 
 User consent 
 

 IdP Discovery 

<p>When a user logs in to a SaaS app, they need to tell the app how to find their enterprise IdP. This is commonly done by either asking the user to enter their work email, or asking the user to enter their tenant URL at the service.</p>

<p></p>

<p>Neither of these is really a great user experience. It would be a lot better if the browser already knew which enterprise IdP the user should be sent to. This is one of my goals with the work happening in <a href="https://github.com/w3c-fedid/FedCM">FedCM</a>. With this new browser API, the browser can mediate the login, telling the SaaS app which enterprise IdP to use automatically only needing the user to click their account icon rather than type anything in.</p>

 User Consent 

<p>Another point of friction in the enterprise happens when a user starts connecting multiple applications to each other within the company. For example, if you drop in a Google Docs link into Slack, Slack will prompt you to connect your Google account to preview the link. Multiply this by N number of applications that can preview links, and M number of applications you might drop links to, and you end up sending the user through a huge number of OAuth consent flows.</p>

<p>The problem is only made worse with the explosion of AI tools. Every AI tool will need access to data in every other application in the enterprise. That is a lot of OAuth consent flows for the user to manage. Plus, the user shouldn't really be the one granting consent for Slack to access the company Google Docs account anyway. That consent should ideally be managed by the enterprise IT admin.</p>

<p>What we actually need is a way to enable the IT admin to grant consent for apps to talk to each other company-wide, removing the need for users to be sent through an OAuth flow at all.</p>

<p>This is the basis of another OAuth spec I've been working on, the <a href="https://datatracker.ietf.org/doc/draft-parecki-oauth-identity-assertion-authz-grant/">Identity Assertion Authorization Grant</a>.</p>

<p>The same problem applies to MCP Servers, and with the separation of concerns laid out above, it becomes straightforward to add this extension to move the consent to the enterprise and streamline the user experience.</p>

<p>Get in touch!</p>

<p>If these sound like interesting problems, please get in touch! You can find me on <a href="https://linkedin.com/in/aaronparecki">LinkedIn</a> or reach me via email at <code>aaron@parecki.com</code>.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://aaronparecki.com/2025/04/03/15/oauth-roles.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   23:39 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 02. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.windley.com/'>Phil Windleys Technometria</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.technometria.com/p/long-live-rss'>Long Live RSS</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Like many, I've grown increasingly disinterested in social media over the last year.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 <a href="https://substackcdn.com/image/fetch/$s_!ZaH4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36b7bf0c-ded1-4a6a-b7b3-a799256563a5_1792x1024.heic">          </a> <p>Like many, I've grown increasingly disinterested in social media over the last year. I still have accounts on X, Substack, Facebook, and others. But I find that there's less and less there that I actually care about. I find myself wading through lots of crap to find the things I want to see. Still, I do like to browse through ideas while waiting in line, at the doctor's office, and so on.</p><p>This post from Cory Doctorow entitled <a href="https://pluralistic.net/2024/10/16/keep-it-really-simple-stupid/#read-receipts-are-you-kidding-me-seriously-fuck-that-noise">You should be using an RSS reader</a> caught my eye a while ago. I used to use an RSS reader all the time. I've tried to get back into it a time or two, but it didn't stick. Inspired by Cory to try one more time, I downloaded the RSS reader he uses (<a href="https://newsblur.com/">Newsblur</a>) and gave it another shot. This time I coupled it with moving the RSS reader app on my phone to a more prominent position so when I'm looking for something to browse, I see it first.</p><p>First I found RSS feeds for some of my friends and publications I like to read. I also discovered that <a href="https://support.substack.com/hc/en-us/articles/360038239391-Is-there-an-RSS-feed-for-my-publication">Substack publications have RSS feeds</a>. Many publications don't make finding RSS easy, but it's almost always there. There are a few authors I like on The New Stack and I found that if you enter <code>/rss</code> after the URL for an author or category page, you get RSS for that author or category. For example, here's the <a href="https://thenewstack.io/author/jon-udell/rss">RSS page for Jon Udell.</a> Cool. </p><p>After adding RSS feeds for things I like (most online publications have one), I started reading things on Newsblur instead of opening up X. Most newsreaders don't have an algorithm. If they do, it is not usually the default setting and can be turned off. If you subscribe to too many feeds, turn it on and let it prioritize your reading. But regardless, you control the experience. Just like you should.</p><p>At the end of his post, Cory writes:</p><blockquote><p>Using RSS to follow the stuff that matters to you will have an immediate, profoundly beneficial impact on your own digital life—and it will appreciably, irreversibly nudge the whole internet towards a better state.</p><p>From <a href="https://pluralistic.net/2024/10/16/keep-it-really-simple-stupid/">You should be using an RSS reader</a><br>Referenced 2025-04-02T13:02:11-0400</p></blockquote><p>Cory's right, using an RSS reader will make your digital life better. I'm wasting less time scrolling past stuff I don't care about and more time reading things I enjoy. That's a win.</p>   <p>Photo Credit: <a>Robot Reading on Phone</a> from DALL-E (public domain)</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://substackcdn.com/image/fetch/$s_!ZaH4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36b7bf0c-ded1-4a6a-b7b3-a799256563a5_1792x1024.heic'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:00 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.webistemology.com/'>Webistemology - John Wunderlich</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.webistemology.com/privacy-freedom/'>Privacy ≠ Freedom (but it should)</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  The data is in. Privacy is not correlated to Freedom. It is time to rethink how we write privacy laws.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>In 1967, Alan Westin published Privacy and Freedom in response to growing concerns in the 1960s about computer databases and surveillance. Westin argued that encroachments on privacy were also encroachments on 'American liberty.' When he stated that "Privacy is the claim of individuals, groups, or institutions to determine for themselves when, how, and to what extent information about them is communicated to others"<a>[1]</a>, he argued that privacy (liberty) was not possible without individuals having both the autonomy to both make these claims and to have them respected.</p><p>In the 60s, there was a growing concern about technology encroaching on privacy. He argued, "The real need is to move from public awareness of the problem to a sensitive discussion of what can be done to protect privacy in an age when so many forces of science, technology, environment, and society press against it from all sides."<a>[2]</a></p><p>The US Privacy Act (1974) was the first legislative response, followed by the OECD privacy guidelines (1980) and the Council of Europe Data Protection Convention in 1981. Data protection or privacy laws have become the norm in the 50 years since the US Privacy Act. However, the concerns expressed then are just as valid today, whether from a left view of <a href="https://monthlyreview.org/2014/07/01/surveillance-capitalism/?ref=webistemology.com">Surveillance Capitalism</a> or a business school description of an <a href="https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism?ref=webistemology.com">Age of Surveillance Capitalism</a>.&nbsp;Despite the proliferation of privacy laws, privacy is as much under threat today as it was then.</p><p>Returning to "Privacy and Freedom", does the failure of privacy mean a failure of freedom? Is the likelihood of a country being free, partially free, or not free uncorrelated with whether or not the government has data protection or privacy laws? There are more than 200 countries in the world, 150 of which have some form of privacy or data protection legislation<a>[3]</a>. Freedom House's Annual <a href="https://freedomhouse.org/report/freedom-world?ref=webistemology.com">Freedom in the World</a> report categorises countries as "Free", "Partially Free", or "Not Free" based on a set of 25 indicators<a>[4]</a>. When you compare the percentages of countries' freedom ratings, the impact of having privacy or data protection legislation on whether or not a country is free is minimal.</p>




Total Countries
208
100 %

DP Countries
150
100%




Free
86
41.3%

Free
69
46.0%


Partially Free
55
26.4%

Partially Free
38
25.3%


Not Free
67
32.2%

Not Free
43
28.7%




<p>This suggests that privacy itself is not related to freedom (or liberty) OR that there is a problem with the way that privacy laws have been written or implemented. The proposition that privacy should be concomitant with individual freedom and that the ability of groups to organise seems almost axiomatically true. And recent writings suggest that, as currently architected, privacy laws can be helpful for authoritarian governments.<a>[5]</a>. This echoes critiques from privacy scholars such as Woodrow Hartzog<a>[6]</a> or Ignacio Cofone<a>[7]</a>. In a recent article, Daniel Solove says, "To adequately regulate government surveillance, it is essential to also regulate surveillance capitalism. Government surveillance and surveillance capitalism are two sides of the same coin. It is impossible to protect privacy from authoritarianism without addressing consumer privacy."<a>[8]</a></p><p>Without trying to be hyperbolic, the current trajectory for privacy laws and regulations is leading down a path of digital alienation. It is time for privacy laws and practices to support digital autonomy.</p> <p>Footnotes</p>   Westin, Alan F.. Privacy and Freedom (p. 5). <a>↩︎</a>  Westin, Alan F., Privacy and Freedom (pp. 1-2). <a>↩︎</a>  See UNCTAD <a href="https://unctad.org/page/data-protection-and-privacy-legislation-worldwide?ref=webistemology.com">Data Protection and Privacy Legislation Worldwide</a> <a>↩︎</a>  See the Methodology Scoring Process at <a href="https://freedomhouse.org/reports/freedom-world/freedom-world-research-methodology?ref=webistemology.com">https://freedomhouse.org/reports/freedom-world/freedom-world-research-methodology</a> <a>↩︎</a>  Jia, Mark (2024) "Authoritarian Privacy," University of Chicago Law Review: Vol. 91: Iss. 3, Article 2. Available at: <a href="https://chicagounbound.uchicago.edu/uclrev/vol91/iss3/2?ref=webistemology.com">https://chicagounbound.uchicago.edu/uclrev/vol91/iss3/2</a> <a>↩︎</a>  Privacy's Blueprint: The Battle to Control the Design of New Technologies <a href="https://www.hup.harvard.edu/books/9780674976009?ref=webistemology.com">https://www.hup.harvard.edu/books/9780674976009</a> <a>↩︎</a>  The Privacy Fallacy: Harm and Power in the Information Economy <a href="https://www.privacyfallacy.com/?ref=webistemology.com">https://www.privacyfallacy.com/</a> <a>↩︎</a>  Solove, Daniel J., Privacy in Authoritarian Times: Surveillance Capitalism and Government Surveillance (January 19, 2025). Available at SSRN: <a href="https://ssrn.com/abstract=5103271?ref=webistemology.com">https://ssrn.com/abstract=5103271</a> or <a href="https://dx.doi.org/10.2139/ssrn.5103271?ref=webistemology.com">http://dx.doi.org/10.2139/ssrn.5103271</a> <a>↩︎</a>  
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://images.unsplash.com/photo-1576568699714-a3f4950805d5?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDM0fHxmcmVlZG9tJTIwcHJpdmFjeXxlbnwwfHx8fDE3NDMzNTM3NjB8MA&ixlib=rb-4.0.3&q=80&w=2000'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   16:32 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 01. April 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2025/04/mini-summit-two/'>2025 Postgres Extensions Mini Summit Two</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A transcript of from the second PostgreSQL Extension Mini Summit,
“Implementing an Extension Search Path”, by Peter Eisentraut.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	<a href="https://www.youtube.com/watch?v=xtnt06zhONk">
			
		</a>
	

         
<p>Last Wednesday, March 26, we hosted the second of five virtual <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/" title="Postgres Extension Ecosystem Mini-Summit on Meetup">Extension
Mini-Summits</a> in the lead up to <a href="https://www.pgevents.ca/events/pgconfdev2025/schedule/session/241/" title="PGConf.dev: Extensions Ecosystem Summit">the big one</a> at the
Postgres Development Conference (<a href="https://2025.pgconf.dev" title="PostgreSQL Development Conference 2025">PGConf.dev</a>) on May 13 in Montréal, Canada.
<a href="https://peter.eisentraut.org">Peter Eisentraut</a> gave a very nice presentation on the history, design
decisions, and problems solved by “Implementing an Extension Search Path”.
That talk, plus another 10-15m of discussion, is now available for your
viewing pleasure:</p>
 
 <a href="https://www.youtube.com/watch?v=xtnt06zhONk">Video</a> 
 <a href="https://justatheory.com/shared/extension-ecosystem-summit/implementing-an-extension-search-path.pdf">Slides</a> 
 
<p>If you’d like to attend any of the next three Mini-Summits, join <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/" title="Postgres Extension Ecosystem Mini-Summit on Meetup">the
Meetup</a>!</p>
<p>Once again, with many thanks again to <a href="https://dev.to/@floord">Floor Drees</a> for the effort, here’s the
transcript from the session.</p>
 Introduction 
<p>Floor Drees introduced the organizers:</p>
 
 <a href="https://justatheory.com/">David Wheeler</a>, Principal Architect at <a href="https://tembo.io/">Tembo</a>, maintainer of <a href="https://pgxn.org/">PGXN</a> 
 <a href="https://ca.linkedin.com/in/yrashk">Yurii Rashkovskii</a>, <a href="https://omnigres.com/">Omnigres</a> 
 <a href="https://pgxn.org/user/keithf4/">Keith Fiske</a>, <a href="https://www.crunchydata.com/">Crunchy Data</a> 
 <a href="https://dev.to/@floord">Floor Drees</a>, Principal Program Manager at <a href="https://enterprisedb.com" title="EnterpriseDB">EDB</a>, PostgreSQL CoCC member,
PGDay Lowlands organizer 
 
<p><a href="https://peter.eisentraut.org">Peter Eisentraut</a>, contributor to PostgreSQL development since 1999, talked
about implementing an extension search path.</p>
<p>The stream and the closed captions available for the recording are supported
by PGConf.dev and their gold level <a href="https://2025.pgconf.dev/sponsors.html">sponsors</a>, Google, AWS, Huawei, Microsoft,
and EDB.</p>
 Implementing an extension search path 
<p>Peter: Thank you for having me!</p>
<p>I’m gonna talk about a current project by me and a couple of people I have
worked with, and that will hopefully ship with Postgres 18 in a few months.</p>
<p>So, what do I know about extensions? I’m a Postgres core developer, but I’ve
developed a few extensions in my time, here’s a list of extensions that I’ve
built over the years.</p>
 
 <a href="https://github.com/petere/plsh">plsh</a> 
 <a href="https://github.com/petere/pguint">pguint</a> 
 <a href="https://github.com/petere/pgpcre">pgpcre</a> 
 <a href="https://github.com/petere/pguri">pguri</a> 
 <a href="https://github.com/petere/plxslt">plxslt</a> 
 <a href="https://github.com/petere/pgemailaddr">pgemailaddr</a> 
 <a href="https://github.com/petere/pgtrashcan">pgtrashcan</a> 
 
<p>Some of those are experiments, or sort of one-offs. Some of those are actually
used in production.</p>
<p>I’ve also contributed to well-known extensions: <a href="https://pgxn.org/dist/orafce/">orafce</a>; and back in the day,
<a href="https://github.com/2ndQuadrant/pglogical">pglogical</a>, <a href="https://www.enterprisedb.com/docs/pgd/4/bdr/">BDR</a>, and <a href="https://github.com/EnterpriseDB/pg_failover_slots">pg_failover_slots</a>, at <a href="https://enterprisedb.com" title="EnterpriseDB">EDB</a>, and previously
2ndQuadrant. Those are obviously used widely and in important production
environments.</p>
<p>I also wrote an extension installation manager called <a href="https://github.com/petere/pex">pex</a> at one point. The
point of pex was to do it in one shell script, so you don’t have any
dependencies. It’s just a shell script, and you can say <code>pex install orafce</code>
and it installs it. This was a proof of concept, in a sense, but was actually
quite useful sometimes for development, when you just need an extension and
you don’t know where to get it.</p>
<p>And then I wrote, even more experimental, a follow-on project called
<a href="https://github.com/petere/autopex">autopex</a>, which is a plugin module that you load into Postgres that
automatically installs an extension if you need it. If you call <code>CREATE EXTENSION orafce</code>, for example, and you don’t have it installed, autopex
downloads and installs it. Obviously highly insecure and dubious in terms of
modern software distribution practice, but  it does work: you can just run
<code>CREATE EXTENSION</code>, and it just installs it if you don’t have it. That kind of
works.</p>
<p>So anyways, so I’ve worked on these various aspects of these over time. If
you’re interested in any of these projects, they’re all under my <a href="https://github.com/petere/">GitHub
account</a>.</p>
<p>In the context of this presentation…this was essentially not my idea. People
came to me and asked me to work on this, and as it worked out, multiple people
came to me with their problems or questions, and then it turned out it was all
the same question. These are the problems I was approached about.</p>
<p>The first one is extension management in the Kubernetes environment. we’ll
hear about this in a <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306551747/" title="PostgreSQL Extension Mini Summit: Extension Management in CNPG">future talk</a> in this series. <a href="https://www.gabrielebartolini.it">Gabriele Bartolini</a> from
the <a href="https://cloudnative-pg.io">CloudNativePG</a> project approached me and said that the issue in a
Kubernetes environment is that if you launch a Postgres service, you don’t
install packages, you have a pre-baked disk image that contains the software
that you need. There’s a Postgres server and maybe some backup software in
that image, and if you want to install an extension, and the extension is not
in that image, you need to rebuild the image with the extension. That’s very
inconvenient.</p>
<p>The ideal scenario would be that you have additional disk images for the
extensions and you just somehow attach them. I’m hand waving through the
Kubernetes terminology, and again, there will be <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306551747/" title="PostgreSQL Extension Mini Summit: Extension Management in CNPG">a presentation</a>
about that in more detail. But I think the idea is clear: you want to have
these immutable disk images that contain your pieces of software, and if you
want to install more of them, you just wanna have these disk images augment
’em together, and that doesn’t work at the moment.</p>
<p>Problem number two is: I was approached by a maintainer of the <a href="https://postgresapp.com">Postgres.app</a>
project, a Mac binary distribution for Postgres. It’s a nice, user-friendly
binary distribution for Postgres. This is sort of a similar problem: on macOS
you have these <code>.app</code> files to distribute software. They’re this sort of weird
hybrid between a zip file with files in it and a directory you can look into,
so it’s kind of weird. But it’s basically an archive with software in it. And
in this case it has Postgres in it and it integrates nicely into your system.
But again, if you want to install an extension, that doesn’t work as easily,
because you would need to open up that archive and stick the extension in
there somehow, or overwrite files.</p>
<p>And there’s also a tie in with the way these packages are signed by Apple, and
if you, mess with the files in the package, then the signature becomes
invalid. It’s the way it’s been explained to me. I hope this was approximately
accurate, but you already get the idea, right? There’s the same problem where
you have this base bundle of software that is immutable or that you want to
keep immutable and you want to add things to it, which doesn’t work.</p>
<p>And then the third problem I was asked to solve came from the Debian package
maintainer, who will also <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306682786/" title="PostgreSQL Extension Mini Summit: Apt Extension Packaging">speak later</a> in this presentation series. What he
wanted to do was to run the tests of an extension while the package is being
built. That makes sense. You wanna run the tests of the software that you’re
building the package for in general. But in order to do that, you have to
install the extension into the the normal file system location, right? That
seems bad. You don’t want to install the software while you’re into the main
system while you’re building it. He actually wrote a custom patch to be able
to do that, which then my work was inspired by.</p>
<p>Those are the problems I was approached about.</p>
<p>I had some problems I wanted to solve myself based on my experience working
with extensions. While I was working on these various extensions over the
years, one thing that never worked is that you could never run <code>make check</code>.
It wasn’t supported by the PGXS build system. Again, it’s the same issue.</p>
<p>It’s essentially a subset of the Debian problem: you want to run a test of the
software before you install it, but Postgres can only load an extension from a
fixed location, and so this doesn’t work. It’s very annoying because it makes
the software development cycle much more complicated. You always have to then,
then run <code>make all</code>, <code>make install</code>, make sure you have a server running,
<code>make installcheck</code>. And then you would want to test it against various
different server versions. Usually they have to run this in some weird loop.
I’ve written custom scripts and stuff all around this, but it’s was never
satisfactory. It should just work.</p>
<p>That’s the problem I definitely wanted to solve. The next problem  — and
these are are all subsets of each other — that if you have Postgres
installed from a package, like an RPM package for example, and then you build
the extension locally, you have to install the extension into the directory
locations that are controlled by your operating system. If you have Postgres
under <code>/usr</code>, then the extensions also have to be installed under <code>/usr</code>,
whereas you probably want to install them under <code>/usr/local</code> or somewhere
else. You want to keep those locally built things separately, but that’s not
possible.</p>
<p>And finally — this is a bit more complicated to explain — I’m mainly using
macOS at the moment, and the <a href="https://brew.sh" title="The Missing Package Manager for macOS (or Linux)">Homebrew</a> package manager is widely used there.
But it doesn’t support extensions very well at all. It’s really weird because
the way it works is that each package is essentially installed into a separate
subdirectory, and then it’s all symlinked together. And that works just fine.
You have a bunch of <code>bin</code> directories, and it’s just a bunch of symlinks to
different subdirectories and that works, because then you can just swap these
things out and upgrade packages quite easily. That’s just a design choice and
it’s fine.</p>
<p>But again, if you wanna install an extension, the extension would be its own
package — PostGIS, for example — and it would go into its own directory.
But that’s not the directory where Postgres would look for it. You would have
to install it into the directory structure that belongs to the other package.
And that just doesn’t work. It’s just does not fit with that system at all.
There are weird hacks at the moment, but it’s not satisfactory. Doesn’t work
at all.</p>
<p>It turned out, all of these things have sort of came up over the years and
some of these, people have approached me about them, and I realized these are
essentially all the same problem. The extension file location is hard-coded to
be inside the Postgres installation tree. Here as an example: it’s usually
under something like <code>/usr/share/postgresql/extension/</code>, and you can’t install
extensions anywhere else. If you want to  keep this location managed by the
operating system or managed by your package management or in some kind of
immutable disk image, you can’t. And so these are essentially all versions of
the same problem. So that’s why I got engaged and tried to find a solution
that addresses all of ’em.</p>
<p>I had worked on this already before, a long time ago, and then someone broke
it along the way. And now I’m fixing it again. If you go way, way back, before
extensions as such existed in Postgres in 9.1, when you wanted to install a
piece of software that consists of a shared library object and some SQL, you
had to install the shared library object into a predetermined location just
like you do now. In addition, you had to run that SQL file by hand, basically,
like you run <code>psql -f install_orafce.sql</code> or something like that. Extensions
made that a little nicer, but it’s the same idea underneath.</p>
<p>In 2001, I realized this problem already and implemented a configuration
setting called <code>dynamic_library_path</code>, which allows you to set a different
location for your shared library. Then you can say</p>
  <code>dynamic_library_path = '/usr/local/my-stuff/something'
</code>  <p>And then Postgres would look there. The SQL file just knows where is
because you run it manually. You would then run</p>
  <code>psql -f /usr/local/my-stuff/something/something.sql
</code>  <p>That fixed that problem at the time. And when extensions were implemented, I
was essentially not paying attention or, you know, nobody was paying
attention. Extension support were a really super nice feature, of course, but
it broke this previously-available feature: then you couldn’t install your
extensions anywhere you wanted to; you were tied to this specific file system,
location, <code>dynamic_library_path</code> still existed: you could still set it
somewhere, but you couldn’t really make much use of it. I mean, you could make
use of it for things that are not extensions. If you have some kind of plugin
module or modules that install hooks, you could still do that. But not for an
extension that consist of a set of SQL scripts and a control file and
<code>dynamic_library_path</code>.</p>
<p>As I was being approached about these things, I realized that was just the
problem and we should just now fix that. The recent history went as follows.</p>
<p>In April, 2024, just about a year ago now, David Wheeler started <a href="https://postgr.es/m/E7C7BFFB-8857-48D4-A71F-88B359FADCFD@justatheory.com">a hackers
thread</a> suggesting <a href="https://www.df7cb.de">Christoph Berg</a>’s Debian patch as a starting point for
discussions. Like, “here’s this thing, shouldn’t we do something about this?”</p>
<p>There was, a fair amount of discussion. I was not really involved at the time.
This was just after feature freeze,and so I wasn’t paying much attention to
it. But the discussion was quite lively and a lot of people pitched in and
had their ideas and thoughts about it. And so a lot of important, filtering
work was done at that time.</p>
<p>Later, in September, <a href="https://www.gabrielebartolini.it">Gabriele</a>, my colleague from EDB who
works on <a href="https://cloudnative-pg.io">CloudNativePG</a>, approached me about this issue and said like: “hey,
this is important, we need this to make extensions useful in the Kubernetes
environment.” And he said, “can you work, can you work on this?”</p>
<p>I said, “yeah, sure, in a couple months I might have time.” [Laughs]. But it
sort of turns out that, at <a href="https://pgconf.eu">PGConf.EU</a> we had a big brain trust meeting of
various people who basically all came and said, “hey, I heard you’re working
on <code>extension_control_path</code>, I also need that!”</p>
<p><a href="https://www.gabrielebartolini.it">Gabriele</a> was there, and <a href="https://github.com/tbussmann">Tobias Bussmann</a> from
<a href="https://postgresapp.com">Postgres.app</a> was there ,and <a href="https://www.df7cb.de">Christoph</a>, and I was like,
yeah, I really need this <code>extension_control_path</code> to make this work. So I made
sure to talk to everybody there and, and make sure that, if we did this, would
it work for you? And then we kind of had a good idea of how it should work.</p>
<p>In November the first patch was posted and last week it was <a href="https://github.com/postgres/postgres/commit/4f7f7b0">committed</a>. I
think there’s still a little bit of discussion of some details and, we
certainly still have some time before the release to fine tune it, but the
main work is hopefully done.</p>
<p>This is <a href="https://github.com/postgres/postgres/commit/4f7f7b0">the commit</a> I made last week. The fact that this presentation was
scheduled gave me additional motivation to get it done. I wanna give some
credits to people who reviewed it. Obviously David did a lot of reviews and
feedback in general. My colleague Matheus, who I think I saw him earlier, he
was also here on the call, did help me quite a bit with sort of finishing the
patch. And then Gabriele, Marco and Nicolò, who work on <a href="https://cloudnative-pg.io">CloudNativePG</a>, did a
large amount of testing.</p>
<p>They set up a whole sort of sandbox environment making test images for
extensions and, simulating the entire process of attaching these to the main
image. Again, I’m butchering the terminology, but I’m just trying to explain
it in general terms. They did the whole end-to-end testing of what that would
then look like with <a href="https://cloudnative-pg.io">CloudNativePG</a>. And again, that will, I assume, be
discussed when Gabriele <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306551747/" title="PostgreSQL Extension Mini Summit: Extension Management in CNPG">presents</a> in a few weeks.</p>
<p>These are the stats from the patch</p>
 <code>commit 4f7f7b03758

doc/src/sgml/config.sgml                                     |  68 +++++
doc/src/sgml/extend.sgml                                     |  19 +-
doc/src/sgml/ref/create_extension.sgml                       |   6 +-
src/Makefile.global.in                                       |  19 +-
src/backend/commands/extension.c                             | 403 +++++++++++++++++----------
src/backend/utils/fmgr/dfmgr.c                               |  77 +++--
src/backend/utils/misc/guc_tables.c                          |  13 +
src/backend/utils/misc/postgresql.conf.sample                |   1 +
src/include/commands/extension.h                             |   2 +
src/include/fmgr.h                                           |   3 +
src/test/modules/test_extensions/Makefile                    |   1 +
src/test/modules/test_extensions/meson.build                 |   5 +
.../modules/test_extensions/t/001_extension_control_path.pl  |  80 ++++++
</code> <p>the reason I show this is that, it’s not big! What I did is use the same
infrastructure and mechanisms that already existed for the
<code>dynamic_library_path</code>. That’s the code in that’s in <code>dfmgr</code> there in the
middle. That’s where this little path search is implemented9. And then of
course, in <code>extension..c</code> there’s some code that’s basically just a bunch of
utility functions, like to list all the extensions and list all the versions
of all the extensions. Those utility functions exist and they needed to be
updated to do the path search. Everything else is pretty straightforward.
There’s just a few configuration settings added to the documentation and the
sample files and so on. It’s not that much really.</p>
<p>One thing we also did was add tests for this, Down there in <code>test_extensions</code>.
We wrote some tests to make sure this works. Well, it’s one thing to make sure
it works, but the other thing is if we wanna make changes or we find problems
with it, or we wanna develop this further in the future, we have a record of
how it works, which is why you write tests. I just wanted to point that out
because we didn’t really have that before and it was quite helpful to build
confidence that we know how this works.</p>
<p>So how does it work? Let’s say you have your Postgres installation in a
standard Linux file system package controlled location. None of the actual
packages look like this, I believe, but it’s a good example. You have your
stuff under the <code>/usr/bin/</code>, you have the shared libraries in the
<code>/usr/lib/something</code>, you have the extension control files and SQL files in
the <code>/usr/share/</code> or something. That’s your base installation. And then you
wanna install your extension into some other place to keep these things
separate. So you have <code>/usr/local/mystuff/</code>, for example.</p>
<p>Another thing that this patch implemented is that you can now also do this:
when you build an extension, you can write <code>make install prefix=something</code>.
Before you couldn’t do that, but there was also no point because if you
installed it somewhere else, you couldn’t do anything with it there. Now you
can load it from somewhere else, but you can also install it there — which
obviously are the two important sides of that.</p>
<p>And then you set these two settings: <code>dynamic_library_path</code> is an existing
configuration setting, yYou set that to where your lib directory is, and then
the <code>extension_control_path</code> is a new setting. The titular setting of this
talk, where you tell it where your extension control files are.</p>
<p>There’s these placeholders, <code>$libdir</code> and <code>$system</code> which mean the system
location, and then the other locations are your other locations, and it’s
separated by colon (and semi-colon on Windows). We had some arguments about
what exactly the <code>extension_control_path</code> placeholder should be called and,
people continue to have different opinions. What it does is it looks in the
list directories for the control file, and then where it finds the control
file from there, it loads all the other files.</p>
<p>And there’s a fairly complicated mechanism. There’s obviously the actual SQL
files, but there’s also these auxiliary control files, which I didn’t even
know that existed. So you can have version specific control files. It’s a
fairly complicated system, so we wanted to be clear  that what is happening is
the, the main control file  is searched for in these directories, and then
wherever it’s found, that’s where it looks for the other things. You can’t
have the control file in one path and then the SQL files in another part of
the path; that’s not how it works.</p>
<p>That solves problem number five. Let’s see what problem number five was. I
forgot [Chuckles]. This is the basic problem, that you no longer have to
install the extensions in the directories that are ostensibly controlled by
the operating system or your package manager.</p>
<p>So then how would Debian packaging use this? I got this information from
<a href="https://www.df7cb.de">Christoph</a>. He figured out how to do this. He just said, “Oh,
I did this, and that’s how it works.” During packaging, the packaging scripts
that built it up in packages that you just pass these:</p>
  <code>PKGARGS="--pgoption extension_control_path=$PWD/debian/$PACKAGE/usr/share/postgresql/$v/extension:\$system
--pgoption dynamic_library_path=$PWD/debian/$PACKAGE/usr/lib/postgresql/$v/lib:/usr/lib/postgresql/$v/lib"
</code>  <p>These options set the control path and the <code>dynamic_library_path</code> and these
versions and then it works. This was confirmed that this addresses his
problem. He no longer has to carry his custom patch. This solves problem
number three.</p>
<p>The question people ask is, “why do we have two?” Or maybe you’ve asked
yourself that. Why do we need two settings. We have the
<code>dynamic_library_path</code>, we have the <code>extension_control_path</code>. Isn’t that kind
of the same thing? Kind of, yes! But in general, it is not guaranteed that
these two things are in a in a fixed relative location.</p>
<p>Let’s go back to our fake example. We have the libraries in
<code>/usr/lib/postgresql</code> and the SQL and control files in
<code>/usr/share/postgresql</code>, for example. Now you could say, why don’t we just set
it to <code>/usr</code>? Or, for example, why don’t we just set the path to
<code>/usr/local/mystuff</code> and it should figure out the sub directories. That would
be nice, but it doesn’t quite work in general because it’s not guaranteed that
those are the subdirectories. There could be, for example. <code>lib64</code>, for
example, right? Or some other so architecture-specific subdirectory names. Or
people can just name them whatever they want. So, this may be marginal, but it
is possible. You need to keep in mind that the subdirectory structure is not
necessarily fixed.</p>
<p>So we need two settings. The way I thought about this, if you compile C code,
you also have two settings. And if you think about it, it’s exactly the same
thing. When you compile C code, you always have to do <code>-I</code> and <code>-L</code>: <code>I</code> for
the include files, <code>L</code> for the  lib files. This is basically the same thing.
The include file is also the text file that describes the interfaces and the
libraries are the libraries. Again, you need two options, because you can’t
just tell the compiler, oh, look for it in <code>/usr/local</code> because the
subdirectories could be different. There could be architecture specific lib
directories. That’s a common case. You need those two settings. Usually they
go in parallel. If somebody has a plan on how to do it simpler, follow up
patches are welcome.</p>
<p>But the main point of why this approach was taken is also to get it done in a
few months. I started thinking about this, or I was contacted about this in
September and I started thinking about it seriously in the October/November
timeframe. That’s quite late in the development cycle to start a feature like
this, which I thought would be more controversial! People haven’t really
complained that this breaks the security of extensions or anything like that.
I was a little bit afraid of that.</p>
<p>So I wanted to really base it on an existing facility that we already had, and
that’s why I wanted to make sure it works exactly in parallel to the other
path that we already have, and that has existed for a long time, and was
designed for this exact purpose. That was also the reason why we chose this
path of least resistance, perhaps.</p>
<p>This is the solution progress for the six problems that I described initially.
The <a href="https://cloudnative-pg.io">CloudNativePG</a> folks obviously have accompanied this project actively and
have already prototyped the integration solution. And, and presumably we will
hear about some of that at the <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306551747/" title="PostgreSQL Extension Mini Summit: Extension Management in CNPG">meeting on May 7th</a>, where
Gabriele will talk about this.</p>
<p><a href="https://postgresapp.com">Postgres.app</a> I haven’t been in touch with, but one of the maintainers is
here, maybe you can give feedback later. Debian is done as I described, and
they will also be at <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306682786/" title="PostgreSQL Extension Mini Summit: Apt Extension Packaging">the next meeting</a>, maybe there will be some
comment on that.</p>
<p>One thing that’s not fully implemented is the the <code>make check</code> issue. I did
send a follow-up patch about that, which was a really quick prototype hack,
and people really liked it. I’m slightly tempted to give it a push and try to
get it into Postgres 18. This is a work in progress, but it’s, there’s sort of
a way forward. The local install problem I said is done.</p>
<p><a href="https://brew.sh" title="The Missing Package Manager for macOS (or Linux)">Homebrew</a>, I haven’t looked into. It’s more complicated, and I’m also not
very closely involved in the development of that. I’ll just be an outsider
maybe sending patches or suggestions at some point, maybe when the release is
closer and, and we’ve settled everything.</p>
<p>I have some random other thoughts here. I’m not actively working on these
right now, but I have worked on it in the past and I plan to work on it again.
Basically the conversion of all the building to <a href="https://www.postgresql.org/docs/current/install-meson.html">Meson</a> is on my mind, and
other people’s mind.</p>
<p>Right now we have two build systems: the <code>make</code> build system and the <a href="https://www.postgresql.org/docs/current/install-meson.html">Meson</a>
build system, and all the production packages, as far as I know, are built
with <code>make</code>. Eventually we wanna move all of that over to Meson, but we want
to test all the extensions and if it still works. As far as I know, it does
work; there’s nothing that really needs to be implemented, but we need to go
through all the extensions and test them.</p>
<p>Secondly — this is optional; I’m not saying this is a requirement — but
you may wish to also build your own extensions with Meson. But that’s in my
mind, not a requirement. You can also use <code>cmake</code> or do whatever you want. But
there’s been some prototypes of that. Solutions exist if you’re interested.</p>
<p>And to facilitate the second point, there’s been the proposal — which I
think was well received, but it just needs to be fully implemented — to
provide a <code>pkg-config</code> file to build against the server, and <code>cmake</code> and Meson
would work very well with that. Then you can just say  here’s a <code>pkg-config</code>
file to build against the server. It’s much easier than setting all the
directories yourself or extracting them from <a href="https://www.postgresql.org/docs/current/app-pgconfig.html"><code>pg_config</code></a>. Maybe that’s
something coming for the next release cycle.</p>
<p>That’s what I had. So <code>extension_control_path</code> is coming in Postgres 18. What
you can do is test and validate that against your use cases and and help
integration into the downstream users. Again, if you’re sort of a package or
anything like that, you know, you can make use of that. That is all for me.</p>
<p>Thank you!</p>
 Questions, comments 
 
 
<p>Reading the comments where several audience members suggested Peter
follows Conference Driven Development he confirmed that that’s definitely
a thing.</p>
 
 
<p>Someone asked for the “requirements gathering document”. Peter said that
that’s just a big word for “just some notes I have”. “It’s not like an
actual document. I called it the requirements gathering. That sounds very
formal, but it’s just chatting to various people and someone at the next
table overheard us talking and it’s like, ‘Hey! I need that too!’”</p>
 
 
<p>Christoph: I tried to get this fixed or implemented or something at least
once over the last 10 something-ish years, and was basically shot down on
grounds of security issues if people mess up their system. And what
happens if you set the extension path to something, install an extension,
and then set the path to something else and then you can’t upgrade. And
all sorts of weird things that people can do with their system in order to
break them. Thanks for ignoring all that bullshit and just getting it
done! It’s an administrator-level setting and people can do whatever they
want with it.</p>
<p>So what I then did is just to implement that patch and, admittedly I never
got around to even try to put it upstream. So thanks David for pushing
that ahead. It was clear that the Debian version of the patch wasn’t
acceptable because it was too limited. It made some assumptions about the
direct restructure of Debian packages. So it always included the prefix in
the path. The feature that Peter implemented solves my problem. It does
solve a lot of more problems, so thanks for that.</p>
 
 
<p>Peter: Testing all extensions. What we’ve talked about is doing this
through the Debian packaging system because the idea was to maybe make a
separate branch or a separate sub-repository of some sort, switch it to
build Meson, and rebuild all the extension packages and see what happens.
I guess that’s how far we’ve come. I doesn’t actually mean they all work,
but I guess that most of them has tests, so we just wanted to test, see
if it works.</p>
<p>There are some really subtle problems. Well, the ones I know of have been
fixed, but there’s some things that certain compilation options are not
substituted into the <code>Makefile</code>s correctly, so then all your extensions
are built without any optimizations, for example, without any <code>-O</code>
options. I’m not really sure how to detect those automatically, but at
least, just rebuild everything once might be an option. Or just do it
manually. There are not thousands of extensions. There are not even
hundreds that are relevant. There are several dozens, and I think that’s
good coverage.</p>
 
 
<p>Christoph: I realize that doing it on the packaging side makes sense
because we all have these tests running. So I was looking into it. The
first time I tried, I stopped once I realized that Meson doesn’t support
LLVM yet; and the second time I tried, I just <code>diff</code>-ed the generated
<code>Makefile</code>s to see if there’s any difference that looks suspicious. At
thus point I should just continue and do compilation run and see what the
tests are doing and and stuff.</p>
<p>So my hope would be that I could run <code>diff</code> on the results; the problem is
compiling with Postgres with Autoconf once and then with Meson the second
time, then see if it has an impact on the extensions compiled. But my idea
was that if I’m just running <code>diff</code> on the two compilations and there’s no
difference, there’s no point in testing because they’re identical anyway.</p>
 
 
<p>Peter Oooh, you want the actual compilation, for the <code>Makefile</code> output to
be the same.</p>
 
 
<p>Christoph: Yeah. I don’t have to run that test, But the <code>diff</code> was a bit
too big to be readable. There was lots of white space noise in there. But
there were also some actual changes. Some were not really bad, like9 in
some points variables were using a fully qualified path for the <code>make</code>
directory or something, and then some points not; but, maybe we can just
work on making that difference smaller and then arguing about correctness
is easier.</p>
 
 
<p>Peter: Yeah, that sounds like a good approach.</p>
 
 
<p>Jakob: Maybe I can give some feedback from <a href="https://postgresapp.com">Postgres.app</a>. So, thank you
very much. I think this solves a lot of problems that we have had with
extensions over the years, especially because it allows us to separate the
extensions and the main Postgres distribution. For Postgres.app we
basically have to decide which extensions to include and we can’t offer
additional extensions when people ask for them without shipping them for
everyone. So that’s a big win.</p>
<p>One question I am wondering about is the use case of people building their
own extensions. As far as I understand, you have to provide the prefix/
And one thing I’m wondering whether there is there some way to give a
default value for the prefix. Like in <a href="https://www.postgresql.org/docs/current/app-pgconfig.html"><code>pg_config</code></a> or in something like
that, so people who just type <code>make install</code> automatically get some path.</p>
 
 
<p>Peter: That might be an interesting follow on. I’m making a note of it.
I’m not sure how you’d…</p>
 
 
<p>Jakob: I’m just thinking because a big problem is that a lot of people who
try things don’t follow the instructions for the specific Postgres. So for
example, if we write documentation how to build extensions and people on a
completely different system — like people Google stuff and they get
instruction — they’ll just try random paths. Right now, if you just
type <code>make install</code>, it works on most systems because it just builds into
the standard directories.</p>
 
 
<p>Peter: Yeah, David puts it like, “should there be a different default
extension location?” I think that’s probably not an unreasonable
direction. I think that’s something we should maybe think about, once this
is stabilized. I think for your <a href="https://postgresapp.com">Postgres.app</a> use case, it, I think you
could probably even implement that yourself with a one or two line patch
so that at least, if you install Postgres.app, then somebody tries to
build an extension, they get a reasonable location.</p>
 
 
<p>David: If I could jump in there, Jakob, my assumption was that
<a href="https://postgresapp.com">Postgres.app</a> would do something like designate the <code>Application Support</code>
directory and <code>Preferences</code> in <code>~/Library</code> as where extensions should be
installed. And yeah, there could be some patch to PGXS to put stuff there
by default.</p>
 
 
<p>Jakob: Yeah, that would be nice!</p>
 
 
<p>Peter: Robert asked a big question here. What do we think the security
consequences of this patch? Well, one of the premises is that we already
have <code>dynamic_library_path</code>, which works exactly the same way, and there
haven’t been any concerns about that. Well, maybe there have been
concerns, but nothing that was acted on. If you set the path to somewhere
where anybody can write stuff, then yeah, that’s not so good. But that’s
the same as anything. Certainly there were concerns as I read through the
discussion.</p>
<p>I assumed somebody would hav security questions, so I really wanted to
base it on this existing mechanism and not invent something completely
new. So far nobody has objected to it [Chuckles]. But yeah, of course you
can make a mess of it if you go into that <code>extension_control_path = /tmp</code>!
That’s probably not good. But don’t do that.</p>
 
 
<p>David: That’s I think in part the <a href="https://en.wikipedia.org/wiki/XZ_Utils_backdoor">xz exploit</a> kind of made people more
receptive to this patch because we want to reduce the number of patches
that packaging maintainers have to maintain.</p>
 
 
<p>Peter: Obviously this is something people do. Better we have one solution
that people then can use and that we at least we understand, as opposed to
everybody going out and figuring out their own complicated solutions.</p>
 
 
<p>David: Peter, I think there are still some issues with the behavior of
<code>MODULEDIR</code> from PGXS and <code>directory</code> in the control file that this
doesn’t quite work with this extension. Do you have some thoughts on how
to address those issues?</p>
 
 
<p>Peter: For those who are not following: there’s an existing, I guess,
rarely used feature that, in the control file, you can specify directory
options, which then specifies where other files are located. And this
doesn’t work the way you think it should maybe it’s not clear what that
should do if you find it in a path somewhere. I guess it’s so rarely used
that we might maybe just get rid of it; that was one of the options.</p>
<p>In my mental model of how the C compiler works, it sets an <a href="https://en.wikipedia.org/wiki/Rpath"><code>rpath</code></a> on
something. If you set an absolute <code>rpath</code> somewhere and you know it’s not
gonna work if you move the thing to a different place in the path. I’m not
sure if that’s a good analogy, but it sort of has similar consequences. If
you hard-code absolute path, then path search is not gonna work. But yeah,
that’s on the list I need to look into.</p>
 
 
<p>David: For what it’s worth, I discovered last week that the part of this
patch where you’re stripping out <code>$libdir</code> and the extension make file that
was in modules, I think? That also needs to be done when you use <code>rpath</code>
to install an extension and point to extensions today with Postgres 17.
Happy to see that one go.</p>
 
 
<p>Christoph: Thanks for fixing that part. I was always wondering why this
was broken. The way it was broken. It looked very weird and it turned out
it was just broken and not me not understanding it.</p>
 
 
<p>David: I think it might have been a documentation oversight back when
extensions were added at 9.1 to say this is how you list the modules.</p>
<p>Anyway, this is great! Im super excited for this patch and where it’s
going and the promise for stuff in the future. Just from your list of the
six issues it addresses, it’s obviously something that covers a
variety of pain points. I appreciate you doing that.</p>
 
 
<p>Peter: Thank you!</p>
 
 
<p>Many thanks and congratulations wrap up this call.</p>
<p>The next Mini-Summit is on <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306682786/" title="PostgreSQL Extension Mini Summit: Apt Extension Packaging">April 9</a>, <a href="https://www.df7cb.de">Christoph Berg</a> (Debian,
and also Cybertec) will join us to talk about Apt Extension Packaging.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
                 <a href="https://justatheory.com/tags/peter-eisentraut/">Peter Eisentraut</a> 
                 <a href="https://justatheory.com/tags/transcript/">Transcript</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/shared/extension-ecosystem-summit/search-path-card.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   19:32 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 30. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://idmlab.eidentity.jp/'>IdM Laboratory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://idmlab.eidentity.jp/2025/03/gidopenid-connect-for-identity-assurance.html'>GビズIDの大幅アップデートとOpenID Connect for Identity Assuranceへの対応</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  こんにちは、富士榮です。    いわゆる法人共通認証基盤と呼ばれる、デジタル庁が提供しているGビズIDの大幅アップデートが公開されましたね。     出典）デジタル庁 - GビズID&nbsp;https://gbiz-id.go.jp/top/         GビズIDについてはこれまでもOpenIDファウンデーションジャパンのイベント等に古くは経産省、デジタル庁へ移管されてからはデジタル庁の方々にお越しいただき技術仕様やトラストフレームワークについてご発表いただいてきました。      OpenID Summit Tokyo 2020 - 2020/1/24  https://www.openid.or.jp/summit/2020/     OpenID BizDay #14 - 2021/1/27  https://openid.connpass.com/event/200742/
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://blogger.googleusercontent.com/img/a/AVvXsEgTibPicVNRfDxPsZskodgXy4LM2pz93eFwUpfp-f3SP_mA9m4oO_sYkBLet4DXRxYm_RjXTCPJcoGH1gj7Z9E8FbKXPZwIrio3zCcLUZG3aShFA6EqCG2VL4J4nni6tRKODac1UdlTXOF7reZxb1PSCqzkMFWqO5GHFVKHYLZJnaSnyCed_AfpbHh9u6MU=w640-h288'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

こんにちは、富士榮です。 <br>  いわゆる法人共通認証基盤と呼ばれる、デジタル庁が提供しているGビズIDの大幅アップデートが公開されましたね。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgTibPicVNRfDxPsZskodgXy4LM2pz93eFwUpfp-f3SP_mA9m4oO_sYkBLet4DXRxYm_RjXTCPJcoGH1gj7Z9E8FbKXPZwIrio3zCcLUZG3aShFA6EqCG2VL4J4nni6tRKODac1UdlTXOF7reZxb1PSCqzkMFWqO5GHFVKHYLZJnaSnyCed_AfpbHh9u6MU"></a>  出典）デジタル庁 - GビズID&nbsp;<a href="https://gbiz-id.go.jp/top/">https://gbiz-id.go.jp/top/</a>   <br>  <br>  GビズIDについてはこれまでもOpenIDファウンデーションジャパンのイベント等に古くは経産省、デジタル庁へ移管されてからはデジタル庁の方々にお越しいただき技術仕様やトラストフレームワークについてご発表いただいてきました。  <br> <blockquote> OpenID Summit Tokyo 2020 - 2020/1/24  <a href="https://www.openid.or.jp/summit/2020/">https://www.openid.or.jp/summit/2020/</a>  <br>  OpenID BizDay #14 - 2021/1/27  <a href="https://openid.connpass.com/event/200742/presentation/">https://openid.connpass.com/event/200742/presentation/</a>  <br>  OpenID BizDay #15 - 2023/1/10  <a href="https://openid.connpass.com/event/266730/">https://openid.connpass.com/event/266730/</a>  <br>  OpenID BizDay #17 - 2025/2/19  <a href="https://openid.connpass.com/event/341152">https://openid.connpass.com/event/341152</a>/ </blockquote> <br>  GビズIDについて  簡単に言うと、GビズIDは企業の代表や従業員などが当該の企業に所属していることを表し、例えば補助金の申請などの行政手続きをオンラインで実施することを可能にするためのID基盤ですね。  そのためには当然、当該の企業が実在していること、そしてGビズIDを利用する代表者や従業員が当該企業と適切に関係しており所属していることを保証していくことが重要です。  <br>  ここは非常に重要な一方でまだまだ課題も多く、例えば現状は法人の実在性について法務局の発行する印鑑証明書や個人事業主の場合は市町村の発行する印鑑登録証明書を使うことで確認することになりますが、アカウントライフサイクルは各利用企業側に任せるしかないという状況があったりします。  <br>  <br>  法人共通認証基盤の必要性  この考え方は何も日本だけで必要とされているわけではなく、海外においても同様の要求はあるわけです。OpenID FoundationのeKYC and Identity Assurance Working Groupでは個人の本人確認がどのようにIdentity Providerで実施されたかという情報をRelying Partyへ伝達するためのOpenID Connect for Identity Assurance（最近<a href="https://idmlab.eidentity.jp/2024/10/openid-connect-for-identity-assurance.html">正式化</a>されましたね！）に加えて、個人が法人とどのような関係性にあるのかを表現するためのAuthority Claims Extensionの開発を進めています。この辺りは日本のOpenIDファウンデーションジャパンのKYC WGの参加メンバーの方々とも協力して国際標準への道筋をうまく作っていきたいところです。  <br>  参考）eKYC and Identity Assurance Working Group  <a href="https://openid.net/wg/ekyc-ida/">https://openid.net/wg/ekyc-ida/</a>  <br>  <br>  GビズIDのアップデート概要  こう言うのは更新履歴を見ていくのが重要ですね。  デジタル庁が公開している<a href="https://gbiz-id.go.jp/top/system_guide/system_guide.html">システム連携ガイド</a>を見ると技術仕様を含め確認ができるので、こちらの更新履歴を見てみましょう。なお、現在「行政サービス向け」のシステム連携ガイドが公開されていますが、そもそも現状のGビズIDは民間サービスとの連携を許可していません。それにもかかわらず行政サービス向け、と明記されているのは今後の民間サービスへの展開を見据えてのことなのかな、、と期待が膨らみますね。  <br>  早速更新履歴を見ていきましょう。すでにバージョン2.3なんですね。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgfQ4z2xiw7RimXDuUduRLk52_I3QL1udr7Lw8POjhlX6To4uNA8mfVsx8UPXG_5trbmH0bGelS7lm_Y1g_aZQSEZ8YR3JjUInw0At1RuTG3Zxdx6VpqD_TkRbb82l0xO_pZ_F6SNzE20kFfH4laWiWZ4tiBOoduiShBbv-I71BSwKNuTNBFO0bpHFzQohG"></a>    <a href="https://blogger.googleusercontent.com/img/a/AVvXsEilYWCowMd-AlVzXELYvrfhgQNaLqIh6o2DW9dERuCa3awENHINDxltgJK13_0wGose1sFXmNQEws7G3GizIeVUXhkNUB4VMxx-S29Nifx_U1I9bAXD_GBbcnO05aNImWRYMwjJSHi7etPsXDt4ButKDfhKvOBzDcc2PddL8WI-xBLWF4svlXToQifWXu3X"></a> <br><br>  結構更新が多いです。さすが大型アップデートです。  <br>  個人的に関心が高かったのは、以下の２点です。    アカウント種別に管理者（GビズIDメンバー（管理者））が増えた  GビズIDトラストフレームワークが策定され、IAL/AALが明記された   アカウント種別はこれまでプライム、メンバー、エントリーの３種類で、原則プライムは法人代表者のためのアカウントでした。そして、メンバーアカウントの作成や管理はプライムの権限者が実施するしかなかったわけですが、いちいち代表者がログインしてアカウント管理をするのか！！という課題も大きかったのだと思います。GビズIDメンバー（管理者）というアカウント管理権限を持ったアカウントを作成することができるようになりました。   ちなみにGビズIDプライムのアカウントはマイナンバーカードを使ったオンライン申請もできるようになってますね。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEjTo43uc1jo9law25KnIwhls5M3QDpO1L4u_RJm2S8O8JGZ0_VFewIJ0LZCTxAAUZYOQZ0vORfr_UesdmzvIndbCeOIb2Ija1nAa3IKjiOT8ajmX2GtSQMa_Inos4CJzG_XqxlVFMHXOqk2aFHK3EP4h93ZBNDSFTwGVSlvi3EdFxjbbRxu4trSdqC3vr2e"></a> <br><br>  トラストフレームワークについても別文書で定義されています。 <blockquote> 法人共通認証基盤におけるトラストフレームワーク  <a href="https://gbiz-id.go.jp/top/trust_framework/trust_framework.pdf">https://gbiz-id.go.jp/top/trust_framework/trust_framework.pdf</a> </blockquote> <br>  システム連携ガイドにもざっくりとしたレベル感は記載されていますので、Relying Partyは扱う情報の機密レベルやリスク度合いに応じてどのアカウント種別を要求するか決めていく必要があります。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEj4jrHEe2EgCsRpXBKyilyL5Zm7w9faMpSPK8OXnlquqz8uE8cZ_s3VJipti0DCY2IxyUdNptW6m-mlTdG3cSQac592jlnxAU6ySeOAvbRSN2gDxk8fozmxcA_suLSIF0jBi7kpZm2790liC4Ot8wmgqhkl01ZT1h3A_6DVh1x5L-rWAauy9UfgXOYAohRB"></a> <br><br>  OpenID Connect for Identity Assuranceへの対応  タイトルにも書いた通り、今回のGビズIDのアップデートの目玉はOpenID Connect for Identity Assurance（OIDC4IDA）への対応です。といっても結論フルスペック対応ではなく、スキーマについてある程度対応した、という程度ではありますが国が提供するサービスに新しい技術仕様が採用されていくのは非常に嬉しいことですね。  <br>  具体的にはscopeにjp_gbizid_v1_idaを指定することでOIDC4IDAに対応した属性情報を取得できるようになるようです。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhSv48L2DuDQibRH7v1lVK2WpvXgb6pFxQ1xS_SKrWv2ft8UtnRn6wLhEy1cSaEAWe5NzKrU0Su4rrtSzS_FvtmlVfSo5jIBdnKWMRQiZfGvEB9pJLreAJjgE99d8DpSisRYFfF0NB2kezPNumAq5pAmUlvsPOU3YfUgfNZ360d3rxH3F5unnFDaONnOIJG"></a> <br>実際に返却される属性（verified_claims）は下記の通りです。  要するにGビズIDのトラストフレームワークに従い、どのような審査・確認が行われたアカウントなのか、という情報がRelying Partyに対して送出されるようになるわけです。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEjcRJrgS_joTeWeAW4Jtyp8mWL--ZwA8oX207_QNL8q3k_LYewnRpx9aseqib0J7RxysdKDDcvE6qay1xrYfILpGpaBmDJPFYkN9i7iBuz_-_ppV3HczFHVmoPgTiEfFFybhSZn5_1nd_qkk8nD3MfMGjP82KWTygoIqN-GpeQkR2v1ShT2B1-I-8kgc7sa"></a>  <br>  よく見るとauthorityに関する属性も返していますね。この辺りは現在eKYC and Identity Assurance Working Groupで開発を進めているAuthority Claims Extensionを先取りした感じです。 <br>サンプルレスポンスも書いてあります。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEi8m1fSmalYsS80Sj7i7RRnWrIzZ7QSBZIiSvRKdzZKXKZB5Ly7xweY2C_gS3Gs9QnnRu5vaLfp5FAPSaUZzXwLfefw4trzXowlQ6UHPP6Nav4iXx5HgTjiR0vW0hAtloeUI5ABTdZIzvukawhHVKCPAeWc6OwVp6b1myKmJ5xkWvW6Vj9vrR0uYmXfkkmE"></a>  <br> 組織情報の詳細についても返却できるようになっていますね。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEjcPr0ngU47RIdsZR65Gd7zTc6rAavK4_DQVMVHLRAsWINxZp18ZxB5B397Cl5ebKO04yPBUrCBDoZfSvdXD9V0AXw4nJP7mAyzYnGrBy6P1X7ju41KwKF4T7d4q2LsBx8CPgw1CxZ1iWyqkfk8TZuq5GLKQw36EgxL6fT6ceMYkcfXicNxHWkFbkbxgyQl"></a>  <br> こんな感じで当該組織でそのアカウントがどのような役割を持っているのかが表現できるようになっています。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEg6-x2GCNkP9xG1RqF19KJeYP90i_4Ur0h7fa-PAOPpI9-t7tLe9hkPvY9_oXXr7fADV5TYdJ3Qt7W9i7WAv-iV24cuLFtGd7JKx5Dd2mmJelPhVQ5TDcpUInAzVPrpcmVkYrvHkjyHDNpV1-LSpBbNZ6I4FbLPLshyS3cU9iaDHzAGUqqwtNDeH3nYnL-l"></a> <br><br><br>これはちゃんとこのドキュメントを英訳してグローバルで発信していかないといけませんね。結構先進的なことをやっているので海外の実装者や政府機関にとっても非常に参考になると思います。＞デジタル庁さん、がんばってください！<br><br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://blogger.googleusercontent.com/img/a/AVvXsEgTibPicVNRfDxPsZskodgXy4LM2pz93eFwUpfp-f3SP_mA9m4oO_sYkBLet4DXRxYm_RjXTCPJcoGH1gj7Z9E8FbKXPZwIrio3zCcLUZG3aShFA6EqCG2VL4J4nni6tRKODac1UdlTXOF7reZxb1PSCqzkMFWqO5GHFVKHYLZJnaSnyCed_AfpbHh9u6MU=w640-h288'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   01:37 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 27. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://kyledenhartog.com/posts/'>Kyle Den Hartog</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://kyledenhartog.com/recreating-web3-cross-origin-tracking/'>Web3 is Reintroducing Cross-Origin Tracking Accidentally</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  We should expect that when the user shares their address that will act as implied consent for cross-origin tracking in the same way cookie notices act as a prompt for tracking.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>In the context of Web3 we’re currently walking down a dangerous path accidentally, and it’s not something being discussed enough. When a user connects to a site with Web3 capabilities enabled the site first requests the user to share a wallet address with them. This paradigm was set primarily by some choices that were made early on by Metamask as a means of protection for the user. At the time these were beneficial, but over time we’ve recognized some tradeoffs between UX and privacy because of it. Let’s explore those further.</p>

 The UX paradigm of sharing an account address is discrete 

<p>The permissions design of this started out as a low level paradigm where the DApp only needed the wallet address and could fetch state itself from the chain. This led to a thin client design where the site and the UX for different interactions are largely determined by the site. However, because the majority of the application logic is handled by the site itself it also means that the site has to operate in a more trusted context. Both in terms of security and privacy.</p>

<p>Additionally, as we’ve added more functionality to the wallet to try and improve the UX, such as EIP-4361 (Sign in With Ethereum) it’s led to an antipattern in the UX. In order to create a “login” flow, the user first has to share the wallet address, then they have to approve a specifically structured transaction using EIP-191. Because of the order of operations of design and the focus on not conducting breaking changes to the Web3 platform APIs (e.g. what the wallet makes accessible to the site) we’ve now added a tiny bit of debt to the UX paradigm rather than combining these operations into a single design interface.</p>

 The account address paradigm trust model doesn’t align with the browsers 

<p>In the context of a modern browser, most sites are isolated into their own sandbox. This occurs both at the OS process level in order to prevent sites open in one tab from tampering with other sites in another tab either at a deeper memory level or at a more functional script injection level. It also happens at a storage layer through the partitioning of localStorage, cookies, IndexedDBs, etc. Essentially, sites are separated into what’s called an “origin” in the browser and that origin identifier (such as https://example.com) becomes the boundary.</p>

<p>This is why “cross-origin” communication is considered an explicit exception. Examples of this would be using CORS for a site to approve the loading of a cross-origin script it trusts. This is ultimately rooted back in the security model (and more recently privacy model) of the browser. Over and over we’ve learned that trusting sites is a mistake because users aren’t always able to identify when sites are doing things that aren’t in their best interest, such as tracking them for dynamic pricing or crowding a page with personalized ads. So what sort of problems should we expect to come in Web3 because our Web3 platform API is too trusting of the site?</p>

 My prediction for problems to occur in Web3 

<p>We should expect that when the user shares their address that will act as implied consent for cross-origin tracking in the same way cookie notices act as a prompt for tracking. The problem here is that as wallets share wallet addresses across different sites, it will become a global identifier used for the purposes of tracking a user and building a copy of their browsing history server side even if the user doesn’t perform an onchain transaction. This could be as simple as an RPC service provider who’s already got a large customer base of wallets and DApps taking this information and building a dataset to sell with it, or it could be a DApp or Wallet doing it directly themselves. Chainalysis has already been doing this for the purposes of correlating wallet addresses to users to sell to governments. What’s to stop someone like them from entering into the web advertising business too because so much of the web3 space is leveraging them for compliance purposes?</p>

<p>Furthermore, once they’ve built this profile all future onchain transactions will be correlated to the shadow copy of the users browsing history (built in the same way they’re built with 3P cookies) and economic activity (such as what they buy with stablecoins) to build deeper behavioral profiles to sell them more goods or serve them more personalized ads. In other words, we really shouldn’t re-introduce this given all major web browser vendors have been moving towards phasing out 3P cookies. But if we can’t share a wallet address how can we solve this problem?</p>

 A paradigm beyond sharing a cross-origin globally unique identifier (wallet address) 

<p>The answer in my opinion here lies in going down the thick client approach rather than thick app approach. What I mean by “thick” is where the majority of application logic is handled. Today, much of the UX, unsigned transaction generation, and many other aspects are handled by the site. This is probably because the site has no way to request the wallet handles this for them and because the site has desires to build a brand recognition around their protocol using the UX from the site as an value differentiator.</p>

<p>However, we can imagine a world where the site casts an intent to the wallet, such that the wallet can display and generate the necessary information to display to the user. A toy example, I like to use here is through a very specific API designed for checking out and paying with Web3.</p>

<p>A wallet could enable the following API to perform a checkout operation without needing to share an address:</p>

   <code>const checkoutTxn = await window.ethereum.request({
    method: "wallet_checkout",
    params: {
        recipient: "eip155:1:0x1234abc", // a pseudonoymous address to send funds to
        amount: "100.01",
        currency: [
            "eip155:1:0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48",
            "eip155:1:0xdAC17F958D2ee523a2206206994597C13D831ec7"
        ]
    }
});
</code>   

<p>In this you’ll notice a different paradigm. First, the wallet doesn’t need to send the wallet address to the site so it can generate the transaction, instead it will leave it up to the wallet to decide this. Second, the site communicates what it desires to the wallet and lets it decide how to handle it. So for example, it wants the user to send $100.01 worth of either USDC on Base L2 or USDT on mainnet which is communicated based on the currency contract address. If the user doesn’t have USDC or USDT on the proper network the wallet can perform the underlying swaps and bridging to assist with completing the intended transaction so that the caller receives the money into the address they expect.</p>

<p>In summary, we shouldn’t be looking to perpetuate the legacy antipatterns of web2 in Web3 like third party cookies. Instead, we should be looking to extend the web platform in ways that browsers aren’t. In this way the value added capabilities we receive from Web3 for asset ownership become an extension of the web by enhancing it so that we can meet the land of web2 where they’re at, rather than building a separate Web3 island and expecting everyone to come join us.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   00:00 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 25. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://idmlab.eidentity.jp/'>IdM Laboratory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://idmlab.eidentity.jp/2025/03/okta-ventures25the-identity-25.html'>Okta Venturesが選ぶ、今年のアイデンティティ界の25人（The Identity 25）に選ばれました</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  こんにちは、富士榮です。    どうやらOkta Venturesが2024年から始めた今年のアイデンティティ界の25人（The Identity 25）に選ばれました。        https://www.okta.com/resources/datasheet-the-identity-25/        このプログラム、2024年からスタートしたもののようで、昨年はSPRIN-Dにいる安田クリスチーナやMicrosoftのPam DIngle、YubicoのJohn Bradleyなどが選ばれていました。     今年はOpenID FoundationのExecutive DirectorのGail HodgesやChairの崎村さんらの錚々たるメンバの中に何故か私も加えていただけたようです。     しかし、最初Okta Ventures側から連絡をもらった時はよくある詐欺かと
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://blogger.googleusercontent.com/img/a/AVvXsEhH4D9wWdZJSHpdmjV9Te3KPHMikbP41uqK9mD5_85fRLRHrcYcnNdYTzvTsyosD-MkIc-a3Q56glF-yv5qt3GWhUodHcrX2_UNMDQJKvxUPsZ7zYSZHQPcrtNPeZrVMGD7MyNQUqFPV7uuFCCFuACe9cCUNKt72jZ7skX4oKFFJt458DEHUkRG0GxGeWnr=w640-h495'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

こんにちは、富士榮です。 <br>  どうやらOkta Venturesが2024年から始めた今年のアイデンティティ界の25人（The Identity 25）に選ばれました。  <br>   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEhH4D9wWdZJSHpdmjV9Te3KPHMikbP41uqK9mD5_85fRLRHrcYcnNdYTzvTsyosD-MkIc-a3Q56glF-yv5qt3GWhUodHcrX2_UNMDQJKvxUPsZ7zYSZHQPcrtNPeZrVMGD7MyNQUqFPV7uuFCCFuACe9cCUNKt72jZ7skX4oKFFJt458DEHUkRG0GxGeWnr"></a> <br><a href="https://www.okta.com/resources/datasheet-the-identity-25/">https://www.okta.com/resources/datasheet-the-identity-25/</a>  <br>  <br>  このプログラム、2024年からスタートしたもののようで、昨年はSPRIN-Dにいる安田クリスチーナやMicrosoftのPam DIngle、YubicoのJohn Bradleyなどが選ばれていました。  <br>  今年はOpenID FoundationのExecutive DirectorのGail HodgesやChairの崎村さんらの錚々たるメンバの中に何故か私も加えていただけたようです。  <br>  しかし、最初Okta Ventures側から連絡をもらった時はよくある詐欺かと思いましたw  いきなりLInked Inで知らない人からCongratulations！でしたから。。。なぜ選ばれたのかは全くわかりませんが、どなたかが推薦していただいたのでしょう。ありがとうございます。光栄です。  <br>  しかしこれ、タイムズスクエアのNASDAQのディスプレイにデカデカと顔が出るらしいです。。  ちょっとこれからニューヨークいってきます（違  <br>  参考）昨年のクリスチーナの写真   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgkXnUcp8izDHWyUZSFTg6grgPi3Jua8RomZkHfcWPjSX7fH63dR-yrSHzZJ0vi2IW2USL80ygH5a32NLfKIXEZxLGcTiMH7gqtm4F4jxPy8svdTbIDwfmVPK9aelgQBnD3NHQsRjtaR_uScj4yCu37w1cqZiySUlaQrkIeXP5DaobPAdY8tYA96ZaWJmH1"></a> <br><br>  いずれにしろ光栄です。感謝申し上げます。  <br>  4/1 追加  タイムズスクエアの写真をもらったので貼っておきます。   <a href="https://blogger.googleusercontent.com/img/a/AVvXsEizXCdvy2KtdTXZdhxo0-HQNEpe8pMstdYjQCzfQZ37526WT5evbZvo7K0eudpi_lbPKcJl-8eKZ-A3_jy6qI-yOCa4lx2qnrhFD4bfReDy9VNRAP75yHxkhyJbYRkKUnDJTIz-F8TFLJrHdBEOrBcAXs0c22CY6FRieQqEksJHM6ePUfOxVayd8Ae7KJWS"></a> <br><br>  <br>  <br> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://blogger.googleusercontent.com/img/a/AVvXsEhH4D9wWdZJSHpdmjV9Te3KPHMikbP41uqK9mD5_85fRLRHrcYcnNdYTzvTsyosD-MkIc-a3Q56glF-yv5qt3GWhUodHcrX2_UNMDQJKvxUPsZ7zYSZHQPcrtNPeZrVMGD7MyNQUqFPV7uuFCCFuACe9cCUNKt72jZ7skX4oKFFJt458DEHUkRG0GxGeWnr=w640-h495'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:18 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 25. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2025/03/25/asp-net-core-delegated-microsoft-obo-access-token-management-entra-only/'>ASP.NET Core delegated Microsoft OBO access token management (Entra only)</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This blog shows how to implement a delegated Microsoft On-Behalf-Of flow in ASP.NET Core, and has a focus on access token management. The solution uses Microsoft.Identity.Web to implement the different flows and it really simple to implement, when you know how to use the Nuget package and use the correct Microsoft documentation. The application can […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This blog shows how to implement a delegated Microsoft On-Behalf-Of flow in ASP.NET Core, and has a focus on access token management. The solution uses Microsoft.Identity.Web to implement the different flows and it really simple to implement, when you know how to use the Nuget package and use the correct Microsoft documentation. The application can request delegated access tokens On-Behalf-Of a user and another application, providing a seamless and secure access to protected resources using a zero trust strategy.</p>



<p>Code: <a href="https://github.com/damienbod/token-mgmt-ui-delegated-obo-entra">https://github.com/damienbod/token-mgmt-ui-delegated-obo-entra</a></p>



 Blogs in this series 



 
 <a href="https://damienbod.com/2025/01/15/asp-net-core-user-delegated-access-token-management/">ASP.NET Core user delegated access token management</a> 



 <a href="https://damienbod.com/2025/01/20/asp-net-core-user-application-access-token-management/">ASP.NET Core user application access token management</a> 



 <a href="https://damienbod.com/2025/02/10/asp-net-core-delegated-oauth-token-exchange-access-token-management/">ASP.NET Core delegated OAuth token exchange access token management</a> 



 ASP.NET Core delegated Microsoft OBO access token management (Entra only) 
 



 Setup 



<p>Three applications can used in this setup. A web UI application, an API and another API which implements the Microsoft On-Behalf-Of flow for the users delegated access token and the application. The Microsoft <a href="https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth2-on-behalf-of-flow">OBO</a> works very like part of the OAuth token exchange standard, but it is not a standard, just a Microsoft flavor for a standard.</p>







<p></p>



 What must an application manage? 



<p>An access token management solution must ensure that tokens are securely stored per user session for delegated downstream API user tokens and updated after each UI authentication or refresh. The solution should be robust to handle token expiration, function seamlessly after restarts, and support multi-instance deployments. The tokens must be persisted safely in multiple instance setups. Additionally, it must effectively manage scenarios involving invalid or missing access tokens. Microsoft.Identity.Web implements this completely as long as as authentication and OAuth flows are implemented using Entra ID.</p>



 Properties of token management in the solution setup: 



 
 The access token is persisted per user session 



 The token expires 



 The token needs to be persisted somewhere safely (Safe and encrypted storage if not in-memory) 



 The token must be replaced after each UI authentication (per user) 



 The solution must work after restarts 



 The solution must work for multiple instances when deployed to multi-instance deployments. 



 The solution must handle invalid access tokens or missing access tokens 



 The application must handle a user logout 
 



 Web UI 



<p>The first step in the Microsoft On-Behalf-Of flow is to authenticate the user and a web application using Entra ID. This is implemented using the Microsoft.Identity.Web Nuget package. The Web application uses OpenID Connect code flow with PKCE and a confidential client. The application requests an access token for the first API. The access token is a delegated access token issued for the user and the specific API. The implementation uses a secret to assert the application. Microsoft recommends using a certificate and client assertions when deploying to production.</p>


  builder.Services.AddHttpClient();

builder.Services.AddOptions();

string[]? initialScopes = builder.Configuration
	.GetValue&lt;string&gt;("WebApiEntraId:ScopeForAccessToken")?
	.Split(' ');

builder.Services.AddDistributedMemoryCache();
builder.Services
	.AddMicrosoftIdentityWebAppAuthentication(builder.Configuration, 
		"EntraID",
        subscribeToOpenIdConnectMiddlewareDiagnosticsEvents: true)
    .EnableTokenAcquisitionToCallDownstreamApi(initialScopes)
    .AddDistributedTokenCaches();

builder.Services
    .AddAuthorization(options =&gt;
    {
        options.FallbackPolicy = options.DefaultPolicy;
    });

builder.Services.AddRazorPages()
    .AddMvcOptions(options =&gt;
    {
        var policy = new AuthorizationPolicyBuilder()
            .RequireAuthenticatedUser()
            .Build();
        options.Filters.Add(new AuthorizeFilter(policy));
    }).AddMicrosoftIdentityUI();

builder.Services.AddServerSideBlazor()
    .AddMicrosoftIdentityConsentHandler();
  


<p>The WebApiEntraIdService class is used to use the access token from the web application and call the downstream API. If the access token is missing, or invalid, an new access token is requested in the application.</p>


  using Microsoft.Extensions.Configuration;
using Microsoft.Identity.Web;
using System;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Threading.Tasks;

namespace RazorPageEntraId.WebApiEntraId;

public class WebApiEntraIdService
{
    private readonly IHttpClientFactory _clientFactory;
    private readonly ITokenAcquisition _tokenAcquisition;
    private readonly IConfiguration _configuration;

    public WebApiEntraIdService(IHttpClientFactory clientFactory,
        ITokenAcquisition tokenAcquisition,
        IConfiguration configuration)
    {
        _clientFactory = clientFactory;
        _tokenAcquisition = tokenAcquisition;
        _configuration = configuration;
    }

    public async Task&lt;string?&gt; GetWebApiEntraIdDataAsync()
    {
        var client = _clientFactory.CreateClient();

        var scope = _configuration["WebApiEntraID:ScopeForAccessToken"];
        var accessToken = await _tokenAcquisition.GetAccessTokenForUserAsync([scope!]);

        client.BaseAddress = new Uri(_configuration["WebApiEntraID:ApiBaseAddress"]!);
        client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue("Bearer", accessToken);
        client.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue("application/json"));

        var response = await client.GetAsync("/api/profiles/photo");
        if (response.IsSuccessStatusCode)
        {
            var responseContent = await response.Content.ReadFromJsonAsync&lt;string&gt;();

            return responseContent;
        }

        throw new ApplicationException($"Status code: {response.StatusCode}, Error: {response.ReasonPhrase}");
    }
}
  


<p></p>



 Web API using On-Behalf-Of Flow 



<p>The first Web API implements the Microsoft On-Behalf-Of flow to acquire a new access token for the existing access token and the user represented in the access token. The access token is a delegated access token. The API has no UI and does not use any UI flows. If the access token used in the request is invalid, a 401 is returned with an exception information on what permission or access token is required to use the API. If the API is requested using a valid access token, the API application uses the default scope and requests a new access token using a secret or a certificate. The new access token can be used to access the downstream API.</p>


  builder.Services.AddTransient&lt;WebApiDownstreamService&gt;();
builder.Services.AddHttpClient();
builder.Services.AddOptions();

builder.Services.AddDistributedMemoryCache();

builder.Services
	.AddMicrosoftIdentityWebApiAuthentication(
		builder.Configuration, "EntraID")
	.EnableTokenAcquisitionToCallDownstreamApi()
	.AddDistributedTokenCaches();
  


<p></p>


  using Microsoft.Identity.Web;
using System.Net.Http.Headers;
using System.Text.Json;

namespace WebApiEntraIdObo.WebApiEntraId;

public class WebApiDownstreamService
{
    private readonly IHttpClientFactory _clientFactory;
    private readonly ITokenAcquisition _tokenAcquisition;
    private readonly IConfiguration _configuration;

    public WebApiDownstreamService(IHttpClientFactory clientFactory,
        ITokenAcquisition tokenAcquisition,
        IConfiguration configuration)
    {
        _clientFactory = clientFactory;
        _tokenAcquisition = tokenAcquisition;
        _configuration = configuration;
    }

    public async Task&lt;string?&gt; GetApiDataAsync()
    {
        var client = _clientFactory.CreateClient();

        // user_impersonation access_as_user access_as_application .default
        var scope = _configuration["WebApiEntraIdObo:ScopeForAccessToken"];
        if (scope == null) throw new ArgumentNullException(nameof(scope));

        var uri = _configuration["WebApiEntraIdObo:ApiBaseAddress"];
        if (uri == null) throw new ArgumentNullException(nameof(uri));

        var accessToken = await _tokenAcquisition
            .GetAccessTokenForUserAsync([scope]);

        client.DefaultRequestHeaders.Authorization
            = new AuthenticationHeaderValue("Bearer", accessToken);

        client.BaseAddress = new Uri(uri);
        client.DefaultRequestHeaders.Accept.Add(
			new MediaTypeWithQualityHeaderValue("application/json"));

        var response = await client.GetAsync("api/profiles/photo");
        if (response.IsSuccessStatusCode)
        {
            var data = await JsonSerializer.DeserializeAsync&lt;string&gt;(
                await response.Content.ReadAsStreamAsync());

            return data;
        }

        throw new ApplicationException($"Status code: {response.StatusCode}, 
			Error: {response.ReasonPhrase}");
    }
}
  


<p></p>



 Web API 



<p>The downstream API validates the request API using standard JWT validation. </p>


  builder.Services.AddControllers(options =&gt;
{
    var policy = new AuthorizationPolicyBuilder()
        .RequireAuthenticatedUser()
        // .RequireClaim("email") // disabled this to test with users that have no email (no license added)
        .Build();
    options.Filters.Add(new AuthorizeFilter(policy));
});

builder.Services.AddHttpClient();
builder.Services.AddOptions();

builder.Services.AddMicrosoftIdentityWebApiAuthentication(
    builder.Configuration, "EntraID");
  


<p></p>



 Running the applications 



<p>When the applications are started, the data from the downstream APIs is returned to the web application.</p>







<p></p>



 Further examples of the Microsoft On-Behalf-Of flow 



<p>Microsoft authentication authorization libraries are complicated and many. They is no one way to implement this. Microsoft provides Microsoft Graph Nuget packages, Azure SDK packages, mixes application and delegation flows, managed identities solutions, direct token acquisition and some legacy Nuget packages to integrate the security. Here are further examples of using the Microsoft On-Behalf-Of flow using different client solutions. </p>



 Microsoft OBO with Azure Blob Storage (delegated) 



<p>ASP.NET Core Razor page using Azure Blob Storage to upload download files securely using OAuth and Open ID Connect</p>



<p><a href="https://github.com/damienbod/AspNetCoreEntraIdBlobStorage">https://github.com/damienbod/AspNetCoreEntraIdBlobStorage</a></p>



 Microsoft OBO with OpenIddict (delegated) 



<p>This demo shows how to implement the On-Behalf-Of flow between an Microsoft Entra ID protected API and an API protected using OpenIddict.</p>



<p><a href="https://github.com/damienbod/OnBehalfFlowOidcDownstreamApi">https://github.com/damienbod/OnBehalfFlowOidcDownstreamApi</a></p>



 ASP.NET Core OBO using Microsoft Graph (delegated) 



<p>Backend for frontend security using Angular Standalone (nx) and ASP.NET Core backend using Microsoft Graph</p>



<p><a href="https://github.com/damienbod/bff-aspnetcore-angular">https://github.com/damienbod/bff-aspnetcore-angular</a></p>



 Note 



<p>The Microsoft OBO flow is only used for integrating with Microsoft Entra. If using any other identity provider, the OAuth token exchange flow should be used for this type of solution.</p>



 Links 



 
<blockquote><a href="https://damienbod.com/2024/02/12/using-blob-storage-from-asp-net-core-with-entra-id-authentication/">Using Blob storage from ASP.NET Core with Entra ID&nbsp;authentication</a></blockquote>
 



 
<blockquote><a href="https://damienbod.com/2023/01/09/implement-the-oauth-2-0-token-exchange-delegated-flow-between-an-azure-ad-api-and-an-api-protected-using-openiddict/">Implement the OAUTH 2.0 Token Exchange delegated flow between an Microsoft Entra ID API and an API protected using&nbsp;OpenIddict</a></blockquote>
 



<p><a href="https://github.com/damienbod/OAuthGrantExchangeOidcDownstreamApi">https://github.com/damienbod/OAuthGrantExchangeOidcDownstreamApi</a></p>



<p><a href="https://docs.duendesoftware.com/identityserver/v7/tokens/extension_grants/token_exchange/">https://docs.duendesoftware.com/identityserver/v7/tokens/extension_grants/token_exchange/</a></p>



<p><a href="https://datatracker.ietf.org/doc/html/rfc8693">https://datatracker.ietf.org/doc/html/rfc8693</a></p>



 
    
 



<p><a href="https://github.com/damienbod/OnBehalfFlowOidcDownstreamApi">https://github.com/damienbod/OnBehalfFlowOidcDownstreamApi</a></p>



<p><a href="https://www.rfc-editor.org/rfc/rfc6749#section-5.2">https://www.rfc-editor.org/rfc/rfc6749#section-5.2</a></p>



<p><a href="https://github.com/blowdart/idunno.Authentication/tree/dev/src/idunno.Authentication.Basic">https://github.com/blowdart/idunno.Authentication/tree/dev/src/idunno.Authentication.Basic</a></p>



<p><a href="https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth2-on-behalf-of-flow">https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth2-on-behalf-of-flow</a></p>



 Standards 



<p><a href="https://datatracker.ietf.org/doc/html/rfc7519">JSON Web Token (JWT)</a></p>



<p><a href="https://datatracker.ietf.org/doc/rfc9700/">Best Current Practice for OAuth 2.0 Security</a></p>



<p><a href="https://datatracker.ietf.org/doc/html/rfc6749">The OAuth 2.0 Authorization Framework</a></p>



<p><a href="https://datatracker.ietf.org/doc/html/rfc9449">OAuth 2.0 Demonstrating Proof of Possession DPoP</a></p>



<p><a href="https://datatracker.ietf.org/doc/rfc9101/">OAuth 2.0 JWT-Secured Authorization Request (JAR) RFC 9101</a></p>



<p><a href="https://datatracker.ietf.org/doc/html/rfc8705">OAuth 2.0 Mutual-TLS Client Authentication and Certificate-Bound Access Tokens</a></p>



<p><a href="https://openid.net/specs/openid-connect-core-1_0-final.html">OpenID Connect 1.0</a></p>



<p><a>Microsoft identity platform and OAuth 2.0 On-Behalf-Of flow</a></p>



<p><a href="https://datatracker.ietf.org/doc/html/rfc8693">OAuth 2.0 Token Exchange</a></p>



<p><a href="https://datatracker.ietf.org/doc/html/rfc9068">JSON Web Token (JWT) Profile for OAuth 2.0 Access Tokens</a></p>



<p><a href="https://datatracker.ietf.org/doc/html/rfc9110#section-15.5.2">HTTP Semantics RFC 9110</a></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2025/02/context-1.png?w=650'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   12:45 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 24. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://identitywoman.net'>Identity Woman</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://identitywoman.net/key-themes-going-into-iiw-xl-40/'>Key themes going into IIW XL (40) </a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   We are heading into the 40th Internet Identity Workshop, an event that I cofounded with Doc Searls and Phil Windley 20 years ago. I am not sure we ever thought it would last this long but it has. We continue to be the world’s leading innovation forum for decentralized digital identity focused on a range […] 
 The post Key themes going into IIW XL (40)&nbsp; appeared first on Identity Woman. 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>We are heading into the 40th Internet Identity Workshop, an event that I cofounded with Doc Searls and Phil Windley 20 years ago. I am not sure we ever thought it would last this long but it has. We continue to be the world’s leading innovation forum for decentralized digital identity focused on a range […]</p>
<p>The post <a href="https://identitywoman.net/key-themes-going-into-iiw-xl-40/">Key themes going into IIW XL (40)&nbsp;</a> appeared first on <a href="https://identitywoman.net">Identity Woman</a>.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   23:27 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 24. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306569342/'>Mini Summit 2: Extension Search Path Patch</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Join us at PostgresSQL Extension Mini Summit #2 this week, where PostgreSQL
committer Peter Eisentraut will discuss the extension search path patch.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	<a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306569342/">
			
		</a>
	

         
<p>This Wednesday, March 26 at noon America/New_York (16:00 UTC), <a href="https://peter.eisentraut.org">Peter
Eisentraut</a> has graciously agreed to give a talk at the <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/events/306569342/" title="Postgres Extensions Ecosystem Mini-Summit #2">Extension Mini Summit
#2</a> on the <a href="https://github.com/postgres/postgres/commit/4f7f7b0">extension search path patch</a> he recently committed to PostgreSQL.
I’m personally stoked for this topic, as freeing extensions from the legacy of
a single directory opens up a number of new patterns for packaging,
installation, and testing extensions. Hit the <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/" title="Postgres Extension Ecosystem Mini-Summit on Meetup">Meetup</a> to register for this
live video conference, and to brainstorm novel uses for this new feature,
expected to debut in PostgreSQL 18.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
                 <a href="https://justatheory.com/tags/search-path/">Search Path</a> 
                 <a href="https://justatheory.com/tags/peter-eisentraut/">Peter Eisentraut</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/shared/extension-ecosystem-summit/search-path-card.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   21:14 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2025/03/mini-summit-one/'>2025 Postgres Extensions Mini Summit One</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A rough transcript of my talk “State of the Extension Ecosystem”.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 
         
<p>Back on March 12, we hosted the first in a series of <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/" title="Postgres Extension Ecosystem Mini-Summit on Meetup">PostgreSQL Extensions
Mini Summits</a> leading up to the <a href="https://www.pgevents.ca/events/pgconfdev2025/schedule/session/241/" title="PGConf.dev: Extensions Ecosystem Summit">Extension Ecosystem
Summit</a> at PGConf.dev on May 13. I once again inaugurated the series
with a short talk on the State of the Extension Ecosystem. The talk was
followed by 15 minutes or so of discussion. Here are the relevant links:</p>
 
 <a href="https://www.youtube.com/watch?v=ebHpEDX9D2Y">Video</a> 
 <a href="https://justatheory.com/2024/06/trunk-oci-poc/#demo" title="POC: Distributing Trunk Binaries via OCI (Demo)">OCI Demo</a> 
 <a href="https://justatheory.com/shared/extension-ecosystem-summit/state-of-the-ecosystem-mini-summit-2025.pdf">Slides</a> 
 
<p>And now, with many thanks to <a href="https://dev.to/@floord">Floor Drees</a> for the effort, the transcript from
the session.</p>
 Introduction 
<p>Floor Drees introduced the organizers:</p>
 
 <a href="https://justatheory.com/">David Wheeler</a>, Principal Architect at <a href="https://tembo.io/">Tembo</a>, maintainer of <a href="https://pgxn.org/">PGXN</a> 
 <a href="https://ca.linkedin.com/in/yrashk">Yurii Rashkovskii</a>, <a href="https://omnigres.com/">Omnigres</a> 
 <a href="https://pgxn.org/user/keithf4/">Keith Fiske</a>, <a href="https://www.crunchydata.com/">Crunchy Data</a> 
 <a href="https://dev.to/@floord">Floor Drees</a>, Principal Program Manager at <a href="https://enterprisedb.com" title="EnterpriseDB">EDB</a>, PostgreSQL CoCC member,
PGDay Lowlands organizer 
 
<p>David presented a State of the Extension Ecosystem at this first event, and
shared some updates from PGXN land.</p>
<p>The stream and the closed captions available for the recording are supported
by PGConf.dev and their gold level <a href="https://2025.pgconf.dev/sponsors.html">sponsors</a>, Google, AWS, Huawei, Microsoft,
and EDB.</p>
 State of the Extensions Ecosystem 
<p>So I wanted to give a brief update on the state of the Postgres extension
ecosystem, the past, present, and future. Let’s give a brie history; it’s
quite long, actually.</p>
<p>There were originally two approaches back in the day. You could use shared
preload libraries to have it preload dynamic shareable libraries into the main
process. And then you could do pure SQL stuff using, including procedural
languages like PL/Perl, PL/Tcl, and such.</p>
<p>And there were a few intrepid early adopters, including PostGIS, BioPostgres,
PL/R, PL/Proxy, and pgTAP, who all made it work. Beginning of Postgres 9.1
Dimitri Fontaine added support for explicit support for extensions in the
Postgres core itself. The key features included the ability to compile and
install extensions. This is again, pure SQL and shared libraries.</p>
<p>There are <code>CREATE</code>, <code>UPDATE</code>, and <code>DROP EXTENSION</code> commands in SQL that you
can use to add extensions to a database, upgrade them to new versions and to
remove them. And then <code>pg_dump</code> and <code>pg_restore</code> support so that extensions
could be considered a single bundle to be backed up and restored with all of
their individual objects being included as part of the backup.</p>
<p>Back then, a number of us, myself included, saw this as an opportunity to have
the extensibility of Postgres itself be a fundamental part of the community
and distribution. I was a long time user of Perl and used CPAN, and I thought
we had something like CPAN for Postgres. So, I proposed <a href="https://pgxn.org/">PGXN</a>, the
PostgreSQL Extension Network, back in 2010. The idea was to do distribution of
source code. You would register namespaces for your extensions.</p>
<p>There was discovery via a website for search, documentation published, tags to
help you find different kinds of objects, and to support installation through
a command line interface. The compile and install stuff that Postgres itself
provides, using PGXS and Configure.</p>
<p>This is what PGXN looks like today. It was launched in 2011. There’s a command
line client, this website, an API an a registry you can upload your extensions
to. The most recent one was <code>pg_task</code> a day or so ago.</p>
<p>In the interim, since that came out in 2011/2012, the cloud providers have
come into their own with Postgres, but their support for extensions tends to
be rather limited. For non-core extension counts, as of yesterday, Azure
provides 38 extensions, GCP provides 44 extensions, and AWS 51. These are the
third party extensions that don’t come with Postgres and its contrib itself.
Meanwhile, PGXN has 420 extensions available to download, compile, build, and
install.</p>
<p>A GitHub project that tracks random extensions on the internet,
(<a href="https://gist.github.com/joelonsql/e5aa27f8cc9bd22b8999b7de8aee9d47">joelonsql/PostgreSQL-EXTENSIONs.md</a>), which is pretty comprehensive,
has almost 1200 extensions listed. So the question is why is the support not
more broad? Why aren’t there a thousand extensions available in every one of
these systems?</p>
<p>Rthis has been a fairly common question that’s come up in the last couple
years. A number of new projects have tired to fill in the gaps. One is
<a href="https://github.com/aws/pg_tle" title="pg_tle: Framework for building trusted language extensions for PostgreSQL">Trusted Language Extensions</a>. They wanted to make it easier to
distribute extensions without needing dynamic shared libraries by adding
additional features in the database itself.</p>
<p>The idea was to empower app developers to make it easy to install extensions
via SQL functions rather than having to access the file system of the database
server system itself. It can be portable, so there’s no compilation required,
it hooks into the create extension command transparently, supports custom data
types, and there have been plans for foreign data wrappers and background
workers. I’m not sure how that’s progressed in the past year. The <a href="https://github.com/aws/pg_tle" title="pg_tle: Framework for building trusted language extensions for PostgreSQL">pg_tle</a>
extension itself was created by AWS and Supabase.</p>
<p>Another recent entrant in tooling for extensions is <a href="https://github.com/pgcentralfoundation/pgrx" title="pgrx: Build Postgres Extensions with Rust!">pgrx</a>, which is native
Rust extensions in Postgres. You build dynamic shared libraries, but write
them in pure Rust. The API for pgrx provides full access to Postgres features,
and still provides the developer-friendly tooling that Rust developers are
used to. There’s been a lot of community excitement the last couple of years
around pgrx, and it remains under active development — version 0.13.0 just
came out a week or so ago. It’s sponsored and run out of the PgCentral
Foundation.</p>
<p>There have also been a several new registries that have come up to try to fill
the gap and make extensions available. They have emphasized different things
than PGXN. One was ease of use. So, for example, here <a href="https://pgxman.com/" title="npm for PostgreSQL">pgxman</a> says it should
be really easy to install a client in a single command, and then it installs
something, and then it downloads and installs a binary version of your an
extension.</p>
<p>And then there was platform neutrality. They wanted to do binary distribution
and support multiple different platform, to know what binary∑ to install for a
given platform. They provide stats. PGXN doesn’t provide any stats, but some
of them are list stats like how many downloads we had, how many in the last
180 days.</p>
<p>And curation. Trunk is another binary extension registry, from my employer,
Tembo. They do categorization of all the extensions on Trunk, which is at 237
now. Quite a few people have come forward to tells us that they don’t
necessarily use Trunk to install extensions, but use them to find them,
because the categories are really helpful for people to figure out what sorts
of things are even available, and an option to use.</p>
<p>So here’s the State of the Ecosystem as I see it today.</p>
 
 
<p>There have been some lost opportunities from the initial excitement around
2010. Extensions remain difficult to find and discover. Some are on PGXN,
some are on GitHub, some are on Trunk, some are on GitLab, etc. There’s no
like one place to go to find them all.</p>
 
 
<p>They remain under-documented and difficult to understand. It takes effort
for developers to write documentation for their extensions, and a lot of
them aren’t able to. Some of them do write the documentation, but they
might be in a format that something like PGXN doesn’t understand.</p>
 
 
<p>The maturity of extensions can be difficult to gauge. If you look at that
<a href="https://gist.github.com/joelonsql/e5aa27f8cc9bd22b8999b7de8aee9d47">list of 1200 extensions</a> on GitHub, which ones are the good ones?
Which ones do people care about? That page in particular show the number
of stars for each extension, but that the only metric.</p>
 
 
<p>They’re difficult to configure and install. This is something TLE really
tried to solve, but the uptake on TLE has not been great so far, and it
doesn’t support all the use cases. There are a lot of use cases that need
to be able to access the internal APIs of Postgres itself, which means
compiling stuff into shared libraries, and writing them in C or Rust or a
couple of other compiled languages.</p>
<p>That makes them difficult to configure. You have ask questions lik: Which
build system do I use? Do I install the tooling? How do I install it and
configure it? What dependencies does it have? Et cetera.</p>
 
 
<p>There’s no comprehensive binary packaging. The Postgres community’s own
packaging systems for Linux — Apt, and YUM — do a remarkably good job
of packaging extensions. They probably have more extensions packaged for
those platforms than any of the others. If they have the extension you
need and you’re using the PGDG repositories, then this stuff is there. But
even those are still like a fraction of all the potential available
extensions that are out there.</p>
 
 
<p>Dependency management can be pretty painful. It’s difficult to know what
you need to install. I was messing around  yesterday with the PgSQL HTTP
extension, which is a great extension that depends on libcurl. I thought
maybe I could build a package that includes libcurl as part of it. But
then I realized that libcurl depends on other packages, other dynamic
libraries. So I’d have to figure out what all those are to get them all
together.</p>
<p>A lot of that goes away if you use a system like apt or yum. But if you,
if you don’t, or you just want to install stuff on your Mac or Windows,
it’s much more difficult.</p>
 
 
<p>Centralized source distribution, we’ve found found, is insufficient. Even
if all the extensions were available on PGXN, not everybody has the
wherewithal or the expertise to find what they need, download it, compile
it, and build it. Moreover, you don’t want to have a compiler on your
production system, so you don’t want to be building stuff from source on
your production system. So then you have to get to the business of
building your own packages, which is a whole thing.</p>
 
 
<p>But in this state of the extension ecosystem we see new opportunities too. One
I’ve been working on for the past year, which we call “PGXN v2”, is made
possible by my employer, Tembo. The idea was to consider the emerging patterns
— new registries and new ways of building and releasing and developing
extensions — and to figure out the deficiencies, and to engage deeply with
the community to work up potential solutions, and to design and implement a
new architecture. The idea is to serve the community for the next decade
really make a PGXN and its infrastructure the source of record for extensions
for Postgres.</p>
<p>In the past year, I did a bunch of design work on it. Here’s a high level
architectural view. We’d have a root registry, which is still the source code
distribution stuff. There’s a web UX over it that would evolve from the
current website. And there’s a command line client that knows how to build
extensions from the registry.</p>
<p>But in addition to those three parts, which we have today, we would evolve a
couple of additional parts.</p>
 
 
<p>One is “interactions”, so that when somebody releases a new extension on
PGXN, some notifications could go out through webhooks or some sort of
queue so that downstream systems like the packaging systems could know
something new has come out and maybe automate building and updating their
packages.</p>
 
 
<p>There could be “stats and reports”, so we can provide data like how many
downloads there are, what binary registries make them available, what
kinds of reviews and quality metrics rate them. We can develop these stats
and display those on the website.</p>
 
 
<p>And, ideally, a “packaging registry” for PGXN to provide binary packages
for all the major platforms of all the extensions we can, to simplify the
installation of extensions for anybody who needs to use them. For
extensions that aren’t available through PGDG or if you’re not using that
system and you want to install extensions. Late last year, I was focused
on figuring out how t build the packaging system.</p>
 
 
<p>Another change that went down in the past year was the Extension Ecosystem
Summit itself. This took place at PGConf.Dev last May. The idea was for a
community of people to come together to collaborate, examine ongoing work in
the extension distribution, examine challenges, identify questions, propose
solutions, and agree on directions for execution. Let’s take a look at the
topics that we covered last year at the summit.</p>
 
 
<p>One was extension metadata, where the topics covered included packaging
and discoverability, extension development, compatibility and taxonomies
as being important to represent a metadata about extensions — as well as
versioning standards. One of the outcomes was <a href="https://github.com/pgxn/rfcs/pull/3" title="RFC: Meta Spec v2">an RFC</a> for
version two of the PGXN metadata that incorporates a lot of those needs
into a new metadata format to describe extensions more broadly.</p>
 
 
<p>Another topic was the binary distribution format and what it should look
like, if we were to have major, distribution format. We talked about being
able to support multiple versions of an extension at one time. There was
some talk about the Python Wheel format as a potential precedent for
binary distribution of code.</p>
<p>There’s also an idea to distribute extensions through Docker containers,
also known as the <a href="https://opencontainers.org">Open Container Initiative</a>. Versioning came up
here, as well. One of the outcomes from this session was another PGXN <a href="https://github.com/pgxn/rfcs/pull/2" title="RFC: Binary Distribution Format">RFC
for binary distribution</a>, which was inspired by Python Wheel among other
stuff.</p>
<p>I wanted to give <a href="https://justatheory.com/2024/06/trunk-oci-poc/#demo" title="POC: Distributing Trunk Binaries via OCI (Demo)">a brief demo</a> build on that format. I hacked
some changes into the PGXS <code>Makefile</code> to add a new target, <code>trunk</code> that
builds a binary package called a “trunk” and uploads it to an OCI registry
for distribution. <a href="https://justatheory.com/2024/06/trunk-oci-poc/#demo" title="POC: Distributing Trunk Binaries via OCI (Demo)">Here’s what it looks like</a>.</p>
 
 
<p>On my Mac I was compiling my semver extension. Then I go into a Linux
container and compile it again for Linux using the <code>make trunk</code>
command. The result is two <code>.trunk</code> files, one for Postgres 16 on
Darwin and one for Postgres 16 on Linux.</p>
 
 
<p>There are also some JSON files that are annotations specifically for
OCI. We have a command where we can push these images to an OCI
registry.</p>
 
 
<p>Then we can then use an install command that knows to download and
install the version of the build appropriate for this platform
(macOS). And then I go into Linux and do the same thing. It also
knows, because of the OCI standard, what the platform is, and so it
installs the appropriate binary.</p>
 
 
 
 
<p>Another topic was ABI and API compatibility. There was some talk at the
Summit about what is the definition of an ABI and an API and how do we
define internal APIs and their use? Maybe there’s some way to categorize
APIs in Postgres core for red, green, or in-between, something like that.
There was desire to have more hooks available into different parts of the
system.</p>
<p>One of the outcomes of this session was that I worked with Peter
Eisentraut on some stability guidance for the API and ABI that is now
committed in the docs. You can <a href="https://www.postgresql.org/docs/devel/xfunc-c.html#XFUNC-API-ABI-STABILITY-GUIDANCE" title="Postgres Docs: Server API and ABI Stability Guidance">read them now</a> on in the developer docs,
they’ll be part of the Postgres 18 release. The idea is that minor version
releases should be safe to use with other minor versions. If you compiled
your extension against one minor version, it should be perfectly
compatible with other minor versions of the same major release.</p>
<p>Interestingly, there was a release earlier this year, like two weeks after
Peter committed this, where there was an API break. It’s the first time in
like 10 years. Robert Treat and I spent quite a bit of time trying to look
for a previous time that happened. I think there was one about 10 years
ago, but then this one happened and, notably it broke the Timescale
database. The Core Team decided to release a fix just a week later to
restore the ABI compatibility.</p>
<p>So it’s clear that even though there’s guidance, you should in general
be able to rely on it, and it was a motivating factor for the a new
release to fix an ABI break, there are no guarantees.</p>
<p>Another thing that might happen is that I <a href="https://wiki.postgresql.org/wiki/GSoC_2025#ABI_Compliance_Checker" title="PostgreSQL Wiki/GSoC 2025: ABI Compliance Checker">proposed a Google Summer of
Code project to build an ABI checker service</a>. Peter
[embarrassing forgetfulness and misattributed national identity omitted]
Geoghegan <a href="https://postgr.es/m/CAH2-Wzm-W6hSn71sUkz0Rem=qDEU7TnFmc7_jG2DjrLFef_WKQ@mail.gmail.com">POC’d an ABI checker</a> in 2023. The project is to take Peter’s
POC and build something that could potentially run on every commit or push
to the back branches of the project. Maybe it could be integrated into the
build farm so that, if there’s a back-patch to an earlier branch and it
turns red, they quickly the ABI was broken. This change could potentially
provide a higher level of guarantee — even if they don’t end up using
the word “guarantee” about the stability of the ABIs and APIs. I’m hoping
this happens; a number of people have asked about it, and at least one
person has written an application.</p>
 
 
<p>Another topic at the summit last year was including or excluding
extensions in core. They’ve talked about when to add something to core,
when to remove something from core,  whether items in contrib should
actually be moved into core itself, and whether to move metadata about
extensions into catalog. And once again, support for multiple versions
came up; this is a perennial challenge! But I’m not aware of much work on
these questions. I’m wondering if it’s time for a revisit,</p>
 
 
<p>As a bonus item — this wasn’t a formal topic at the summit last year,
but it came up many times in the mini-summits — is the challenge of
packaging and lookup. There’s only one path to extensions in <code>SHAREDIR</code>.
This creates a number of difficulties. Christoph Berg has a patch for a
PGDG and Debian that adds a second directory. This allowed the PGDG stuff
to actually run tests against extensions without changing the core
installation of the Postgres service itself. Another one is <a href="https://cloudnative-pg.io" title="Run PostgreSQL. The Kubernetes way.">Cloud Native
Postgres</a> immutability. If that directory is part of the image, for
your CloudNative Postgres, you can’t install extensions into it.</p>
<p>It’s a similar issue, for <a href="https://postgresapp.com">Postgres.app</a> immutability. Postgres.app is a
Mac app, and it’s signed by a  certificate provided by Apple. But that
means that if you install an extension in its <code>SHAREDIR</code>, it changes the
signature of the application and it won’t start. They work around this
issue through a number of symlink shenanigans, but these issues could be
solved by allowing extension to be installed in multiple locations.</p>
<p>Starting with Christoph’s search path patch and a number of discussions we
had at PGConf last year, <a href="https://peter.eisentraut.org">Peter Eisentraut</a> has been working on a search
path patch to the core that would work similar to shared preload
libraries, but it’s for finding extension control files. This would allow
you to have them in multiple directories and it will find them in path.</p>
<p>Another interesting development in this line has been, the
<a href="https://cloudnative-pg.io" title="Run PostgreSQL. The Kubernetes way.">CloudNativePG</a> project has been using that extension search path
patch to prototype a new feature coming to Kubernetes that allows one to
mount a volume that’s actually another Docker image. If you have your
extension distributed as an OCI image, you can specify that it be mounted
and installed via your CNPG cluster configuration. That means when CNPG
spins up, it puts the extension in the right place. It updates the search
path variables and stuff just works.</p>
<p>A lot of the thought about the stuff went into a <a href="https://justatheory.com/2024/11/rfc-extension-packaging-lookup/" title="RFC: Extension Packaging &amp; Lookup">less formal RFC</a> I wrote
up in my blog, rather than on PGXN. The idea is to take these improvements
and try to more formally specify the organization of extensions separate
from how Postgres organizes shared libraries and shared files.</p>
 
 
<p>I said, we’re bringing the Extension Summit back! There will be another
Extension Summit hosted our team of organizers, myself, Floor, Keith Fiske
from Crunchy Data, and Yurii from Omnigres. That will be on May 13th in the
morning at <a href="https://2025.pgconf.dev" title="PostgreSQL Development Conference 2025">PGConf.dev</a>; we appreciate their support.</p>
<p>The idea of these Mini Summits is to bring up a number of topics of interest.
Have somebody come and do a 20 or 40 minute talk about it, and then we can
have discussion about implications.</p>
<p>Floor mentioned the schedule, but briefly:</p>
 
 March 12: <a href="https://justatheory.com/">David Wheeler</a>, PGXN: “State of the Extension Ecosystem” 
 March 24: <a href="https://peter.eisentraut.org">Peter Eisentraut</a>, Core Team: “Implementing an Extension Search Path” 
 April 9: <a href="https://www.df7cb.de">Christoph Berg</a>, Debian: “Apt Extension Packaging” 
 April 23: 
 May 7: <a href="https://www.gabrielebartolini.it">Gabriele Bartolini</a>, CNPG “Extension Management in CloudNativePG” 
 
<p>So, what are your interests in extensions and how they can be improved. There
are a lot of potential topics to talk about at the Summit or at these Mini
Summits: development tools, canonical registry, how easy it is to publish,
continuous delivery, yada, yada, yada, security scanning — all sorts of
stuff that could go into conceiving, designing, developing, distributing
extensions for Postgres.</p>
<p>I hoe you all will participate. I appreciate you taking the time to listen to
me for half an hour. So I’d like to turn it over to, discussion, if people
would like to join in, talk about implications of stuff. Also, we can get to
any questions here.</p>
 Questions, comments, shout-outs 
<p>Floor: David, at one point you talked about, metadata taxonomy. If you can
elaborate on that a little bit, that’s Peter’s question.</p>
<p>David: So one that people told me that they found useful was one provided by
<a href="https://pgt.dev">Trunk</a>. So it has these limited number of categories, so if you’re interested
in machine learning stuff, you could go to the <a href="https://pgt.dev/?cat=machine_learning" title="Trunk Categories: Machine Learning">machine learning</a> stuff and it
shows you what extensions are potentially available. They have 237 extensions
on Trunk now.</p>
<p>PGXN itself allows arbitrary tagging of stuff. It builds <a href="https://pgxn.org/tags" title="PGXN: Release Tags">this little tag
cloud</a>. But if I look at this one here, you can see <a href="https://pgxn.org/dist/uint128/1.0.1/" title="PGXN: uint128 v1.0.1">this one</a> has a bunch of
tags. These are arbitrary tags that are applied by the author. The current
metadata looks <a href="https://api.pgxn.org/src/uint128/uint128-1.0.1/META.json" title="PGXN: uint128 v1.0.1 META.json">like this</a>. It’s just plain JSON, and it has a list of tags.
The <a href="https://github.com/pgxn/rfcs/pull/3" title="RFC: Meta Spec v2">PGXN Meta v2 RFC</a> has a bunch of examples. It’s an evolution of
that <code>META.json</code>, so the idea is to have a classifications that includes tags
as before, but also adds categories, which are a limited list that would be
controlled by the core [he means “root”] registry:</p>
  <code>{
  "classifications": {
    "tags": [
      "testing",
      "pair",
      "parameter"
    ],
    "categories": [
      "Machine Learning"
    ]
  }
}
</code>   Announcements 
<p>Yurii made a number of announcements, summarizing:</p>
 
 
<p>There is a new library that they’ve been developing at Omnigres that
allows you to develop Postgres extensions in C++. For people who are
interested in developing extensions in C++ and gaining the benefits of
that and not having to do all the tedious things that we have to do with C
extensions: look for <a href="https://cppgres.org">Cppgres</a>. Yurii thinks that within a couple of
months it will reach parity with pgrx.</p>
<p>David: So it sounds like it would work more closely to the way PGXS and
C works. Whereas pgrx has all these additional Rust crates you have to
load and like slow compile times and all these dependencies.</p>
<p>Yurii: This is just like a layer over the C stuff, an evolution of that.
It’s essentially a header only library, so it’s a very common thing in the
C++ world. So you don’t have to build anything and you just include a
file. And in fact the way I use it, I amalgamate all the header files that
we have into one. Whenever I include it in the project, I just copy the
amalgamation and it’s just one file. You don’t have any other build chain
associated yet. It is C++ 20, which some people consider new, but by the
time it’s mature it’s already five years old and most compilers support
it. They have decent support of C++ 20 with a few exclusions, but those
are relatively minor. So for that reason, it’s not C++ 23, for example,
because it’s not very well supported across compilers, but C++ 20 is.</p>
 
 
<p>Yurii is giving a talk about <a href="https://github.com/postgres-pm/pgpm">PostgresPM</a> at the Postgres Conference in
Orlando. He’ll share the slides and recording with this group. The idea
behind PostgresPM is that it takes a lot of heuristics, takes the URLs of
packages and of extensions and creates packages for different outputs like
for Red Hat, for Debian, perhaps for some other formats in the future. It
focuses on the idea that a lot of things can be figured out.</p>
<p>For example: do we have a new version? Well, we can look at list of tags
in the Git repo. Very commonly that works for say 80 percent of
extensions. Do we need a C compiler? We can see whether we have C files.
We can figure out a lot of stuff without packagers having to specify that
manually every time they have a new extension. And they don’t have to
repackage every time there is a new release, because we can detect new
releases and try to build.</p>
 
 
<p>Yurii is also running an event that, while not affiliated with PGConf.dev,
is strategically scheduled to happen one day before PGConf.dev: <a href="https://pgext.day">Postgres
Extensions Day</a>. The Call for Speakers is open until April 1st. There’s
also an option for people who cannot or would not come to Montréal this
year to submit a prerecorded talk. The point of the event is not just to
bring people together, but also ti surface content that can be interesting
to other people. The event itself is free.</p>
 
 
<p>Make sure to join our <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/" title="Postgres Extension Ecosystem Mini-Summit on Meetup">Meetup group</a> and join us live, March 26,
when <a href="https://peter.eisentraut.org">Peter Eisentraut</a> joins us to talk about implementing an extension search
path.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
                 <a href="https://justatheory.com/tags/transcript/">Transcript</a> 
             
         
     
   
  
  
  

</div>
</div>

<div class='item-footer'>
   20:46 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://realestatecafe.com/'>Bill Wendels Real Estate Cafe</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://realestatecafe.com/is-proposed-mlspin-settlement-the-april-fools-day-joke-recartel-doesnt-consumers-retelling/'>Is proposed MLSpin Settlement the April Fool’s Day joke #RECartel doesn’t want consumers retelling?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Homebuyers &amp; sellers, intend to buy or sell this Spring and talking to real estate agents about their fees? Have any mentioned that is just… 
The post Is proposed MLSpin Settlement the April Fool’s Day joke #RECartel doesn’t want consumers retelling? first appeared on Real Estate Cafe.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>Homebuyers &amp; sellers, intend to buy or sell this Spring and talking to real estate agents about their fees? Have any mentioned that is just…</p>
The post <a href="https://realestatecafe.com/is-proposed-mlspin-settlement-the-april-fools-day-joke-recartel-doesnt-consumers-retelling/">Is proposed MLSpin Settlement the April Fool’s Day joke #RECartel doesn’t want consumers retelling?</a> first appeared on <a href="https://realestatecafe.com">Real Estate Cafe</a>.
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:23 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 19. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://benwerd.medium.com/forcing-people-back-to-the-office-was-a-choice-im-making-mine-e1e7b8dd0af4?source=rss-3b16402f5b9d------2'>Forcing people back to the office was a choice. I’m making mine.</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

      Here’s why I won’t RTO.  Continue reading on Medium »  
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/0*vOxtl6yDf7Ur-gp2.jpg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   <p><a href="https://benwerd.medium.com/forcing-people-back-to-the-office-was-a-choice-im-making-mine-e1e7b8dd0af4?source=rss-3b16402f5b9d------2"></a></p><p>Here’s why I won’t RTO.</p><p><a href="https://benwerd.medium.com/forcing-people-back-to-the-office-was-a-choice-im-making-mine-e1e7b8dd0af4?source=rss-3b16402f5b9d------2">Continue reading on Medium »</a></p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/0*vOxtl6yDf7Ur-gp2.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   20:32 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 15. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://xditao.blogspot.com/'>The Tao of XDI</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='http://xditao.blogspot.com/2025/03/recognition-identity-and-ai-building.html'>Recognition, Identity, and AI: Building Trust in Digital Agents</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   My first love was self-sovereign distributed data, where each person owns and controls their data, hosting it wherever they choose and permissioning it under their own terms. But I got lost in the complexity of building a robust distributed identity infrastructure. How can you give permission to someone if you can’t 'name' them in a way that is verifiable and resistant to subversion? There's no po
   
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>My first love was self-sovereign distributed data, where each person owns and controls their data, hosting it wherever they choose and permissioning it under their own terms. But I got lost in the complexity of building a robust distributed identity infrastructure. How can you give permission to someone if you can’t 'name' them in a way that is verifiable and resistant to subversion? There's no point in saying "only John can access this" if Tom can show up and convincingly say, "I'm John."</p><p>This issue isn’t theoretical—many modern digital problems stem from weak identity foundations. Take email, for example. SMTP, the core protocol, evolved without a strong sense of identity. Had we designed email with a robust identity layer—and maybe a little reputation—spam might have been less rampant. Instead, we've had to patch identity onto email systems, mostly at the DNS layer. Could better choices early on have changed the landscape of digital trust?</p><p>As we enter the era of AI and Personal AI, this challenge resurfaces. We will increasingly rely on agents to interact, assist, and even make decisions on our behalf. But how can we trust these agents? How do we know they are who they claim to be, and whose interests they truly serve? When I ask my AI how to unwind after a long day, it might suggest a refreshing Diet Coke. But is that suggestion rooted in understanding my preferences, or is it influenced by unseen commercial incentives?</p><p>Recognition and Identity in AI</p><p>In the animal world, intelligence is often measured by the ability to recognize oneself and others. The mirror test is a classic example—when an animal identifies itself in a reflection, it demonstrates a form of self-awareness. Similarly, recognizing specific others—distinguishing one individual from another—marks advanced cognitive development.</p><p>AI, in contrast, remains limited in this capacity. While AI excels at pattern recognition, it lacks the ability to form a persistent sense of identity, either of itself or others. This limitation restricts its ability to build trust and context in interactions. Without a foundation for recognizing specific entities, AI systems risk becoming tools of confusion or exploitation.</p><p>Embedding Identity Systems into AI</p><p>One solution is to deeply embed identity frameworks into AI architectures from the outset. Decentralized Identifiers (DIDs), Verifiable Credentials (VCs), and similar systems could provide AI with a structured way to "recognize" and differentiate entities.</p>  <p>Persistent Identity Chains: AI could track verifiable chains of identity, ensuring that when it reports information—like "Brad says buy this stock"—it can verify that it truly came from the Brad you trust.</p>  <p>Verification of Origin: By leveraging cryptographically verifiable credentials, AI can ensure that information hasn’t been tampered with and originates from a trusted source.</p>  <p>Reputation Frameworks: Identity systems could incorporate reputation mechanisms, helping AI prioritize information from sources that consistently meet a trust threshold.</p>  <p>Chain of Custody: AI could provide transparency on how information was received and processed, ensuring that its recommendations are based on data with verifiable origins.</p>  <p>The Path to Trusted AI</p><p>Trustworthy AI isn’t about making machines socially aware; it’s about ensuring that humans can trust the chain of custody behind AI-generated insights. When AI states that "Brad recommends this action," it should be able to prove that the recommendation came from the right "Brad"—the person you trust, not an imposter or manipulated data source.</p><p>The real question is: How do we create systems where AI is not just technically accurate but verifiably trustworthy? In an era where decisions increasingly rely on AI advice, embedding identity systems at the core isn’t just beneficial—it’s fundamental.</p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:51 &bull;
   4 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 11. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wrenchinthegears.com/2025/03/11/on-hiatus-look-for-me-in-the-garden/'>On Hiatus – Look For Me In The Garden</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Today I realized that even though the Web3 program is ramping up as demonstrated by Silicon Valley’s plans to turn Argentina into a “network state,” few understand what is unfolding with cybernetic governance. For that reason, I decided to take time off this spring to work on my garden and simply be with real people [...]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Today I realized that even though the Web3 program is ramping up as demonstrated by Silicon Valley’s plans to turn Argentina into a “network state,” few understand what is unfolding with cybernetic governance. For that reason, I decided to take time off this spring to work on my garden and simply be with real people in the real world. I made a brief announcement to that effect tonight.</p>
<p></p>
<p>This is a clip I created (6 minutes) summarizing Protocol Lab’s Aleph project with Javier Milei, the guy who gave Musk the chainsaw at the CPAC gathering a few weeks ago. I get that it is difficult to navigate narratives and torrents of information. The current has been against the story I’ve been trying to tell for a decade. It simply didn’t stick, but I did try. I lost my old life in the process, but I gained a sunny yard filled with birdsong and time to read stacks of books on&nbsp;cosmology and consciousness. Until I pop in again, be well everyone.</p>
<p></p>
<p></p>
<p>A work in progress. Yesterday I planted twenty-five asparagus crowns. It will be three years before I can harvest them. It feels kind of good to be on slow vegetable time.</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2025/03/img_7850-1.jpg?resize=768%2C1024&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   04:32 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Saturday, 08. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@rufftimo'>Timothy Ruff</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://rufftimo.medium.com/sedi-details-for-identity-nerds-e1949af5cc30?source=rss-11906019170a------2'>SEDI Details for Identity Nerds</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   This post is a more technical follow-up to the post announcing the passing of SB 260 in Utah, establishing the principles for state-endorsed digital identity (SEDI). While SEDI does not mandate any particular technology, its mandated principles close the door for technologies that do not comply, and open the door for technologies that do. This article explores what SEDI makes technically possible,
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/814/1*9RZYPWX4qb7rxuS3liFxOw.jpeg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This post is a more technical follow-up to the <a href="https://medium.com/@rufftimo/utah-blazes-new-trail-for-ssi-sedi-441d8ce65ab8">post announcing the passing of SB 260 in Utah</a>, establishing the principles for state-endorsed digital identity (SEDI). While SEDI does not mandate any particular technology, its mandated principles close the door for technologies that do not comply, and open the door for technologies that do. This article explores what SEDI makes technically possible, in compliance with SB 260 principles.</p><p>State-endorsed digital identity (SEDI) introduces numerous foundational principles for a government-sponsored digital identity system, several of which were covered in <a href="https://medium.com/@rufftimo/utah-blazes-new-trail-for-ssi-sedi-441d8ce65ab8">Utah Blazes New Trail for SSI:&nbsp;SEDI</a>:</p>  “Endorse” vs. “issue” and “identity” vs. “credential”  Sovereign control  Guardianship  Privacy &amp; no surveillance  State-of-the-art security  <p>This post explores more technical and security principles enabled by SEDI that will be of interest to digital identity&nbsp;experts.</p> “Autonomic” Identifiers <p>SEDI doesn’t mandate any specific technology stack, but it does specify capabilities that are simply unachievable for most digital identity technologies. One foundational example is the requirement in lines (73) — (77) of the <a href="https://le.utah.gov/~2025/bills/static/SB0260.html">Utah bill</a>, that individuals must first bring their “identity” to the state, after which it can be endorsed.</p><p>What kind of “identity” might that be? That will be specified as the details of Utah’s future identity program are fleshed out. As a Utah citizen and as an outside expert invited to consult on these new identity policies, I will push for stringent minimum standards for the security, utility, privacy, and autonomy of a digital identity to qualify for state endorsement. Needless to say, your social media login — or even your bank login — should not&nbsp;suffice.</p><p>One “identity” that meets my preferred standard is an “Autonomic Identifier” or AID, as first described in Dr. Samuel Smith’s seminal paper: <a href="https://trustoverip.github.io/tswg-keri-specification/">Key Event Receipt Infrastructure</a> (KERI).</p><p>Some of the capabilities enabled by an AID-powered SEDI ecosystem include:</p><p>BYOID (Bring Your Own Identifier) — In the digital realm, for an identity to be self sovereign, it must begin with a unique identifier that is under the sovereign control of an individual. When a state endorses an identity, in practice they are endorsing an identifier that an individual has brought to the state that is provably under the individual’s control.</p><p>Persistent Identifiers — When that individual moves their residence to another state (within the U.S., for example), necessitating a new identity endorsement from the new state, the old state can revoke their endorsement of the same identifier that the new state will be endorsing; the identifier is persistent. This is a KERI capability that is very different from most PKI-based systems where the identifier is a public key that is discarded upon rotation or revocation.</p><p>Reputation — This persistence creates a continuity for the individual to retain and build their digital reputation independent from any particular government endorsement. In fact, it would be inclusive of every endorsement they have ever had, and all their provable behaviors using those endorsements, no matter how many states or countries have given them past endorsements of identity. And it would be inclusive of non-state endorsements such as educational, work, or anything else,&nbsp;too.</p> Putting It All&nbsp;Together <p>A simple depiction of SEDI built on AIDs might look like&nbsp;this:</p><p>AIDs enable true self-sovereignty as the foundation, right where it belongs and where it provides the greatest utility for all aspects of digital life. State endorsement is also situated right where it belongs: as only one of many endorsements an individual may receive through their digital lifetime.</p> State-Issued ‘Jenga’ <p>Conversely, building digital identity on top of a state-issued credential instead of a state-endorsed identifier lasts only until the state revokes that credential. An attached academic credential, for example, is lost when the individual moves from one state to another and obtains a new identity in the new state, and the old state revokes the now-obsolete issuance.</p><p>Look at this visually:</p><p>This is like a game of Jenga: pull out the state-issued ID — by moving from one state or country to another — and the whole stack falls down. When the state revokes the ID they’ve issued, you also lose everything cryptographically bound to it, and must start&nbsp;over.</p><p>Even if the old state kept their credential alive, there is no binding between old and new credentials, no way to build a growing, persistent reputation that connects a person to their cross-boundary behaviors in the digital world the same way a consistent legal name connects their behaviors across boundaries in the physical&nbsp;world.</p> Human-Friendly? <p>Autonomic identifiers cannot be human-friendly. Human-friendly namespaces require human management, recurring cost, and have scarcity problems like <a href="https://en.wikipedia.org/wiki/Cybersquatting">cybersquatting</a>, or missing the opportunity to register Timothy.com or timothy@gmail; the 1,000th Timothy won’t have much of a selection. AIDs are long random strings of letters and numbers, and are so incredibly random it would take a million computers a million years to guess one, which ensures universal uniqueness and brings a strong measure of security.</p><p>AIDs have cryptographic properties, so we can prove control over them in the digital realm and recover control when keys are compromised. And keys *will* get compromised (see “Expected Compromise” below). Without these special mathematical properties, they’d be just letters and numbers anyone can copy and use to impersonate, like a social security&nbsp;number.</p> Many Identifiers <p>In real life, we have identifiers that are public — our legal names — that typically remain constant throughout our lives. We use these identifiers to build reputations over time, as patterns of behaviors accumulate that are associated with them. With SEDI, the identifier that is endorsed by the state will work in the same manner: it will be persistent, and as public as one chooses to make it, used and disclosed (always under the hood) whenever one might disclose their legal&nbsp;name.</p><p>In digital situations where one prefers to not use their legal name, there’s nothing in SEDI preventing that individual from using a completely different identifier, unrelated to their endorsed one. In fact there is no limit on how many different unique identifiers one may create and use for whatever purpose they choose; and each state can choose whether or not only one at a time can be officially endorsed (Utah allows only one physical identity at a time, and may do the same digitally).</p> Potential Security Advantages <p>SEDI’s architecture enables numerous critical security advantages that could be implemented:</p><p>Zero Trust — ”Never trust, always verify” is the ideal of zero trust. SEDI can not only embrace zero trust, it can enable true zero trust, which does away with all shared secrets, described briefly&nbsp;below.</p><p>No Shared Secrets — SEDI doesn’t employ bearer tokens, symmetric keys, or phishable passwords, unless they are used in conjunction with digital signatures. (Pins and passwords can be helpful when used in addition to digital signatures or physical&nbsp;keys)</p><p>Anti-fragile — A breach of one person or system does not facilitate the breach of the entire system or another connected one, as with most current digital identity systems, which are predicated on shared secrets and perimeter security. With SEDI, most breaches can be detectable, preventable, and recoverable.</p><p>Expected Compromise — Key compromise should be expected and anticipated — and not surprising and catastrophic — in addition to robust prevention and detection capabilities, bringing powerful means for recovery.</p><p>Multi-signature — Keys and credentials can be protected with m-of-n weighted multi-signature capabilities, enabling creative familial, professional, and social protection and recovery&nbsp;schemes.</p><p>Mutual authentication — Individuals could authenticate each other, in any setting. For example, a citizen could authenticate a police officer as readily as a police officer can authenticate the citizen. Any person, organization, or thing claiming authority from the state could be instantly verified to have the claimed authority.</p> Artificially Intelligent Agents (“Agentic AI”) <p>IBM predicts that 2025 may be “<a href="https://www.ibm.com/think/insights/ai-agents-2025-expectations-vs-reality">the year of the AI agent</a>”, and the CEO of Microsoft predicts that agentic AI will “<a href="https://www.youtube.com/results?search_query=satya+nadella+agentic+ai">replace all software</a>.” Whether that happens or not, AI agents are coming and will soon affect the way we interact with organizations of all kinds. This can take the form of you interacting with some organization’s agent, some organization interacting with your agent, or two AI agents interacting with each&nbsp;other.</p><p>SEDI paves the way for verifying that an AI agent has the delegated authority it claims to have, and constraining what an agent is authorized to do. SEDI enables authority to be delegated to people, organizations, and things, and AI agents are things. SEDI is uniquely well suited for controlling, securing, and navigating a world of ubiquitous AI&nbsp;agents.</p><p>Many don’t like the idea of having to deal with AI agents in any form, and I can’t blame them. But like the digital transformation itself, that world is coming whether we like it or not, and the more controls and constraints we can have for it, the&nbsp;better.</p> Comprehensive Digital&nbsp;Identity <p>In sum, SEDI enables a cohesive, comprehensive digital identity system for a state or for any other ecosystem. It requires no proprietary platforms, products, or providers, though it doesn’t preclude their participation, and requires no shared or trusted intermediaries or blockchains. Through robust guardianship, delegation, and multi-sig, it extends digital capabilities to populations that have been previously excluded: minors (including infants), the elderly, disabled persons, homeless persons, migrants, and any other population. And it does this while maximizing individual autonomy and control over one’s data, with powerful means for consent, and numerous breakthroughs in security.</p><p>In short, SEDI heralds a new era of utility, security and autonomy for comprehensive digital identity ecosystems, and I am proud of my home state of Utah for introducing it to the&nbsp;world.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/814/1*9RZYPWX4qb7rxuS3liFxOw.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   00:15 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 07. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@rufftimo'>Timothy Ruff</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://rufftimo.medium.com/utah-blazes-new-trail-for-ssi-sedi-441d8ce65ab8?source=rss-11906019170a------2'>Utah Blazes New Trail for SSI: SEDI</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Finally!  On March 4, 2025, the Utah legislature unanimously passed SB 260, a groundbreaking digital identity bill that carries this description:  “This bill enacts provisions related to a state-endorsed digital identity” (SEDI).  Now it’s off to the governor for signature, who has been publicly positive about it. Huge credit to Sen. Cullimore, the bill’s sponsor, and Christopher Bramwell, the sta
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*x3UpsjrhELH4iGU6guOwjA.jpeg'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Finally! <p>On March 4, 2025, the Utah legislature unanimously passed <a href="https://le.utah.gov/~2025/bills/static/SB0260.html">SB 260</a>, a groundbreaking digital identity bill that carries this description:</p><p>“This bill enacts provisions related to a state-endorsed digital identity” (SEDI).</p><p>Now it’s off to the governor for signature, who has been publicly positive about it. Huge credit to Sen. Cullimore, the bill’s sponsor, and Christopher Bramwell, the state’s Chief Privacy Officer, who helped draft and guide its content. SB 260 will likely soon become the law in Utah, and with it the possibility of an exciting new era for self-sovereign identity&nbsp;(SSI).</p><p>Passage of SB 260 is the partial realization of a dream I’ve had for more than a dozen years, since co-founding <a href="https://www.evernym.com/">Evernym</a> with Jason Law in 2012. It is not the complete realization of that dream, but it lays the necessary groundwork and carries with it momentum to go the distance over the next few&nbsp;years.</p><p>SB 260 does not call for the creation of a new identity system for the state of Utah — yet — that will be done later, hopefully in next year’s session. This bill establishes the principles Utah politicians stand behind — unanimously — that any future Utah statewide identity system must adhere to. If you are an SSI aficionado, you will recognize and love the principles in this&nbsp;bill.</p> Is State-Endorsed Identity (SEDI) Still&nbsp;SSI? <p>For some in the SSI community, the concept of SSI does not involve a state actor — at&nbsp;all.</p><p>To these SSI purists, SSI becomes trustworthy through a <a href="https://en.wikipedia.org/wiki/Web_of_trust">web of trust</a> that replaces the need for a state actor as a root of trust. While theoretically possible — and still a worthwhile end-goal — this ideal vision of SSI isn’t feasible anytime soon. In the meantime, the credibility of government can help accelerate SSI toward an end-goal of not needing government. How? By making ubiquitous the technologies and ceremonies necessary for a web of trust to&nbsp;emerge.</p><p>At the end of the day, if independent control of strong digital identity rests fully with the individual, with no visibility by the state for if, how, or when it is used, then advocates of SSI should be in favor of SSI strengthened by a state endorsement. That is how physical identity works today and how digital identity should work, too. And there’s no reason other, non-government flavors of SSI can’t continue to develop and&nbsp;coexist.</p> State-Endorsed Digital Identity&nbsp;(SEDI) <p>The words “state-endorsed” used throughout this bill carry a lot of meaning; most emerging digital identity systems use the word “issue“ instead of “endorse”. This is a distinction with a profound difference.</p><p>In real life, does a state issue identity or endorse it? When a baby is born, who gives the baby its identifier, its name? The parents do. The state waits for parents to pick a name, puts the name into a certificate, endorses the certificate, then gives it to the parents. Parents determine identity, the state endorses&nbsp;it.</p><p>A couple months ago, a 60 year-old friend of mine decided to change her last name. The decision was hers alone to make, but her new name would not be believable or provable to third parties until she has an endorsement of the new name from the&nbsp;state.</p><p>In real life the state issues credentials, not identity. In this bill Utah acknowledges this reality and explicitly states that individuals first assert identity and only then can the state endorse it. That will be the guardrails for any future state-endorsed digital identity system. (See “<a href="https://medium.com/@rufftimo/sedi-details-for-identity-nerds-e1949af5cc30">SEDI Details for Identity Nerds</a>” for more&nbsp;info.)</p><p>Through SEDI, SB 260 codifies several foundational SSI principles:</p> Control <p>In lines (65) and (120), it is explicit that the individual is in control of their digital identity.</p> Guardianship &amp; Age Verification <p>Lines (35–40) and (97–98) lay the groundwork for digital guardianship, bringing broad digital capabilities for the first time to dependent populations such as children (including infants), the elderly, disabled persons, homeless persons, and&nbsp;more</p><p>Of particular note in Utah, digital guardianship is how age verification can work for all ages and capabilities, something Utah has been on the forefront of, policy-wise. In the future, guardianship could even work for animals and things,&nbsp;too.</p><p>See “<a href="https://libertas.institute/privacy/finally-a-path-forward-for-online-age-verification/">Finally, a Path Forward for Age Verification</a>” by the Libertas Institute for more info about how SB 260 could facilitate age verification.</p> Privacy &amp; No Surveillance <p>SB 260 is explicit that individuals have the right to not be surveilled. This means that SEDI cannot have any form of “phone home”, which differentiates it from the mobile drivers license (mDL) systems based on ISO 18013, and from the federated identity systems now being implemented in Europe and Australia, which all have inherent phone-home surveillance capabilities.</p><p>In lines (78–79), (84–96), (99–102), (116–117), (122–125), and (128–139) the bill details the principles of privacy and autonomy that must guide any future state-endorsed digital identity system in&nbsp;Utah.</p> State-of-the-Art Security <p>Line (113) stipulates that Utah SEDI must incorporate “state-of-the-art safeguards” for protecting identity. Current industry standards are not good enough, and in fact are unacceptably fragile in my view. My interpretation of “state-of-the-art” means something akin to KERI (Key Event Receipt Infrastructure), where breaches are preventable, detectable, and recoverable in ways that current identity standards cannot do, and which is quantum-resistant. (See the ‘<a href="https://medium.com/@rufftimo/sedi-details-for-identity-nerds-e1949af5cc30">Details for Nerds</a>’ for more details about security.)</p> Now the Work&nbsp;Begins <p>In some ways SB 260 is the end of a long, hard journey for me, and the partial realization of an audacious dream. In reality it’s one bill, in one state, that establishes the principles and guardrails for a future identity system without creating one&nbsp;today.</p><p>In any case, it’s the closest thing I’ve seen to true self-sovereign identity being officially and legally embraced at a significant level, an entire state of the United States. The next step is far harder: turning concepts into&nbsp;reality.</p><p>Stay tuned.</p><p>For more technical information, see <a href="https://medium.com/@rufftimo/sedi-details-for-identity-nerds-e1949af5cc30">SEDI Details for Identity&nbsp;Nerds</a>.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*x3UpsjrhELH4iGU6guOwjA.jpeg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   23:30 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.jonudell.net'>Jon Udell</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.jonudell.net/2025/03/07/making-the-fediverse-more-accessible-with-claude-3-7-sonnet/'>Making the Fediverse More Accessible With Claude 3.7 Sonnet</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  A few years ago I abandoned Twitter in favor of Mastodon. Recent events validate that choice and underscore the strategic importance of a decentralized fediverse that can’t be owned by a single corporate or state actor. But while Mastodon meets my needs, much of the Twitter diaspora has gone to Bluesky. That’s fine for now … Continue reading Making the Fediverse More Accessible With Claude 3.7 Sonn
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>A few years ago I abandoned Twitter in favor of Mastodon. Recent events validate that choice and underscore the strategic importance of a decentralized fediverse that can’t be owned by a single corporate or state actor. But while Mastodon meets my needs, much of the Twitter diaspora has gone to Bluesky. That’s fine for now but might not always be. In an article titled “Science Must Step Away From Nationally Managed Infrastructure,” Dan Goodman writes:</p>
<blockquote><p>Many scientists put huge efforts into building networks to communicate with colleagues and the general public. But all that work and the value in those networks was lost when many scientists felt compelled to leave following Elon Musk’s takeover of the platform (now X). The process of rebuilding on Bluesky is underway, but it will take years and may never reach the same critical mass. Even if the transition is successful, the same thing may happen to Bluesky in a few years.</p></blockquote>
<p>How can we prepare for a future migration from Bluesky to Mastodon? Bridgy Fed — a service that enables you to connect together your website, fediverse account and Bluesky account — will help. But Bridgy Fed needs to be easier to use. So I recruited Claude’s new Sonnet 7 model to do that.</p>
<p><a href="https://thenewstack.io/making-the-fediverse-more-accessible-with-claude-3-7-sonnet/">Making the Fediverse More Accessible With Claude 3.7 Sonnet</a></p>
<p><a href="https://jonudell.info/newstack/archive.html">LLM series at The New Stack</a></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   17:54 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 05. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://justatheory.com'>Just a Theory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://justatheory.com/2025/03/extension-ecosystem-summit/'>Extension Ecosystem Summit 2025</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  We’re doing it again! The PostgreSQL Extension Ecosystem Summit returns to
PGConf.dev. We’ll again be looking at indexing, discovery, packaging, and
core support. And we’ll once again be hosting a series of “Mini Summits”
leading up to the event. Join us!
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 


	<a href="https://pgconf.dev">
			
		</a>
	

         
<p>I’m happy to announce that some PostgreSQL colleagues and have once again
organized the <a href="https://www.pgevents.ca/events/pgconfdev2025/schedule/session/241/" title="PGConf.dev: Extensions Ecosystem Summit">Extension Ecosystem Summit</a> at <a href="https://2025.pgconf.dev" title="PostgreSQL Development Conference 2025">PGConf.dev</a> in Montréal on May
13. <a href="https://www.linkedin.com/in/floordrees/">Floor Drees</a>, <a href="https://yrashk.com">Yurii Rashkovskii</a>, <a href="https://www.keithf4.com">Keith Fiske</a> will be on hand to kick
off this <a href="https://en.wikipedia.org/wiki/Unconference" title="Wikipedia: Unconference">unconference</a> session:</p>
  
    <blockquote>
      <p>Participants will collaborate to learn about and explore the ongoing work on
PostgreSQL development and distribution, examine challenges, identify
questions, propose solutions, and agree on directions for execution.</p>
    </blockquote>
  
<p>Going to PGConf.dev? Select it as an “Additional Option” when you <a href="https://www.pgevents.ca/events/pgconfdev2025/register/" title="Register for PGConf.dev or Update your Registration">register</a>,
or <a href="https://www.pgevents.ca/events/pgconfdev2025/register/" title="Register for PGConf.dev or Update your Registration">update your registration</a> if you’ve already registered. Hope to
see you there!</p>
 


	<a href="https://www.eventbrite.com/e/851125899477">
			
		</a>
	

 Extension Ecosystem Mini-Summit 2.0 
<p>We are also once again hosting a series of virtual gatherings in the lead-up
to the Summit, the <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/" title="Postgres Extension Ecosystem Mini-Summit on Meetup">Postgres Extension Ecosystem Mini-Summit</a>.</p>
<p>Join us for an hour or so every other Wednesday starting March 12 to hear
contributors to a variety of community and commercial extension initiatives
outline the problems they want to solve, their attempts to so, challenges
discovered along the way, and dreams for an ideal extension ecosystem in the
future. Tentative speaker lineup (will post updates as the schedule fills in):</p>
 
 March 12: <a href="https://justatheory.com/">David Wheeler</a>, PGXN: “State of the Extension Ecosystem” 
 March 24: <a href="https://peter.eisentraut.org">Peter Eisentraut</a>, Core Team: “Implementing an Extension Search Path” 
 April 9: <a href="https://www.df7cb.de">Christoph Berg</a>, Debian: “Apt Extension Packaging” 
 April 23: <a href="https://www.linkedin.com/in/celeste-horgan-b65b5a1a/">Celeste Horgan</a>, <a href="https://www.linkedin.com/in/sonia-valeja-69517a140/">Sonia Valeja</a>, and <a href="https://www.linkedin.com/in/alexeypalazhchenko/overlay/about-this-profile/">Alexey Palazhchenko</a>: “The
User POV” 
 May 7: <a href="https://www.gabrielebartolini.it">Gabriele Bartolini</a>, CNPG “Extension Management in CloudNativePG” 
 
<p>Join the <a href="https://www.meetup.com/postgres-extensions-ecosystem-mini-summits/" title="Postgres Extension Ecosystem Mini-Summit on Meetup">meetup</a> for details. These sessions will be recorded and Posted to
the <a href="https://www.youtube.com/@pgconfdev">PGConf.dev YouTube</a> and we’ll have again detailed transcripts. Many
thanks to my co-organizers <a href="https://www.linkedin.com/in/floordrees/">Floor Drees</a> and <a href="https://yrashk.com">Yurii Rashkovskii</a>, as well as
the <a href="https://2025.pgconf.dev/about/">PGConf.dev organizers</a> for making this all happen!</p>
<p>Update 2025-04-14: Added the April 23 session topic and panelists.</p>

         

         
             More about… 
             
                 <a href="https://justatheory.com/tags/postgres/">Postgres</a> 
                 <a href="https://justatheory.com/tags/pgxn/">PGXN</a> 
                 <a href="https://justatheory.com/tags/extensions/">Extensions</a> 
                 <a href="https://justatheory.com/tags/pgconf/">PGConf</a> 
                 <a href="https://justatheory.com/tags/montr%C3%A9al/">Montréal</a> 
                 <a href="https://justatheory.com/tags/summit/">Summit</a> 
             
         
     
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://justatheory.com/shared/extension-ecosystem-summit/pgconf.dev.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   00:35 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 03. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://kayaelle.medium.com/'>Kerri Lemole</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.dcconsortium.org/dcc-at-the-2025-1edtech-digital-credentials-summit-e506d7c8051c?source=rss-eccb4b25c339------2'>DCC at the 2025 1EdTech Digital Credentials Summit</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   LCW Experience Badge &amp; Introduction to Open Badges 3.0 Course at Participate  This week is the 2025 Digital Credentials Summit hosted by the organization that shepherds the Open Badges 3.0 standard. This is a great space to meet with implementers, educators, technologists, funders, and policy makers from all over the world to&nbsp;connect.  I will be attending this year and have the privilege 
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*C5f97BW3SnBW2wqXkKcWrA.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>LCW Experience Badge &amp; Introduction to Open Badges 3.0 Course at Participate</p><p>This week is the <a href="https://www.cvent.com/c/abstracts/3a9f6baa-770e-437c-a9bc-dd31785476bf">2025 Digital Credentials Summit </a>hosted by the organization that shepherds the <a href="https://www.imsglobal.org/spec/ob/v3p0">Open Badges 3.0 standard</a>. This is a great space to meet with implementers, educators, technologists, funders, and policy makers from all over the world to&nbsp;connect.</p><p>I will be attending this year and have the privilege to participate in five sessions covering topics like degree data models, trust, identity, open communities and open source software. I’ve listed my sessions at the bottom for those who may be attending.</p><p>In time for the Summit, check out the two projects the DCC team has launched to contribute to advancing understanding Open Badges&nbsp;3.0.</p> LCW Experience Badge <p>The <a href="https://blog.dcconsortium.org/the-lcw-experience-6a5bef5cca4d">LCW Experience</a> is an introduction to the experience of receiving, claiming, and storing of an Open Badges 3.0 badge to the <a href="https://lcw.app/">Learner Credential Wallet</a>. Get started&nbsp;<a href="https://badging.dcconsortium.org/lcw-experience-badge">here</a>.</p> Introduction to Open Badges 3.0 Course at Participate <p><a href="https://medium.com/p/660c2bf922dd/edit">Introduction to Open Badges 3.0 </a>is a short, videos based course hosted at <a href="https://www.participate.com/">Participate.com</a>, a community learning platform that issues Open Badges 3.0. To get started, join the <a href="https://participate.community/tokens/nw2kjzoKZgGh8x7rtcPeGHN1y1WukaNegBmTW47WeC0">The Digital Credentials Consortium Learning Hub</a> and there you will find the course. Watch six short videos and earn a DCC Intro to OBv3&nbsp;badge.</p> Sessions at Digital Credentials Summit <p>The DCC has been busy this year working with our members and colleagues on great projects about digital degrees, issuer trust, open web, and open source software. Here’s a list of these sessions:</p>  March 3, 3:30–4:15 PM MT <a href="https://web.cvent.com/event/2813bf9a-1231-4a65-9a3f-97e1fe604213/websitePage:b02ad9d9-260b-4530-bdd2-b863dbc3921f?session=27796bf9-d038-40a6-9d88-d036011259ff&amp;shareLink=true">Using Open Badges 3.0 to Issue Portable and Verifiable Digital Degrees</a>, with Eric Sembrant from Georgia&nbsp;Tech  March 4, 9:20–10:20 AM MT <a href="https://web.cvent.com/event/2813bf9a-1231-4a65-9a3f-97e1fe604213/websitePage:b02ad9d9-260b-4530-bdd2-b863dbc3921f?session=c7d2bc60-8be1-4ec3-bc4b-5c3add3100b6&amp;shareLink=true">Understanding Issuer Identity in the LER Verifiable Credentials Ecosystem</a>, with Jeanne Kitchens from Credential Engine  March 4, 1–1:40 PM MT <a href="https://web.cvent.com/event/2813bf9a-1231-4a65-9a3f-97e1fe604213/websitePage:b02ad9d9-260b-4530-bdd2-b863dbc3921f?session=b664a83f-4025-4e59-8503-e962408d1932&amp;shareLink=true">Quality in Digital Credentialing: A Multifaceted Approach</a>, panel with National Student Clearinghouse, Territorium, Strada Education Foundation, and Western Governors University  March 4, 3–3:40 PM MT <a href="https://web.cvent.com/event/2813bf9a-1231-4a65-9a3f-97e1fe604213/websitePage:b02ad9d9-260b-4530-bdd2-b863dbc3921f?session=b3bf6991-ecfc-4725-8838-f03444960e00&amp;shareLink=true">The Story of Open — How the Web, the World, and Credentials Thrive When Collaboration &amp; Community is Prioritized</a>, with Kate Giovacchini from&nbsp;ASU  March 4, 4:40–5:20 PM MT, <a href="https://web.cvent.com/event/2813bf9a-1231-4a65-9a3f-97e1fe604213/websitePage:b02ad9d9-260b-4530-bdd2-b863dbc3921f?session=0a4c4991-f6d1-44d8-8139-5c86766567f5&amp;shareLink=true">The Power of Open-Source Badging in Micro-Credentialing</a>, with colleagues from Digital&nbsp;Promise   <p><a href="https://blog.dcconsortium.org/dcc-at-the-2025-1edtech-digital-credentials-summit-e506d7c8051c">DCC at the 2025 1EdTech Digital Credentials Summit</a> was originally published in <a href="https://blog.dcconsortium.org">Digital Credentials Consortium</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*C5f97BW3SnBW2wqXkKcWrA.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:13 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>



<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.dcconsortium.org/open-badges-3-0-basics-watch-learn-and-earn-on-participate-660c2bf922dd?source=rss-eccb4b25c339------2'>Open Badges 3.0 Basics: Watch, Learn, and Earn on Participate</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Join the Digital Credentials Consortium Learning&nbsp;Hub  The Digital Credential has started a series of short posts explaining the characteristics and trust models of W3C Verifiable Credentials and Open Badges 3.0. To accompany this series, we collaborated with our colleagues as Participate to create the The Digital Credentials Consortium Learning Hub and offer a short course, “Introduction to O
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*It45Esz2T1Ve5DfuyKoS4A.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

 Join the Digital Credentials Consortium Learning&nbsp;Hub <p>The Digital Credential has started a series of short posts explaining the characteristics and trust models of <a href="https://blog.dcconsortium.org/explaining-verifiable-credentials-and-open-badges-3-0-34ae898b98b2">W3C Verifiable Credentials and Open Badges 3.0</a>. To accompany this series, we collaborated with our colleagues as <a href="https://www.participate.com/">Participate</a> to create the <a href="https://participate.community/tokens/nw2kjzoKZgGh8x7rtcPeGHN1y1WukaNegBmTW47WeC0">The Digital Credentials Consortium Learning Hub</a> and offer a short course, “Introduction to Open Badges 3.0”. To get started, join the <a href="https://participate.community/tokens/nw2kjzoKZgGh8x7rtcPeGHN1y1WukaNegBmTW47WeC0">The Digital Credentials Consortium Learning Hub</a> and there you will find the&nbsp;course.</p><p>The course consists of six short (approx 2 min), informal videos. Once you watch all six videos, click on the bell in the right top corner to see your “DCC Introduction to OBV3 badge”. After installing and setting up the <a href="https://lcw.app/">Learner Credential Wallet</a> on your mobile device, click on “Add to Wallet” and scan the QR code that displays in the pop up with the camera on your phone or using the “Scan QR code” option in the ‘Add Credential screen of the LCW app. If you’d like to practice claiming an Open Badges 3.0 badge first, try the <a href="https://kayaelle.medium.com/the-lcw-experience-6a5bef5cca4d">LCW Experience</a>.</p><p>Thanks to the <a href="https://www.participate.com/">Participate</a> team for setting this up with us. Participate is one of the first (maybe THE first) community learning platform to issue Open Badges 3.0. Their team has been involved with Open Badges since the very beginning. They host the <a href="https://participate.community/tokens/z3EvdXD_QO0eWQGzQ2V83YPw01chchEjidkUhiQfQzc">Open Recognition Community</a> where you will find many with a long history in this space who are discussing the value of Open Badges at the cross sections of formal, informal, nonformal learning.</p><p>For more information on the DCC, including membership, projects, and events, visit our <a href="http://digitalcredentials.mit.edu/">website</a>, <a href="https://wiki.dcconsortium.org/">wiki</a>, and <a href="https://www.linkedin.com/company/dccconsortium">Linkedin</a>. To receive our newsletter and invitations to webinars and events sign up for our <a href="https://mit.us6.list-manage.com/subscribe?u=ad81d725159c1f322a0c54837&amp;id=3621913fe4">community mailing&nbsp;list</a>.</p> <p><a href="https://blog.dcconsortium.org/open-badges-3-0-basics-watch-learn-and-earn-on-participate-660c2bf922dd">Open Badges 3.0 Basics: Watch, Learn, and Earn on Participate</a> was originally published in <a href="https://blog.dcconsortium.org">Digital Credentials Consortium</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*It45Esz2T1Ve5DfuyKoS4A.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:12 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://ggreve.medium.com/'>Georg C. F. Greve</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://ggreve.medium.com/a-future-for-self-sovereign-identity-c237caa5e46f?source=rss-f390f7bc98b4------2'>A future for Self Sovereign Identity?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Symbolic representation of Web Security applied to&nbsp;SSI. Many children in Europe grew up with the tales of Baron Münchhausen, who claims to have lifted himself and his horse out of a mire by pulling his own hair. The image is so powerful because the problem of the circular dependency is so clearly visible. In real life, circular dependencies are often far less&nbsp;obvious.  Which is why the fi
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/0*k7oiynNT2-M61k1X'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

Symbolic representation of Web Security applied to&nbsp;SSI.<p>Many children in Europe grew up with the tales of Baron Münchhausen, who claims to have lifted himself and his horse out of a mire by pulling his own hair. The image is so powerful because the problem of the circular dependency is so clearly visible. In real life, circular dependencies are often far less&nbsp;obvious.</p><p>Which is why the <a href="https://ggreve.medium.com/self-sovereign-identity-over-before-it-started-661b4b0dbdc6">first article</a> in this series was primarily focused on looking behind the SSI smoke and mirrors around Web based identifiers and communication protocols. The resulting discussions in the <a href="https://www.weboftrust.info/">Rebooting the Web Of Trust (RWOT)</a> community were quite enlightening, and included a deeper look at the <a href="https://ec.europa.eu/digital-building-blocks/sites/display/EUDIGITALIDENTITYWALLET/Technical+Specifications">EU Digital Identity Wallet Technical specifications</a>.</p><p>One of the mirrors basically broke when claims of OpenID4VC supporting decentralized identifiers were shattered when someone pointed out that while the EU Wallet is marketed on digital sovereignty and privacy, but in reality does not <a href="https://github.com/openid/federation-wallet/issues/43#issuecomment-2539862593">does not allow decentralized identifiers</a>:</p>The current EUDI approach: No decentralized identifiers allowed<p>So while it was clear that OpenID4VC and did:web* do not qualify as decentralized, Self-Sovereign Identity, some people advocated to just embrace the false marketing in the hope that it would create wider acceptance and the appearance of adoption for&nbsp;SSI.</p><p>But has that approach ever really&nbsp;worked?</p><p>More often this kind of “sovereignwashing” appears to run a high risk of creating false expectations, disappointment. Which would ultimately cement the status quo of the federated platform identity lock-in for the next 20 years. As a community we should focus on building actual decentralized identifiers, communication protocols, and applications.</p><blockquote>Because the true social and economic value of SSI is not just in the identity layer itself, it is in the decentralized applications enabled as a&nbsp;result.</blockquote><p>Some of which would be in direct competition to the champions of the platform age, who are investing their financial and political capital into OpenID4VC and Web based identifiers to prevent that competition from ever getting off the ground. A classic “old industry vs new technologies” battle.</p><p>There are real opportunity costs across most of economy and society if the old encumbents manage to postpone or kill innovation.</p>Symbolic representation of eIDAS 2.0 after successful lobbying by the platforms and trust intermediaries Security and privacy for a globally networked society <p>Technology and corresponding security have been head to head in a special kind of race for a long time, dating back to an Egyptian inscription around 1900 BC in the main chamber of the tomb of Khnumhotep II, over Julius Caesar using a ROT-3 cypher in 100 BC, all the way to the famous Enigma machine used in World War II. The more people potentially had access to a message, the harder the encryption had to&nbsp;become.</p><p>The encryption used by Julius Caesar was not particularly strong, because it relied on a supposedly secret algorithm. Once parties know the secret, encryption and decryption become trivial. Over time this moved to well-known algorithms using shared secrets. And even though the shared secrets are more complex on today’s internet, this fundamental principle hasn’t&nbsp;changed:</p><blockquote>If you know the shared secret, and can intercept the encrypted message, you will be able to read, and also impersonate and falsify communication.</blockquote><p>In contrast, Engima was quite strong for its day because it combined a rotating cypher with a codebook that was famously carried by U-Boats allowing them to choose the correct settings. Literally handed over to the commander of the boat by hand in a secure location before departure, these code books effectively represented a cryptographic key, shared over a second channel — the physical handover.</p><p>Which makes any well-designed encryption system almost impossible to break. Unless, of course, you have intimate knowledge of the inner workings of the rotating cypher, and can guess certain messages, like weather reports, to then use brute force to arrive back at the settings for the day. Those settings then allowed to read other messages, which would otherwise have been unbreakable.</p>Digital identity should be based on an&nbsp;advance<p>In other words: The cryptography of the Enigma machine itself was solid, and essentially unbroken. But the Allied Forces were able to exploit structural weaknesses designed into the operation of Engima to attack the key generation for the&nbsp;day.</p> Security in Swiss Healthcare <p>That particular race accelerated when the Internet was born. In 1996, when the internet was still young, the US Congress deliberated and passed the Health Insurance Portability and Accountability Act (HIPAA). That same year, the <a href="https://www.fmh.ch/">Swiss Medical Association (FMH)</a>, realized patient data had to be better secured on the internet, leading to the creation of <a href="https://hin.ch">Health Info Net (HIN)</a>. Starting from encrypted email, Swiss doctors have relied on HIN for decades to keep their patient data&nbsp;safe.</p><p>But technology years are a lot like dog years. And 28 years is a very long&nbsp;time.</p><p>HIN is constantly working to innovate and improve its solutions. Which is how <a href="https://vereign.com">Vereign</a>, working closely with our partner <a href="https://www.linkedin.com/company/more-than-bits-gmbh">More than Bits</a>, started to run some POCs with HIN in 2023, and ended up working all of 2024 almost exclusively for the Swiss healthcare sector.</p><blockquote>Our challenge: Design a system that starts from what today’s users are used to, while re-thinking the system architecture using SSI and modern data ecosystem architectures, based on the work we had done for&nbsp;Gaia-X.</blockquote><p>The starting point was obvious: Email is the world’s largest distributed identity database and communication protocol. It is the use case with which HIN started, and it is the singular product that all users rely on mutliple times each day to communicate with colleagues, laboratories, and patients.</p><p>Email is also facing challenges of concentration and capture by the large, federated platforms. And its lack of an identity layer has made it a fertile ground for attacks by malicious governments, corporations, and common criminals.</p><p>Vereign showcased its first prototype to harden email using SSI in 2019, which earned us a nomination as the hottest new innovation for the Swiss <a href="https://www.digitaleconomyaward.ch/">Digital Economy Award</a> in Zurich. COVID-19 had other plans, but our experience proved invaluable when working on the POCs with&nbsp;HIN.</p><p>This time, we built out peer to peer email exchange via <a href="https://didcomm.org/">DIDComm</a>. Secure, encrypted, authentic and designed in a way that it can be plugged into any legacy email system to gradually switch to a new, identity verified transport layer reaching all the way to the people themselves.</p> From prototyping to production: Quest for the identifier <p>We built these prototypes using did:web, because it is a great placeholder to stand in for decentralized identifiers while rapidly prototyping around user flow and experience.</p><p>But from the onset it was clear that did:web would not be the choice for production, because for all the reasons also <a href="https://medium.com/@ggreve/self-sovereign-identity-over-before-it-started-661b4b0dbdc6">highlighted in the last&nbsp;article</a>:</p><blockquote>Web based identifiers must never be used for personal identity.</blockquote><p>Our preferred choice would have been <a href="https://keri.one">KERI</a> due to its robust security and privacy architecture. But with the official implementation being Python only, we had concerns about efforts required in supporting a secure, long term solution across the range of platforms we anticipated.</p><p>The <a href="https://github.com/THCLab/keriox">Rust implementation</a> by the Human Colossus Foundation fared better on that front. But there seems to be a rift in the community, causing concerns of diverging implementations, as well as long-term support. Which are exacerbated by the choice for European Public License&nbsp;(EUPL).</p><p>We could not find information about adoption, nor community. And finally, the security of KERI as a concept critically depends on the networks of Witnesses and Watchers, for which we could not find information about size, health and long term viability of these networks for either implementation.</p><p>Had we chosen KERI in February 2024, we would not have been able to go productive before these issues had been resolved. And our time line dictated we had to be ready for initial production by late 2024. As a result, KERI was a non-starter.</p><p>Other methods, such as did:indy, have been in decline for some time, and Sovrin is <a href="https://www.linkedin.com/posts/sovrin-foundation_to-the-sovrin-community-after-seven-years-activity-7253082729443930112-069C?utm_source=share&amp;utm_medium=member_desktop">shutting down</a> in just a couple of weeks. Methods like did:peer on the other hand are not great in scenarios where long-lived connections are desirable.</p><blockquote>So in the end, our search for production ready decentralized identifiers that could safely be used for natural persons left us empty&nbsp;handed.</blockquote>A classic. And good&nbsp;advice. Ignoring XKCD <p>The <a href="https://xkcd.com/927/">competing standards</a> comic by XKCD is a classic. As far as rules go, it is a good one. But there are no rules without exceptions. Having exhausted every other path, we decided to ignore XKCDs’ best practice. Only, we did not aim to create the universal solution — that’s KERI — but to create the simplest possible, yet still sufficiently safe identifier for the requirements of our specific use&nbsp;case.</p><p>Like any good design, it should build on existing technologies as much as possible, be simple enough to be implemented within a reasonable time frame, and to be supportable for at least 5–10 years, when potentially it would be replaced by something better.</p> Designing a decentralized identifier <p>Our requirements asked for an identifier that was truly secure and private. We explicitly sought to minimize dependencies on infrastructure such as DNS, Web Servers and Certificate Authorities. Blockchain would have fit these criteria, but we do not require a global consensus. All we needed was a decentralized storage system that would guarantee integrity and availability of&nbsp;records.</p><p><a href="https://git-scm.com/">Git</a> might have been an option. It is <a href="https://en.wikipedia.org/wiki/Content-addressable_storage">Content-Addressable Storage</a>, so objects are referenced by their hash, any modification creates a new object. But Git would add unnecessary overhead, and there is a central repository. The <a href="https://ipfs.tech/">Interplanetary File System (IPFS)</a> on the other hand is built for peer to peer distribution between nodes without a central&nbsp;server.</p><p>Like Git, IPFS is built on Content-Addressable Storage (CAS). Objects are referenced by their sha256 hashes. Users can request data at any node, and if that node does not have this particular object, it will use peer-to-peer network connectivity between nodes to obtain a copy of the data and provide it to the user. It is open, verifiable, and resilient.</p><blockquote>Its function allows DID documents to be uploaded onto any node and be referenced by their hash on any node in the network. Modifications to the document modify the hash, so documents are integrity protected by design. Simultaneously, the entire DID storage and distribution mechanism is robust regarding the well-known attacks against Web based identifiers.</blockquote><p>In addition, the hash for the document contains no Personally Identifiable Information (PII) and unless we’d make the mistake of adding PII to the DID documents themselves, our design would not expose any kind of PII anywhere.</p><p>Of course we were not the first, nor the only ones to realize the potential of IPFS for decentralized identifiers. There has been a prior attempt at using IPFS for DID documents, the <a href="https://did-ipid.github.io/ipid-did-method/">IPID DID Method</a>. But it never got much traction, and its use of the <a href="https://docs.ipfs.tech/concepts/ipns/">InterPlanetary Name System (IPNS)</a> made it less robust. Also, it did not have provisions for the rotation of keys, which is crucial for long-term connections with the same identifier, as well as the ability to switch wallets or upgrade crypto algorithms.</p>Swiss Healthcare: Innovating together toward the gold standard of decentralized, secure, private identity and applications An identifier for Sovereign Data Exchange&nbsp;(SVDX) <p>The result is did:svdx, our DID method for Sovereign Data Exchange.</p><p>Agents generate their active key locally, as well as a key that can be used for the update of the identifier later. The public key of the first key is used as the persistent identifier, creating a persistent Autonomous Identifier (AID).</p><p>The second key, which is used for the update of the identifier, is never shared. Only its hash is declared in the document as a next key commitment. Because this key is never actively used until it is time to rotate, it is well protected against being compromised.</p><p>Each revision of the decentralized identity documents representing a Decentralized Identifier has a Content Identifier (CID) when stored in IPFS, so the resulting identifier is always the combination of the AID with the CID of the latest revision of the identifier.</p><p>Since each revision of the identifier refers back to the previous version by its CID, the result is a sha-256 hash based Key Event Chain of IPFS objects, all the way back to the inception document, the root of the AID in question.</p><p>did:svdx:z6MknHKiY477mH97qryHv3zjuHaTLvBbbp6tHS5SvZv67uR4:QmecqVGBxvW7gjffxmYTGFZNPmJcWmYPdD8azB1cZYaY6F</p><p>Because the identifier also contains the CID of the current state, starting verification of the Key Event Chain is trivial: Just pull the corresponding object out of IPFS and verify. Check for ancestor, rinse and repeat until you’re at the beginning of the chain. Check whether the AID matches the initial key.&nbsp;Done.</p> Trivial to implement in web based tool&nbsp;chains <p>No native IPFS support? No problem. Just pick one of the <a href="https://ipfs.github.io/public-gateway-checker/">public IPFS gateways</a>, and with a single request pull the DID document, e.g. <a href="https://ipfs.io/ipfs/QmecqVGBxvW7gjffxmYTGFZNPmJcWmYPdD8azB1cZYaY6F">https://ipfs.io/ipfs/QmecqVGBxvW7gjffxmYTGFZNPmJcWmYPdD8azB1cZYaY6F</a>.</p><p>Thanks to content based addressing, you will get the same document no matter which gateway you use. And you’re welcome to use as many of them as you would like to compare. Although for production use cases it is highly recommended to run your own, which is&nbsp;trivial.</p><blockquote>In other words, IPFS allows to integrate classic web based tool chains with decentralized storage and delivery of integrity protected DID documents. It’s as easy as any of the did:web* methods to work with, but does not suffer from the attack surfaces of DNS, TLS and Certificate Authorities.</blockquote><p>In addition, it is robust against a number of DDOS scenarios, allows for low impact self-hosting, and eliminates the web server as a central point of attack, surveillance and compromise.</p><p>Also, it plays well with DIDComm and other communication protocols, but if you really require web based interaction protocols, they can also be encoded into the identifier. But unlike web based identifiers, exchanging key material via did:svdx mitigates a substantial number of attack scenarios for web connection protocols.</p> Layering trust <p>By design did:svdx contains zero personal information. It is deliberately focused on secure key exchange of an Autonomous Identifier, only.</p><p>So any relationship starts from a reliable assumption the AID controllers have a strong connection to one another and can maintain it over a longer period of time, including throughout key rotation and changes in cryptography. But they start from zero trust in one&nbsp;another.</p><p>Trust is built gradually, through Verifiable Presentations securely exchanged over the connection. Similar to what Christopher Allen describes as “<a href="https://www.lifewithalacrity.com/article/progressive-trust/">Building Trust in Gradients</a>.”</p><p>For SVDX, given it is built for a true P2P, decentralized ecosystem, we surmise that the party initiating a connection first authenticates itself toward the recipient of the connection request before requesting reciprocal information. That should also make data mining or identifier scraping much&nbsp;harder.</p> Limits of&nbsp;did:svdx <p>For any design, it is crucial to know its limits. Firstly, the identifier specification does not contain any of the multi-signature capabilities of systems like KERI. Because we did not require it for our use case at hand, we pushed that complexity, along the complexity of secure restore and key rotation, onto the clients — which we control for the use case at&nbsp;hand.</p><p>Also, while IPFS plays a role similar to that of Witnesses in KERI, there are no Watchers. So there is no built-in detection of duplicity, as Sam Smith calls it. And while parties can update each other on key rotations using DIDComm, allowing each other to verify they are still talking to the same party, the design has no built-in protections against a controller forking their identity.</p><p>For our use case this was not an issue, because there is a central catalogue for the ecosystem to allow looking up the latest, known version of an AID. Which is not ideal for some scenarios. But we considered the solution good enough for what we needed to achieve, given that all controllers need to also maintain their identity and trustworthiness with HIN as the central ecosystem fiduciary.</p><p>That said, it should be possible to design a robust duplicity detection on top of did:svdx, and there may even be scenarios where duplicity is not a primary concern as long as agents always ensure to only consider the latest version of an AID authoritative.</p><blockquote>So did:svdx is not a replacement for KERI. But it is a replacement for web based DID methods, offering far better security, and similar efforts of adoption and support. From our own experience we know it took around 6-8 weeks to implement in JavaScript.</blockquote> What’s next? <p>The first application using did:svdx in production will have ramped up by April&nbsp;2025.</p><p>By mid 2025 we expect hundreds of thousands of production messages sent each month containing verifiable credentials backed by did:svdx. Our roadmap has us building out additional applications until all the institutions and eventually all the patients in Switzerland will have identifiers within the next 2-3&nbsp;years.</p><p>We have already Open Sourced the initial implementation and will continue to add additional implementations. Also, we would love to finalize the specification so that it can be maximally useful to others. And there may be features that would be required for additional use cases, as well as community-based methods for duplicity detection.</p> Open questions   Where is the right place to finalize, publish and maintain did:svdx?  Who would be interested in participating?  What are the critical capabilities that may still be&nbsp;missing?  What kind of best practice operational RFCs should we develop as a community?  <p>If you’re at <a href="https://diceurope.org/">DICE in Zurich</a> this year, I’d love to sit down and discuss these questions with you — alongside everything else you would like to know about our vision for the Sovereign Data Exchange.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/0*k7oiynNT2-M61k1X'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:34 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://kayaelle.medium.com/'>Kerri Lemole</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.dcconsortium.org/the-lcw-experience-6a5bef5cca4d?source=rss-eccb4b25c339------2'>The LCW Experience</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   The Digital Credentials Consortium, our team and our members, are aiming to advance the understanding &amp; use of privacy-enhanced, portable, verifiable digital credentials like Open Badges 3.0. We’ve noticed that while many are interested in the opportunities LERs as digital credentials may offer, many haven’t yet experienced what it is like to claim and accept an Open Badge 3.0 credential with 
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/1*wUjr6cZfljV8fEVBp7oQmQ.png'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>The Digital Credentials Consortium, our team and our members, are aiming to advance the understanding &amp; use of privacy-enhanced, portable, verifiable digital credentials like <a href="https://blog.dcconsortium.org/explaining-verifiable-credentials-and-open-badges-3-0-5bf2f482b383">Open Badges 3.0</a>. We’ve noticed that while many are interested in the opportunities LERs as digital credentials may offer, many haven’t yet experienced what it is like to claim and accept an Open Badge 3.0 credential with a digital wallet like the <a href="https://lcw.app/">LearnerCredential Wallet</a>&nbsp;(LCW).</p><p>We created the <a href="https://badging.dcconsortium.org/lcw-experience-badge">LCW Experience</a> to make it as easy as possible to try this out. It uses the open source <a href="https://wiki.dcconsortium.org/app/page/15C5lTeh5KkszSqfnc0B_KHUf4rte0ub0?p=1h1VJHHv2zSe0n9Ltg-KQevSuqWbGcExy">DCC Issuing System</a> and Learner Credential Wallet to demonstrate the experience of requesting and storing an Open Badges 3.0. You can learn more about how it works&nbsp;<a href="https://badging.dcconsortium.org/lcw-experience-badge#explainer">here</a>.</p><p>To add the LCW Experience Badge to your Learner Credential Wallet, follow these&nbsp;steps:</p>  Install &amp; set up the <a href="https://lcw.app/">LCW.app</a> on your mobile&nbsp;device  Go to the <a href="https://badging.dcconsortium.org/lcw-experience-badge">LCW Experience site </a>to enter your name &amp; email&nbsp;address  <p>After you click the “Award Badge” button, a message will be sent to the email you provided with a link to your claim page. There you will find instructions on how to use your LCW app to claim your credential.</p><p>The email notification is a typical way to let earners know that they have been awarded a badge. If you would rather go to the claim page directly, use this link, replacing the name with your own (or someone you’d like to send it to): <a href="https://badging.dcconsortium.org/collect?recipientName=FirstName+LastName">https://badging.dcconsortium.org/collect?recipientName=FirstName+LastName</a></p><p>Note that your name &amp; email are not saved in our system. The email is used to send you the message and your name is included so that it will show who it was issued to in the accepted badge data and&nbsp;display.</p><p>Big shout out to the DCC team, <a href="https://www.linkedin.com/in/james-chartrand-4544b717/">James Chartrand</a>, <a href="https://www.linkedin.com/in/alexhiguera/">Alex Higuera</a>, <a href="https://www.linkedin.com/in/dzagidulin/">Dmitri Zagidulin</a>, and <a href="https://www.linkedin.com/in/gwalsh311/">Gillian Walsh</a> for developing this project and brainstorming fun, accessible, and pragmatic approaches to learning about Verifiable Credentials and Open Badges. We hope you enjoy this experience!</p><p>For more information on the DCC, including membership, projects, and events, visit our <a href="http://digitalcredentials.mit.edu/">website</a>, <a href="https://wiki.dcconsortium.org/">wiki</a>, and <a href="https://www.linkedin.com/company/dccconsortium">Linkedin</a>. To receive our newsletter and invitations to webinars and events sign up for our <a href="https://mit.us6.list-manage.com/subscribe?u=ad81d725159c1f322a0c54837&amp;id=3621913fe4">community mailing&nbsp;list</a>.</p> <p><a href="https://blog.dcconsortium.org/the-lcw-experience-6a5bef5cca4d">The LCW Experience</a> was originally published in <a href="https://blog.dcconsortium.org">Digital Credentials Consortium</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/1*wUjr6cZfljV8fEVBp7oQmQ.png'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   14:14 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2025/03/03/implement-phone-verification-2fa-using-asp-net-core-identity/'>Implement Phone verification, 2FA using ASP.NET Core Identity</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  This post shows how to implement phone (SMS) verification and two-factor authentication (2FA) using ASP.NET Core Identity. The solution integrates phone-based verification and 2FA mechanisms. The implementation uses ASP.NET Core Identity’s extensibility to incorporate SMS-based verification during user registration and login processes. SMS is no longer a recommended authentication method due to sec
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>This post shows how to implement phone (SMS) verification and two-factor authentication (2FA) using ASP.NET Core Identity. The solution integrates phone-based verification and 2FA mechanisms. The implementation uses ASP.NET Core Identity’s extensibility to incorporate SMS-based verification during user registration and login processes. SMS is no longer a recommended authentication method due to security risks but does provide a good solution for some business cases or user flows like onboarding phone users or phone applications, frontline workers with no desktop or other such solutions with limited security possibilities.</p>



<p></p>



<p>Code: <a href="https://github.com/damienbod/IdentityOidcPhone2fa">https://github.com/damienbod/IdentityOidcPhone2fa</a></p>



<p></p>



 Setup 



<p>The ASP.NET Core Identity application integrates the SMS provider using the Identity PhoneNumberTokenProvider and an SMS verification service. </p>


  builder.Services.AddDbContext&lt;ApplicationDbContext&gt;(options =&gt;
	options.UseSqlServer(builder.Configuration.GetConnectionString("DefaultConnection")));

builder.Services.Configure&lt;EmailSettings&gt;(builder.Configuration.GetSection("EmailSettings"));
builder.Services.AddTransient&lt;IEmailSender, EmailSender&gt;();

builder.Services.Configure&lt;SmsOptions&gt;(builder.Configuration.GetSection("SmsOptions"));

var authorization = Convert.ToBase64String(Encoding.ASCII.GetBytes(
	$"{builder.Configuration["SmsOptions:Username"]}:{builder.Configuration["SmsOptions:Password"]}"));

builder.Services.AddHttpClient(Consts.SMSeColl, client =&gt;
{
	client.BaseAddress = new Uri($"{builder.Configuration["SmsOptions:Url"]}");
	client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue("Basic", authorization);
});

builder.Services.AddScoped&lt;SmsProvider&gt;();

builder.Services.AddIdentity&lt;ApplicationUser, IdentityRole&gt;()
	.AddEntityFrameworkStores&lt;ApplicationDbContext&gt;()
	.AddTokenProvider&lt;DataProtectorTokenProvider&lt;ApplicationUser&gt;&gt;(TokenOptions.DefaultProvider)
	.AddTokenProvider&lt;AuthenticatorTokenProvider&lt;ApplicationUser&gt;&gt;(TokenOptions.DefaultAuthenticatorProvider)
	.AddTokenProvider&lt;PhoneNumberTokenProvider&lt;ApplicationUser&gt;&gt;(Consts.Phone)
	.AddTokenProvider&lt;EmailTokenProvider&lt;ApplicationUser&gt;&gt;(Consts.Email);
  


<p></p>



<p>The ApplicationUser needs some new properties to support multiple authentication methods. The properties are used to allow a user to use the selected authentication method or force an authentication on a OpenID Connect client.</p>


  public bool Phone2FAEnabled { get; set; }
public bool Email2FAEnabled { get; set; }
public bool AuthenticatorApp2FAEnabled { get; set; }
public bool Passkeys2FAEnabled { get; set; }
  


<p></p>



<p>An SMS service are used to integrate the SMS, the SmsProvider class. In this demo, the <a href="https://ecall-messaging.com/">eColl</a> messaging service is used to send SMS. The implementation and the configuration would vary if you use a different service.</p>



<p>The SmsProvider is used to verify a phone number, to enable SMS 2FA and to force SMS 2FA. The service uses a HttpClient to access the SMS service rest API. </p>


  using IdentityProvider.Models;
using Microsoft.AspNetCore.Identity;
using Microsoft.Extensions.Options;

namespace IdentityProvider.Services;

public class SmsProvider
{
    private readonly HttpClient _httpClient;
    private readonly UserManager&lt;ApplicationUser&gt; _userManager;
    private readonly SmsOptions _smsOptions;
    private readonly ILogger&lt;SmsProvider&gt; _logger;

    private const string Message = "message";

    public SmsProvider(IHttpClientFactory clientFactory,
        UserManager&lt;ApplicationUser&gt; userManager,
        IOptions&lt;SmsOptions&gt; smsOptions,
        ILogger&lt;SmsProvider&gt; logger)
    {
        _httpClient = clientFactory.CreateClient(Consts.SMSeColl);
        _userManager = userManager;
        _smsOptions = smsOptions.Value;
        _logger = logger;
    }

    public async Task&lt;(bool Success, string? Error)&gt; Send2FASmsAsync(ApplicationUser user, string phoneNumber)
    {
        var code = await _userManager.GenerateTwoFactorTokenAsync(user, Consts.Phone);
        var ecallMessage = new EcallMessage
        {
            To = phoneNumber,
            From = _smsOptions.Sender,
            Content = new EcallContent
            {
                Text = $"2FA code: {code}"
            }
        };

        var result = await _httpClient.PostAsJsonAsync(Message, ecallMessage);

        string? messageResult;
        if (result.IsSuccessStatusCode)
        {
            messageResult = await result.Content.ReadAsStringAsync();
        }
        else
        {
            _logger.LogWarning("Error sending SMS 2FA, {ReasonPhrase}", result.ReasonPhrase);
            return (false, result.ReasonPhrase);
        }

        return (true, messageResult);
    }

    public async Task&lt;(bool Success, string? Error)&gt; StartVerificationAsync(ApplicationUser user, string phoneNumber)
    {
        var token = await _userManager.GenerateChangePhoneNumberTokenAsync(user, phoneNumber);
        var ecallMessage = new EcallMessage
        {
            To = phoneNumber,
            From = _smsOptions.Sender,
            Content = new EcallContent
            {
                Text = $"Verify code: {token}"
            }
        };

        var result = await _httpClient.PostAsJsonAsync(Message, ecallMessage);

        string? messageResult;
        if (result.IsSuccessStatusCode)
        {
            messageResult = await result.Content.ReadAsStringAsync();
        }
        else
        {
            _logger.LogWarning("Error sending SMS for phone Verification, {ReasonPhrase}", result.ReasonPhrase);
            return (false, result.ReasonPhrase);
        }

        return (true, messageResult);
    }

    public async Task&lt;bool&gt; CheckVerificationAsync(ApplicationUser user, string phoneNumber, string verificationCode)
    {
        var is2faTokenValid = await _userManager
            .VerifyChangePhoneNumberTokenAsync(user, verificationCode, phoneNumber);

        return is2faTokenValid;
    }

    public async Task&lt;(bool Success, string? Error)&gt; EnableSms2FaAsync(ApplicationUser user, string phoneNumber)
    {
        var token = await _userManager.GenerateChangePhoneNumberTokenAsync(user, phoneNumber);
        var message = $"Enable phone 2FA code: {token}";

        var ecallMessage = new EcallMessage
        {
            To = phoneNumber,
            From = _smsOptions.Sender,
            Content = new EcallContent
            {
                Text = message
            }
        };

        var result = await _httpClient.PostAsJsonAsync(Message, ecallMessage);

        string? messageResult;
        if (result.IsSuccessStatusCode)
        {
            messageResult = await result.Content.ReadAsStringAsync();
        }
        else
        {
            _logger.LogWarning("Error sending SMS to enable phone 2FA, {ReasonPhrase}", result.ReasonPhrase);
            return (false, result.ReasonPhrase);
        }

        return (true, messageResult);
    }
}
  


<p></p>



 Flow 1: Verify phone 



<p>Once a user has authenticated with email and password, the user can verify a phone. To verify the phone, the user MUST be authenticated. If not, a malicious program may send multiple SMS and cause financial harm.   The Add phone number link can be used to start the verification process.</p>







<p></p>



<p>The VerifyPhone Razor page allows the user to enter an mobile phone number to send the SMS. This should be validated for real phone numbers at least. The StartVerificationAsync method is used to send the SMS. The ASP.NET Core Identity method GenerateChangePhoneNumberTokenAsync is used to generate the challenge for the verification.</p>


  var user = await _userManager.GetUserAsync(User);
if (user == null)
{
    return NotFound($"Unable to load user with ID '{_userManager.GetUserId(User)}'.");
}

var result = await _client.StartVerificationAsync(user, Input.PhoneNumber);
  


<p></p>



<p>The UI is not styled, just uses the standard bootstrap styles.</p>







<p></p>



<p>The confirm Razor Page accepts the verification code which was sent to the phone and uses the VerifyAndProcessCode method to validate. The ASP.NET Core Identity VerifyChangePhoneNumberTokenAsync method is used to validate the code.</p>


    public async Task&lt;IActionResult&gt; OnPostAsync()
  {
      if (!ModelState.IsValid)
      {
          return Page();
      }

      try
      {
          if (Input.PhoneNumber != null &amp;&amp; Input.VerificationCode != null)
          {
              return await VerifyAndProcessCode(Input.PhoneNumber, Input.VerificationCode);
          }
          else
          {
              ModelState.AddModelError("", "Input.PhoneNumber or Input.VerificationCode missing");
          }
      }
      catch (Exception)
      {
          ModelState.AddModelError("", "There was an error confirming the code, please check the verification code is correct and try again");
      }

      return Page();
  }

  private async Task&lt;IActionResult&gt; VerifyAndProcessCode(string phoneNumber, string code)
  {
      var applicationUser = await _userManager.GetUserAsync(User);

      if (applicationUser != null)
      {
          var validCodeForUserSession = await _client.CheckVerificationAsync(applicationUser,
              phoneNumber, code);

          return await ProcessValidCode(applicationUser, validCodeForUserSession);
      }
      else
      {
          ModelState.AddModelError("", "No user");
          return Page();
      }
  }

  private async Task&lt;IActionResult&gt; ProcessValidCode(ApplicationUser applicationUser, bool validCodeForUserSession)
  {
      if (validCodeForUserSession)
      {
          var phoneNumber = await _userManager.GetPhoneNumberAsync(applicationUser);
          if (Input.PhoneNumber != phoneNumber)
          {
              await _userManager.SetPhoneNumberAsync(applicationUser, Input.PhoneNumber);
          }

          applicationUser.PhoneNumberConfirmed = true;
          var updateResult = await _userManager.UpdateAsync(applicationUser);

          if (updateResult.Succeeded)
          {
              return RedirectToPage("ConfirmPhoneSuccess");
          }
          else
          {
              ModelState.AddModelError("", "There was an error confirming the verification code, please try again");
          }
      }
      else
      {
          ModelState.AddModelError("", "There was an error confirming the verification code");
      }

      return Page();
  }
  


<p></p>



<p>The UI displays the input for the code and the number it was sent to.</p>







<p></p>



 Flow 2: Enable phone 2FA 



<p>Once the phone is verified, it can be used for an SMS 2FA. </p>







<p></p>



<p>The EnableSms2FaAsync method is used to enable the SMS 2FA.</p>


  var user = await _userManager.GetUserAsync(User);
if (user == null)
{
 return NotFound($"Unable to load user with ID '{_userManager.GetUserId(User)}'.");
}

if (user.PhoneNumber != Input.PhoneNumber)
{
 ModelState.AddModelError("Input.PhoneNumber", 
	"Phone number does not match user user, please update or add phone in your profile");
}

await _smsVerifyClient.EnableSms2FaAsync(user, Input.PhoneNumber!);

return RedirectToPage("./VerifyPhone2Fa", new { Input.PhoneNumber });
  


<p></p>



<p>The EnablePhone2Fa Razor page is used to validate the phone number before activating the 2FA.</p>







<p></p>



<p>The VerifyChangePhoneNumberTokenAsync is used to validate and the 2FA is activated.</p>


  var user = await _userManager.GetUserAsync(User);
if (user == null)
{
	return NotFound($"Unable to load user with ID '{_userManager.GetUserId(User)}'.");
}

var verificationCode = Input.Code.Replace(" ", string.Empty).Replace("-", string.Empty);
var is2faTokenValid = await _userManager
	.VerifyChangePhoneNumberTokenAsync(user, verificationCode, user.PhoneNumber!);

if (!is2faTokenValid)
{
	ModelState.AddModelError("Input.Code", "Verification code is invalid.");
	return Page();
}

await _userManager.SetTwoFactorEnabledAsync(user, true);
  


<p></p>



<p>The Verify Phone 2FA Razor page displays the input field for the code.</p>







<p></p>



<p>Once activated, the user should persist some recovery codes.</p>







<p></p>



 Flow 3: SMS 2FA using phone 



<p>Next time the user authenticates, the SMS 2FA is required. The user can use multiple authentication methods, not only SMS. If possible, passkeys or strong authentication should be used.</p>


  if (user.Phone2FAEnabled)
{
 IsPhone = true;
 if (!user.AuthenticatorApp2FAEnabled)
 {
	 await _smsVerifyClient
		.Send2FASmsAsync(user, user.PhoneNumber!);
 }
}
  


<p></p>



 Further flows 



 Phone only authentication 



<p>Requires mass usage protection</p>



 Recover account using Phone authentication 



<p>Requires mass usage protection</p>



 Links 



<p><a href="https://learn.microsoft.com/en-us/aspnet/core/security/authentication/2fa">https://learn.microsoft.com/en-us/aspnet/core/security/authentication/2fa</a></p>



<p><a href="https://github.com/andrewlock/TwilioSamples/blob/master/src/SendVerificationSmsDemo">https://github.com/andrewlock/TwilioSamples/blob/master/src/SendVerificationSmsDemo</a></p>



<p></p><blockquote><a href="https://ecall-messaging.com/">Professionell Online SMS senden</a></blockquote><p></p>



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2025/02/identity-sms-add-phone_01.png?w=733'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   06:05 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 02. March 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://medium.com/@benwerd'>Werdmüller on Medium</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://benwerd.medium.com/the-web-was-always-about-redistribution-of-power-lets-bring-that-back-d3d855509f9c?source=rss-3b16402f5b9d------2'>The web was always about redistribution of power. Let’s bring that back.</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

      It’s time to rekindle the movement.  Continue reading on Medium »  
   
  
  
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://cdn-images-1.medium.com/max/1024/0*kzhSPojFtZ34KFje'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

   <p><a href="https://benwerd.medium.com/the-web-was-always-about-redistribution-of-power-lets-bring-that-back-d3d855509f9c?source=rss-3b16402f5b9d------2"></a></p><p>It’s time to rekindle the movement.</p><p><a href="https://benwerd.medium.com/the-web-was-always-about-redistribution-of-power-lets-bring-that-back-d3d855509f9c?source=rss-3b16402f5b9d------2">Continue reading on Medium »</a></p> 
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://cdn-images-1.medium.com/max/1024/0*kzhSPojFtZ34KFje'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:00 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 27. February 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.moxytongue.com/'>Moxy Tongue</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='http://www.moxytongue.com/2025/02/if-im-problem-you-might-be-reason.html'>If Im The Problem, You Might Be The Reason</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   &nbsp;Tune your metaphors..&nbsp;             &nbsp;From where I dance and glance in America...   All over the world, men and women are not having the same experience. Generational conflicts are arising, influencing everything, causing chaos to status quo, and making clear that the ability to r/w/rw is not shared equally. Computational supremacy, computational illiteracy, what does the data say? W
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>&nbsp;Tune your metaphors..&nbsp;</p><p></p>      <br> <br> <br>&nbsp;From where I dance and glance in America... <p></p><p>All over the world, men and women are not having the same experience. Generational conflicts are arising, influencing everything, causing chaos to status quo, and making clear that the ability to r/w/rw is not shared equally. Computational supremacy, computational illiteracy, what does the data say? Who? What? When? Why? Where? How? Now what?&nbsp;&nbsp;<br></p><p>FREEdumb self-certifies.&nbsp;</p><p>Structure yields results... not empathy, not emotion, not opinion.&nbsp;</p><p>You own root, or you don't.&nbsp;</p><p>Owners own the world. One contract at a time. Data, under Terms.</p><p>Sovereignty emerges from people, Individuals all.&nbsp;</p><p>Freedom self-certifies.&nbsp;</p><p>Left Cult == Right Cult political paradigm is collapsing.</p><p>Something new is possible.</p><p>You, Sovereign.&nbsp;</p><p>Data structure yields life results... own root authority, people, it is your Right as Individuals.</p><p>The ultimate minority of one. Love somebody.&nbsp;</p><p></p>  <br>&nbsp;<br><p></p> 
   
  
  
  

</div>
</div>

<div class='item-footer'>
   22:08 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Tuesday, 25. February 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://www.patrick-breyer.de/'>Patrick Breyer</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://www.patrick-breyer.de/chatkontrolle-eu-buergerbeauftragte-kritisiert-drehtuerwechsel-von-europol-beamten-zu-chatkontrolle-lobbyist-thorn/'>Chatkontrolle: EU-Bürgerbeauftragte kritisiert Drehtürwechsel von Europol-Beamten zu Chatkontrolle-Lobbyist Thorn</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Die scheidende EU-Bürgerbeauftragte Emily O’Reilly hat es auf die Beschwerde des ehemaligen Europaabgeordneten der Piratenpartei Patrick Breyer als “Missstand” bewertet, dass ein Europol-Beamter trotz potenzieller Interessenskonflikte ohne jegliche Auflagen zum … 
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Die scheidende EU-Bürgerbeauftragte Emily O’Reilly hat es auf die Beschwerde des ehemaligen Europaabgeordneten der Piratenpartei Patrick Breyer als “Missstand” <a href="https://www.ombudsman.europa.eu/en/decision/en/200017">bewertet</a>, dass ein Europol-Beamter trotz potenzieller Interessenskonflikte ohne jegliche Auflagen zum Chatkontrolle-Dienstleister Thorn wechseln und zuvor noch zwei Monate bei Europol in derselben Funktion weiter arbeiten durfte. “Europol hat es versäumt, den oben genannten Interessenkonflikt zu lösen, wodurch die Integrität und Unparteilichkeit seiner Maßnahmen gefährdet wurden”, heißt es in der Entscheidung der Bürgerbeauftragten. Der Beamte hatte bei Europol an einem KI-Pilotprojekt zur CSAM-Erkennung gearbeitet und war nach seinem Wechsel<a href="https://balkaninsight.com/2023/09/29/europol-sought-unlimited-data-access-in-online-child-sexual-abuse-regulation/"> beim Bundestag als Lobbyist für Thorn registriert und fuhr in seiner neuen Funktion zu einem Europol-Treffen mit seinen ehemaligen Kollegen für eine Produktpräsentation. </a>Europol verspricht ab 2025 nun zusätzliche Maßnahmen zur Vorbeugung von Interessenskonflikten wie Versetzungen, veränderte Aufgaben und den Einzug von Informationszugang für wechselwillige Mitarbeiter.<br><br>Breyer begrüßt das Ergebnis: „Wenn ein ehemaliger Europol-Bediensteter sein internes Wissen und seine Kontakte verkauft, um ihm persönlich bekannte Mitarbeiter der EU-Kommission zu lobbyieren, ist dies genau das, was es zu verhindern gilt. Seit der Enthüllung des ‘<a href="https://netzpolitik.org/2023/anlasslose-massenueberwachung-recherchen-decken-netzwerk-der-chatkontrolle-lobby-auf/">Chatcontrol-Gate</a>‘ wissen wir, dass der EU-Vorschlag zu Chatkontrolle letztlich ein Produkt der Lobby eines internationalen überwachungsbehördlich-industriellen Komplexes ist. Damit so etwas nie wieder passiert, muss der Überwachungslobbysumpf trockengelegt werden.“</p>



<p>Hinweis: Die Entscheidung der EU-Bürgerbeauftragten ist <a href="https://www.ombudsman.europa.eu/en/decision/en/200017">hier</a> abrufbar. Der hochkontroverse EU-Vorschlag zur Chatkontrolle steckt fest, weil es <a href="https://www.patrick-breyer.de/beitraege/chatkontrolle/#WasTun">weder für die Aufhebung des digitalen Briefgeheimnisses und die Zerstörung sicherer Verschlüsselung eine qualifizierte Mehrheit unter den EU-Staaten gibt</a> <a href="https://netzpolitik.org/2025/internes-protokoll-mehrheit-der-eu-staaten-beharrt-auf-verpflichtender-chatkontrolle/">noch eine Mehrheit für die Streichung der Chatkontrolle aus dem Vorschlag</a>.<br></p>
   
  
  
  

</div>
</div>

<div class='item-footer'>
   12:20 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Monday, 24. February 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://realestatecafe.com/'>Bill Wendels Real Estate Cafe</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://realestatecafe.com/how-are-political-economic-uncertainty-impacting-real-estate-consumers/'>How are political & economic uncertainty impacting real estate consumers?</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   Writing that he knows “a bunch of people who held their nose and voted for Trump,” the president of HousingWire media issued this challenge on… 
The post How are political &amp; economic uncertainty impacting real estate consumers? first appeared on Real Estate Cafe.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>Writing that he knows “a bunch of people who held their nose and voted for Trump,” the president of HousingWire media issued this challenge on…</p>
The post <a href="https://realestatecafe.com/how-are-political-economic-uncertainty-impacting-real-estate-consumers/">How are political &amp; economic uncertainty impacting real estate consumers?</a> first appeared on <a href="https://realestatecafe.com">Real Estate Cafe</a>.
   
  
  
  

</div>
</div>

<div class='item-footer'>
   16:14 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




  <hr class='item-seperator'>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://damienbod.com/'>Damien Bod</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://damienbod.com/2025/02/24/use-client-assertions-in-openid-connect-and-asp-net-core/'>Use client assertions in OpenID Connect and ASP.NET Core</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Client assertions is a method of client authentication which can be used in OpenID Connect. This provides an alternative to client secrets. This approach enhances security by using signed tokens (JWTs) to authenticate clients during the token request process or the OAuth PAR request. In ASP.NET Core, client assertions is not supported per default, a […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Client assertions is a method of client authentication which can be used in OpenID Connect. This provides an alternative to client secrets. This approach enhances security by using signed tokens (JWTs) to authenticate clients during the token request process or the OAuth PAR request. In ASP.NET Core, client assertions is not supported per default, a small implementation is required.</p>



<p>Code: <a href="https://github.com/damienbod/oidc-client-assertion">https://github.com/damienbod/oidc-client-assertion</a></p>



<p>Blogs in this series</p>



 
 <a href="https://damienbod.com/2025/04/21/implement-client-assertions-for-oauth-client-credential-flows-in-asp-net-core/">Implement client assertions for OAuth client credential flows in ASP.NET Core</a> 



 <a href="https://damienbod.com/2025/02/24/use-client-assertions-in-openid-connect-and-asp-net-core/">Using client assertions in OpenID Connect and ASP.NET Core</a> 



 <a href="https://damienbod.com/2025/05/12/implement-client-assertions-with-client-credentials-flow-using-oauth-dpop/">Implement client assertions with client credentials flow using OAuth DPoP</a> 
 



 Setup 



<p>A client assertion can be added to the token request which is sent from the ASP.NET Core backend to the OpenID Connect server. This is sent instead of the a client secret. The client is authenticated using the assertion. If using OAuth, the assertion is sent in the PAR request.</p>







<p></p>



 Create client assertion JWT 



<p>The application uses Microsoft Entra ID as the OpenID Connect server and the identity provider. The assertion was created using the Microsoft documentation.</p>



<p>Original src: <a href="https://learn.microsoft.com/en-us/entra/msal/dotnet/acquiring-tokens/web-apps-apis/confidential-client-assertions#crafting-the-assertion">https://learn.microsoft.com/en-us/entra/msal/dotnet/acquiring-tokens/web-apps-apis/confidential-client-assertions#crafting-the-assertion</a></p>



<p>A X509Certificate2 is used to create the certificate and validate the certificate. The audience, the clientId and the sub claims are sent in the JWT client assertion as well as the standard claims signed with the private key of the certificate used to validate the assertion in Microsoft Entra ID. As always, if using this against a different OpenID Connect server, JWT will be validated differently. Microsoft Entra ID requires a RSA key size 2048. </p>


  public static class CertService
{
    public static string GetSignedClientAssertion(
          X509Certificate2 certificate, string aud, string clientId)
    {
        // no need to add exp, nbf as JsonWebTokenHandler will add them by default.
        var claims = new Dictionary&lt;string, object&gt;()
        {
            { "aud", aud },
            { "iss", clientId },
            { "jti", Guid.NewGuid().ToString() },
            { "sub", clientId }
        };

        var securityTokenDescriptor = new SecurityTokenDescriptor
        {
            Claims = claims,
            SigningCredentials = new X509SigningCredentials(certificate)
        };

        var handler = new JsonWebTokenHandler();
        var signedClientAssertion = handler.CreateToken(securityTokenDescriptor);

        return signedClientAssertion;
    }
}
  


<p></p>



<p>Using Micrsoft Entra ID as the OpenID Connect server, the client assertion is created using the token endpoint. In production, the certificate can be generated using Azure Key Vault. The certificate can also be read from the operating system store. This can all be set in the app.settings.</p>


          // single tenant
        var aud = $"https://login.microsoftonline.com/{builder.Configuration["AzureAd:TenantId"]!}/oauth2/v2.0/token";

        var clientAssertion = CertService.GetSignedClientAssertion(
            X509CertificateLoader.LoadPkcs12FromFile("cert_rsa512.pfx", "1234"),
            aud,
            builder.Configuration["AzureAd:ClientId"]!);
  


<p></p>



 Use the client assertion in the OpenID connect client 



<p>Once ready, the OnAuthorizationCodeReceived event can be used to added the assertion in the OpenID Connect client. If using PAR, the par event is used.</p>


  builder.Services.AddAuthentication(OpenIdConnectDefaults.AuthenticationScheme)
    .AddCookie(CookieAuthenticationDefaults.AuthenticationScheme)
    .AddOpenIdConnect(OpenIdConnectDefaults.AuthenticationScheme, oidcOptions =&gt;
    {
        oidcOptions.SignInScheme = CookieAuthenticationDefaults.AuthenticationScheme;
        oidcOptions.Scope.Add(OpenIdConnectScope.OpenIdProfile);
        oidcOptions.Scope.Add("user.read");
        oidcOptions.Scope.Add(OpenIdConnectScope.OfflineAccess);
        oidcOptions.Authority 
         = $"https://login.microsoftonline.com/{builder.Configuration["AzureAd:TenantId"]}/v2.0/";
        oidcOptions.ClientId = builder.Configuration["AzureAd:ClientId"];
        //oidcOptions.ClientSecret = builder.Configuration["AzureAd:ClientSecret"];
        oidcOptions.ResponseType = OpenIdConnectResponseType.Code;
        oidcOptions.MapInboundClaims = false;
        oidcOptions.SaveTokens = true;
        oidcOptions.TokenValidationParameters.NameClaimType 
          = JwtRegisteredClaimNames.Name;
        oidcOptions.TokenValidationParameters.RoleClaimType = "role";

        oidcOptions.Events = new OpenIdConnectEvents
        {
            // Add client_assertion            
            OnAuthorizationCodeReceived = context =&gt;
            {
                context.TokenEndpointRequest!.ClientAssertion = clientAssertion;
                context.TokenEndpointRequest.ClientAssertionType 
                 = "urn:ietf:params:oauth:client-assertion-type:jwt-bearer";
                return Task.FromResult(0);
            }
        };
    });
  


<p></p>



 Setup Entra ID App registration 



<p>An Azure App registration is used to authenticate using Micrsoft Entra ID. The public key of the certificate can be uploaded to to the App registration. </p>







<p></p>



<p>Client assertions can be used for any OpenID Connect confidential client if the OpenID Connect server supports this. The assertion can be sent in the token request or in the PAR request depending on the flavor of the OpenID Connect code flow used.</p>



 Creating the certificate 



<p>The github <a href="https://github.com/damienbod/oidc-client-assertion/blob/main/CreateSelfSignedCertsConsole/Program.cs">repository</a> linked at the top, provides an example to create your own certificate which can be used in this flow. You can also use Azure Key vault or any other tool to create the certificate.</p>



 Links 



<p><a href="https://learn.microsoft.com/en-us/entra/msal/dotnet/acquiring-tokens/web-apps-apis/confidential-client-assertions">https://learn.microsoft.com/en-us/entra/msal/dotnet/acquiring-tokens/web-apps-apis/confidential-client-assertions</a></p>



<p><a href="https://github.com/AzureAD/microsoft-identity-web/blob/2b8fbf0104d820bba8785c41b2ef9e6f801b5e73/src/Microsoft.Identity.Web.TokenAcquisition/MsAuth10AtPop.cs#L48">https://github.com/AzureAD/microsoft-identity-web/blob/2b8fbf0104d820bba8785c41b2ef9e6f801b5e73/src/Microsoft.Identity.Web.TokenAcquisition/MsAuth10AtPop.cs#L48</a></p>



<p><a href="https://datatracker.ietf.org/doc/html/rfc7521">https://datatracker.ietf.org/doc/html/rfc7521</a></p>



<p><a href="https://oauth.net/private-key-jwt/">https://oauth.net/private-key-jwt/</a></p>



<p><a href="https://learn.microsoft.com/en-us/aspnet/core/security/authentication/configure-oidc-web-authentication">https://learn.microsoft.com/en-us/aspnet/core/security/authentication/configure-oidc-web-authentication</a></p>



<p><a href="https://datatracker.ietf.org/doc/html/rfc7523">RFC 7523 – JSON Web Token (JWT) Profile for OAuth 2.0 Client Authentication and Authorization Grants</a></p>



<p></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://damienbod.com/wp-content/uploads/2025/02/oidc-confidential-pkce-code-flow-assertion.drawio.png?w=800'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   06:43 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Sunday, 23. February 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://idmlab.eidentity.jp/'>IdM Laboratory</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://idmlab.eidentity.jp/2025/02/fapiverifiable-credentials.html'>FAPIとVerifiable Credentialsに関するイベントをやります</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   こんにちは、富士榮です。  3月頭はFintech Weekということもあり、あちこちでFintech系のイベントが開催されますね。そのうちの一つである4F（Future Frontier Fes by FINOLAB）の一コマをいただきAuthlete川崎さんと一緒にFAPIとVerifiable Credentialsの話をします。  こちらのイベントですね。  https://4f-otmcbldg.tokyo/2025-jp/       このうち、3/4の午前中のセッションです。    セッションの詳細と申し込みはこちらからしていただけます。  https://fapi-vc.peatix.com/        &nbsp;私は慶應の鈴木先生と一緒に先日発行したデジタルクレデンシャルの管理要件に関するディスカッションペーパーの中身の話を解説させていただきます。みなさん色々とデジ
   
  
  
    
      <div class="preview">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_400/https://blogger.googleusercontent.com/img/a/AVvXsEiL-xr9-AwFn3lzZg_ll4OXfBISk6pjKu1Atf4gTWiZ_2ktRpt-mw37pXLoFozrsYrvoy-gK9jpeCoQGiUn5sjj1bCVb671Vn3tAX7CUXfcH5FYM7sf8On5HADiF2KT6LD9AA8KTdCy29v8wOJdQUBxMPFVjwVfPEqZCkC9yttDtS7419N6QLzrJh0ScLcO=w640-h360'>
      </div>
    
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>こんにちは、富士榮です。</p><p>3月頭はFintech Weekということもあり、あちこちでFintech系のイベントが開催されますね。そのうちの一つである4F（Future Frontier Fes by FINOLAB）の一コマをいただきAuthlete川崎さんと一緒にFAPIとVerifiable Credentialsの話をします。</p><p>こちらのイベントですね。</p><p><a href="https://4f-otmcbldg.tokyo/2025-jp/">https://4f-otmcbldg.tokyo/2025-jp/</a></p><p></p> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEiL-xr9-AwFn3lzZg_ll4OXfBISk6pjKu1Atf4gTWiZ_2ktRpt-mw37pXLoFozrsYrvoy-gK9jpeCoQGiUn5sjj1bCVb671Vn3tAX7CUXfcH5FYM7sf8On5HADiF2KT6LD9AA8KTdCy29v8wOJdQUBxMPFVjwVfPEqZCkC9yttDtS7419N6QLzrJh0ScLcO"></a> <p><br></p>このうち、3/4の午前中のセッションです。<br><br><p></p><p>セッションの詳細と申し込みはこちらからしていただけます。</p><p><a href="https://fapi-vc.peatix.com/">https://fapi-vc.peatix.com/</a></p><p></p> <a href="https://blogger.googleusercontent.com/img/a/AVvXsEiZh5Z1JsGcqR4w3aGphbG2GnCqNoYse8TtIjp6u0oO1JprMPjbr7WYBTFU8h6Pd39Jrqg5tcSVFsGl8RgURsiOcHAXt0qS58Jy4BD_yR4vmQvrquVfGfP4hf8n6lzM3lPrwgcInK2lhzmtIC5Q7JlbfSzm3bMQ_heXHr3lBJAvpGob6GytG0mimu52Bcm2"></a> <br><br><p></p><p>&nbsp;私は慶應の鈴木先生と一緒に先日発行したデジタルクレデンシャルの管理要件に関する<a href="https://dal.sfc.keio.ac.jp/ja/TR/management-requirements-for-digital-credentials/">ディスカッションペーパー</a>の中身の話を解説させていただきます。みなさん色々とデジタルクレデンシャルを発行しますが、ちゃんと用途に応じた管理をしないとダメですよ、って話です。</p><p>ぜひお越しください！</p><p><br></p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://blogger.googleusercontent.com/img/a/AVvXsEiL-xr9-AwFn3lzZg_ll4OXfBISk6pjKu1Atf4gTWiZ_2ktRpt-mw37pXLoFozrsYrvoy-gK9jpeCoQGiUn5sjj1bCVb671Vn3tAX7CUXfcH5FYM7sf8On5HADiF2KT6LD9AA8KTdCy29v8wOJdQUBxMPFVjwVfPEqZCkC9yttDtS7419N6QLzrJh0ScLcO=w640-h360'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   02:00 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Thursday, 20. February 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://wrenchinthegears.com/'>Wrench in the Gears</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://wrenchinthegears.com/2025/02/19/a-visit-to-tulsa-with-microtuble-symbolic-communication-thoughts-on-doge-against-a-upenn-quaker-kabbalah-backdrop/'>A Visit To Tulsa With Microtuble Symbolic Communication + Thoughts on DOGE Against A UPenn “Quaker Kabbalah” Backdrop</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  Posting this late, so I won’t be writing an introduction. As is the case for me lately, the offering is a somewhat rambling discussion that surfaces quiet a few nuggets for you to chew on.&nbsp; Life in this absurdist improvisational theater might easier if we can practice the Tao of Pooh. Smile. Here’s a link [...]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>Posting this late, so I won’t be writing an introduction. As is the case for me lately, the offering is a somewhat rambling discussion that surfaces quiet a few nuggets for you to chew on.&nbsp; Life in this absurdist improvisational theater might easier if we can practice the Tao of Pooh. Smile.</p>
<p></p>
<p>Here’s a <a href="https://www.youtube.com/playlist?list=PLnNSjVGWqTO5e0FCLHjSdkfn5Nq96kbv6">link</a> to the playlist of DOGE clips. An overview of them is presented in the last half hour of the video.</p>
<p><a href="https://www.youtube.com/playlist?list=PLnNSjVGWqTO5e0FCLHjSdkfn5Nq96kbv6"></a></p>
<p>Be sure to check out the story of the Hot Springs, AR IQ Zoo run by BF Skinner behaviorist animal trainers. Hat Tip to Emily of “Save Maine Schools” who first alerted me to BF Skinner’s Pigeon Project. I promise you’ll never think of Cheerios the same way!</p>
<p><a href="https://www3.uca.edu/iqzoo/iqzoo.htm"></a></p>
<p>Click <a href="https://www3.uca.edu/iqzoo/History/history.htm">here</a> for historical background and <a href="https://www3.uca.edu/iqzoo/Exhibits/exhibits.htm">here</a> for interesting projects (<a href="https://www3.uca.edu/iqzoo/Exhibits/spy_raven.htm">Spy Birds!</a>).</p>
<p>Also, here is a <a href="https://bantamjoe.com/2025/02/16/consciousness-driven-energy-warfare/">link</a> to Joseph’s blog on “free energy,” which I equate more with manifestation and the ritual opening of “gates” into the quantum field.</p>
<p><a href="https://bantamjoe.com/2025/02/16/consciousness-driven-energy-warfare/"></a></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://i0.wp.com/wrenchinthegears.com/wp-content/uploads/2025/02/playlist.png?resize=1020%2C937&ssl=1'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   04:55 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Wednesday, 19. February 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='https://blog.talkingidentity.com'>Talking Identity</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://blog.talkingidentity.com/2025/02/signing-off-from-uniken-thank-you-for-the-adventure.html'>Signing Off from Uniken: Thank You for the Adventure</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

  It’s the end of an era: after eight incredible years, it felt like the right time to close the chapter on my story at Uniken. When I joined as CTO, Uniken was an ambitious startup with a unique core technology and a vision to make a global impact. What made the ensuing journey so rewarding […]
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

<p>It’s the end of an era: after eight incredible years, it felt like the right time to close the chapter on my story at <a href="http://www.uniken.com">Uniken</a>. When <a href="https://www.streetinsider.com/Press+Releases/Uniken+Announces+Nishant+Kaushik+as+Chief+Technology+Officer/12739642.html">I joined as CTO</a>, Uniken was an ambitious startup with a unique core technology and a vision to make a global impact. What made the ensuing journey so rewarding was the team – exceptionally talented, deeply committed, and relentlessly innovative. Together, we navigated complex challenges, scaled through uncertainty (including a global pandemic), and built Uniken into a global cybersecurity leader with customer engagements on every continent (except Antarctica, because sadly it would appear that penguins aren’t really concerned about digital trust).&nbsp;</p>



<p>I am incredibly proud of the team, and the platform we built – one that empowers businesses to engage securely and seamlessly with their customers, and creates lasting value for our customers and partners.&nbsp; I want to express my deepest gratitude to my colleagues and partners at Uniken. Our collaborative efforts transformed the company and positioned it for long-term success, and I look forward to seeing you scale new heights.</p>



<a href="https://blog.talkingidentity.com/wp-content/uploads/2025/02/scrabble_quote.jpg"></a>



<p></p>



<p>As for what’s next – that’s TBD. Digital identity has never been more critical, yet the challenges facing the industry have never been more complex. Let’s see what the next chapter brings.</p>
   
  
  
    
      <div class="photo">
        
        <img src='https://res.cloudinary.com/identosphere/image/fetch/q_auto:eco,f_auto,w_800/https://blog.talkingidentity.com/wp-content/uploads/2025/02/scrabble_quote-1024x412.jpg'>
      </div>
    
  

</div>
</div>

<div class='item-footer'>
   15:11 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->




 <h2 class='new-date'>
  Friday, 14. February 2025
 </h2>



<article class='item'>


<h4 class='feed-title'>
  <a href='http://realestatecafe.com/'>Bill Wendels Real Estate Cafe</a>
</h4>


<h3 class='item-title'>
 <i class='icon-caret-down item-collapse item-close item-opened'></i>
 <i class='icon-caret-right item-expand item-open item-closed'></i>

 <a href='https://realestatecafe.com/multiply-tiny-house-love-story-with-million-backyard-march/'>Multiply Tiny House Love Story with Million Backyard March</a>

</h3>

<div class='item-body'>

<div class='item-snippet'>
<!-- use 'smarter' algorithm for cutoff snippet text
   note: summary goes first; than try content -->

   After a post about a college that’s teaching students how to build their own tiny houses for just $5,000 drew a lot of attention, decided… 
The post Multiply Tiny House Love Story with Million Backyard March first appeared on Real Estate Cafe.
   
  
  
  
  
  

</div>
<div class='item-content item-summary'>

<!-- todo: use sanitize or simplify or whitelist
   - todo/fix: check why summary can be nil/null
   note: content goes first; than try summary
 -->
<div class='wrap'>

  <p>After a post about a college that’s teaching students how to build their own tiny houses for just $5,000 drew a lot of attention, decided…</p>
The post <a href="https://realestatecafe.com/multiply-tiny-house-love-story-with-million-backyard-march/">Multiply Tiny House Love Story with Million Backyard March</a> first appeared on <a href="https://realestatecafe.com">Real Estate Cafe</a>.
   
  
  
  

</div>
</div>

<div class='item-footer'>
   21:49 &bull;
   5 months ago
</div>

</div><!-- item-body -->

</article><!-- item -->

<!-- each item -->



</body>
</html>
